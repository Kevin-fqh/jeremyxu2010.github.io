[{"categories":["nodejs开发"],"contents":"最近在很紧张地使用typescript开发一个项目。和正常的业务项目很类似，前期的设计及技术框架搭建还有些意思，目前在做的业务功能开发就比较按部就班，今天终于有时间，先将前期技术框架搭建过程中的一些技术点记录下来。\n这个项目是一个正常的前后端单体应用，在项目启动之初，考虑到团队内部人员的技术积累，最终选择以typescript为前后端主要开发语言。\n前端项目初始化 前端选用VueJS及与之配套的一组成熟的库，使用vue-cli创建项目：\n$ npm install -g @vue/cli $ vue create xx_frontend 这里选择Manually select features，选择Babel、TypeScript、Router、Vuex、CSS Pre-processors、Linter/Formatter这些特征。\n后端项目初始化 后端我以一个starter骨架项目为基础，在此基础上开发业务API接口：\n$ git clone git@github.com:ddimaria/koa-typescript-starter.git $ cd koa-typescript-starter $ git checkout -b upgrade-to-typescript-3 origin/upgrade-to-typescript-3 $ yarn install $ yarn run watch 前后端接口协议约定 这个项目里前后端使用swagger.json描述交互的API接口，事实上koa-typescript-starter里本身已经集成了swagger的功能。但经过调研，发现让业务开发同学直接编写swagger.json难度较大。于是找了种自动生成swagger.json文档的方案koa-swagger-decorator，我对这个库进行了一些增强。\n$ npm install --registry=https://npm.pkg.github.com @jeremyxu2010/koa-swagger-decorator@1.6.1 --save 然后就参考https://github.com/jeremyxu2010/koa-swagger-decorator/blob/master/example/routes/index.js简单配置下SwaggerRouter，在Controller里通过decorator描述每个API接口入参及响应schema就可以了。\nimport { request, summary, body, tags, middlewares, path, description, producesAll, responses } from \u0026#39;@jeremyxu2010/koa-swagger-decorator\u0026#39;; const tag = tags([\u0026#39;User\u0026#39;]); const logTime = () =\u0026gt; async (ctx, next) =\u0026gt; { console.log(`start: ${new Date()}`); await next(); console.log(`end: ${new Date()}`); }; const userSchema = { name: { type: \u0026#39;string\u0026#39;, required: true, default: \u0026#39;jeremyxu\u0026#39; }, password: { type: \u0026#39;string\u0026#39;, required: true, default: \u0026#39;123456\u0026#39; } }; const userRespSchema = { type: \u0026#39;object\u0026#39;, properties: { name: { type: \u0026#39;string\u0026#39;, required: true } }, example: { name: \u0026#39;jeremyxu\u0026#39; } }; @producesAll([\u0026#39;application/json\u0026#39;]) export default class UserRouter { @request(\u0026#39;POST\u0026#39;, \u0026#39;/user/register\u0026#39;) @summary(\u0026#39;register user\u0026#39;) @description(\u0026#39;example of api\u0026#39;) @tag @middlewares([logTime()]) @body(userSchema) @responses({ 200: { description: \u0026#39;file upload success\u0026#39;, schema: userRespSchema} }) static async register(ctx) { const { name } = ctx.validatedBody; const user = { name }; ctx.body = { user }; } // ..... other API } 后端项目运行起来即可通过http://127.0.0.1:3000/swagger-html访问到接口文档。\n前端同学根据swagger文档快速构建一个可用的mock服务也很方便：\n# 转化swagger.json为openapi.yaml $ curl -o swagger.json http://127.0.0.1:3000/swagger-json $ npm install -g swagger2openapi \u0026amp;\u0026amp; swagger2openapi -p -w -y -o openapi.yaml swagger.json # 根据openapi.yaml自动启动一个mock server $ wget https://github.com/danielgtaylor/apisprout/releases/download/v1.3.0/apisprout-v1.3.0-mac.tar.xz $ tar -Jxf apisprout-v1.3.0-mac.tar.xz $ ./apisprout -p 4000 ./openapi.yaml 生产环境部署 虽然通过npm run start可以将服务跑起来，但生产环境还需要利用服务器的多核性能，可以利用pm2来自动完成这点：\n$ npm install -g pm2 $ pm2 start ./dist/index.js --name xx_backend --instances 4 The End！\n参考  https://juejin.im/post/6844904088048500744 https://github.com/ddimaria/koa-typescript-starter/tree/upgrade-to-typescript-3 https://github.com/jeremyxu2010/koa-swagger-decorator https://github.com/danielgtaylor/apisprout http://www.doocr.com/articles/5894d4713c6bfb7e3b7fe20e  ","permalink":"https://jeremyxu2010.github.io/2020/08/nodejs%E9%A1%B9%E7%9B%AE%E6%8A%80%E6%9C%AF%E5%B0%8F%E7%BB%93/","tags":["javascript","typescript","nodejs","swagger","VueJS","koa2"],"title":"nodejs项目技术小结"},{"categories":["微服务"],"contents":"最近在工作中碰到一个分布式锁问题，这个问题之前做项目的过程也搞过，不过没有深入整理，这个周末有时间刚好整理一把。\n为什么需要分布式锁？ 在分布式系统中，常常需要协调他们的动作。如果不同的系统或是同一个系统的不同主机之间共享了一个或一组资源，那么访问这些资源的时候，往往需要互斥来防止彼此干扰进而保证一致性，这个时候，便需要使用到分布式锁。\n常见的实现方案 基于数据库实现 数据表方案 最容易想到的一种实现方案就是基于数据库的。可以在数据库中创建一张锁表，然后通过操作该表中的数据来实现。\n可以这样创建一张表：\ncreate table TDistributedLock ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;主键\u0026#39;, `lock_key` varchar(64) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;锁的键值\u0026#39;, `lock_timeout` datetime NOT NULL DEFAULT NOW() COMMENT \u0026#39;锁的超时时间\u0026#39;, `create_time` datetime NOT NULL DEFAULT NOW() COMMENT \u0026#39;记录创建时间\u0026#39;, `modify_time` datetime NOT NULL DEFAULT NOW() COMMENT \u0026#39;记录修改时间\u0026#39;, PRIMARY KEY(`id`), UNIQUE KEY `uidx_lock_key` (`lock_key`) USING BTREE ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=\u0026#39;分布式锁表\u0026#39;; 这样在需要加锁时，只需要往这张表里插入一条数据就可以了：\nINSERT INTO TDistributedLock(lock_key, lock_timeout) values(\u0026#39;lock_xxxx\u0026#39;, \u0026#39;2020-07-19 18:20:00\u0026#39;); 当对共享资源的操作完毕后，可以释放锁：\nDELETE FROM TDistributedLock where lock_key=\u0026#39;lock_xxxx\u0026#39;; 该方案简单方便，主要利用数据库表的唯一索引约束特性，保证多个微服务模块同时申请分布式锁时，只有一个能够获得锁。\n下面讨论下一些相关问题：\n 获得锁的微服务模块意外crash，来不及释放锁怎么办？因为在插入锁记录时，同时设置了一个lock_timeout属性，可以另外跑一个lock_cleaner，将超时的锁记录删除。当然为了安全，lock_timeout最好设置一个合理的值，以确保在这之后正常的共享资源操作一定是完成了的。 获得锁失败的微服务模块如何继续尝试获得锁？搞一个while循环，反复尝试，如能成功获得锁就跳出循环，否则sleep一会儿重新进入循环体继续尝试。 数据库单点不安全怎么办？数据库领域有主从、一主多从、多主多从等复制方案，可保证一个数据库实例挂掉时，其它实例可以接管过来继续提供服务。不过需要注意的是有一些复制方案它是异步的，并不能保证写入一个数据库实例的数据立马可以在另外一个数据库实例中查询到， 这就容易造成锁丢失，导致授予了两把锁的问题。 获得锁的微服务模块想重入地获得锁怎么办？在数据库表中加个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库可以查到的话，直接把锁分配给它就可以了。同样为了确保安全，必须在本微服务模块没有主动释放锁，同时离锁的超时时间还很久远的情况下，才可以安全地直接把锁再次分配给它，同时更新锁的超时时间。  该方案的缺陷：\n 该方案利用数据库表自身的唯一键约束，性能相比后面提到的redis方案会差一点。 该方案中其它尝试获得锁的客户端会反复尝试插入数据，消耗数据库资源。 该方案中需要为数据库找一种比较可靠的高可用方案，同时还得确保数据库实例之间复制方案是满足一致性要求的。  数据库表的行排它锁方案 除了动态地往数据库表里插入数据外，还可以预先将锁信息写入数据库表，然后利用数据库的行排它锁来进行加锁与释放锁操作。\n例如加锁时可以执行以后命令：\nSET autocommit = 0; START TRANSACTION; SELECT * FROM TDistributedLock WHERE lock_key=\u0026#39;lock_xxxx\u0026#39; FOR UPDATE; 当对共享资源的操作完毕后，可以释放锁：\nCOMMIT; 该方案也很简单，利用数据库表的行排它锁特性，保证多个微服务模块同时申请分布式锁时，只有一个能够获得锁。\n下面讨论下一些相关问题：\n 获得锁的微服务模块意外crash，来不及释放锁怎么办？没有COMMIT，数据库会自动将锁释放掉。 获得锁失败的微服务模块如何继续尝试获得锁？for update语句会在执行成功后立即返回，在执行失败时一直处于阻塞状态，直到成功，因此不用写while循环反复尝试获得锁了。 数据库单点问题跟上面那个方案一致，不再赘述。 在这个方案下，好像很难做到锁可重入了。  该方案的缺陷：\n 该方案利用数据库表的行排它锁特性，性能上比上面那个方案好一点，但相比后面提到的redis方案还是差一点。 该方案中需要为该表的数据库事务配置较高的事务超时时间，而且一个排他锁长时间不提交，会占用数据库连接。 该方案中同样需要为数据库找一种比较可靠的高可用方案，同时还得确保数据库实例之间复制方案是满足一致性要求的。  基于缓存redis实现 使用redis做分布式锁的思路大概是这样的：在redis中设置一个值表示加了锁，然后释放锁的时候就把这个key删除。\n具体代码如下：\n获取锁\n-- NX是指如果key不存在就成功，key存在返回false，PX可以指定过期时间 redis.call(\u0026#34;set\u0026#34;, key, ARGV[1], \u0026#34;NX\u0026#34;, \u0026#34;PX\u0026#34;, ARGV[2]) 释放锁\n-- 释放锁涉及到两条指令，这两条指令不是原子性的 -- 需要用到redis的lua脚本支持特性，redis执行lua脚本是原子性的 if redis.call(\u0026#34;get\u0026#34;, key) == ARGV[1] then return redis.call(\u0026#34;del\u0026#34;, key) else return 0 end 上述的lua代码比较简单，不具体解释了。这里要注意给锁键的value值要保证唯一，这个是为了避免释放错了锁。场景如下：假设A获取了锁，过期时间30s，此时35s之后，锁已经自动释放了，A去释放锁，但是此时可能B获取了锁。加上释放锁前的value值判断，A客户端就不能删除B的锁了。\n如果是javascript的项目，可以使用redislock库，它封装了上述加锁、释放锁等操作逻辑，用起来很方便。\n才区区几行代码就优雅地搞定了分布式锁，而且redis的写入、删除键值的性能超高，看样子很完美，但事实上并非如此。\n键的超时时间 在redis中设置键时可以通过PX指定过期时间，这个时间不宜设置得太大，否则万一获得锁的进程crash了，要等很久此键才能过期自动删除。这个时间也不宜设得过小，否则对共享资源的操作还没完成，锁就释放了，其它服务就又能获得锁了。这里可以采用watchdog之类的方案，获得锁时设置一个较小的超时时间，然后在持有锁的过程中定期对锁的租期进行延长。\nredis的高可用架构 为了解决redis的单点问题，一般在生产环境会为redis实施高可用架构方案。可问题是redis主从复制方案均是异步的，在主从切换过程中有可能造成锁丢失。Redis的作者Antirez为此提出了一个RedLock的算法方案，这个算法的大概逻辑如下：\n假设存在多个Redis实例，这些节点是完全独立的，不需要使用复制或者任何协调数据的系统，多个redis实例中获取锁的过程就变成了如下步骤：\n 以毫秒为单位获取当前的服务器时间 尝试使用相同的key和随机值来获取锁，对每一个机器获取锁时都应该有一个超时时间，比如锁的过期时间为10s，那么获取单个节点锁的超时时间就应该为5到50毫秒左右，他这样做的目的是为了保证客户端与故障的机器连接不耗费多余的时间！超时间时间内未获取数据就放弃该节点，从而去下一个节点获取，直至将所有节点全部获取一遍！ 获取完成后，获取当前时间减去步骤一获取的时间，当且仅当客户端半数以上获取成功且获取锁的时间小于锁额超时时间，则证明该锁生效！ 获取锁之后，锁的超时时间等于设置的有效时间-获取锁花费的时间 如果获取锁的机器不满足半数以上，或者锁的超时时间计算完毕后为负数等异常操作，则系统会尝试解锁所有实例，即使有些实例没有获取锁成功，依旧会被尝试解锁！ 释放锁，只需在所有实例中释放锁，无论客户端是否认为它能够成功锁定给定的实例。  这个方案看似很好，但仔细审视后还是发现一些问题的，分布式架构师Martin就提出了自己的意见。这些意见总结下来如下：\n 分布式锁的用途无非两种：\n 要么为了提升效率，用锁来保证一个任务没有必要被执行两次。比如（很昂贵的计算） 保证正确，使用锁来保证任务按照正常的步骤执行，防止两个节点同时操作一份数据，造成文件冲突，数据丢失。这种情况下对锁是有一定宽容度的，就算发生了两个节点同时工作，对系统的影响也仅仅是多付出了一些计算的成本，没什么额外的影响。这个时候 使用单点的 Redis 就能很好的解决问题，没有必要使用RedLock，维护那么多的Redis实例，提升系统的维护成本。 要么为了安全性，此时必须从理论上确保拿到的锁是安全的，两个进程不可能同时持有某把锁，因为很可能涉及到一些金钱交易，如果锁定失败，并且两个节点同时处理同一数据，则结果将导致文件损坏，数据丢失，永久性不一致，或者金钱方面的损失！而RedLock方案从理论上说并不能保证这一点。   RedLock方案从理论上说并不能保证锁的安全性，主要有以下几点原因：\n 锁的自动过期机制很可能导致锁的意外丢失。因此采用watchdog机制确保在持有锁之后持续地给锁续期是有必要的。 RedLock方案对于系统时钟有强依赖。假设有A、B、C、D、E 5个redis节点：  客户端1获取节点A，B，C的锁定。由于网络问题，无法访问D和E。 节点C上的时钟向前跳，导致锁过期。 客户端2获取节点C，D，E的锁定。由于网络问题，无法访问A和B。 现在，客户1和2都认为他们持有该锁。   RedLock方案同样无法避免redis实例意外重启导致的问题。假设有A、B、C、D、E 5个redis节点：  客户端1获取节点A，B，C的锁定。由于网络问题，无法访问D和E。 节点C上的redis实例意外重启，重启后原来写入的键值丢失。 客户端2获取节点C，D，E的锁定。由于网络问题，无法访问A和B。 现在，客户1和2都认为他们持有该锁。    虽说RedLock从理论上说确实无法100%保证锁的安全性，但以上列举的场景极为严苛，事实上在现实中很难碰到。而由于该方案获取锁的效率确实很高，事实上还是有不少业务场景就是使用的该方案。\n如果是javascript的项目，配合着使用timers库，即可实施watchdog方案，在持有锁的过程中定期对锁的租期进行延长。如果要使用RedLock方案，可以使用node-redlock库，它封装了上述RedLock方案的复杂逻辑，用起来也很方便。\n如果对锁的安全性要求极高，真的不允许任何锁安全性问题，还可以试试下面的zookeeper方案。\n基于zookeeper实现 zookeeper是一种提供配置管理、分布式协同以及命名的中心化服务。很明显zookeeper本身就是为了分布式协同这个目标而生的，它采用一种被称为ZAB(Zookeeper Atomic Broadcast)的一致性协议来保证数据的一致性。基于zk的一些特性，我们很容易得出使用zk实现分布式锁的落地方案：\n 使用zk的临时节点和有序节点，每个线程获取锁就是在zk创建一个临时有序的节点，比如在/lock/目录下。 创建节点成功后，获取/lock目录下的所有临时节点，再判断当前线程创建的节点是否是所有的节点的序号最小的节点 如果当前线程创建的节点是所有节点序号最小的节点，则认为获取锁成功。 如果当前线程创建的节点不是所有节点序号最小的节点，则对节点序号的前一个节点添加一个事件监听。 比如当前线程获取到的节点序号为/lock/003,然后所有的节点列表为[/lock/001,/lock/002,/lock/003],则对/lock/002这个节点添加一个事件监听器。  如果锁释放了，会唤醒下一个序号的节点，然后重新执行第3步，判断是否自己的节点序号是最小。\n比如/lock/001释放了，/lock/002监听到时间，此时节点集合为[/lock/002,/lock/003],则/lock/002为最小序号节点，获取到锁。\n从数据一致性的角度来说，zk的方案无疑是最可靠的，而且等候锁的客户端也不用不停地轮循锁是否可用，当锁的状态发生变化时可以自动得到通知。\n但zookeeper实现也存在较大的缺陷：\n 性能问题  zookeeper作为分布式协调系统，不适合作为频繁的读写存储系统。而且通过增加zookeeper服务器来提高集群的读写能力也是有上限的，因为zookeeper集群规模越大，zookeeper数据需要同步到更多的服务器。同时zookeeper分布式锁每一次都要申请锁和释放锁，都要动态创建删除临时节点，所以zookeeper不能维护大量的分布式锁，也不能维护大量的客户端心跳长连接。在分布式定理中，zookeeper追求的是CP，也就是zookeeper保证集群向外提供统一的视图，但是zookeeper牺牲了可用性，在极端情况下，zookeeper可能会丢弃一些请求。并且zookeeper集群在进行leader选举的时候整个集群也是不可用的，集群选举时间长达30 ~ 120s。\n 惊群效应  在获取锁的时候有一个细节，客户端在获取锁失败的情况下，会监听/lock节点。这会存在性能问题，如果在/lock节点下排队等待的有1000个进程，那么锁持有者释放锁(删除/lock节点下的临时节点)时，zookeeper会通知监听/lock的1000个进程。然后这1000个进程会读取zookeeper下/lock节点的全部临时节点，然后判断自己是否为最小的临时节点。但是这1000个进程中只有一个最小序号的进程会持有分布式锁，也就是说999个进程做的都是无用功。这些无用功会对zookeeper造成较大压力的读负载。为了解决惊群效应，需要对zookeeper分布式锁监听逻辑进行优化，实际上，排队进程真正感兴趣的是比自己临时节点序号小的节点，我们只需要监听序号比自己小的节点。\n另外还需要注意的是，使用zookeeper方案也不是说就高枕无忧了。假设某一个客户端通过上述方案获得了锁，但由于网络问题，该客户端与zookeeper集群间失去了联系，zookeeper的心跳无效后，该客户端会收到了一个zookeeper的SessionTimeout的事件。为了保证分布式锁的有效性，这个时候客户端就需要在下一个等侯者获得锁之前，中断对共享资源的访问，然后继续尝试获取锁。\n如果是javascript的项目，可以使用zk-lock库，它封装了上述方案的复杂逻辑，用起来也很方便。\n总结 总的来说，目前分布式锁领域暂时没有出现十分完美、无懈可击的方案。个人觉得，综合对比下，还是推荐采用缓存redis方案。如果项目较小，影响面不大，采用单实例redis就差不多了。如果对redis的高可用有要求，可以采用RedLock方案，配合watchdog机制定期延长key的续约租期，不过要通过合理的部署、运维等方式规避系统时钟、网络分区问题。如果真的对分布锁有着极高的一致性要求，同时对锁的性能不太在意的话，也可以采用zookeeper方案。\n参考  http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html https://redis.io/topics/distlock  ","permalink":"https://jeremyxu2010.github.io/2020/07/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%B8%AD%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E6%96%B9%E6%A1%88/","tags":["microservice","lock","redis","zk"],"title":"微服务中的分布式锁方案"},{"categories":["python开发"],"contents":"为了便于以后维护代码，最近花了些时间将历史遗留代码迁移到python3，整个迁移还是比较顺利的，在做这个的过程中有一些经验，这里记录一下。\n字面值字符串的编码问题 python2中有一处很恶心的设计，同样声明一个字面值字符串，会产生两种不同的写法，如下：\n\u0026gt;\u0026gt;\u0026gt; a = \u0026#39;abc\u0026#39; \u0026gt;\u0026gt;\u0026gt; typeof(a) === \u0026#39;str\u0026#39; true \u0026gt;\u0026gt;\u0026gt; a2 = b\u0026#39;abc\u0026#39; \u0026gt;\u0026gt;\u0026gt; typeof(a2) === \u0026#39;str\u0026#39; true \u0026gt;\u0026gt;\u0026gt; b = u\u0026#39;中国\u0026#39; \u0026gt;\u0026gt;\u0026gt; typeof(b) === \u0026#39;unicode\u0026#39; true 字面值字符串中如出现非ascii编码才能表达的字符，则只能使用u'xxx'来声明。也就是说python2里的str类型代表的是采用某种具体编码格式编码后的二进制字节码，unicode类型代表的是还未采用某种具体编码格式编码的统一unicode码序列，而这两种类型的字符串混用经常会报错。\n\u0026gt;\u0026gt;\u0026gt; hi = u\u0026#34;今天\u0026#34; + \u0026#34;天气真好\u0026#34; Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; UnicodeDecodeError: \u0026#39;ascii\u0026#39; codec can\u0026#39;t decode byte 0xe5 in position 0: ordinal not in range(128) 网上对上述问题的解释都不太清楚，我经过分析终于知道了问题的根因。\n首先这里的\u0026quot;天气真好\u0026quot;是一个str类型的二进制字节码，既然是二进制字节码，那么必然需要使用某种具体的编码格式进行编码，那么python2如何知道用何种编码将字符串\u0026quot;天气真好\u0026quot;编码成二进制字节码呢？一般情况下py文件头部会标明源码文件所用的编码（如下如示，如果文件中没标注，则会使用默认的ASCII编码），python就是用这个编码将字面值字符串编码成二进制字节码的。如果文件用的编码与标识的编码不一致，也会导致编码失败。\n#!/usr/bin/env python # -*- coding: utf-8 -*- u\u0026quot;今天\u0026quot;只是一个统一unicode码序列。这两个类型进行+操作，必然先需要转换为相同的类型，这里实际上是将str类型的字符串转换为unicode类型的。这个转换会将二进制字符码采用某种具体的编码格式解码为统一unicode码序列。采用什么编码格式进行转换呢？实际上python2中是使用从sys.getdefaultencoding()得到的编码格式，默认情况下就是ascii编码，所以这里会导致解码失败。\n代码里出现两种不同类型的字符串会导致很多类似的问题，之前为了规避这些问题将代码中出现的所有字符串都转型为一个类型，于是在代码中写了很多encode的代码。\n这个问题在python3中有了较好的方案，所有代码中出现的字面值字符串都是统一unicode码序列，其类型为str，这个str类型与python2中的str类型有本质区别，类似于python2中的unicode类型。而且python3中sys.getdefaultencoding()已经与时俱进地改为了utf-8。其实在python2中也可以采用这个方案，那就是使用from __future__ import unicode_literals，这个在之前的博文中有提到过，参见写py2py3兼容的代码。\n知道了上述问题的原因，升级就比较容易处理了：删除原来很多无用的encode代码、注释sys.setdefaultencoding()相关的代码、代码中不要再使用u'xxxx'。\ndict的相关方法调整 dict的.keys()、.items 和.values()方法返回迭代器，而之前的iterkeys()等函数都被废弃。同时去掉的还有 dict.has_key()，用 in替代它。\n还可以six模块，它提供兼容API，使得代码可以在python2、python3中运行。\n   six.iterkeys(dictionary, **kwargs)\nReturns an iterator over dictionary‘s keys. This replaces dictionary.iterkeys() on Python 2 and dictionary.keys() on Python 3. kwargs are passed through to the underlying method.\n  six.itervalues(dictionary, **kwargs)\nReturns an iterator over dictionary‘s values. This replaces dictionary.itervalues() on Python 2 and dictionary.values() on Python 3. kwargs are passed through to the underlying method.\n  six.iteritems(dictionary, **kwargs)\nReturns an iterator over dictionary‘s items. This replaces dictionary.iteritems() on Python 2 and dictionary.items() on Python 3. kwargs are passed through to the underlying method.\n  six.iterlists(dictionary, **kwargs)\nCalls dictionary.iterlists() on Python 2 and dictionary.lists() on Python 3. No builtin Python mapping type has such a method; this method is intended for use with multi-valued dictionaries like Werkzeug’s.kwargs are passed through to the underlying method.\n  six.viewkeys(dictionary)\nReturn a view over dictionary‘s keys. This replaces dict.viewkeys() on Python 2.7 and dict.keys() on Python 3.\n  six.viewvalues(dictionary)\nReturn a view over dictionary‘s values. This replaces dict.viewvalues() on Python 2.7 and dict.values() on Python 3.\n  six.viewitems(dictionary)\nReturn a view over dictionary‘s items. This replaces dict.viewitems() on Python 2.7 and dict.items() on Python 3.\n   print函数 py3中print语句没有了，取而代之的是print()函数。 Python 2.6与Python 2.7部分地支持这种形式的print语法。因此保险起见，新写的代码最好都使用print函数。\nfrom __future__ import print_function print(\u0026#34;fish\u0026#34;, \u0026#34;panda\u0026#34;, sep=\u0026#39;, \u0026#39;) 异常的处理 在 Python 3 中处理异常也轻微的改变了，在 Python 3 中我们现在使用 as 作为关键词。\n捕获异常的语法由 except exc, var 改为 except exc as var。\n使用语法except (exc1, exc2) as var可以同时捕获多种类别的异常。 Python 2.6已经支持这两种语法。\n 在2.x时代，所有类型的对象都是可以被直接抛出的，在3.x时代，只有继承自BaseException的对象才可以被抛出。 2.x raise语句使用逗号将抛出对象类型和参数分开，3.x取消了这种奇葩的写法，直接调用构造函数抛出对象即可。  这里倒没有异议了，本来原来py2那种奇葩写法很奇怪，基本都只使用的是py3的写法。\ntry: raise BaseException(\u0026#39;fdf\u0026#39;) except BaseException as err: print(err) 标准库及函数名称变更 py3重新组织了一些标准库及一些函数，还可以使用six的兼容API保证代码在python2、python3中都正常运行。\nfrom six.moves.cPickle import loads Supported renames:\n   Name Python 2 name Python 3 name     builtins __builtin__ builtins   configparser ConfigParser configparser   copyreg copy_reg copyreg   cPickle cPickle pickle   cStringIO cStringIO.StringIO() io.StringIO   dbm_gnu gdbm dbm.gnu   _dummy_thread dummy_thread _dummy_thread   email_mime_multipart email.MIMEMultipart email.mime.multipart   email_mime_nonmultipart email.MIMENonMultipart email.mime.nonmultipart   email_mime_text email.MIMEText email.mime.text   email_mime_base email.MIMEBase email.mime.base   filter itertools.ifilter() filter()   filterfalse itertools.ifilterfalse() itertools.filterfalse()   getcwd os.getcwdu() os.getcwd()   getcwdb os.getcwd() os.getcwdb()   http_cookiejar cookielib http.cookiejar   http_cookies Cookie http.cookies   html_entities htmlentitydefs html.entities   html_parser HTMLParser html.parser   http_client httplib http.client   BaseHTTPServer BaseHTTPServer http.server   CGIHTTPServer CGIHTTPServer http.server   SimpleHTTPServer SimpleHTTPServer http.server   input raw_input() input()   intern intern() sys.intern()   map itertools.imap() map()   queue Queue queue   range xrange() range   reduce reduce() functools.reduce()   reload_module reload() imp.reload(), importlib.reload() on Python 3.4+   reprlib repr reprlib   shlex_quote pipes.quote shlex.quote   socketserver SocketServer socketserver   _thread thread _thread   tkinter Tkinter tkinter   tkinter_dialog Dialog tkinter.dialog   tkinter_filedialog FileDialog tkinter.FileDialog   tkinter_scrolledtext ScrolledText tkinter.scrolledtext   tkinter_simpledialog SimpleDialog tkinter.simpledialog   tkinter_ttk ttk tkinter.ttk   tkinter_tix Tix tkinter.tix   tkinter_constants Tkconstants tkinter.constants   tkinter_dnd Tkdnd tkinter.dnd   tkinter_colorchooser tkColorChooser tkinter.colorchooser   tkinter_commondialog tkCommonDialog tkinter.commondialog   tkinter_tkfiledialog tkFileDialog tkinter.filedialog   tkinter_font tkFont tkinter.font   tkinter_messagebox tkMessageBox tkinter.messagebox   tkinter_tksimpledialog tkSimpleDialog tkinter.simpledialog   urllib.parse See six.moves.urllib.parse urllib.parse   urllib.error See six.moves.urllib.error urllib.error   urllib.request See six.moves.urllib.request urllib.request   urllib.response See six.moves.urllib.response urllib.response   urllib.robotparser robotparser urllib.robotparser   urllib_robotparser robotparser urllib.robotparser   UserDict UserDict.UserDict collections.UserDict   UserList UserList.UserList collections.UserList   UserString UserString.UserString collections.UserString   winreg _winreg winreg   xmlrpc_client xmlrpclib xmlrpc.client   xmlrpc_server SimpleXMLRPCServer xmlrpc.server   xrange xrange() range   zip itertools.izip() zip()   zip_longest itertools.izip_longest() itertools.zip_longest()    这里用得比较多的是：\nimport six.moves.configparser import six.moves.cPickle import six.moves.cStringIO import six.moves.filter import six.moves.filterfalse import six.moves.getcwd import six.moves.http_cookies import six.moves.html_entities import six.moves.html_parser import six.moves.http_client import six.moves.BaseHTTPServer import six.moves.CGIHTTPServer import six.moves.SimpleHTTPServer import six.moves.input import six.moves.map import six.moves.queue import six.moves.range import six.moves.reduce import six.moves.socketserver import six.moves.zip import six.moves.zip_longest import six.moves.urllib.parse import six.moves.urllib.error import six.moves.urllib.request import six.moves.urllib.response 只有按这个方案导入其它模块，即可保证在py2、py3下都可正确导入模块，详细可参看six模块的文档。\n类的比较特性 在python2中，如果要为某类添加比较特性，只需要为该类添加__cmp__方法就可以了。\nclass Person(object): def __init__(self, firstname, lastname): self.first = firstname self.last = lastname def __cmp__(self, other): return cmp((self.last, self.first), (other.last, other.first)) def __repr__(self): return \u0026#34;%s%s\u0026#34; % (self.first, self.last) 在python3中推荐使用total_ordering这个decorator，并添加三个方法__eq__、__ne__、__lt__。\nfrom functools import total_ordering @total_ordering class Person(object): def __init__(self, firstname, lastname): self.first = firstname self.last = lastname def __eq__(self, other): return ((self.last, self.first) == (other.last, other.first)) def __ne__(self, other): return not (self == other) def __lt__(self, other): return ((self.last, self.first) \u0026lt; (other.last, other.first)) def __repr__(self): return \u0026#34;%s%s\u0026#34; % (self.first, self.last) python3中删除了cmp函数，可以很方便地补一个。\ndef cmp(x, y): \u0026#34;\u0026#34;\u0026#34;Replacement for built-in function cmp that was removed in Python 3Compare the two objects x and y and return an integer according tothe outcome. The return value is negative if x \u0026lt; y, zero if x == yand strictly positive if x \u0026gt; y.\u0026#34;\u0026#34;\u0026#34; return (x \u0026gt; y) - (x \u0026lt; y) 排序函数相关 在python2中，.sort() or sorted() 函数有一个 cmp参数，这个参数决定了排序。\n\u0026gt;\u0026gt;\u0026gt; def cmp_last_name(a, b): ... \u0026#34;\u0026#34;\u0026#34;Compare names by last name\u0026#34;\u0026#34;\u0026#34; ... return cmp(a.last, b.last) ... \u0026gt;\u0026gt;\u0026gt; sorted(actors, cmp=cmp_last_name) [\u0026#39;John Cleese\u0026#39;, \u0026#39;Terry Gilliam\u0026#39;, \u0026#39;Eric Idle\u0026#39;, \u0026#39;Terry Jones\u0026#39;, \u0026#39;Michael Palin\u0026#39;] 在python3中，排序函数只有一个 key参数，这个参数指定的函数直接返回用于进行排序的键值。\n\u0026gt;\u0026gt;\u0026gt; def keyfunction(item): ... \u0026#34;\u0026#34;\u0026#34;Key for comparison by last name\u0026#34;\u0026#34;\u0026#34; ... return item.last ... \u0026gt;\u0026gt;\u0026gt; sorted(actors, key=keyfunction) [\u0026#39;John Cleese\u0026#39;, \u0026#39;Terry Gilliam\u0026#39;, \u0026#39;Eric Idle\u0026#39;, \u0026#39;Terry Jones\u0026#39;, \u0026#39;Michael Palin\u0026#39;] 本次迁移大概就遇到上述这些问题。\n其实还有一个办法搞定历史遗留代码迁移，那就是直接使用python3运行脚本代码，遇报错则在Conservative Python 3 Porting Guide中查找相关事项，并根据建议作相应改动。\nThe End！\n参考  https://portingguide.readthedocs.io/en/latest/process.html https://jeremyxu2010.github.io/2017/11/%E5%86%99py2py3%E5%85%BC%E5%AE%B9%E7%9A%84%E4%BB%A3%E7%A0%81/  ","permalink":"https://jeremyxu2010.github.io/2020/06/%E8%BF%81%E7%A7%BB%E5%8E%86%E5%8F%B2%E9%81%97%E7%95%99%E4%BB%A3%E7%A0%81%E5%88%B0python3/","tags":["python"],"title":"迁移历史遗留代码到python3"},{"categories":["工具"],"contents":"最近干的工作比较杂，没有时间进行记录，很久没有写博客了，惭愧！正好五一假日期间有一些时间，把一些点滴记录下来。\n一个事情是最近给组内的同学进行了一个技术分享，分享的具体的内容就不复述了，分享完后有同学问到我使用的工具，觉得挺棒的，这里记录一下。\nmarp 首先要介绍的一个工具是marp。进行技术分享时少不了做一个简单的ppt，对着ppt给小伙伴讲讲技术。而以前我也一直是做微软的PowerPoint的，发现在做ppt的过程中浪费了大量的精力去调整字体、样式之类的东东，而且调整样式的活很容易让自己慢慢失去关注点，忘记了内容本身的脉络。\n而marp这款工具可以让我不用太在意样式，关注地以markdown格式编写ppt的内容，写完之后简单地分隔一下，对样式作稍许调整，一个精美的ppt就形成了。\n简易使用方法 先用vscode安装 Marp for VS Code插件，然后在vscode里新建一个markdown文件，参照example.md写markdown文件内容就可以了。\n当然marp通过一些注释扩展了一些功能，要用到这些功能需要了解下这些注释的写法，这个学起来很简单，参照官方文档20分钟就可以完全学会。\n最后写完的markdown文件可以很方便地转换为pptx、pdf、png等格式。\n为啥不用deckset，因为deckset收费，而且不能导出为pptx格式，不便于跟文档保管员交流，汗。。。\ndemo-magic 在做技术分享时，干对着ppt讲概念是有些不直观的，有时需要运行一些小demo进行演示。我们可以提前准备好要演示的小脚本，但脚本的问题在于观众可能还没太明白，脚本就一把跑过了。这样演示的体验不是太好。\n而在脚本中运用demo-magic，可以让脚本的执行按照演讲者的节奏随时暂停。\n简易使用方法 仔细看下demo-magic的源码，其实它核心就只是提供了4个bash函数：p、pe、wait、cmd。分别用来帮助打印提示、打印要执行的命令并执行、暂停脚本执行等待用户按Enter、进入交互式命令模式。而且demo-magic在输出提示或命令时会模拟人的输入过程，很有趣。用法很简单，参照官方的示例写几个小脚本就学会了。\n实作示例 最后将使用上述两个工具做的一次技术分享的原始素材公布出来，大家可以参考学习。\n参考  https://marp.app/ https://github.com/paxtonhare/demo-magic  ","permalink":"https://jeremyxu2010.github.io/2020/05/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB%E4%B9%8B%E5%B7%A5%E5%85%B7%E6%8E%A8%E8%8D%90/","tags":["ppt","markdown","bash"],"title":"技术分享之工具推荐"},{"categories":["数据库开发"],"contents":"最近做的工作与数据库操作有一些关系，这里将数据库的一些基础知识总结一下以备忘。\n为什么需要ACID？ 读书时学过关系型数据库必须具备四个特性：原子性（atomicity，或称不可分割性）、一致性（consistency）、隔离性（isolation，又称独立性）、持久性（durability）。为啥呢？\n读了一些资料，我觉得可以这样理解。关系型数据库内部存储着一堆数据资料，当然为了让这堆数据资料产生价值，必须要允许通过某些方式操作（查询、修改、删除）这些数据。当然这些操作不能毫无规则地进行，必须要遵守一定的规则，从而使数据库存储的数据始终是一致的。于是人们发现了事务这个概念，事务其实就是单个数据逻辑单元组成的对象操作集合。事务的执行遵循ACID特性，通过事务的执行，即可使数据库从一个一致的状态转换到另一个一致的状态。\n事务为啥要遵守原子性？一个事务是一连串的操作组成，增删改查的集合。原子性就要求一个事务要么全部执行成功，要么就不执行，不允许只执行一半，因此原子性是达成一致性的必要条件。但是只要保证了原子性就可以保证一致性了吗？显然不是，所以原子性是一致性的一个必要条件，但是不充分条件。\n事务为啥要遵守持久性？假设现在原子性保证了，一个事务未提交的时候，发生了错误就执行rollback，那么事务就不会提交了。但是当我们事务执行成功了，执行commit指令之后，遇到了错误会怎么样？正常情况下执行commit后会让事务刷盘，进行持久化操作。进行刷盘操作时是需要一定时间的，在这个刷盘过程中出现宕机、停电、系统崩溃等等可以中断刷盘的操作，那么这个过程理论上有可能导致一半数据刷盘成功，另一半没有刷进去，这显然不是期望的。持久性保证了一旦提交事务commit之后，事务一定会持久化到数据库中。即使刷盘过程中宕机了，导致只有一半数据刷盘成功。当数据库下一次重启的时候，会根据提交日志进行重放，将另一半的数据也进行写入。同样持久性是事务一致性的充分条件，但是还无法构成必要条件。\n事务为啥要隔离性？隔离性说的是多个并发事务实际上都是独立事务上下文，多个事务上下文之间彼此隔离，互不干扰。但是多个事务如果对共享数据进行查看、删除、修改，如果不加以控制，就会出现线程安全问题。如何解决这个线程安全问题？很自然会想到用锁的方案，但在数据库里直接用排它锁必然导致性能大打折扣。为了兼顾效率，前人已经为数据库设计了四种不同级别的锁，即四种隔离级别：读未提交、读已提交、可重复读、串行化。\n至此，我们终于明白为啥关系型数据库必须遵守ACID特性。\n何为事务隔离级别？ 很多书中都讲到关系型数据库存在4个事务隔离级别：由低到高依次为Read uncommitted 、Read committed 、Repeatable read 、Serializable，这4个级别可以逐个解决脏读、不可重复读、幻读这几个问题。\n   隔离级别 脏读（Dirty Read） 不可重复读（NonRepeatable Read） 幻读（Phantom Read）     未提交读（Read uncommitted） 可能 可能 可能   已提交读（Read committed） 不可能 可能 可能   可重复读（Repeatable read） 不可能 不可能 可能   可串行化（Serializable ） 不可能 不可能 不可能    级别越高，数据越安全，但性能越低。\n何为脏读、不可重复读、幻读？\n 1.脏读： 脏读就是指当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。\n2.不可重复读： 是指在一个事务内，多次读同一数据。在这个事务还没有结束时，另外一个事务也访问该同一数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改，那么第一个事务两次读到的的数据可能是不一样的。这样就发生了在一个事务内两次读到的数据是不一样的，因此称为是不可重复读。（即不能读到相同的数据内容） 例如，一个编辑人员两次读取同一文档，但在两次读取之间，作者重写了该文档。当编辑人员第二次读取文档时，文档已更改。原始读取不可重复。如果只有在作者全部完成编写后编辑人员才可以读取文档，则可以避免该问题。\n3.幻读: 是指当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入一行新数据。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象 发生了幻觉一样。 例如，一个编辑人员更改作者提交的文档，但当生产部门将其更改内容合并到该文档的主复本时，发现作者已将未编辑的新材料添加到该文档中。如果在编辑人员和生产部门完成对原始文档的处理之前，任何人都不能将新材料添加到文档中，则可以避免该问题。\n 何为MVCC？ 继续接上文。我们MySQL中InnoDB的默认隔离级别是REPEATABLE-READ。这个是如此实现的？拍脑袋一想，也许它是通过行锁来实现的吧。已有人好奇地做了相关实验，事实证明并不是通过行锁来实现，而是用Multiversion Concurrency Control来实现的。\n大多数的MySQL事务型存储引擎，如InnoDB都不止使用简单的行加锁机制，都和MVCC-多版本并发控制一起使用。锁机制可以控制并发操作，但是其系统开销较大，而MVCC可以在大多数情况下代替行级锁,使用MVCC,能降低其系统开销。\nMVCC是通过保存数据在某个时间点的快照来实现的。不同存储引擎的MVCC实现是不同的，下面分析一下MySQL的InnoDB的MVCC实现。\nInnoDB的MVCC，是通过在每行记录后面保存两个隐藏的列来实现的，这两个列，分别保存了这个行的创建时间，一个保存的是行的删除时间。这里存储的并不是实际的时间值,而是系统版本号(可以理解为事务的ID)，每开始一个新的事务，系统版本号就会自动递增，事务开始时刻的系统版本号会作为事务的ID。\n 例子 create table yang( id int primary key auto_increment, name varchar(20));  假设系统的版本号从1开始。\n INSERT InnoDB为新插入的每一行保存当前系统版本号作为版本号。 第一个事务ID为1；\nstart transaction; insert into yang values(NULL,\u0026#39;yang\u0026#39;) ; insert into yang values(NULL,\u0026#39;long\u0026#39;); insert into yang values(NULL,\u0026#39;fei\u0026#39;); commit; 对应在数据中的表如下(后面两列是隐藏列,我们通过查询语句并看不到)\n   id name 创建时间（事务ID) 删除时间（事务ID）     1 yang 1 undefined   2 long 1 undefined   3 fel 1 undefined    SELECT InnoDB会根据以下两个条件检查每行记录：\n  InnoDB只会查找版本早于当前事务版本的数据行(也就是,行的系统版本号小于或等于事务的系统版本号)，这样可以确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的\n  行的删除版本要么未定义,要么大于当前事务版本号,这可以确保事务读取到的行，在事务开始之前未被删除。\n只有a,b同时满足的记录，才能返回作为查询结果.\n  DELETE InnoDB会为删除的每一行保存当前系统的版本号(事务的ID)作为删除标识。 看下面的具体例子分析： 第二个事务，ID为2：\nstart transaction; select * from yang; //(1) select * from yang; //(2) commit; 假设1 假设在执行这个事务ID为2的过程中，刚执行到(1)，这时,有另一个事务ID为3往这个表里插入了一条数据， 第三个事务ID为3。\nstart transaction; insert into yang values(NULL,\u0026#39;tian\u0026#39;); commit; 这时表中的数据如下:\n   id name 创建时间（事务ID) 删除时间（事务ID）     1 yang 1 undefined   2 long 1 undefined   3 fel 1 undefined   4 tian 3 undefined    然后接着执行事务2中的(2)，由于id=4的数据的创建时间(事务ID为3)，执行当前事务的ID为2，而InnoDB只会查找事务ID小于等于当前事务ID的数据行，所以id=4的数据行并不会在执行事务2中的(2)被检索出来，在事务2中的两条select 语句检索出来的数据都只会下表：\n   id name 创建时间（事务ID) 删除时间（事务ID）     1 yang 1 undefined   2 long 1 undefined   3 fel 1 undefined    假设2 假设在执行这个事务ID为2的过程中，刚执行到(1)，假设事务执行完事务3后，接着又执行了事务4， 第四个事务：\nstart transaction; delete from yang where id=1; commit; 此时数据库中的表如下：\n   id name 创建时间（事务ID) 删除时间（事务ID）     1 yang 1 4   2 long 1 undefined   3 fel 1 undefined   4 tian 3 undefined    接着执行事务ID为2的事务(2)，根据SELECT 检索条件可以知道，它会检索创建时间(创建事务的ID)小于当前事务ID的行和删除时间(删除事务的ID)大于当前事务的行，而id=4的行上面已经说过，而id=1的行由于删除时间(删除事务的ID)大于当前事务的ID，所以事务2的(2)select * from yang也会把id=1的数据检索出来。所以，事务2中的两条select语句检索出来的数据都如下：\n   id name 创建时间（事务ID) 删除时间（事务ID）     1 yang 1 4   2 long 1 undefined   3 fel 1 undefined    UPDATE InnoDB执行UPDATE，实际上是新插入了一行记录，并保存其创建时间为当前事务的ID，同时保存当前事务ID到要UPDATE的行的删除时间。\n假设3 假设在执行完事务2的(1)后又执行，其它用户执行了事务3，4，这时，又有一个用户对这张表执行了UPDATE操作。 第5个事务：\nstart transaction; update yang set name=\u0026#39;Long\u0026#39; where id=2; commit; 根据update的更新原则：会生成新的一行，并在原来要修改的列的删除时间列上添加本事务ID，得到表如下：\n   id name 创建时间（事务ID) 删除时间（事务ID）     1 yang 1 4   2 long 1 5   3 fel 1 undefined   4 tian 3 undefined   2 Long 5 undefined    继续执行事务2的(2)，根据select 语句的检索条件，得到下表：\n   id name 创建时间（事务ID) 删除时间（事务ID）     1 yang 1 4   2 long 1 5   3 fel 1 undefined    还是和事务2中(1)select 得到相同的结果。\n 上面这个加两个隐藏字段的理解方案其实并不准确，顶多算一个简化版的理解方法。事实上InnoDB的MVCC是结合使用事务版本号、表的隐藏列、undo log、read view这几个概念实现的，参见这里。\n有些文章里说到MVVC可以解决幻读问题，但按上述MVVC原理的分析，实际上MVVC还是无法解决幻读问题的。\n参考  https://zh.wikipedia.org/wiki/ACID https://blog.csdn.net/qq_25448409/article/details/78110430 https://blog.csdn.net/JIESA/article/details/51317164 https://blog.csdn.net/pengzonglu7292/article/details/86562799 https://www.jianshu.com/p/c51ba403ce07 https://zhuanlan.zhihu.com/p/52977862  ","permalink":"https://jeremyxu2010.github.io/2020/03/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","tags":["mysql","transaction","isolation"],"title":"数据库基础知识"},{"categories":["微服务"],"contents":"最近在工作中碰到一个分布式事务问题，这个问题之前做项目的过程也搞过，不过没有深入整理，这个周末有时间刚好整理一把。\n问题引出 在微服务架构中，随着服务的逐步拆分，数据库私有已经成为共识，这也导致所面临的分布式事务问题成为微服务落地过程中一个非常难以逾越的障碍，但是目前尚没有一个完整通用的解决方案。为了保证分布式事务一致性目前业内成熟的解决方案有\n 强一致分布式事务方案：其中包括两段式提交协议2PC、三段式提交协议3PC。 最终一致分布式事务方案：其中包括事件通知模式（本地异步事件服务模式、外部事件服务模式、事务消息模式、最大努力通知模式）、事务补偿模式（Saga、TCC）。  各模式分析 本地事务 在分析各模式之前，先回顾一下传统的单机本机事务。\n传统单机应用使用一个RDBMS作为数据源。应用开启事务，进行CRUD，提交或回滚事务，统统发生在本地事务中，由资源管理器（RM）直接提供事务支持。数据的一致性在一个本地事务中得到保证。\n强一致分布式事务方案 两阶段提交协议（2PC） 显然随着服务的逐步拆分，各个服务均有自己的数据库，这个时候本地事务已经无法满足数据一致性的要求。由于多个数据源的同时访问，事务需要跨多个数据源管理。于是参考设计本地事务的思想，前人很自然研究出了两阶段提交（2PC）这种分布式事务模式。两阶段提交分为准备阶段和提交阶段。\n两阶段提交-commit\n两阶段提交-rollback\n2PC看着挺美好，但其实是存在缺陷的：\n 2PC在执行过程中可能发生协调者或者参与者突然宕机的情况，在不同时期宕机可能有不同的现象。\n 情况一：协调者挂了，参与者没挂 这种情况其实比较好解决，只要找一个协调者的替代者。当他成为新的协调者的时候，询问所有参与者的最后那条事务的执行情况，他就可以知道是应该做什么样的操作了。所以，这种情况不会导致数据不一致。\n 情况二：参与者挂了，协调者没挂 这种情况其实也比较好解决。如果协调者挂了。那么之后的事情有两种情况：\n 第一个是挂了就挂了，没有再恢复。那就挂了呗，反正不会导致数据一致性问题。 第二个是挂了之后又恢复了，这时如果他有未执行完的事务操作，直接取消掉，然后询问协调者目前我应该怎么做，协调者就会比对自己的事务执行记录和该参与者的事务执行记录，告诉他应该怎么做来保持数据的一致性。   情况三：参与者挂了，协调者也挂了 这种情况比较复杂，我们分情况讨论。\n 协调者和参与者在第一阶段挂了。  由于这时还没有执行commit操作，新选出来的协调者可以询问各个参与者的情况，再决定是进行commit还是roolback。因为还没有commit，所以不会导致数据一致性问题。   第二阶段协调者和参与者挂了，挂了的这个参与者在挂之前并没有接收到协调者的指令，或者接收到指令之后还没来的及做commit或者roolback操作。  这种情况下，当新的协调者被选出来之后，他同样是询问所有的参与者的情况。只要有机器执行了abort（roolback）操作或者第一阶段返回的信息是No的话，那就直接执行roolback操作。如果没有人执行abort操作，但是有机器执行了commit操作，那么就直接执行commit操作。这样，当挂掉的参与者恢复之后，只要按照协调者的指示进行事务的commit还是roolback操作就可以了。因为挂掉的机器并没有做commit或者roolback操作，而没有挂掉的机器们和新的协调者又执行了同样的操作，那么这种情况不会导致数据不一致现象。   第二阶段协调者和参与者挂了，挂了的这个参与者在挂之前已经执行了操作。但是由于他挂了，没有人知道他执行了什么操作。  这种情况下，新的协调者被选出来之后，如果他想负起协调者的责任的话他就只能按照之前那种情况来执行commit或者roolback操作。这样新的协调者和所有没挂掉的参与者就保持了数据的一致性，我们假定他们执行了commit。但是，这个时候，那个挂掉的参与者恢复了怎么办，因为他之前已经执行完了之前的事务，如果他执行的是commit那还好，和其他的机器保持一致了，万一他执行的是roolback操作那？这不就导致数据的不一致性了么？虽然这个时候可以再通过手段让他和协调者通信，再想办法把数据搞成一致的，但是，这段时间内他的数据状态已经是不一致的了！    所以，2PC协议中，如果出现协调者和参与者都挂了的情况，有可能导致数据不一致。\n 三段式提交协议（3PC） 为了解决上述问题，前人在2PC基础上又研究出3PC的方案。\n3PC最关键要解决的就是协调者和参与者同时挂掉的问题，所以3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。在第一阶段，只是询问所有参与者是否可可以执行事务操作，并不在本阶段执行事务操作。当协调者收到所有的参与者都返回YES时，在第二阶段才执行事务操作，然后在第三阶段在执行commit或者rollback。\n3PC相对2PC的优点：\n 直接分析协调者和参与者都挂的情况。\n 第二阶段协调者和参与者挂了，挂了的这个参与者在挂之前已经执行了操作。但是由于他挂了，没有人知道他执行了什么操作。  这种情况下，当新的协调者被选出来之后，他同样是询问所有的参与者的情况来觉得是commit还是roolback。这看上去和二阶段提交一样啊？他是怎么解决一致性问题的呢？ 看上去和二阶段提交的那种数据不一致的情况的现象是一样的，但仔细分析所有参与者的状态的话就会发现其实并不一样。我们假设挂掉的那台参与者执行的操作是commit。那么其他没挂的操作者的状态应该是什么？他们的状态要么是prepare-commit要么是commit。因为3PC的第三阶段一旦有机器执行了commit，那必然第一阶段大家都是同意commit。所以，这时，新选举出来的协调者一旦发现未挂掉的参与者中有人处于commit状态或者是prepare-commit的话，那就执行commit操作。否则就执行rollback操作。这样挂掉的参与者恢复之后就能和其他机器保持数据一致性了。（为了简单的让大家理解，笔者这里简化了新选举出来的协调者执行操作的具体细节，真实情况比我描述的要复杂）    简单概括一下就是，如果挂掉的那台机器已经执行了commit，那么协调者可以从所有未挂掉的参与者的状态中分析出来，并执行commit。如果挂掉的那个参与者执行了rollback，那么协调者和其他的参与者执行的肯定也是rollback操作。\n所以，再多引入一个阶段之后，3PC解决了2PC中存在的那种由于协调者和参与者同时挂掉有可能导致的数据一致性问题。\n 3PC也不是完美的，同样存在问题：\n 在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，会在等待超时之后，会继续进行事务的提交。\n所以，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。\n 最终一致分布式事务方案 尽管2PC/3PC存在一些问题，但其实是通过提升服务运营能力部分克服问题，那是不是2PC/3PC就可以满足微服务场景下分布式事务的需求了呢？答案是否定的，原因有三点：\n  由于微服务间无法直接进行数据访问，微服务间互相调用通常通过RPC（Dubbo）或Http API（Spring Cloud）进行，所以已经无法使用TM统一管理微服务的RM。 不同的微服务使用的数据源类型可能完全不同，如果微服务使用了NoSQL之类不支持事务的数据库，则事务根本无从谈起。 即使微服务使用的数据源都支持事务，那么如果使用一个大事务将许多微服务的事务管理起来，这个大事务维持的时间，将比本地事务长几个数量级。如此长时间的事务及跨服务的事务，将为产生很多锁及数据不可用，严重影响系统性能。   由此可见，传统的分布式事务已经无法满足微服务架构下的事务管理需求。那么，既然无法满足传统的ACID事务，在微服务下的事务管理必然要遵循新的法则－－BASE理论。\n BASE理论由eBay的架构师Dan Pritchett提出，BASE理论是对CAP理论的延伸，核心思想是即使无法做到强一致性，应用应该可以采用合适的方式达到最终一致性。BASE是指基本可用（Basically Available）、软状态（ Soft State）、最终一致性（ Eventual Consistency）。\n 基本可用：指分布式系统在出现故障的时候，允许损失部分可用性，即保证核心可用。 软状态：允许系统存在中间状态，而该中间状态不会影响系统整体可用性。分布式存储中一般一份数据至少会有三个副本，允许不同节点间副本同步的延时就是软状态的体现。 最终一致性：最终一致性是指系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。弱一致性和强一致性相反，最终一致性是弱一致性的一种特殊情况。   BASE中的最终一致性是对于微服务下的事务管理的根本要求，即虽然基于微服务的事务管理无法达到强一致性，但必须保证最终一致性，这就是所说的柔性事务。\n实现事务最终一致性的方案主要有事件通知模式、事务补偿模式两种。\n事件通知方案 事件通知模式的设计理念比较容易理解，即是主服务完成后将结果通过事件（常常是消息队列）传递给从服务，从服务在接受到消息后进行消费，完成业务，从而达到主服务与从服务间的消息一致性。\n设计理念很简单，那是不是真的很简单呢？小白可能很简单地想到以下方案：\n上面的逻辑看上去天衣无缝，如果数据库操作失败则直接退出，不发送消息；如果发送消息失败，则数据库回滚；如果数据库操作成功且消息发送成功，则业务成功，消息发送给下游消费。然后仔细思考后，这种同步消息通知有两个问题：\n   在微服务的架构下，有可能出现网络IO问题或者服务器宕机的问题，如果这些问题出现在时序图的第7步，使得消息投递后无法正常通知主服务（网络问题），或无法继续提交事务（宕机），那么主服务将会认为消息投递失败，会滚主服务业务，然而实际上消息已经被从服务消费，那么就会造成主服务和从服务的数据不一致。具体场景可见下面两张时序图。\n  事件服务（在这里就是消息服务）与业务过于耦合，如果消息服务不可用，会导致业务不可用。应该将事件服务与业务解耦，独立出来异步执行，或者在业务执行后先尝试发送一次消息，如果消息发送失败，则降级为异步发送。\n   本地异步事件服务模式 为了解决上述同步事件中描述的同步事件的问题，异步事件通知模式被发展了出来，既业务服务和事件服务解耦，事件异步进行，由单独的事件服务保证事件的可靠投递。\n当业务执行时，在同一个本地事务中将事件写入本地事件表，同时投递该事件，如果事件投递成功，则将该事件从事件表中删除。如果投递失败，则由定时任务异步统一地处理投递失败的事件，进行重新投递，直到事件被正确投递，并将事件从事件表中删除。这个在本服务内部的定时任务一般叫它本地事件服务。这种方式最大可能地保证了事件投递的实效性，并且当第一次投递失败后，也能使用本地事件服务保证事件至少被投递一次。\n外部异步事件服务模式 上述使用本地事件服务保证可靠事件通知的方式也有它的不足之处，那便是业务仍旧与事件服务有一定耦合（第一次同步投递时），更为严重的是，本地事务需要负责额外的事件表的操作，为数据库带来了压力，在高并发的场景，由于每一个业务操作就要产生相应的事件表操作，几乎将数据库的可用吞吐量砍了一半，这无疑是无法接受的。正是因为这样的原因，可靠事件通知模式进一步地发展－外部事件服务出现在了人们的眼中。\n外部事件服务在本地事件服务的基础上更进了一步，将事件服务独立出主业务服务，主业务服务不在对事件服务有任何强依赖。\n 业务服务在提交前，向事件服务发送事件，事件服务只记录事件，并不发送。业务服务在提交或回滚后通知事件服务，事件服务发送事件或者删除事件。不用担心业务系统在提交或者会滚后宕机而无法发送确认事件给事件服务，因为事件服务会定时获取所有仍未发送的事件并且向业务系统查询，根据业务系统的返回来决定发送或者删除该事件。\n外部事件虽然能够将业务系统和事件系统解耦，但是也带来了额外的工作量：外部事件服务比起本地事件服务来说多了两次网络通信开销（提交前、提交／回滚后），同时也需要业务系统提供单独的查询接口给事件系统用来判断未发送事件的状态。\n MQ事务消息模式 上述外部事件服务虽然解耦得比较成功，但其实还是需要自行开发一个独立的事件服务系统的，比较麻烦。其实还有更优雅地方案：事务消息模式。假如使用的MQ本身支持事务消息，这样业务应用就能以某种方式确保消息正确投递到MQ。这个方案的代表是RocketMQ。\n在RocketMQ这个方案里，事务消息作为一种异步确保型事务， 将两个事务分支通过 MQ 进行异步解耦，RocketMQ 事务消息的设计流程同样借鉴了两阶段提交理论，整体交互流程如下图所示：\n  事务发起方首先发送 prepare 消息到 MQ。 在发送 prepare 消息成功后执行本地事务。 根据本地事务执行结果返回 commit 或者是 rollback。 如果消息是 rollback，MQ 将删除该 prepare 消息不进行下发，如果是 commit 消息，MQ 将会把这个消息发送给 consumer 端。 如果执行本地事务过程中，执行端挂掉，或者超时，MQ 将会不停的询问其同组的其他 producer 来获取状态。 Consumer 端的消费成功机制有 MQ 保证。   这里浏览下RocketMQ这种方案的代码：\nTransactionProducer\npublic class TransactionProducer { public static void main(String[] args) throws MQClientException, InterruptedException { TransactionListener transactionListener = new TransactionListenerImpl(); TransactionMQProducer producer = new TransactionMQProducer(\u0026#34;please_rename_unique_group_name\u0026#34;); ExecutorService executorService = new ThreadPoolExecutor(2, 5, 100, TimeUnit.SECONDS, new ArrayBlockingQueue\u0026lt;Runnable\u0026gt;(2000), new ThreadFactory() { @Override public Thread newThread(Runnable r) { Thread thread = new Thread(r); thread.setName(\u0026#34;client-transaction-msg-check-thread\u0026#34;); return thread; } }); producer.setNamesrvAddr(\u0026#34;10.10.15.246:9876;10.10.15.247:9876\u0026#34;); producer.setExecutorService(executorService); producer.setTransactionListener(transactionListener); producer.start(); String[] tags = new String[] {\u0026#34;TagA\u0026#34;, \u0026#34;TagB\u0026#34;, \u0026#34;TagC\u0026#34;, \u0026#34;TagD\u0026#34;, \u0026#34;TagE\u0026#34;}; for (int i = 0; i \u0026lt; 10; i++) { try { Message msg = new Message(\u0026#34;TranTest\u0026#34;, tags[i % tags.length], \u0026#34;KEY\u0026#34; + i, (\u0026#34;Hello RocketMQ \u0026#34; + i).getBytes(RemotingHelper.DEFAULT_CHARSET)); SendResult sendResult = producer.sendMessageInTransaction(msg, null); System.out.printf(\u0026#34;%s%n\u0026#34;, sendResult); Thread.sleep(10); } catch (MQClientException | UnsupportedEncodingException e) { e.printStackTrace(); } } for (int i = 0; i \u0026lt; 100000; i++) { Thread.sleep(1000); } producer.shutdown(); } } TransactionListener\npublic class TransactionListenerImpl implements TransactionListener { private AtomicInteger transactionIndex = new AtomicInteger(0); private ConcurrentHashMap\u0026lt;String, Integer\u0026gt; localTrans = new ConcurrentHashMap\u0026lt;\u0026gt;(); public LocalTransactionState executeLocalTransaction(Message msg, Object arg) { int value = transactionIndex.getAndIncrement(); int status = value % 3; localTrans.put(msg.getTransactionId(), status); return LocalTransactionState.UNKNOW; } public LocalTransactionState checkLocalTransaction(MessageExt msg) { Integer status = localTrans.get(msg.getTransactionId()); if (null != status) { switch (status) { case 0: return LocalTransactionState.UNKNOW; case 1: return LocalTransactionState.COMMIT_MESSAGE; case 2: return LocalTransactionState.ROLLBACK_MESSAGE; } } return LocalTransactionState.COMMIT_MESSAGE; } } 如果从代码的角度，可以看到在RocketMQ这个方案里，生产者TransactionProducer涉及到2个角色：本地事务执行器（代码中的TransactionListenerImpl的executeLocalTransaction方法）、服务器回查客户端Listener（代码中TransactionListenerImpl的checkLocalTransaction方法）。\n如果事务消息发送到MQ上后，会回调本地事务执行器；但是此时事务消息是prepare状态，对消费者还不可见，需要本地事务执行器返回RMQ一个确认消息。只有当确认完之后，才会将消息的状态由消费端不可见的prepare状态更新为消费者端可见的commied状态，如果本地事务执行器返回的是rollbak，则RMQ直接删除该prepare状态的消息。\n为了处理由于异常情况导致RMQ收不到本地事务执行器的确认消息的问题，RMQ会通过服务器回查客户端Listener接口反查prepare状态事务消息最终应该的状态，从而将消息由消费端不可见的prepare状态更新为消费者端可见的commied状态或直接删除prepare状态的消息。\nRocketMQ的实现代码也挺有意思，可以看看这篇文章。\n这里想到一个问题，为啥只有RocketMQ实现了事务消息的功能，其它MQ基本都没有实现这个功能。对于分布事务场景来说，这个功能不香吗？\n其实业界各种MQ均有各自的适用场景，很多通用性MQ关注点是消息的高效流转，如果加上事务消息这一特性会导致MQ的性能打一些折扣。而解决消息的可靠投递并不一定需要使用事务消息方案，采用上面介绍的两种方法也可以。一句话，技术上并不是一定要追求架构的最优，还是要考虑综合效能。\n事件最大努力通知 上面说到的3种模式，均可以保证事件消息可靠地投递到下游服务那儿去。但有些场景是允许一定程度地丢消息的。于是就发展出事件最大努力通知模式。最大努力通知型的特点是，业务服务在提交事务后，进行有限次数（设置最大次数限制）的消息发送，比如发送三次消息，若三次消息发送都失败，则不予继续发送。所以有可能导致消息的丢失。同时，主业务方需要提供查询接口给从业务服务，用来恢复丢失消息。最大努力通知型对于时效性保证比较差（既可能会出现较长时间的软状态），所以对于数据一致性的时效性要求比较高的系统无法使用。这种模式通常使用在不同业务平台服务或者对于第三方业务服务的通知，如银行通知、商户通知等。\n事件通知模式要注意的点：\n通过上面的描述，我们知道只要是使用事件通知模式，那么消费端收到的消息就有可能是重复的。那么就需要消费端保证同一条事件不会重复被消费，简而言之就是保证事件消费的幂等性。\n如果事件本身是具备幂等性的状态型事件，如订单状态的通知（已下单、已支付、已发货等），则需要判断事件的顺序。一般通过时间戳来判断，既消费过了新的消息后，当接受到老的消息直接丢弃不予消费。如果无法提供全局时间戳，则应考虑使用全局统一的序列号。\n对于不具备幂等性的事件，一般是动作行为事件，如扣款100，存款200，则应该将事件ID及事件结果持久化，在消费事件前查询事件ID，若已经消费则直接返回执行结果；若是新消息，则执行，并存储执行结果。\n事务补偿方案 除了事件通知模式外，事务补偿模式也可以实现事务的最终一致性。\n补偿模式比起事件通知模式最大的不同是，补偿模式的上游服务依赖于下游服务的运行结果，而事件通知模式上游服务不依赖于下游服务的运行结果。\nSaga模式 Saga是一种纯业务补偿模式，其设计理念为，业务在调用的时候正常提交，当一个服务失败的时候，所有其依赖的上游服务都进行业务补偿操作。\n业务补偿模式要求每个服务都提供补偿接口，且这种补偿一般来说是不完全补偿，既即使进行了补偿操作，那条取消的火车票记录还是一直存在数据库中可以被追踪（一般是有相信的状态字段“已取消”作为标记），毕竟已经提交的线上数据一般是不能进行物理删除的。\n业务补偿模式最大的缺点是软状态的时间比较长，既数据一致性的时效性很低，多个服务常常可能处于数据不一致的情况。\n这种方案我以前在项目中深入实践中，可以看看这些文章：servicecomb-saga开发实战、servicecomb-saga源码解读。\nTCC模式 TCC模式是一种优化了的业务补偿模式，它可以做到完全补偿，既进行补偿后不留下补偿的纪录，就好像什么事情都没有发生过一样。同时，TCC的软状态时间很短，原因是因为TCC是一种两阶段型模式，只有在所有的服务的第一阶段（try）都成功的时候才进行第二阶段确认（Confirm）操作，否则进行补偿(Cancel)操作，而在try阶段是不会进行真正的业务处理的。\nTCC模式的具体流程为两个阶段：\n Try，业务服务完成所有的业务检查，预留必需的业务资源 如果Try在所有服务中都成功，那么执行Confirm操作，Confirm操作不做任何的业务检查（因为try中已经做过），只是用Try阶段预留的业务资源进行业务处理；否则进行Cancel操作，Cancel操作释放Try阶段预留的业务资源。  TCC模式跟纯业务补偿模式相比，需要每个服务都需要实现Confirm和Cancel两个接口，因此落地实施上比纯业务补偿模式复杂一些，但好处是数据一致的实时性高，因此其在很多金融、电商场景中大量采用。\n在事务补偿方案中，由于上游服务依赖于下游服务的结果，考虑到上下游服务间网络有可能是不稳定的，因此业务接口、补偿接口（Saga模式中）和Try接口、Confirm接口、Cancel接口（TCC模式中）均有可能会被多次调用，因此这些接口在实现时需要考虑幂等性。幂等性的实现方式可以是：\n1、通过唯一键值做处理，即每次调用的时候传入唯一键值，通过唯一键值判断业务是否被操作，如果已被操作，则不再重复操作\n2、通过状态机处理，给业务数据设置状态，通过业务状态判断是否需要重复执行\n最终一致分布式方案对比 最终一致分布式方案如此之多，如何选择呢？其实还是要根据具体的业务场景。\n   类型 模式 数据一致性的实时性 开发成本 上游服务是否依赖下游服务结果 事件消息发送链路 业务耦合事件发送 绑定MQ特性     事件通知 本地异步事件服务模式 高 高 不依赖 短 高 否   事件通知 外部异步事件服务模式 高 高 不依赖 长 低 否   事件通知 MQ事务消息模式 高 低 不依赖 中 低 是   事件通知 事件最大努力通知模式 低 低 不依赖 短 高 否   事务补偿 Saga纯业务补偿模式 低 低 依赖 - - -   事务补偿 TCC业务补偿模式 高 高 依赖 - - -    各个方案均有优缺点，按照业务场景及开发团队的能力水平选择一种合适的方案即可。\nDONE！\n参考  https://mp.weixin.qq.com/s/7AGxzyy05PBT857_NNdr-Q http://www.hollischuang.com/archives/1580 https://itzones.cn/2019/07/09/RocketMQ%E4%BA%8B%E5%8A%A1%E6%B6%88%E6%81%AF/ https://www.zhihu.com/question/48627764 https://blog.roncoo.com/article/124243  ","permalink":"https://jeremyxu2010.github.io/2020/03/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%B8%AD%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%96%B9%E6%A1%88/","tags":["microservice","transaction","tcc","saga"],"title":"微服务中的分布式事务方案"},{"categories":["python开发"],"contents":"复工后第一周的工作是用python写一个图遍历的算法，周末稍微总结一下。\n纯手工写 最开始想挑战一下，于是不借助任何第三方库，纯手工编写，其实也不太难。\n# 用一个字典表示有向图中各节点到其它节点的边 graph = {\u0026#39;A\u0026#39;: [\u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;D\u0026#39;], \u0026#39;B\u0026#39;: [\u0026#39;E\u0026#39;], \u0026#39;C\u0026#39;: [\u0026#39;D\u0026#39;, \u0026#39;F\u0026#39;], \u0026#39;D\u0026#39;: [\u0026#39;B\u0026#39;, \u0026#39;E\u0026#39;, \u0026#39;G\u0026#39;], \u0026#39;E\u0026#39;: [], \u0026#39;F\u0026#39;: [\u0026#39;D\u0026#39;, \u0026#39;G\u0026#39;], \u0026#39;G\u0026#39;: [\u0026#39;E\u0026#39;]} # 查找从一个节点到另一个节点的路径 def findPath(graph, start, end, path=[]): path = path + [start] if start == end: return path for node in graph[start]: if node not in path: newpath = findPath(graph, node, end, path) if newpath: return newpath return None # 找到所有从start到end的路径 def findAllPath(graph, start, end, path=[]): path = path + [start] if start == end: return [path] paths = [] # 存储所有路径 for node in graph[start]: if node not in path: newpaths = findAllPath(graph, node, end, path) for newpath in newpaths: paths.append(newpath) return paths # 查找最短路径 def findShortestPath(graph, start, end, path=[]): path = path + [start] if start == end: return path shortestPath = [] for node in graph[start]: if node not in path: newpath = findShortestPath(graph, node, end, path) if newpath: if not shortestPath or len(newpath) \u0026lt; len(shortestPath): shortestPath = newpath return shortestPath # 查找最短路径 def findLongestPath(graph, start, end, path=[]): path = path + [start] if start == end: return path longestPath = [] for node in graph[start]: if node not in path: newpath = findLongestPath(graph, node, end, path) if newpath: if not longestPath or len(newpath) \u0026gt; len(longestPath): longestPath = newpath return longestPath 代码不太难，可以看到上述方法均使用了递归。\n然后测试一下：\nonepath = findPath(graph, \u0026#39;A\u0026#39;, \u0026#39;E\u0026#39;) print(\u0026#39;一条路径:\u0026#39;, onepath) allpath = findAllPath(graph, \u0026#39;A\u0026#39;, \u0026#39;E\u0026#39;) print(\u0026#39;\\n所有路径：\u0026#39;, allpath) shortpath = findShortestPath(graph, \u0026#39;A\u0026#39;, \u0026#39;E\u0026#39;) print(\u0026#39;\\n最短路径：\u0026#39;, shortpath) longpath = findLongestPath(graph, \u0026#39;A\u0026#39;, \u0026#39;E\u0026#39;) print(\u0026#39;\\n最长路径：\u0026#39;, longpath) 为了便于观看这个有向图到底长成什么样子了，这里用graphviz画一下这个有向图：\nimport tempfile from graphviz import Digraph g = Digraph(name=\u0026#39;G\u0026#39;) g.node_attr.update(shape=\u0026#39;circle\u0026#39;) g.node(\u0026#39;A\u0026#39;, shape=\u0026#39;doublecircle\u0026#39;) g.node(\u0026#39;E\u0026#39;, shape=\u0026#39;doublecircle\u0026#39;) for tail, v in graph.items(): for head in v: g.edge(tail, head) g.view(tempfile.mktemp(\u0026#39;.gv\u0026#39;)) 使用networkx库实现 纯手工写可以锻炼下动手能力，但真正在生产实践中，面对时刻变化的需求，还是找一个成熟的图算法库好一点，这里我找到了networkx这个python库。\nnetworkx这个库支持多种类型的图：无向图、有向图、允许平行边的无向图、允许平行边的有向图。\n   Networkx Class Type Self-loops allowed Parallel edges allowed     Graph undirected Yes No   DiGraph directed Yes No   MultiGraph undirected Yes Yes   MultiDiGraph directed Yes Yes    如果对排序一致性有要求的话，还可以用**OrderedGraph**、**OrderedDiGraph**、**OrderedMultiGraph**、**OrderedMultiDiGraph**这四个变种。\n选择合适的图类型即可创建图，如下：\nimport networkx as nx g = nx.OrderedDiGraph() 接下来就可以向图中添加节点和边了：\n# 只要对象是hashable的，即可添加进图 g.add_node(1) g.add_node(2) g.add_nodes_from([3, 4, 5]) g.add_edge(1, 2) g.add_edges_from([(2, 3), (3, 4), (4, 5)]) 可以访问图的节点或边：\ng.nodes() g.edges() 访问图中节点的邻居：\ng.adj[1] 访问图中某些节点相关的度量：\ng.degree([1, 2]) 图、节点、边上都可以添加属性：\ng.graph[\u0026#39;desc\u0026#39;]=\u0026#39;This is a demo graph\u0026#39; g.nodes[1][\u0026#39;name\u0026#39;] = \u0026#39;node1\u0026#39; g.edges[(1, 2)][\u0026#39;desc\u0026#39;] = \u0026#39;from 1 to 2\u0026#39; 用相关的API构造出图后，即可采用一定的算法处理这个图，networkx提供了很多相关的算法Algorithms，这个就是这个库的关键所在了，思考下业务场景找到对应的算法调用即可。\n比如想搜索一下图中出现的环，使用这个包Cycles下的方法就可以了：\nfrom networkx.algorithms.cycles import simple_cycles cycles = simple_cycles(g) 探索从某节点到某节点的路径列表，使用这个包Simple Paths下的方法：\nfrom networkx.algorithms.simple_paths import all_simple_paths, shortest_simple_paths all_paths = all_simple_paths(g, \u0026#39;S\u0026#39;, \u0026#39;E\u0026#39;) shortest_paths = shortest_simple_paths(g, \u0026#39;S\u0026#39;, \u0026#39;E\u0026#39;) 除此以外，这个库还提供一些生成器方法，用来生成一些业界很知名的图，这个可以很方便地用来进行测试，如：\npetersen = nx.petersen_graph() tutte = nx.tutte_graph() maze = nx.sedgewick_maze_graph() tet = nx.tetrahedral_graph() 最后为了让开发人员可以直观地看图，networkx也提供将图画出来的能力，不过我还是习惯用graphviz来画，这个样式调整起来更方便，见上面graphviz的小例子。\nDONE！\n参考  https://networkx.github.io/ https://graphviz.readthedocs.io/  ","permalink":"https://jeremyxu2010.github.io/2020/02/python%E5%9B%BE%E7%AE%97%E6%B3%95%E5%AE%9E%E6%88%98/","tags":["python","networkx","graphviz"],"title":"python图算法实战"},{"categories":["linux"],"contents":"最近在家办公，发现天朝的网络限制越来越严了。周末终于有时间，把家里电脑的上网状况改善一下了，这里记录一下。\n搭FQ的梯子 在香港买了台服务器，在上面装了个shadowsocks-libev。为什么用shadowsocks-libev，因为买的服务器配置不太高，这个是用c语言和libev写的，资源占用低。\n$ yum install -y epel-release $ curl -o /etc/yum.repos.d/librehat-shadowsocks-epel-7.repo https://copr.fedorainfracloud.org/coprs/librehat/shadowsocks/repo/epel-7/librehat-shadowsocks-epel-7.repo $ yum install -y shadowsocks-libev 编辑shadowsocks-libev的配置文件：\n$ vim /etc/shadowsocks-libev/config.json { \u0026#34;server\u0026#34;:\u0026#34;0.0.0.0\u0026#34;, \u0026#34;server_port\u0026#34;:55387, \u0026#34;local_port\u0026#34;:1080, \u0026#34;password\u0026#34;:\u0026#34;\u0026lt;A-PASSWORD\u0026gt;\u0026#34;, \u0026#34;timeout\u0026#34;:60, \u0026#34;method\u0026#34;:\u0026#34;xchacha20-ietf-poly1305\u0026#34; } 这里用xchacha20-ietf-poly1305这个算法，据说效率好一点，而且也能稍微抗点干扰。\n启动并设置开机自启动：\n$ systemctl enable shadowsocks-libev \u0026amp;\u0026amp; systemctl start shadowsocks-libev 在家里的Mac电脑上也安装shadowsocks-libev：\n$ brew install shadowsocks-libev 编辑shadowsocks-libev的配置文件：\n$ vim /usr/local/etc/shadowsocks-libev.json { \u0026#34;server\u0026#34;: \u0026#34;xx.xx.xx.xx\u0026#34;, \u0026#34;server_port\u0026#34;: 55387, \u0026#34;local_port\u0026#34;: 1086, \u0026#34;password\u0026#34;: \u0026#34;\u0026lt;A-PASSWORD\u0026gt;\u0026#34;, \u0026#34;timeout\u0026#34;: 600, \u0026#34;method\u0026#34;: \u0026#34;xchacha20-ietf-poly1305\u0026#34; } 这里的server设置为上面那台服务器的公网IP，password、method要与上面的设置保持一致。\n启动它并设置开机自启动：\n$ brew services enable shadowsocks-libev 至此即在本机Run起了一个Socks5代理127.0.0.1:1086，任何程序使用该代理上网，即是FQ了。\n自动使用FQ梯子 显然所有的请求都走上述Socks5代理，上网速度肯定很慢，还好已经有人整理了gfwlist，即应该FQ的网站地址列表。那么我们只需要搭建一个智能代理，当遇到FQ地址时使用上述Socks5代理，否则就直接访问。在网上找了下，这里使用privoxy这个软件完成这件事。\n首先用autoproxy2privoxy这个工具将gfwlist转成privoxy识别的actionsfile文件。\n$ make -B proxy=socks5://127.0.0.1:1086 # 这里会生成gfwlist.action 安装privoxy:\n$ brew install privoxy 将上述生成的gfwlist.action放到privoxy的配置目录，并进行相应的配置：\n$ cp -r gfwlist.action /usr/local/etc/privoxy/gfwlist.action $ vim /usr/local/etc/privoxy/config .... actionsfile default.action # Main actions file actionsfile user.action # User customizations actionsfile gfwlist.action # 增加gfwlist.action actionsfile custom.action # 增加自定义代理规则 .... 有些地址虽然不在gfwlist列表里，但也要FQ才能访问，因此这里添加了一个custom.action：\n$ cat /usr/local/etc/privoxy/custom.action {+forward-override{forward-socks5 127.0.0.1:1086 .}} .githubusercontent.com github.com 这里提一句，actionsfile文件里除了设置这些地址走Socks5代理外，还可以设置其它类型的代理，因此很适合做层次结构的代理服务器方案，参见这里。\n启动它并设置开机自启动：\n$ brew services start privoxy 然后在系统的网络设置处设置全局代理，System Preferences-Network-Advanced...-Proxies，这里设置Web Proxy（HTTP）和Secure Web Proxy（HTTPS）均为127.0.0.1:8118。\n注意一直明显不需要走代理的IP或域名填写到下面的Bypass proxy settings for these Hosts \u0026amp; Domains里，如localhost、127.0.0.1、114.114.114.114\n现在打开游览器应该可以正常FQ上网了，而且访问网内的网站速度也很快。\n处理DNS污染问题 以为万事大吉了，但有些网站仍然打不开，最终查明是域名的DNS的域名解析结果被污染了。这里在本机搭建DNS服务解决该问题。\n安装dnsmasq和dnscrypt-proxy\n$ brew install dnsmasq $ brew install dnscrypt-proxy 国内的一些域名可以安全地使用国内的DNS服务进行解析，这样快一点，有人已经将这些域名整理出来了，直接使用它。\n$ /usr/bin/curl -x socks5://127.0.0.1:1086 -o /usr/local/etc/dnsmasq.d/accelerated-domains.china.conf https://raw.githubusercontent.com/felixonmars/dnsmasq-china-list/master/accelerated-domains.china.conf 这里用的是电信的DNS服务器，如果你是联通的网络，你可以执行下面的脚本将配置文件中的DNS服务器换成联通的：\nsed -i -e \u0026#39;s/114.114.114.114/221.6.4.66/g\u0026#39; /usr/local/etc/dnsmasq.d/accelerated-domains.china.conf 然后配置dnsmasq的主配置文件：\n$ vim /usr/local/etc/dnsmasq.conf listen-address=127.0.0.1 no-resolv conf-dir=/usr/local/etc/dnsmasq.d server=127.0.0.1#5300 这里除上面的accelerated-domains.china.conf列表中的域名使用114.114.114.114解析外，其它域名解析转发给dnscrypt-proxy处理。\n配置dnscrypt-proxy的主配置文件：\n$ vim /usr/local/etc/dnscrypt-proxy.toml # 监听5300端口 listen_addresses = [\u0026#39;127.0.0.1:5300\u0026#39;, \u0026#39;[::1]:5300\u0026#39;] # 使用下面3个公开的DNS服务 server_names = [\u0026#39;google\u0026#39;, \u0026#39;cloudflare\u0026#39;, \u0026#39;cloudflare-ipv6\u0026#39;] # 如果找不到合适的公开DNS服务，则使用下面的DNS服务 fallback_resolvers = [\u0026#39;9.9.9.9:53\u0026#39;, \u0026#39;8.8.8.8:53\u0026#39;] # 担心这些DNS请求被墙，设置使用代理发送DNS请求 force_tcp = true proxy = \u0026#39;socks5://127.0.0.1:1086\u0026#39; 启动dnsmasq和dnscrypt-proxy：\n$ sudo brew services start dnsmasq $ brew services start dnscrypt-proxy 注意因为dnsmasq要监听53端口，因此要使用root权限启动\n然后在网络设置处设置全局DNS服务器为127.0.0.1，System Preferences-Network-Advanced...-DNS，在这个界面中删除原来的所有DNS Servers，添加127.0.0.1。\n强制老旧程序使用全局代理 有些老旧程序并不会使用系统的全局代理，这点很讨厌，我这里可以使用proxychains-ng强制这些老旧程序使用全局HTTP代理。\n安装proxychains-ng：\n$ brew install proxychains-ng 编辑proxychains-ng的主配置文件：\n$ vim /usr/local/etc/proxychains.conf # Quiet mode (no output from library) quiet_mode # Proxy DNS requests - no leak for DNS data proxy_dns [ProxyList] # add proxy here ... # meanwile # defaults set to \u0026#34;tor\u0026#34; # socks4 127.0.0.1 9050 http 127.0.0.1 8118 然后在启动老旧程序前加上/usr/local/bin/proxychains4即可，也可以写成脚本，如下：\n#!/bin/bash  (/usr/local/bin/proxychains4 \u0026#39;/Applications/Navicat for MySQL.app/Contents/MacOS/Navicat for MySQL\u0026#39; \u0026lt;/dev/null \u0026gt;/dev/null 2\u0026gt;\u0026amp;1) \u0026amp; 上面的例子里，启动的Navicat即会使用到全局的HTTP代理。\n至此系统中所有程序均可以自动选择连网的方式，终于可以愉快地享受自由的网络了。\nDONE！\n参考  https://www.howru.cc/articles/350.html https://github.com/jeremyxu2010/autoproxy2privoxy https://www.privoxy.org/user-manual/actions-file.html#FORWARD-OVERRIDE https://w2x.me/2019/06/12/Mac-OS%E9%85%8D%E7%BD%AEDnsmasq-Dnscrypt%E6%9D%A5%E8%A7%A3%E5%86%B3DNS%E6%B1%A1%E6%9F%93%E9%97%AE%E9%A2%98/ https://github.com/felixonmars/dnsmasq-china-list https://www.hi-linux.com/posts/48321.html  ","permalink":"https://jeremyxu2010.github.io/2020/02/%E4%BA%AB%E5%8F%97%E8%87%AA%E7%94%B1%E7%9A%84%E7%BD%91%E7%BB%9C/","tags":["linux","privoxy","dnsmasq","dnscrypt-proxy","proxychains-ng"],"title":"享受自由的网络"},{"categories":["golang开发"],"contents":"工作中一个小问题，Android手机上的APP访问后台服务频繁出现网络超时。以下为分析此问题的简要步骤，记录一下以备忘。\n了解网络链路 首先大致了解下该场景的网络链路，比较简单。\nAPP -\u0026gt; HTTP Proxy Server -\u0026gt; APP backend Service 估算网络请求压力 在测试场景里，有70台手机上的APP向对后端服务发出HTTP请求。假设每个APP每秒发出一个请求，这样算得后端服务需要扛住的请求压力至少为70qps。\n对后端服务进行压测 首先检测下后端服务是否可以扛住此压力。使用以下命令：\n$ hey -z 60s -c 70 -q 1 -m \u0026#39;POST\u0026#39; -d \u0026#39;xxxxx\u0026#39; \\  -T \u0026#34;application/x-www-form-urlencoded; charset=UTF-8\u0026#34; \\  -H \u0026#39;accept: */*\u0026#39; \\  -H \u0026#39;accept-encoding: gzip, deflate\u0026#39; \\  -H \u0026#39;connection: keep-alive\u0026#39; \\  -H \u0026#39;content-length: 111\u0026#39; \\  -H \u0026#39;user-agent: python-requests/2.10.0\u0026#39; \\ \t\u0026#34;http://xxx/yyyy\u0026#34; Summary: Total:\t10.7173 secs Slowest:\t0.7229 secs Fastest:\t0.3527 secs Average:\t0.4343 secs Requests/sec:\t65.3150 ...... Latency distribution: 10% in 0.3806 secs 25% in 0.3908 secs 50% in 0.4115 secs 75% in 0.4431 secs 90% in 0.4941 secs 95% in 0.6704 secs 99% in 0.7166 secs Details (average, fastest, slowest): DNS+dialup:\t0.0034 secs, 0.3527 secs, 0.7229 secs DNS-lookup:\t0.0017 secs, 0.0000 secs, 0.0188 secs req write:\t0.0001 secs, 0.0000 secs, 0.0012 secs resp wait:\t0.4305 secs, 0.3525 secs, 0.7228 secs resp read:\t0.0002 secs, 0.0000 secs, 0.0038 secs Status code distribution: [200]\t4200 responses 关注下结果中各关键指标，效果还是挺好的，因此认为后端服务没有问题。\n这里为啥要用hey，而不是ab？因为hey是用golang语言编写的，很方便编译出android手机上可运行的二进制版本。\n在手机上压测 首先要编译出一个可在Android手机上运行的hey二进制文件：\n$ brew cask install android-ndk $ export ANDROID_NDK_HOME=/usr/local/share/android-ndk $ git clone https://github.com/rakyll/hey.git $ cd hey $ go mod download $ env GOOS=android GOARCH=arm64 CC=/usr/local/share/android-ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/aarch64-linux-android21-clang CXX=/usr/local/share/android-ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/aarch64-linux-android21-clang++ CGO_ENABLED=1 go build -o hey . 将编译出的二进制文件上传至Android手机：\n$ adb connect xxx.xxx.xxx.xxx:yyy $ adb -s xxx.xxx.xxx.xxx:yyy push hey /data/local/tmp/hey 最后在Android手机上执行压测命令：\n$ adb -s xxx.xxx.xxx.xxx:yyy shell $ cd /data/local/tmp $ chmod +x ./hey $ ./hey -z 60s -c 70 -q 1 -x http://zzz.zzz.zzz.zzz:aaa -m \u0026#39;POST\u0026#39; -d \u0026#39;xxxxx\u0026#39; \\  -T \u0026#34;application/x-www-form-urlencoded; charset=UTF-8\u0026#34; \\  -H \u0026#39;accept: */*\u0026#39; \\  -H \u0026#39;accept-encoding: gzip, deflate\u0026#39; \\  -H \u0026#39;connection: keep-alive\u0026#39; \\  -H \u0026#39;content-length: 111\u0026#39; \\  -H \u0026#39;user-agent: python-requests/2.10.0\u0026#39; \\ \t\u0026#34;http://xxx/yyyy\u0026#34; 这里按我们的场景，压测时还指定了HTTP代理\n果然这样测得QPS差了很多。找维护代理服务器的同学咨询了下，果然是代理服务器存在性能瓶颈。\n参考  https://github.com/rakyll/hey https://golang.org/misc/android/README  ","permalink":"https://jeremyxu2010.github.io/2020/01/%E5%88%86%E6%9E%90app%E8%AE%BF%E9%97%AE%E5%90%8E%E5%8F%B0%E6%9C%8D%E5%8A%A1%E7%9A%84%E7%BD%91%E7%BB%9C%E6%95%85%E9%9A%9C/","tags":["golang","http","android","hey"],"title":"分析APP访问后台服务的网络故障"},{"categories":["python开发"],"contents":"项目中使用了django，这个之前并没有深度使用过，今天花了些时间把官方文档大概浏览了一遍，简单学习了一下，这里简单记录一下以备忘。\n快速上手步骤 安装django 为了能使用django-admin命令，先将django安装到全局：\n$ pip install Django 创建项目 $ django-admin startproject django_demo $ cd django_demo # 创建较干净的python virtualenv环境 $ virtualenv --no-site-packages .venv $ source .venv/bin/activate # 在此python virtualenv环境安装Django、mysqlclient $ pip install Django $ pip install mysqlclient $ python manage.py startapp test_app 创建模型类 编辑项目的settings文件：\n$ vim django_demo/settings.py 修改如下的片断：\nINSTALLED_APPS = [ 'test_app.apps.TestAppConfig', ...... ] DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'NAME': 'test', 'USER': 'root', 'PASSWORD': '123456', 'HOST': '127.0.0.1', 'PORT': '3306', } } TIME_ZONE = 'Asia/Shanghai' 修改模型文件：\n$ vim test_app/models.py test_app/models.py的内容如这个文件。\n根据模型文件修改数据库schema：\n$ python ./manage.py makemigrations $ python ./manage.py migrate 写模型类对应的测试用例 个人不太喜欢使用django的方案写测试用例，还是喜欢写标准的测试用例：\n$ vim test_app/models_test.py test_app/models_test.py的内容如这个文件。\n运行测试用例：\n$ export PYTHONPATH=`pwd`:${PYTHONPATH} $ python test_app/models_test.py 编写视图 编写视图文件：\n$ vim test_app/views.py test_app/views.py的内容如这个文件。\n编写模板文件：\n$ vim test_app/templates/index.html $ vim test_app/templates/detail.html 这两个文件的内容如这些文件。\n配置url：\n$ vim test_app/urls.py $ vim django_demo/urls.py 主要修改了以下几点：\nfrom test_app import views app_name = 'test_app' urlpatterns = [ url(r'^$', views.index, name='index'), url(r'^(?P\u0026lt;question_id\u0026gt;[0-9]+)/$', views.detail, name=\u0026quot;detail\u0026quot;) ] url(r'^', include('test_app.urls')) 运行开发Server $ python ./manage.py runserver 运行生产Server $ pip install gunicorn==19.10.0 $ gunicorn -b 127.0.0.1:8000 -w 5 django_demo.wsgi 亮点 之前接触过其它语言的web开发框架，django给我印象比较深刻的是其自带的ORM框架，可参考Model相关的API。\n\u0026gt;\u0026gt;\u0026gt; from polls.models import Question, Choice # Import the model classes we just wrote. # No questions are in the system yet. \u0026gt;\u0026gt;\u0026gt; Question.objects.all() \u0026lt;QuerySet []\u0026gt; # Create a new Question. # Support for time zones is enabled in the default settings file, so # Django expects a datetime with tzinfo for pub_date. Use timezone.now() # instead of datetime.datetime.now() and it will do the right thing. \u0026gt;\u0026gt;\u0026gt; from django.utils import timezone \u0026gt;\u0026gt;\u0026gt; q = Question(question_text=\u0026#34;What\u0026#39;s new?\u0026#34;, pub_date=timezone.now()) # Save the object into the database. You have to call save() explicitly. \u0026gt;\u0026gt;\u0026gt; q.save() # Now it has an ID. Note that this might say \u0026#34;1L\u0026#34; instead of \u0026#34;1\u0026#34;, depending # on which database you\u0026#39;re using. That\u0026#39;s no biggie; it just means your # database backend prefers to return integers as Python long integer # objects. \u0026gt;\u0026gt;\u0026gt; q.id 1 # Access model field values via Python attributes. \u0026gt;\u0026gt;\u0026gt; q.question_text \u0026#34;What\u0026#39;s new?\u0026#34; \u0026gt;\u0026gt;\u0026gt; q.pub_date datetime.datetime(2012, 2, 26, 13, 0, 0, 775217, tzinfo=\u0026lt;UTC\u0026gt;) # Change values by changing the attributes, then calling save(). \u0026gt;\u0026gt;\u0026gt; q.question_text = \u0026#34;What\u0026#39;s up?\u0026#34; \u0026gt;\u0026gt;\u0026gt; q.save() # objects.all() displays all the questions in the database. \u0026gt;\u0026gt;\u0026gt; Question.objects.all() \u0026lt;QuerySet [\u0026lt;Question: Question object\u0026gt;]\u0026gt; 这个确实极大简化了操作数据库的复杂度，详情的配置文档可参考这里。\n如果想直接操作SQL，方法也很简单，参考这里\n另外在web开发领域，一些常见问题均有推荐的解决方案：\n 处理Form表单 使用middleware 文件上传 视图的装饰器 复用视图 使用会话 页面数据分页 发送邮件 处理文件 日志处理 认证鉴权 web安全防御 序列化反序列化 性能优化  还是挺全面的。\nDONE!\n参考  https://docs.djangoproject.com/en/1.11/intro/ https://docs.djangoproject.com/en/1.11/topics/ https://books.agiliq.com/projects/django-orm-cookbook/en/latest/introduction.html  ","permalink":"https://jeremyxu2010.github.io/2019/12/%E5%AD%A6%E4%B9%A0django/","tags":["python","django","web"],"title":"学习django"},{"categories":["linux"],"contents":"偶然间看到一篇英文文档，觉得还挺有用的，尝试翻译一下以加深记忆。\n翻译自https://medium.com/netflix-techblog/linux-performance-analysis-in-60-000-milliseconds-accc10403c55。\n问题背景 当登录到一台有性能问题的Linux服务器，第一分钟要检查什么？\n在Netflix，我们拥有庞大的EC2 Linux虚拟机云，我们有众多性能分析工具来监视和诊断这些Linux服务器的性能。这些工具包括Atlas（负责整个虚拟机云的监控）和Vector（负责按需对虚拟机实例进行性能分析）。这些工具可以帮助我们解决大多数问题，但有时我们需要登录到虚拟机实例，并运行一些标准的Linux性能工具。\n前60秒：摘要 在本文中，Netflix性能工程团队将使用您应该使用的标准Linux工具在命令行中向您展示一个性能诊断过程的前60秒。在60秒内，您可以通过运行以下十个命令来了解有关系统资源使用和运行进程的信息。最应该关注的是一些很容易理解的错误、饱和度指标和资源利用率等指标。饱和度是衡量资源负载超出其处理能力的指标，它可以通过观察请求队列的长度或等待时间反映出来。\nuptime dmesg | tail vmstat 1 mpstat -P ALL 1 pidstat 1 iostat -xz 1 free -m sar -n DEV 1 sar -n TCP,ETCP 1 top 其中的一些命令需要安装sysstat软件包。这些命令暴露出的指标将帮助您完成一些USE方法：一种查找性能瓶颈的方法。它们涉及检查所有资源（CPU、内存、磁盘等）的利用率，饱和度和错误指标。在诊断过程中还应该注意检查和排除某些资源的问题。因为通过排除某些资源的问题，可以缩小诊断的范围，并指民后续的诊断。\n以下各节通过生产系统中的示例总结了这些命令。有关这些工具更多的信息，请参见其手册页。\n1. uptime $ uptime 23:51:26 up 21:31, 1 user, load average: 30.02, 26.43, 19.02 这是快速查看平均负载的方法，该平均负载指标了要运行的任务（进程）的数量。在Linux系统上，这些数字包括要在CPU上运行的进程以及在不中断IO（通常是磁盘IO）中阻塞的进程。这里给出了资源负载高层次的概览，但是没有其它工具就很难正确理解，值得快速看一眼。\n这三个数字是指数衰减移动平均值，分别代表了1分钟、5分钟、15分钟的平均值。这三个数字使我们对负载如何随时间变化有了一定的了解。例如，如果您去诊断一个有问题的服务器，发现1分钟的值比15分钟的值低很多，那么您可能已经登录得太晚了，错过了问题。\n在上面的例子中，平均负载有所增加，因为1分钟的值30相对15分钟的值19来说大了一些。数字变大意味着很多种可能：有可能是CPU的需求变多了，使用3和4中提到的vmstat或mpstat命令将可以进一步确认问题。\n2. dmesg | tail $ dmesg | tail [1880957.563150] perl invoked oom-killer: gfp_mask=0x280da, order=0, oom_score_adj=0 [...] [1880957.563400] Out of memory: Kill process 18694 (perl) score 246 or sacrifice child [1880957.563408] Killed process 18694 (perl) total-vm:1972392kB, anon-rss:1953348kB, file-rss:0kB [2320864.954447] TCP: Possible SYN flooding on port 7001. Dropping request. Check SNMP counters. 该命令展示最近 10条系统消息。在这些系统消息中查找有可能引起性能问题的报错。上面的例子包括oom-killer和TCP丢弃了一个请求。\n不能忘记这个步骤，dmesg通常对诊断问题很有价值。\n3. vmstat 1 $ vmstat 1 procs ---------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 34 0 0 200889792 73708 591828 0 0 0 5 6 10 96 1 3 0 0 32 0 0 200889920 73708 591860 0 0 0 592 13284 4282 98 1 1 0 0 32 0 0 200890112 73708 591860 0 0 0 0 9501 2154 99 1 0 0 0 32 0 0 200889568 73712 591856 0 0 0 48 11900 2459 99 0 0 0 0 32 0 0 200890208 73712 591860 0 0 0 0 15898 4840 98 1 1 0 0 ^C vmstat是虚拟内存统计(Virtual Memory Stat)的缩写，vmstat(8)是一个通常可用的工具(最初是在之前的BSD时代创建的)，它每行打印一行服务器关键统计的概览。\nvmstat使用参数1运行，意味着每1秒打印打印一次概览。命令输出的第一行展示的是从启动开始的平均值，而不是最近一秒的平均值。因此跳过第一行，除非您想学习并记住哪一列是哪一列。\n要检查的列：\n r：在CPU上运行并等待回合的进程数。由于它不包含IO，因此它比指示CPU饱和的平均负载提供了更多的信息。一个大于CPU核数的r值就是饱和的。 free：空闲的内存（单位的KB）。如果计数很大，说明服务器有足够的内存，free -m命令将对空闲内存的状态有更好的说明。 si、so：交换置入和交换置出。如果这两个值是非空，说明物理内存用完了，现在在使用交换内存了。 us、sy、id、wa、st：这些是CPU时间的分类，其是所有CPU的平均值。它们是用户时间、系统时间(内核)、空闲时间、等待IO和被偷窃时间（被其它宾客系统进行使用，或宾客系统隔离的驱动程序域Xen）  通过将用户时间和系统时间这两个分类相加，即可判断CPU是否繁忙。一定的等待IO时间说明磁盘有可能是性能瓶颈。你可以认为等待IO时间是另一种形式的空闲时间，它提供了它是如何空闲的线索。\nIO处理需要占用CPU系统时间。一个较高的CPU系统时间（超过20%）可能会很有趣，有必要进一步研究：也许内核在很低效地处理IO。\n在上面的示例中，CPU时间基本全在用户时间，这说明应用程序本身在大量占用CPU时间。CPU的平均利用率也远远超过90%。这不一定是问题，可以使用r列来检查饱和度。\n4. mpstat -P ALL 1 $ mpstat -P ALL 1 Linux 3.13.0-49-generic (titanclusters-xxxxx) 07/14/2015 _x86_64_ (32 CPU) 07:38:49 PM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle 07:38:50 PM all 98.47 0.00 0.75 0.00 0.00 0.00 0.00 0.00 0.00 0.78 07:38:50 PM 0 96.04 0.00 2.97 0.00 0.00 0.00 0.00 0.00 0.00 0.99 07:38:50 PM 1 97.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 2.00 07:38:50 PM 2 98.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 07:38:50 PM 3 96.97 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 3.03 [...] 此命令显示每个CPU的CPU时间明细，可用于检查不平衡的情况。单个热CPU说明是单线程应用程序在大量占用CPU时间。\n5. pidstat 1 $ pidstat 1 Linux 3.13.0-49-generic (titanclusters-xxxxx) 07/14/2015 _x86_64_ (32 CPU) 07:41:02 PM UID PID %usr %system %guest %CPU CPU Command 07:41:03 PM 0 9 0.00 0.94 0.00 0.94 1 rcuos/0 07:41:03 PM 0 4214 5.66 5.66 0.00 11.32 15 mesos-slave 07:41:03 PM 0 4354 0.94 0.94 0.00 1.89 8 java 07:41:03 PM 0 6521 1596.23 1.89 0.00 1598.11 27 java 07:41:03 PM 0 6564 1571.70 7.55 0.00 1579.25 28 java 07:41:03 PM 60004 60154 0.94 4.72 0.00 5.66 9 pidstat 07:41:03 PM UID PID %usr %system %guest %CPU CPU Command 07:41:04 PM 0 4214 6.00 2.00 0.00 8.00 15 mesos-slave 07:41:04 PM 0 6521 1590.00 1.00 0.00 1591.00 27 java 07:41:04 PM 0 6564 1573.00 10.00 0.00 1583.00 28 java 07:41:04 PM 108 6718 1.00 0.00 0.00 1.00 0 snmp-pass 07:41:04 PM 60004 60154 1.00 4.00 0.00 5.00 9 pidstat ^C pidstat有点像top的每个进程摘要，但是会滚动打印，而不是清屏再打印。这对于观察一段时间内的模式以及将所看到的内容（复制\u0026amp;粘贴）记录到调查记录中很有用。\n上面的示例显示两个Java进程要为消耗大量CPU负责。%CPU这一列是所有CPU核的总和，1591%说明Java进程差不多消耗了16个核的CPU。\n6. iostat -xz 1 $ iostat -xz 1 Linux 3.13.0-49-generic (titanclusters-xxxxx) 07/14/2015 _x86_64_ (32 CPU) avg-cpu: %user %nice %system %iowait %steal %idle 73.96 0.00 3.73 0.03 0.06 22.21 Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util xvda 0.00 0.23 0.21 0.18 4.52 2.08 34.37 0.00 9.98 13.80 5.42 2.44 0.09 xvdb 0.01 0.00 1.02 8.94 127.97 598.53 145.79 0.00 0.43 1.78 0.28 0.25 0.25 xvdc 0.01 0.00 1.02 8.86 127.79 595.94 146.50 0.00 0.45 1.82 0.30 0.27 0.26 dm-0 0.00 0.00 0.69 2.32 10.47 31.69 28.01 0.01 3.23 0.71 3.98 0.13 0.04 dm-1 0.00 0.00 0.00 0.94 0.01 3.78 8.00 0.33 345.84 0.04 346.81 0.01 0.00 dm-2 0.00 0.00 0.09 0.07 1.35 0.36 22.50 0.00 2.55 0.23 5.62 1.78 0.03 [...] ^C 这是了解块设备（磁盘），应用的工作负载和产生的性能影响的绝佳工具。重点关注下面的指标：\n r/s、w/s、 rkB/s、 wkB/s：这些是设备每秒交付的读取、写入、读取千字节和写入千字节。使用这些来表征块设备的工作负载。性能问题可能是由于向块设备施加了过多的工作负载。 await：IO的平均时间，以毫秒为单位。这是应用程序所感受到的时间，它包括IO排队时间和IO服务时间。大于预期的平均时间可能表示块设备饱和或设备出现问题了。 avgqu-sz：发给设备的平均请求数。值大于1可以表明已达到饱和状态（尽管设备通常可以并行处理请求，尤其是在多个后端磁盘所组成的前端虚拟设备的情况下）。 %util：设备利用率。这是一个表征繁忙度的百分比，它表示设备每秒工作的时间。尽管它的值取决于设备，但值大于60%通常会导致性能不佳（也会通过await的值观察到）。接近100\u0010%的值通常表示饱和。  如果存储设备是有许多后端磁盘组成的前端逻辑磁盘设备，则100%的利用率可能仅意味着100%的时间正在处理某些IO，但是后端磁盘可能远远没有饱和，并且可能还可以处理更多的工作。\n请记住，性能不佳的磁盘IO不一定是应用问题，通常可以使用许多技术以执行异步IO，以便使应用程序不会被阻塞住而产生直接产生IO延迟（例如，预读和缓冲写入技术）\n7. free -m $ free -m total used free shared buffers cached Mem: 245998 24545 221453 83 59 541 -/+ buffers/cache: 23944 222053 Swap: 0 0 0 右边两列：\n buffers：缓冲区高速缓存，用于块设备I / O cached：页面缓存，由文件系统使用  我们只需要检查下它们的大小是否接近零。如果接近零的话，这可能导致较高的磁盘IO（可以使用iostat进行确认）和较差的性能。上面的示例看起来不错，每列都有较大的数据。\n-/+ buffers/cache为已用和空闲内存提供较少让人产生混乱的值。Linux将可用内存用于高速缓存，但是如果应用程序需要，它们可以快速被回收。因此应以某种方式将缓存的内存包括在free列中，这也就是这一行的所做的。甚至还有一个网站专门讨论了这种混乱。\n如果在Linux上使用ZFS，就像我们对某些服务所做的那么，因为ZFS具有自己的文件系统缓存，它们并不会反映在free -m的列中，因此这种场景下这种混乱还将存在。所以会看到似乎系统的可用内存不足，而实际上可根据需要从ZFS缓存中申请到内存。\n8. sar -n DEV 1 $ sar -n DEV 1 Linux 3.13.0-49-generic (titanclusters-xxxxx) 07/14/2015 _x86_64_ (32 CPU) 12:16:48 AM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil 12:16:49 AM eth0 18763.00 5032.00 20686.42 478.30 0.00 0.00 0.00 0.00 12:16:49 AM lo 14.00 14.00 1.36 1.36 0.00 0.00 0.00 0.00 12:16:49 AM docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 12:16:49 AM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil 12:16:50 AM eth0 19763.00 5101.00 21999.10 482.56 0.00 0.00 0.00 0.00 12:16:50 AM lo 20.00 20.00 3.25 3.25 0.00 0.00 0.00 0.00 12:16:50 AM docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ^C 此工具可以检查网络接口的吞吐量：rxkB/s和txkB/s，作为工作负载的度量，还可以检查是否已达到网络接口的限制。在上面的示例中，eth0接收速率达到22MB/s，即176Mbit/s（远低于1Gbit/s的网络接口限制，假设是千兆网卡）。\n此版本还具有%ifutil用来指示设备利用率（全双工双向），这也是我们使用的Brendan的nicstat工具测量出来的。就像nicstat一样，这个指标很难计算正确，而且在本例中好像不起作用（数据是0.00）。\n9. sar -n TCP,ETCP 1 $ sar -n TCP,ETCP 1 Linux 3.13.0-49-generic (titanclusters-xxxxx) 07/14/2015 _x86_64_ (32 CPU) 12:17:19 AM active/s passive/s iseg/s oseg/s 12:17:20 AM 1.00 0.00 10233.00 18846.00 12:17:19 AM atmptf/s estres/s retrans/s isegerr/s orsts/s 12:17:20 AM 0.00 0.00 0.00 0.00 0.00 12:17:20 AM active/s passive/s iseg/s oseg/s 12:17:21 AM 1.00 0.00 8359.00 6039.00 12:17:20 AM atmptf/s estres/s retrans/s isegerr/s orsts/s 12:17:21 AM 0.00 0.00 0.00 0.00 0.00 ^C 这是一些关键的TCP指标的摘要，包括：\n active / s：每秒本地启动的TCP连接数（例如，通过connect（））。 passive/s：每秒远程启动的TCP连接数（例如，通过accept（））。 retrans / s：每秒TCP重传的次数。  主动和被动计数通常作为服务器TCP负载的粗略度量：新接受的连接数（被动）和新出站的连接数（主动）。将主动视为出站，将被动视为入站可能对理解这两个指标有些帮助，但这并不是严格意义上的（例如，考虑从localhost到localhost的连接）。\n重新传输是网络或服务器问题的迹象；它可能是不可靠的网络（例如，公共Internet），也可能是由于服务器过载并丢弃了数据包。上面的示例仅显示每秒一个新的TCP连接。\n10. top $ top top - 00:15:40 up 21:56, 1 user, load average: 31.09, 29.87, 29.92 Tasks: 871 total, 1 running, 868 sleeping, 0 stopped, 2 zombie %Cpu(s): 96.8 us, 0.4 sy, 0.0 ni, 2.7 id, 0.1 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem: 25190241+total, 24921688 used, 22698073+free, 60448 buffers KiB Swap: 0 total, 0 used, 0 free. 554208 cached Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 20248 root 20 0 0.227t 0.012t 18748 S 3090 5.2 29812:58 java 4213 root 20 0 2722544 64640 44232 S 23.5 0.0 233:35.37 mesos-slave 66128 titancl+ 20 0 24344 2332 1172 R 1.0 0.0 0:00.07 top 5235 root 20 0 38.227g 547004 49996 S 0.7 0.2 2:02.74 java 4299 root 20 0 20.015g 2.682g 16836 S 0.3 1.1 33:14.42 java 1 root 20 0 33620 2920 1496 S 0.0 0.0 0:03.82 init 2 root 20 0 0 0 0 S 0.0 0.0 0:00.02 kthreadd 3 root 20 0 0 0 0 S 0.0 0.0 0:05.35 ksoftirqd/0 5 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0H 6 root 20 0 0 0 0 S 0.0 0.0 0:06.94 kworker/u256:0 8 root 20 0 0 0 0 S 0.0 0.0 2:38.05 rcu_sched top命令包括我们之前检查的许多指标。运行它可以很方便地查看是否有任何东西与以前的命令有很大不同，这表明负载是可变的。\ntop命令不太好的地方是，随着时间的推移很难看到指标变化的模式，这在提供滚动输出的vmstat和pidstat之类的工具中可能更清楚一点。如果您没有足够快地暂停输出（Ctrl-S暂停，Ctrl-Q继续），在屏幕输出被top命令清除后，间歇性问题的证据也可能被丢失了。\n后续分析 您可以使用更多的命令和方法来进行更深入的研究。可以看一下Brendan的Linux Performance Tools tutorial，该教程讲述了40多个命令，涉及可观察性、基准测试、性能调优、静态性能调优、性能分析和跟踪。\n解决一定网络规模后系统的可靠性和性能问题是我们特别热衷的事情之一。\n总结 除了这篇文章外，还看到一篇使用eBPF技术完成对内核操作的tracing的文档，后面准备对这块也详细学习一下。\n参考  https://medium.com/netflix-techblog/netflix-at-velocity-2015-linux-performance-tools-51964ddb81cf https://medium.com/netflix-techblog/linux-performance-analysis-in-60-000-milliseconds-accc10403c55  ","permalink":"https://jeremyxu2010.github.io/2019/12/60%E7%A7%92%E5%AE%8C%E6%88%90linux%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E8%AF%91/","tags":["linux","tracing","profiling","observability"],"title":"60秒完成Linux系统的性能分析(译)"},{"categories":["工具"],"contents":"出差大半个月，一直在客户现场处理各类疑难杂症，当中遇到一个小问题，有点意思，花了些时间诊断该问题，这里记录一下。\n问题引出 突然有需求要临时搭建一个http file server，以方便其它人从这个web站点下载文件。\n最简单的做法，直接用python自带的SimpleHTTPServer：\n$ cd ${some_dir} $ nohup python -m SimpleHTTPServer 8888 \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp; 本以为这样就OK了，结果发现只要ssh会话一中断，就没法从这个http file server下载文件了。\n探究原因 是不是python自带的SimpleHTTPServer本身有些问题？先换个工具试试\n$ curl -s -o - -L https://github.com/codeskyblue/gohttpserver/releases/download/1.0.5/gohttpserver_1.0.5_linux_amd64.zip | bsdtar -xvf- $ nohup ./gohttpserver -r ./ --port 8888 --upload \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp; 好了，即使ssh会话中断，依旧可以从这个http file server下载文件了。\n难道python自带的SimpleHTTPServer真有这么大的bug？个人觉得不太可能。\n参考DebuggingWithGdb，使用gdb调试下hung住的python进程，发现进程是因为读取标准输入卡住了。\n意识到应该是ssh会话中断后，进程依赖的标准输入文件句柄不存在，导致卡住了。换个命令试一下：\n$ nohup python -m SimpleHTTPServer 8888 \u0026lt;/dev/null \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp; 这下终于没问题了。\n进一步思考 怎么诊断进程hang在哪里了？ 参考DebuggingWithGdb，可以用gdb attach进程，使用bt、py-bt命令查看进程当前的运行栈：\n$ gdb ${process_binary_file} ${process_pid} (gdb) bt (gdb) py-bt (gdb) info threads (gdb) py-list (gdb) thread apply all py-list 怎么查看进程运行时依赖的标准输入、标准输出、标准错误文件？ 很简单，直接查看proc文件系统就可以了：\n$ ll /proc/${process_pid}/fd total 0 lrwx------ 1 root root 64 Dec 7 23:14 0 -\u0026gt; /dev/pts/0 (deleted) l-wx------ 1 root root 64 Dec 7 23:14 1 -\u0026gt; /dev/null l-wx------ 1 root root 64 Dec 7 23:14 2 -\u0026gt; /dev/null ... 这里0、1、2指向的文件就是标准输入、标准输出、标准错误文件。\n怎么在不重启进程的前提下改变其标准输入、标准输出、标准错误？ 很幸运，找到一个工具dupx:\n$ dupx -q ${process_pid} \u0026lt;/dev/null \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 这个工具的原理也很简单，其实就是使用gdb attach到目标进程，然后执行open、dup、dup2、close等系统调用。\nhttps://github.com/oudream/dupx/blob/master/dupx#L145\ngdb_cmds () { local _name=$1 local _mode=$2 local _desc=$3 local _msgs=$4 local _len [ -w \u0026#34;/proc/$PID/fd/$_desc\u0026#34; ] || _msgs=\u0026#34;\u0026#34; if [ -d \u0026#34;/proc/$PID/fd\u0026#34; ] \u0026amp;\u0026amp; ! [ -e \u0026#34;/proc/$PID/fd/$_desc\u0026#34; ]; then warn \u0026#34;Attempting to remap non-existent fd $nof PID ($PID)\u0026#34; fi [ -z \u0026#34;$_name\u0026#34; ] \u0026amp;\u0026amp; return echo \u0026#34;set \\$fd=open(\\\u0026#34;$_name\\\u0026#34;, $_mode)\u0026#34; echo \u0026#34;set \\$xd=dup($_desc)\u0026#34; echo \u0026#34;call dup2(\\$fd, $_desc)\u0026#34; echo \u0026#34;call close(\\$fd)\u0026#34; if [ $((_mode \u0026amp; 3)) ] \u0026amp;\u0026amp; [ -n \u0026#34;$_msgs\u0026#34; ]; then _len=$(echo -en \u0026#34;$_msgs\u0026#34; | wc -c) echo \u0026#34;call write(\\$xd, \\\u0026#34;$_msgs\\\u0026#34;, $_len)\u0026#34; fi echo \u0026#34;call close(\\$xd)\u0026#34; } 参考  https://www.ibm.com/developerworks/cn/linux/l-cn-nohup/index.html https://github.com/codeskyblue/gohttpserver https://wiki.python.org/moin/DebuggingWithGdb https://www.isi.edu/~yuri/dupx/  ","permalink":"https://jeremyxu2010.github.io/2019/12/%E8%A7%A3%E5%86%B3http-file-server-hang%E4%BD%8F%E9%97%AE%E9%A2%98/","tags":["linux","gdb","python"],"title":"解决HTTP File Server Hang住问题"},{"categories":["容器编排"],"contents":"这两天遇到一个很有意思的应用场景：有一个业务应用部署在kubernetes容器中，如果将该应用以Kubernetes Service NodePort暴露出来，这时测试人员测得应用的页面响应性能较高，可以达到2w多的QPS；而将这个Kubernetes Service再用Ingress暴露出来，测试人员测得的QPS立马就较得只有1w多的QPS了。这个性能开销可以说相当巨大了，急需进行性能调优。花了一段时间分析这个问题，终于找到原因了，这里记录一下。\n问题复现 问题是在生产环境出现了，不便于直接在生产环境调参，这里搭建一个独立的测试环境以复现问题。\n首先在一台16C32G的服务器上搭建了一个单节点的kubernetes集群，并部署了跟生产环境一样的nginx-ingress-controller。然后进行基本的调优，以保证尽量与生产环境一致，涉及的调优步骤如下：\n  ClusterIP使用性能更优异的ipvs实现\n$ yum install -y ipset $ cat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; \u0026gt; /etc/sysconfig/modules/ipvs.modules #!/bin/bash ipvs_modules=(ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4) for kernel_module in ${ipvs_modules[*]}; do /sbin/modinfo -F filename ${kernel_module} \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 if [ $? -eq 0 ]; then /sbin/modprobe ${kernel_module} fi done EOF $ chmod +x /etc/sysconfig/modules/ipvs.modules $ /etc/sysconfig/modules/ipvs.modules $ kubectl -n kube-system edit cm kube-proxy ...... mode: \u0026#34;ipvs\u0026#34; ...... $ kubectl -n kube-system get pod -l k8s-app=kube-proxy | grep -v \u0026#39;NAME\u0026#39; | awk \u0026#39;{print $1}\u0026#39; | xargs kubectl -n kube-system delete pod $ iptables -t filter -F; iptables -t filter -X; iptables -t nat -F; iptables -t nat -X;   flannel使用host-gw模式\n$ kubectl -n kube-system edit cm kube-flannel-cfg ...... \u0026#34;Backend\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;host-gw\u0026#34; } ...... $ kubectl -n kube-system get pod -l k8s-app=flannel | grep -v \u0026#39;NAME\u0026#39; | awk \u0026#39;{print $1}\u0026#39; | xargs kubectl -n kube-system delete pod   集群node节点及客户端配置内核参数\n$ cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/sysctl.conf net.core.somaxconn = 655350 net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_timestamps = 1 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_fin_timeout = 30 net.ipv4.tcp_max_tw_buckets = 5000 net.nf_conntrack_max = 2097152 net.netfilter.nf_conntrack_max = 2097152 net.netfilter.nf_conntrack_tcp_timeout_close_wait = 15 net.netfilter.nf_conntrack_tcp_timeout_fin_wait = 30 net.netfilter.nf_conntrack_tcp_timeout_time_wait = 30 net.netfilter.nf_conntrack_tcp_timeout_established = 1200 EOF $ sysctl -p --system   集群node节点及客户端配置最大打大文件数\n$ ulimit -n 655350 $ cat /etc/sysctl.conf ... fs.file-max=655350 ... $ sysctl -p --system $ cat /etc/security/limits.conf ... *\thard\tnofile\t655350 *\tsoft\tnofile\t655350 *\thard\tnproc\t6553 *\tsoft\tnproc\t655350 root hard nofile 655350 root soft nofile 655350 root hard nproc 655350 root soft nproc 655350 ... $ echo \u0026#39;session required pam_limits.so\u0026#39; \u0026gt;\u0026gt; /etc/pam.d/common-session   然后在集群中部署了一个测试应用，以模拟生产环境上的业务应用：\n$ cat web.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: labels: app: web name: web namespace: default spec: selector: matchLabels: app: web template: metadata: labels: app: web spec: containers: - image: nginx:1.17-alpine imagePullPolicy: IfNotPresent name: nginx resources: limits: cpu: 60m --- apiVersion: v1 kind: Service metadata: labels: app: web name: web namespace: default spec: externalTrafficPolicy: Cluster ports: - nodePort: 32380 port: 80 protocol: TCP targetPort: 80 selector: app: web sessionAffinity: None type: NodePort --- apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: ingress.kubernetes.io/ssl-redirect: \u0026#34;false\u0026#34; kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/proxy-connect-timeout: \u0026#34;300\u0026#34; nginx.ingress.kubernetes.io/proxy-read-timeout: \u0026#34;300\u0026#34; nginx.ingress.kubernetes.io/proxy-send-timeout: \u0026#34;300\u0026#34; nginx.ingress.kubernetes.io/ssl-redirect: \u0026#34;false\u0026#34; nginx.ingress.kubernetes.io/connection-proxy-header: \u0026#34;keep-alive\u0026#34; labels: app: web name: web namespace: default spec: rules: - host: web.test.com http: paths: - backend: serviceName: web servicePort: 80 path: / $ kubectl apply -f web.yaml 注意：这里故意将pod的cpu限制在60m，这样一个pod副本可同时处理的页面请求数有限，以模拟真正的业务应用\n接下来简单测试一下：\n# 使用httpd-utils中的ab命令直接压测Kubernetes Service NodePort，并发请求数为10000，总发出1000000个请求，此时测得QPS为2.4w $ ab -r -n 1000000 -c 10000 http://${k8s_node_ip}:32380/ 2\u0026gt;\u0026amp;1 | grep \u0026#39;Requests per second\u0026#39; Requests per second: 24234.03 [#/sec] (mean) # 再在客户端的/etc/hosts中将域名web.test.com指向${k8s_node_ip}，通过Ingress域名压测业务应用，测得QPS为1.1w $ ab -r -n 1000000 -c 10000 http://web.test.com/ 2\u0026gt;\u0026amp;1 | grep \u0026#39;Requests per second\u0026#39; Requests per second: 11736.21 [#/sec] (mean) 可以看到访问Ingress域名后，确实QPS下降很明显，跟生产环境的现象一致。\n分析原因 我们知道，nginx-ingress-controller的原理实际上是扫描Kubernetes集群中的Ingress资源，根据Ingress资源的定义自动为每个域名生成一段nginx虚拟主机及反向代理的配置，最后由nginx读取这些配置，完成实际的HTTP请求流量的处理，整个HTTP请求链路如下：\n client -\u0026gt; nginx -\u0026gt; upstream(kubernetes service) -\u0026gt; pods nginx的实现中必然要对接收的HTTP请求进行7层协议解析，并根据请求信息将HTTP请求转发给upstream。\n而client直接请求kubernetes service有不错的QPS值，说明nginx这里存在问题。\n解决问题 虽说nginx进行7层协议解析、HTTP请求转发会生产一些性能开销，但nginx-ingress-controller作为一个kubernetes推荐且广泛使用的ingress-controller，参考业界的测试数据，nginx可是可以实现百万并发HTTP反向代理的存在，照理说才一两万的QPS，其不应该有这么大的性能问题。所以首先怀疑nginx-ingress-controller的配置不够优化，需要进行一些调优。\n我们可以从nginx-ingress-controller pod中取得nginx的配置文件，再参考nginx的常用优化配置，可以发现有些优化配置没有应用上。\nkubectl -n kube-system exec -ti nginx-ingress-controller-xxx-xxxx cat /etc/nginx/nginx.conf \u0026gt; /tmp/nginx.conf 对比后，发现server context中keepalive_requests、keepalive_timeout，upstream context中的keepalive、keepalive_requests、keepalive_timeout这些配置项还可以优化下，于是参考nginx-ingress-controller的配置方法，这里配置了下：\n$ kubectl -n kube-system edit configmap nginx-configuration ... apiVersion: v1 data: keep-alive: \u0026#34;60\u0026#34; keep-alive-requests: \u0026#34;100\u0026#34; upstream-keepalive-connections: \u0026#34;10000\u0026#34; upstream-keepalive-requests: \u0026#34;100\u0026#34; upstream-keepalive-timeout: \u0026#34;60\u0026#34; kind: ConfigMap ... 再次压测：\n$ ab -r -n 1000000 -c 10000 http://web.test.com/ 2\u0026gt;\u0026amp;1 | grep \u0026#39;Requests per second\u0026#39; Requests per second: 22733.73 [#/sec] (mean) 此时发现性能好多了。\n分析原理 什么是Keep-Alive模式？ HTTP协议采用请求-应答模式，有普通的非KeepAlive模式，也有KeepAlive模式。\n非KeepAlive模式时，每个请求/应答客户和服务器都要新建一个连接，完成 之后立即断开连接（HTTP协议为无连接的协议）；当使用Keep-Alive模式（又称持久连接、连接重用）时，Keep-Alive功能使客户端到服 务器端的连接持续有效，当出现对服务器的后继请求时，Keep-Alive功能避免了建立或者重新建立连接。\n启用Keep-Alive的优点 启用Keep-Alive模式肯定更高效，性能更高。因为避免了建立/释放连接的开销。下面是RFC 2616 上的总结：\n  TCP连接更少，这样就会节约TCP连接在建立、释放过程中，主机和路由器上的CPU和内存开销。\n  网络拥塞也减少了，拿到响应的延时也减少了\n  错误处理更优雅：不会粗暴地直接关闭连接，而是report，retry\n  性能大提升的原因 压测命令ab并没有添加-k参数，因此client-\u0026gt;nginx的HTTP处理并没有启用Keep-Alive。\n但由于nginx-ingress-controller配置了upstream-keepalive-connections、upstream-keepalive-requests、upstream-keepalive-timeout参数，这样nginx-\u0026gt;upstream的HTTP处理是启用了Keep-Alive的，这样到Kuberentes Service的TCP连接可以高效地复用，避免了重建连接的开销。\nDONE.\n参考  https://www.jianshu.com/p/024b33d1a1a1 https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/ https://zhuanlan.zhihu.com/p/34052073 http://nginx.org/en/docs/http/ngx_http_core_module.html#keepalive_requests http://nginx.org/en/docs/http/ngx_http_upstream_module.html#keepalive https://kiswo.com/article/1018  ","permalink":"https://jeremyxu2010.github.io/2019/11/%E4%BC%98%E5%8C%96nginx-ingress-controller%E5%B9%B6%E5%8F%91%E6%80%A7%E8%83%BD/","tags":["nginx","kubernetes","http","tcp","conntrack"],"title":"优化nginx-ingress-controller并发性能"},{"categories":["容器编排"],"contents":"最近为项目奔波，都没有多少时间写博文了。。。不过这大半个月在客户现场处理了大量kubernetes集群部署运营的相关工作，这里总结一下。\nkubernetes大规模集群优化 内核参数调化 增大部分内核选项，在/etc/sysctl.conf文件中添加下面片断：\nfs.file-max=1000000 # max-file 表示系统级别的能够打开的文件句柄的数量， 一般如果遇到文件句柄达到上限时，会碰到 # \u0026quot;Too many open files\u0026quot;或者Socket/File: Can’t open so many files等错误。 # 配置arp cache 大小 net.ipv4.neigh.default.gc_thresh1=1024 # 存在于ARP高速缓存中的最少层数，如果少于这个数，垃圾收集器将不会运行。缺省值是128。 net.ipv4.neigh.default.gc_thresh2=4096 # 保存在 ARP 高速缓存中的最多的记录软限制。垃圾收集器在开始收集前，允许记录数超过这个数字 5 秒。缺省值是 512。 net.ipv4.neigh.default.gc_thresh3=8192 # 保存在 ARP 高速缓存中的最多记录的硬限制，一旦高速缓存中的数目高于此，垃圾收集器将马上运行。缺省值是1024。 # 以上三个参数，当内核维护的arp表过于庞大时候，可以考虑优化 net.netfilter.nf_conntrack_max=10485760 # 允许的最大跟踪连接条目，是在内核内存中netfilter可以同时处理的“任务”（连接跟踪条目） net.netfilter.nf_conntrack_tcp_timeout_established=300 net.netfilter.nf_conntrack_buckets=655360 # 哈希表大小（只读）（64位系统、8G内存默认 65536，16G翻倍，如此类推） net.core.netdev_max_backlog=10000 # 每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目。 fs.inotify.max_user_instances=524288 # 默认值: 128 指定了每一个real user ID可创建的inotify instatnces的数量上限 fs.inotify.max_user_watches=524288 # 默认值: 8192 指定了每个inotify instance相关联的watches的上限 etcd性能优化  Etcd对磁盘写入延迟非常敏感，因此对于负载较重的集群，etcd一定要使用local SSD或者高性能云盘。可以使用fio测量磁盘实际顺序 IOPS。  fio -filename=/dev/sda1 -direct=1 -iodepth 1 -thread -rw=write -ioengine=psync -bs=4k -size=60G -numjobs=64 -runtime=10 -group_reporting -name=file 由于etcd必须将数据持久保存到磁盘日志文件中，因此来自其他进程的磁盘活动可能会导致增加写入时间，结果导致etcd请求超时和临时leader丢失。因此可以给etcd进程更高的磁盘优先级，使etcd服务可以稳定地与这些进程一起运行。  ionice -c2 -n0 -p $(pgrep etcd) 默认etcd空间配额大小为 2G，超过 2G 将不再写入数据。通过给etcd配置 --quota-backend-bytes 参数增大空间配额，最大支持 8G。  --quota-backend-bytes 8589934592 如果etcd leader处理大量并发客户端请求，可能由于网络拥塞而延迟处理follower对等请求。在follower 节点上可能会产生如下的发送缓冲区错误的消息：  dropped MsgProp to 247ae21ff9436b2d since streamMsg's sending buffer is full dropped MsgAppResp to 247ae21ff9436b2d since streamMsg's sending buffer is full 可以通过提高etcd对于对等网络流量优先级来解决这些错误。在 Linux 上，可以使用 tc 对对等流量进行优先级排序：\ntc qdisc add dev eth0 root handle 1: prio bands 3 tc filter add dev eth0 parent 1: protocol ip prio 1 u32 match ip sport 2380 0xffff flowid 1:1 tc filter add dev eth0 parent 1: protocol ip prio 1 u32 match ip dport 2380 0xffff flowid 1:1 tc filter add dev eth0 parent 1: protocol ip prio 2 u32 match ip sport 2379 0xffff flowid 1:1 tc filter add dev eth0 parent 1: protocol ip prio 2 u32 match ip dport 2379 0xffff flowid 1:1 为了在大规模集群下提高性能，可以将events存储在单独的 ETCD 实例中，可以配置kube-apiserver参数：  --etcd-servers=\u0026quot;http://etcd1:2379,http://etcd2:2379,http://etcd3:2379\u0026quot; \\ --etcd-servers-overrides=\u0026quot;/events#http://etcd4:2379,http://etcd5:2379,http://etcd6:2379\u0026quot; docker优化  配置docker daemon并行拉取镜像，以提高镜像拉取效率，在/etc/docker/daemon.json中添加以下配置：  \u0026quot;max-concurrent-downloads\u0026quot;: 10 可以使用local SSD或者高性能云盘作为docker容器的持久数据目录，在/etc/docker/daemon.json中添加以下配置：  \u0026quot;data-root\u0026quot;: \u0026quot;/ssd_mount_dir\u0026quot; 启动pod时都会拉取pause镜像，为了减小拉取pause镜像网络带宽，可以每个node预加载pause镜像，在每个node节点上执行以下命令：  docker load -i /tmp/preloaded_pause_image.tar kubelet优化   设置 --serialize-image-pulls=false， 该选项配置串行拉取镜像，默认值时true，配置为false可以增加并发度。但是如果docker daemon 版本小于 1.9，且使用 aufs 存储则不能改动该选项。\n  设置--image-pull-progress-deadline=30， 配置镜像拉取超时。默认值时1分，对于大镜像拉取需要适量增大超时时间。\n  kubelet 单节点允许运行的最大 Pod 数：--max-pods=110（默认是 110，可以根据实际需要设置）\n  kube-apiserver优化   设置 --apiserver-count 和 --endpoint-reconciler-type，可使得多个 kube-apiserver 实例加入到 Kubernetes Service 的 endpoints 中，从而实现高可用。\n  设置 --max-requests-inflight 和 --max-mutating-requests-inflight，默认是 200 和 400。 节点数量在 1000 - 3000 之间时，推荐：\n  --max-requests-inflight=1500 --max-mutating-requests-inflight=500 节点数量大于 3000 时，推荐：\n--max-requests-inflight=3000 --max-mutating-requests-inflight=1000 使用--target-ram-mb配置kube-apiserver的内存，按以下公式得到一个合理的值：  --target-ram-mb=node_nums * 60 kube-controller-manager优化  kube-controller-manager可以通过 leader election 实现高可用，添加以下命令行参数：  --leader-elect=true --leader-elect-lease-duration=15s --leader-elect-renew-deadline=10s --leader-elect-resource-lock=endpoints --leader-elect-retry-period=2s 限制与kube-apiserver通信的qps，添加以下命令行参数：  --kube-api-qps=100 --kube-api-burst=150 kube-scheduler优化  kube-scheduler可以通过 leader election 实现高可用，添加以下命令行参数：  --leader-elect=true --leader-elect-lease-duration=15s --leader-elect-renew-deadline=10s --leader-elect-resource-lock=endpoints --leader-elect-retry-period=2s 限制与kube-apiserver通信的qps，添加以下命令行参数：  --kube-api-qps=100 --kube-api-burst=150 pod优化 在运行Pod的时候也需要注意遵循一些最佳实践。\n 为容器设置资源请求和限制，尤其是一些基础插件服务  spec.containers[].resources.limits.cpu spec.containers[].resources.limits.memory spec.containers[].resources.requests.cpu spec.containers[].resources.requests.memory spec.containers[].resources.limits.ephemeral-storage spec.containers[].resources.requests.ephemeral-storage 在k8s中，会根据pod的limit 和 requests的配置将pod划分为不同的qos类别：\n* Guaranteed * Burstable * BestEffort 当机器可用资源不够时，kubelet会根据qos级别划分迁移驱逐pod。被驱逐的优先级：BestEffort \u0026gt; Burstable \u0026gt; Guaranteed。\n 对关键应用使用 nodeAffinity、podAffinity 和 podAntiAffinity 等保护，使其调度分散到不同的node上。比如kube-dns 配置：\n  尽量使用控制器来管理容器（如 Deployment、StatefulSet、DaemonSet、Job 等）\n  kubernetes集群数据备份与还原 etcd数据 备份 备份数据前先找到etcd集群当前的leader：\nETCDCTL_API=3 etcdctl --endpoints=127.0.0.1:2379 --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt endpoint --cluster status | grep -v \u0026#39;false\u0026#39; | awk -F \u0026#39;[/ :]\u0026#39; \u0026#39;{print $4}\u0026#39; 然后登录到leader节点，备份snapshot db文件：\nrsync -avp /var/lib/etcd/member/snap/db /tmp/etcd_db.bak 还原 将备份的snaphost db文件上传到各个etcd节点，使用以下命令还原数据：\nETCDCTL_API=3 etcdctl snapshot restore \\  /tmp/etcd_db.bak \\  --endpoints=192.168.0.11:2379 \\  --name=192.168.0.11 \\  --cert=/etc/kubernetes/pki/etcd/server.crt \\  --key=/etc/kubernetes/pki/etcd/server.key \\  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\  --initial-advertise-peer-urls=https://192.168.0.11:2380 \\  --initial-cluster-token=etcd-cluster-0 \\  --initial-cluster=192.168.0.11=https://192.168.0.11:2380,192.168.0.12=https://192.168.0.12:2380,192.168.0.13=https://192.168.0.13:2380 \\  --data-dir=/var/lib/etcd/ \\  --skip-hash-check=true harbor 如果使用harbor作为镜像仓库与chart仓库，可使用脚本将harbor中所有的镜像和chart导入导出。\n备份 #!/bin/bash  harborUsername=\u0026#39;admin\u0026#39; harborPassword=\u0026#39;Harbor12345\u0026#39; harborRegistry=\u0026#39;registry.test.com\u0026#39; harborBasicAuthToken=$(echo -n \u0026#34;${harborUsername}:${harborPassword}\u0026#34; | base64) docker login --username ${harborUsername} --password ${harborPassword} ${harborRegistry} rm -f dist/images.list rm -f dist/charts.list # list projects projs=`curl -s -k -H \u0026#34;Authorization: Basic ${harborBasicAuthToken}\u0026#34; \u0026#34;https://${harborRegistry}\u0026#34;\u0026#39;/api/projects?page=1\u0026amp;page_size=1000\u0026#39; | jq -r \u0026#39;.[] | \u0026#34;\\(.project_id)=\\(.name)\u0026#34;\u0026#39;` for proj in ${projs[*]}; do projId=`echo $proj|cut -d \u0026#39;=\u0026#39; -f 1` projName=`echo $proj|cut -d \u0026#39;=\u0026#39; -f 2` # list repos in one project repos=`curl -s -k -H \u0026#34;Authorization: Basic ${harborBasicAuthToken}\u0026#34; \u0026#34;https://${harborRegistry}\u0026#34;\u0026#39;/api/repositories?page=1\u0026amp;page_size=1000\u0026amp;project_id=\u0026#39;\u0026#34;${projId}\u0026#34; | jq -r \u0026#39;.[] | \u0026#34;\\(.id)=\\(.name)\u0026#34;\u0026#39;` for repo in ${repos[*]}; do repoId=`echo $repo|cut -d \u0026#39;=\u0026#39; -f 1` repoName=`echo $repo|cut -d \u0026#39;=\u0026#39; -f 2` # list tags in one repo tags=`curl -s -k -H \u0026#34;Authorization: Basic ${harborBasicAuthToken}\u0026#34; \u0026#34;https://${harborRegistry}\u0026#34;\u0026#39;/api/repositories/\u0026#39;\u0026#34;${repoName}\u0026#34;\u0026#39;/tags?detail=1\u0026#39; | jq -r \u0026#39;.[].name\u0026#39;` for tag in ${tags[*]}; do #echo ${tag}; # pull image docker pull ${harborRegistry}/${repoName}:${tag} # tag image docker tag ${harborRegistry}/${repoName}:${tag} ${repoName}:${tag} # save image mkdir -p $(dirname dist/${repoName}) docker save -o dist/${repoName}:${tag}.tar ${repoName}:${tag} # record image to list file echo \u0026#34;${repoName}:${tag}\u0026#34; \u0026gt;\u0026gt; dist/images.list done done # list charts in one project charts=`curl -s -k -H \u0026#34;Authorization: Basic ${harborBasicAuthToken}\u0026#34; \u0026#34;https://${harborRegistry}\u0026#34;\u0026#39;/api/chartrepo/\u0026#39;\u0026#34;${projName}\u0026#34;\u0026#39;/charts\u0026#39; | jq -r \u0026#39;.[].name\u0026#39;` for chart in ${charts[*]}; do #echo ${chart} # list download urls in one chart durls=`curl -s -k -H \u0026#34;Authorization: Basic ${harborBasicAuthToken}\u0026#34; \u0026#34;https://${harborRegistry}\u0026#34;\u0026#39;/api/chartrepo/\u0026#39;\u0026#34;${projName}\u0026#34;\u0026#39;/charts/\u0026#39;\u0026#34;${chart}\u0026#34; | jq -r \u0026#39;.[].urls[0]\u0026#39;` #echo ${durl[*]} for durl in ${durls[*]}; do #echo ${durl}; # download chart mkdir -p $(dirname dist/${projName}/${durl}) curl -s -k -H \u0026#34;Authorization: Basic ${harborBasicAuthToken}\u0026#34; -o dist/${projName}/${durl} \u0026#34;https://${harborRegistry}/chartrepo/${projName}/${durl}\u0026#34; # record chart to list file echo \u0026#34;${projName}/${durl}\u0026#34; \u0026gt;\u0026gt; dist/charts.list done done done 还原 #!/bin/bash  harborUsername=\u0026#39;admin\u0026#39; harborPassword=\u0026#39;Harbor12345\u0026#39; harborRegistry=\u0026#39;registry.test.com\u0026#39; harborBasicAuthToken=$(echo -n \u0026#34;${harborUsername}:${harborPassword}\u0026#34; | base64) docker login --username ${harborUsername} --password ${harborPassword} ${harborRegistry} while IFS=\u0026#34;\u0026#34; read -r image || [ -n \u0026#34;$image\u0026#34; ] do projName=${image%%/*} # echo ${projName} # create harbor project curl -k -X POST -H \u0026#34;Authorization: Basic ${harborBasicAuthToken}\u0026#34; \u0026#34;https://${harborRegistry}/api/projects\u0026#34; -H \u0026#34;accept: application/json\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;project_name\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$projName\u0026#34;\u0026#39;\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;public\u0026#34;: \u0026#34;true\u0026#34; }}\u0026#39; # load image docker load -i dist/${image}.tar # tag image docker tag ${image} ${harborRegistry}/${image} # push image docker push ${harborRegistry}/${image} done \u0026lt; dist/images.list while IFS=\u0026#34;\u0026#34; read -r chart || [ -n \u0026#34;$chart\u0026#34; ] do projName=${chart%%/*} # echo ${projName} # create harbor project curl -k -X POST -H \u0026#34;Authorization: Basic ${harborBasicAuthToken}\u0026#34; \u0026#34;https://${harborRegistry}/api/projects\u0026#34; -H \u0026#34;accept: application/json\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;project_name\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$projName\u0026#34;\u0026#39;\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;public\u0026#34;: \u0026#34;true\u0026#34; }}\u0026#39; # upload chart curl -s -k -H \u0026#34;Authorization: Basic ${harborBasicAuthToken}\u0026#34; -X POST \u0026#34;https://${harborRegistry}/api/chartrepo/${projName}/charts\u0026#34; -H \u0026#34;accept: application/json\u0026#34; -H \u0026#34;Content-Type: multipart/form-data\u0026#34; -F \u0026#34;chart=@dist/${chart};type=application/gzip\u0026#34; done \u0026lt; dist/charts.list pvc对应的存储卷 集群中其它应用数据一般是保存在pvc对应的存储卷中的。\n备份 首先根据pvc找到对应的pv：\nkubectl -n test get pvc demo-pvc -o jsonpath=\u0026#39;{.spec.volumeName}\u0026#39; 找到pv的挂载目录：\nmount | grep pvc-xxxxxxxxxxxxxxxxxxx 使用rsync命令备份数据：\nrsync -avp --delete /var/lib/kubelet/pods/xxxxxx/volumes/xxxxxxx/ /tmp/pvc-data-bak/test/demo-pvc/ 还原 首先根据pvc找到对应的pv：\nkubectl -n test get pvc demo-pvc -o jsonpath=\u0026#39;{.spec.volumeName}\u0026#39; 找到pv的挂载目录：\nmount | grep pvc-xxxxxxxxxxxxxxxxxxx 使用rsync命令备份数据：\nrsync -avp --delete /tmp/pvc-data-bak/test/demo-pvc/ /var/lib/kubelet/pods/xxxxxx/volumes/xxxxxxx/ 备份数据管理 所有备份出的数据可以存放在一个目录下，并使用restic工具保存到多个后端存储系统上，如：\n# 初始化备份仓库 restic --repo sftp:user@host:/srv/restic-repo init # 将目录备份到备份仓库 restic --repo sftp:user@host:/srv/restic-repo backup /data/k8s-all-data DONE.\n参考  https://www.flftuu.com/2019/03/12/%E5%A4%A7%E8%A7%84%E6%A8%A1%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E4%BC%98%E5%8C%96/ https://testerhome.com/topics/7509 https://etcd.io/docs/v3.4.0/tuning/ https://docs.docker.com/engine/reference/commandline/dockerd/ https://kubernetes.io/docs/setup/best-practices/cluster-large/ https://www.jianshu.com/p/904a3f2b6579 https://github.com/restic/restic https://restic.readthedocs.io/en/latest/030_preparing_a_new_repo.html#sftp  ","permalink":"https://jeremyxu2010.github.io/2019/11/kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E8%BF%90%E8%90%A5%E5%AE%9E%E8%B7%B5%E6%80%BB%E7%BB%93/","tags":["kubernetes","large-cluster","etcd","harbor","restic"],"title":"kubernetes集群部署运营实践总结"},{"categories":["容器编排"],"contents":"今天测试环境遇到一个问题，一个Java的容器由于OOM频繁被Killed掉。这个问题还经常出现的，这里记录下解决过程。\n为啥会频繁OOM？ 首先排除Java程序的问题，因为基本上Java程序刚运行起来没一会儿，容器就由于OOM被Killed掉了，料想程序还不会写得这么烂。\n经诊断，频繁OOM的容器是设置了memory quota的，因此这里做一个实验：\n$ docker run -m 100MB openjdk:8u121-alpine java -XshowSettings:vm -version VM settings: Max. Heap Size (Estimated): 239.75M Ergonomics Machine Class: client Using VM: OpenJDK 64-Bit Server VM openjdk version \u0026#34;1.8.0_121\u0026#34; OpenJDK Runtime Environment (IcedTea 3.3.0) (Alpine 8.121.13-r0) OpenJDK 64-Bit Server VM (build 25.121-b13, mixed mode) 这里发现给容器设置了100MB的memory quota，但JVM运行时实际最大的Heap Size却大于这个值。为啥会这样呢？\n查阅资料，发现JVM默认分配本机内存的大约25%作为Max Heap Size。\n Java Heap Sizing Basics Per default the JVM automatically configures heap size according to the spec of the machine it is running on. On my brand new MacBook Pro 2018 this yields the following heap size:\n $ java -XX:+PrintFlagsFinal -version | grep -Ei \u0026#34;maxheapsize|maxram\u0026#34; uintx DefaultMaxRAMFraction = 4 {product} uintx MaxHeapSize := 8589934592 {product} uint64_t MaxRAM = 137438953472 {pd product} uintx MaxRAMFraction = 4 {product}  As you can see, the JVM defaults to 8.0 GB max heap (8589934592 / 1024^3) and 0.5 GB initial heap on my machine. The formula behind this is straight forward. Using the JVM configuration parameter names, we end up with: MaxHeapSize = MaxRAM * 1 / MaxRAMFraction where MaxRAM is the available RAM (1) and MaxRAMFraction is 4 (2) by default. That means the JVM allocates up to 25% of your RAM per JVM running on your machine.\n 而在容器中运行的Java进程默认取到的系统内存是宿主机的内存信息：\n$ docker run -m 100MB openjdk:8u121-alpine cat /proc/meminfo MemTotal: 1008492 kB MemFree: 144328 kB MemAvailable: 548688 kB Buffers: 69864 kB Cached: 421352 kB ... 如果宿主机上的内存容量较大，通过上述计算公式自然得到一个较大的Max Heap Size，这样Java程序在运行时如果频繁申请内存，而由于并没有接近Max Heap Size，因此不会去GC，这样运行下去，最终申请的内存超过了容器的memory quota，因而被cgroup杀掉容器进程了。\n解决方案 容器如此火热的今天，这个问题自然有解决方案了。\n方案1 如果java可以升级到Java 10，则使用-XX:+UseContainerSupport打开容器支持就可以了，这时容器中运行的JVM进程取到的系统内存即是施加的memory quota了：\n$ docker run -m 400MB openjdk:10 java -XX:+UseContainerSupport -XX:InitialRAMPercentage=40.0 -XX:MaxRAMPercentage=90.0 -XX:MinRAMPercentage=50.0 -XshowSettings:vm -version VM settings: Max. Heap Size (Estimated): 348.00M Using VM: OpenJDK 64-Bit Server VM openjdk version \u0026#34;10.0.2\u0026#34; 2018-07-17 OpenJDK Runtime Environment (build 10.0.2+13-Debian-2) OpenJDK 64-Bit Server VM (build 10.0.2+13-Debian-2, mixed mode) 同时还可以通过-XX:InitialRAMPercentage、-XX:MaxRAMPercentage、-XX:MinRAMPercentage这些参数控制JVM使用的内存比率。因为很多Java程序在运行时会调用外部进程、申请Native Memory等，所以即使是在容器中运行Java程序，也得预留一些内存给系统的。所以-XX:MaxRAMPercentage不能配置得太大。\n进行一步查阅资料，发现-XX:+UseContainerSupport这个标志选项在Java 8u191已经被backport到Java 8了。因此如果使用的jdk是Java 8u191之后的版本，上述那些JVM参数依然有效：\n$ docker run -m 400MB openjdk:8u191-alpine java -XX:+UseContainerSupport -XX:InitialRAMPercentage=40.0 -XX:MaxRAMPercentage=90.0 -XX:MinRAMPercentage=50.0 -XshowSettings:vm -version VM settings: Max. Heap Size (Estimated): 348.00M Ergonomics Machine Class: client Using VM: OpenJDK 64-Bit Server VM openjdk version \u0026#34;1.8.0_191\u0026#34; OpenJDK Runtime Environment (IcedTea 3.10.0) (Alpine 8.191.12-r0) OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode) 方案2 如果使用的jdk是Java 8u131之后的版本，可使用-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap选项，如下：\n$ docker run -m 400MB openjdk:8u131-alpine java -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:MaxRAMFraction=2 -XshowSettings:vm -version VM settings: Max. Heap Size (Estimated): 193.38M Ergonomics Machine Class: client Using VM: OpenJDK 64-Bit Server VM openjdk version \u0026#34;1.8.0_131\u0026#34; OpenJDK Runtime Environment (IcedTea 3.4.0) (Alpine 8.131.11-r2) OpenJDK 64-Bit Server VM (build 25.131-b11, mixed mode) -XX:MaxRAMFraction就是刚才那个公式里的MaxRAMFraction，默认值是4，代表默认分配系统内存大约25%给Heap Size，可以减小这个参数，从而使JVM尽量地使用memory quota。同时这个值不能太小，设置为1还是有些危险，见这里的说明，一般设置为2。\n方案3 容器运行时会将容器的quota等cgroup目录挂载进容器，因此可以通过entrypoint脚本自行读取这些信息，并给JVM设置合理的-Xms、-Xmx等参数，参考这里的脚本。\n当然最好是能升级到Java 8u191或Java 10。\n参考  https://medium.com/adorsys/jvm-memory-settings-in-a-container-environment-64b0840e1d9e https://blog.csanchez.org/2017/05/31/running-a-jvm-in-a-container-without-getting-killed/ https://stackoverflow.com/questions/49854237/is-xxmaxramfraction-1-safe-for-production-in-a-containered-environment https://stackoverflow.com/questions/42187085/check-mem-limit-within-a-docker-container https://github.com/jeremyxu2010/rocketmq-docker  ","permalink":"https://jeremyxu2010.github.io/2019/10/%E9%81%BF%E5%85%8D%E5%AE%B9%E5%99%A8%E4%B8%AD%E8%BF%90%E8%A1%8C%E7%9A%84java%E5%BA%94%E7%94%A8%E8%A2%AB%E6%9D%80%E6%8E%89/","tags":["java","docker"],"title":"避免容器中运行的Java应用被杀掉"},{"categories":["容器编排"],"contents":"工作中需要在一台x86服务器从写好的golang程序源码生成linux/amd64、linux/arm64 docker镜像，查阅了下资料，这里记录一下操作过程。\n安装docker 查阅docker官方文档，需要使用buildx，而Docker 19.03版本已经捆绑了buildx，方便起见，这里就直接使用19.03版本的docker了，过程如下：\n$ sudo yum remove docker \\  docker-client \\  docker-client-latest \\  docker-common \\  docker-latest \\  docker-latest-logrotate \\  docker-logrotate \\  docker-engine $ sudo yum install -y yum-utils \\  device-mapper-persistent-data \\  lvm2 $ sudo yum-config-manager \\  --add-repo \\  https://download.docker.com/linux/centos/docker-ce.repo # 列一下可安装的docker版本 $ yum list docker-ce --showduplicates | sort -r # 安装19.03.2版本的docker $ sudo yum install docker-ce-19.03.2 docker-ce-cli-19.03.2 containerd.io # 启动docker服务 $ systemctl start docker 安装qemu-user-static 为了让在x86上可以运行arm64的docker镜像，这里需要安装qemu-user-static，过程如下：\n$ docker run --rm --privileged multiarch/qemu-user-static --reset -p yes 创建构建多平台docker镜像的构建器 首先打开docker-cli的experimental开关：\n$ mkdir ~/.docker $ cat \u0026lt;\u0026lt; EOF \u0026gt; ~/.docker/config.json { \u0026#34;experimental\u0026#34;: \u0026#34;enabled\u0026#34; } EOF 创建并启动构建器：\n# 创建构建器 $ docker buildx create --name builder --node default --use # 启动构建器 $ docker buildx inspect builder --bootstrap # 观察下当前使用的构建器及构建器支持的cpu架构，可以看到支持很多cpu架构 $ docker buildx ls 编写脚本生成多平台docker镜像 假设有一个普通的golang程序源码，我们已经写好了Dockerfile生成其docker镜像，如下：\n# Start from the latest golang base imageFROMgolang:latest as go-builder# Add Maintainer InfoLABEL maintainer=\u0026#34;Jeremy Xu \u0026lt;jeremyxu2010@gmail.com\u0026gt;\u0026#34;# Set the Current Working Directory inside the containerWORKDIR/app# Copy go mod and sum filesCOPY go.mod go.sum ./# Download all dependencies. Dependencies will be cached if the go.mod and go.sum files are not changedRUN go mod download# Copy the source from the current directory to the Working Directory inside the containerCOPY ./cmd ./cmd# Build the Go appRUN go build -o output/demo ./cmd# Start from the latest alpine base imageFROMalpine:latest# Set the Current Working Directory inside the containerWORKDIR/app# Copy execute file from go-builderCOPY --from=go-builder /app/output/demo /app/demo# Set docker image commandCMD [ \u0026#34;/app/demo\u0026#34; ]那么现在只需要使用两条命令，即可生成linux/amd64、linux/arm64 docker镜像，如下：\n# 生成linux/amd64 docker镜像 $ docker buildx build --rm -t go-mul-arch-build:latest-amd64 --platform=linux/amd64 --output=type=docker . # 生成linux/arm64 docker镜像 $ docker buildx build --rm -t go-mul-arch-build:latest-arm64 --platform=linux/arm64 --output=type=docker . 最后检查下生成的docker镜像：\n# 运行下linux/amd64的docker镜像，检查镜像的cpu架构 $ docker run --rm -ti go-mul-arch-build:latest-amd64 sh /app # ./demo Hello world! oh dear /app # uname -m x86_64 /app # exit # 运行下linux/arm64的docker镜像，检查镜像的cpu架构 $ docker run --rm -ti go-mul-arch-build:latest-arm64 sh /app # ./demo Hello world! oh dear /app # uname -m aarch64 /app # exit 本操作指引中涉及的示例代码、脚本见github项目。\nDone.\n参考  https://docs.docker.com/install/linux/docker-ce/centos/ https://docs.docker.com/buildx/working-with-buildx/#build-multi-platform-images https://github.com/docker/buildx https://github.com/multiarch/qemu-user-static https://www.callicoder.com/docker-golang-image-container-example/ https://github.com/docker/buildx/issues/138  ","permalink":"https://jeremyxu2010.github.io/2019/09/%E7%94%9F%E6%88%90%E5%A4%9A%E5%B9%B3%E5%8F%B0docker%E9%95%9C%E5%83%8F/","tags":["docker","buildx","arm64"],"title":"生成多平台docker镜像"},{"categories":["容器编排"],"contents":"本周帮助为一个kubernetes CSI插件实现了动态供应(dynamic provisioning)功能，在这个过程中学习并了解了kubernetes CSI插件的实现细节，这里详细记录一下。\nCSI相关概念 在CSI未出现之前，容器编排系统（Container Orchestration Systems，简称COs，如kubernetes）为了能使用外部存储系统，使这些存储系统为容器工作负载提供存储卷。COs需要在自身的代码中嵌入大量与存储相关的代码，参见kubernetes里的volume包，这个包下面大部分就是所谓的in-tree（意思是在kubernetes的代码树里）存储卷插件。\n要支持的存储系统多种多样，而且有些存储系统的支持代码还不便于开源，所以很明显上述设计并不好。\n后面又出现了Flexvolume这种out-tree的存储卷插件机制，允许存储厂商将写好的存储卷插件二进制文件放置到各node节点预设的目录下，kubernetes即可在自动发现它们，并调用它们完成存储卷的供应。详情技术细节可参考这里。\n上述Flexvolume方案很类似于kubernetes里用的网络方案CNI，都是将外部插件放置在预设的目录下，以供kubernetes调用。但总的来说还是跟kubernetes这一容器编排系统绑定得太死了。于是人们又发明了CSI。\nCSI 代表容器存储接口，CSI 试图建立一个行业标准接口的规范，借助 CSI 容器编排系统（CO）可以将任意存储系统暴露给自己的容器工作负载。有关详细信息，请查看设计方案。\ncsi 卷类型也是一种 out-tree（in-tree是指跟其它存储插件在同一个代码路径下，随 Kubernetes 的代码同时编译，out-tree则刚好相反） 的 CSI 卷插件，用于 Pod 与在同一节点上运行的外部 CSI 卷驱动程序交互。部署 CSI 兼容卷驱动后，用户可以使用 csi 作为卷类型来挂载驱动提供的存储。\nCSI 持久化卷支持是在 Kubernetes v1.9 中引入的，作为一个 alpha 特性，必须由集群管理员明确启用。换句话说，集群管理员需要在 apiserver、controller-manager 和 kubelet 组件的 “--feature-gates =” 标志中加上 “CSIPersistentVolume = true”。\nCSI 持久化卷具有以下字段可供用户指定：\n driver：一个字符串值，指定要使用的卷驱动程序的名称。必须少于 63 个字符，并以一个字符开头。驱动程序名称可以包含 “。”、“ - ”、“_” 或数字。 volumeHandle：一个字符串值，唯一标识从 CSI 卷插件的 CreateVolume 调用返回的卷名。随后在卷驱动程序的所有后续调用中使用卷句柄来引用该卷。 readOnly：一个可选的布尔值，指示卷是否被发布为只读。默认是 false。  CSI插件机制分析 光看上面的概念，还是很难理解到底CSI插件是怎样的。其实说到底一个CSI插件就是实现了CSI规范要求的多个gRPC接口的服务程序。\n一个CSI插件一般会以两种形式部署运行着，分别是Controller组件和Node组件。\n Controller Plugin\nThe controller component can be deployed as a Deployment or StatefulSet on any node in the cluster. It consists of the CSI driver that implements the CSI Controller service and one or more sidecar containers. These controller sidecar containers typically interact with Kubernetes objects and make calls to the driver's CSI Controller service.\nIt generally does not need direct access to the host and can perform all its operations through the Kubernetes API and external control plane services. Multiple copies of the controller component can be deployed for HA, however it is recommended to use leader election to ensure there is only one active controller at a time.\nController sidecars include the external-provisioner, external-attacher, external-snapshotter, and external-resizer. Including a sidecar in the deployment may be optional.\nNode Plugin\nThe node component should be deployed on every node in the cluster through a DaemonSet. It consists of the CSI driver that implements the CSI Node service and the node-driver-registrar sidecar container.\nThe Kubernetes kubelet runs on every node and is responsible for making the CSI Node service calls. These calls mount and unmount the storage volume from the storage system, making it available to the Pod to consume. Kubelet makes calls to the CSI driver through a UNIX domain socket shared on the host via a HostPath volume. There is also a second UNIX domain socket that the node-driver-registrar uses to register the CSI driver to kubelet.\n 可以看到Controller组件一般是以Deployment或StatefulSet形式部署的，它实现了CSI Controller service，它会与Kubernetes API、外部存储服务的控制面交互，但它并不会实际处理存储卷在宿主机上的挂载等事宜。\n而Node组件因为要运行在所有node节点上，因此一般是以DaemonSet形式部署的，它实现了CSI Node service，它会暴露出一个UNIX domain socket文件出来，从而让kubelet在进行存储卷操作时，通过这个UNIX domain socket文件调用它的gRPC接口。\n上述两个组件配合，即完成了将存储卷暴露给工作负载的功能。\n下面看一下一个CSI插件要实现的三组gRPC接口服务：\n  Identity Service: Both the Node Plugin and the Controller Plugin MUST implement this sets of RPCs. Controller Service: The Controller Plugin MUST implement this sets of RPCs. Node Service: The Node Plugin MUST implement this sets of RPCs.   service Identity { rpc GetPluginInfo(GetPluginInfoRequest) returns (GetPluginInfoResponse) {} rpc GetPluginCapabilities(GetPluginCapabilitiesRequest) returns (GetPluginCapabilitiesResponse) {} rpc Probe (ProbeRequest) returns (ProbeResponse) {}}service Controller { rpc CreateVolume (CreateVolumeRequest) returns (CreateVolumeResponse) {} rpc DeleteVolume (DeleteVolumeRequest) returns (DeleteVolumeResponse) {} rpc ControllerPublishVolume (ControllerPublishVolumeRequest) returns (ControllerPublishVolumeResponse) {} rpc ControllerUnpublishVolume (ControllerUnpublishVolumeRequest) returns (ControllerUnpublishVolumeResponse) {} rpc ValidateVolumeCapabilities (ValidateVolumeCapabilitiesRequest) returns (ValidateVolumeCapabilitiesResponse) {} rpc ListVolumes (ListVolumesRequest) returns (ListVolumesResponse) {} rpc GetCapacity (GetCapacityRequest) returns (GetCapacityResponse) {} rpc ControllerGetCapabilities (ControllerGetCapabilitiesRequest) returns (ControllerGetCapabilitiesResponse) {} rpc CreateSnapshot (CreateSnapshotRequest) returns (CreateSnapshotResponse) {} rpc DeleteSnapshot (DeleteSnapshotRequest) returns (DeleteSnapshotResponse) {} rpc ListSnapshots (ListSnapshotsRequest) returns (ListSnapshotsResponse) {} rpc ControllerExpandVolume (ControllerExpandVolumeRequest) returns (ControllerExpandVolumeResponse) {}}service Node { rpc NodeStageVolume (NodeStageVolumeRequest) returns (NodeStageVolumeResponse) {} rpc NodeUnstageVolume (NodeUnstageVolumeRequest) returns (NodeUnstageVolumeResponse) {} rpc NodePublishVolume (NodePublishVolumeRequest) returns (NodePublishVolumeResponse) {} rpc NodeUnpublishVolume (NodeUnpublishVolumeRequest) returns (NodeUnpublishVolumeResponse) {} rpc NodeGetVolumeStats (NodeGetVolumeStatsRequest) returns (NodeGetVolumeStatsResponse) {} rpc NodeExpandVolume(NodeExpandVolumeRequest) returns (NodeExpandVolumeResponse) {} rpc NodeGetCapabilities (NodeGetCapabilitiesRequest) returns (NodeGetCapabilitiesResponse) {} rpc NodeGetInfo (NodeGetInfoRequest) returns (NodeGetInfoResponse) {}}上述这堆接口，看着挺多。其实如果不想实现CSI的某些功能，有些接口也不用实现的。比如如果不想实现存储卷的动态供应，Controller的CreateVolume、DeleteVolume即可不实现。另外有些接口属于元数据接口，仅仅是声明该CSI的Capability，Info的，如Identity的所有接口，ControllerGetCapabilities的ControllerGetCapabilities接口，Node的NodeGetCapabilities、NodeGetInfo接口。再看一下storage volume的lifecycle，这些接口之前的调用关系就很清楚了。\n  CreateVolume +------------+ DeleteVolume +-------------\u0026gt;| CREATED +--------------+ | +---+----+---+ | | Controller | | Controller v +++ Publish | | Unpublish +++ |X| Volume | | Volume | | +-+ +---v----+---+ +-+ | NODE_READY | +---+----^---+ Node | | Node Stage | | Unstage Volume | | Volume +---v----+---+ | VOL_READY | +------------+ Node | | Node Publish | | Unpublish Volume | | Volume +---v----+---+ | PUBLISHED | +------------+ Figure 6: The lifecycle of a dynamically provisioned volume, from creation to destruction, when the Node Plugin advertises the STAGE_UNSTAGE_VOLUME capability.  上面这个图是一个较为复杂的卷供应生命周期图，从这个图我们可以看出一个存储卷的供应分别调用了Controller Plugin的CreateVolume、ControllerPublishVolume及Node Plugin的NodeStageVolume、NodePublishVolume这4个gRPC接口，存储卷的销毁分别调用了Node Plugin的NodeUnpublishVolume、NodeUnstageVolume及Controller的ControllerUnpublishVolume、DeleteVolume这4个gRPC接口，这每个接口要完成的工作参见CSI规范。\n只看规范可能会看得云里雾里的，我是参考一个现成的CSI插件实例来理解CSI规范的。这里推荐下tencentcloud cbs块存储的CSI插件，这个CSI插件实现得相当规范，分别在controller.go里实现了Controller Service里的几个gRPC接口、identity.go里实现了Identify Service里的几个gRPC接口、node.go里实现了Node Service里的几个gRPC接口。\nCSI插件的部署 按CSI规范实现了相应的gRPC接口后，一个CSI插件就基本成型了。但这并不是全部，回想下目前整个CSI插件的功能逻辑，我们只是实现了存储卷驱动的核心逻辑，但并没有与Kubernetes产生任何联动啊。这写好的CSI插件如何工作呢？\n与Kubernetes的联动逻辑比较统一，基本就是watch Kubrernetes API，根据watch到的资源状态调用相应的CSI接口，根据CSI接口的返回结果更新Kubernetes里的资源状态。官方为了简化开发CSI插件的复杂度，提供了一系列的sidecar来完成这些工作。而CSI的开发人员要做的就是在部署CSI插件时声明将相应的sidecar与CSI插件捆绑部署在一起。\n Kubernetes CSI Sidecar Containers are a set of standard containers that aim to simplify the development and deployment of CSI Drivers on Kubernetes.\nThese containers contain common logic to watch the Kubernetes API, trigger appropriate operations against the “CSI volume driver” container, and update the Kubernetes API as appropriate.\nThe containers are intended to be bundled with third-party CSI driver containers and deployed together as pods.\nThe containers are developed and maintained by the Kubernetes Storage community.\nUse of the containers is strictly optional, but highly recommended.\n 使用这些sidecar的好处很多，官方文档中下面这两段话说得很清楚。以后做设计时也可以参考这样优雅地实现。\n Benefits of these sidecar containers include:\n Reduction of \u0026ldquo;boilerplate\u0026rdquo; code.  CSI Driver developers do not have to worry about complicated, \u0026ldquo;Kubernetes specific\u0026rdquo; code.   Separation of concerns.  Code that interacts with the Kubernetes API is isolated from (and in a different container then) the code that implements the CSI interface.     会使用到的sidecar如下：\n  external-provisioner\n如果CSI插件要实现CREATE_DELETE_VOLUME能力（即动态供应），则CSI插件需要实现Controller Service的CreateVolume、DeleteVolume接口，并配合上该sidecar就可以了。这样当watch到指定StorageClass的 PersistentVolumeClaim资源状态变更，会自动地调用这两个接口。\n  external-attacher\n如果CSI插件要实现PUBLISH_UNPUBLISH_VOLUME能力，则CSI插件需要实现Controller Service的ControllerPublishVolume、ControllerUnpublishVolume接口，并配合上该sidecar就可以了。这样当watch到VolumeAttachment资源状态变更，会自动地调用这两个接口。\n  external-snapshotter\n如果CSI插件要实现CREATE_DELETE_SNAPSHOT能力，则CSI插件需要实现Controller Service的CreateSnapshot、DeleteSnapshot接口，并配合上该sidecar就可以了。这样当watch到指定SnapshotClass的VolumeSnapshot资源状态变更，会自动地调用这两个接口。\n  external-resizer\n如果CSI插件要实现EXPAND_VOLUME能力，则CSI插件需要实现Controller Service的ControllerExpandVolume接口，并配合上该sidecar就可以了。这样当watch到PersistentVolumeClaim资源的容量发生变更，会自动地调用这个接口。\n  node-driver-registrar\nCSI插件实现Node Service的NodeGetInfo接口后，配合上该sidecar。这样当CSI Node Plugin部署到kubernetes的node节点时，该sidecar会自动调用接口获取CSI插件信息，并向kubelet进行注册。\n  livenessprobe\n配合上该sidecar，kubernetes即可检测到CSI插件相关pod的健康状态，当不正常时自动重启相应pod。\n  怎么将这些sidecar与CSI Driver部署在一起，官方文档其实讲得很清楚的。\n同样个人觉得还是结合实际的示例理解文档更快一点。比如tencentcloud cbs块存储的CSI插件的部署清单里：\n csi-cbsplugin.yaml以DaemonSet方式部署了csi-tencentcloud-cbs:v1.0.0这个CSI插件Driver程序，这个插件Driver程序旁边放了一个csi-node-driver-registrar:v1.0.2的sidecar，这个sidecar会自动调用接口获取CSI插件信息，并向kubelet进行注册。 csi-cbsplugin-provisioner.yaml以StatefulSet方式部署了csi-provisioner:v1.0.1、csi-snapshotter:v1.0.1这两个sidecar，这两个sidecar会watch指定StorageClass的 PersistentVolumeClaim资源状态变更、指定SnapshotClass的VolumeSnapshot资源状态变更，然后通过宿主机上的csi UNIX domain socket与CSI插件驱动通信，调用相应的gRPC接口方法。 csi-cbsplugin-attacher.yaml以StatefulSet方式部署了csi-attacher:v1.0.1这个sidecar，这个sidecar会watch VolumeAttachment资源状态变更，然后通过宿主机上的csi UNIX domain socket与CSI插件驱动通信，调用相应的gRPC接口方法。  其它的sidecar的使用方法都类似上面示例中的玩法，照着配置就可以了。\nCSI的其它技术细节  在进行存储系统操作时会使用到各种密码、身份凭证，而这些需要按照一定的规约进行配置，详见官方文档。 可以通过CSIDriver这一自定义资源，定制Kubernetes与CSI存储插件交互的行为，详见官方文档。 可以通过CSINodeInfo这一自定义资源，将kubernetes的node与CSI Node映射起来。还可以通过CSINodeInfo指定node节点的的topologyKey，这个能实现基于topology的CSI存储卷供应，详见官方文档1和官方文档2。  CSI插件的测试支持 官方还为开发CSI插件提供了单元测试与e2e测试的方案，可惜时间关系，并没有演练一把，后面可以考虑把这一点实操一下，毕竟测试驱动开发是趋势。\n现成的CSI插件驱动 目前各大知名云厂商都实现了自家存储产品的CSI插件驱动，列表在这里。正常情况下直接使用官方提供的CSI插件即可。当然如果要学习下也是可以，CSI插件的代码一般都不难，5-6个go文件而已。\nDONE\n参考  https://kubernetes-csi.github.io/docs https://github.com/TencentCloud/kubernetes-csi-tencentcloud https://github.com/container-storage-interface/spec https://jimmysong.io/kubernetes-handbook/concepts/csi.html https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md https://github.com/kubernetes/kubernetes/tree/master/pkg/volume  ","permalink":"https://jeremyxu2010.github.io/2019/09/kubernetes-csi%E5%AD%98%E5%82%A8%E6%8F%92%E4%BB%B6%E6%8E%A2%E7%A9%B6/","tags":["k8s","devops","csi","volume"],"title":"kubernetes CSI存储插件探究"},{"categories":["devops"],"contents":"本周做了较多的国产化适配工作，虽然主要是拿到源码在国产化平台上编译一下，不是太难，但还是总结一下。\n国产化平台使用的是arm64v8 CPU芯片，因此传统软件厂商提供的x86架构二进制软件包都没法用，都需要在arm64v8 CPU的服务器上拿源码重新编译。\n构建debian deb包 在x86上构建debian的deb包还是比较简单的，过程简述如下：\n# 下面假设要编译curl的deb包 # 安装gcc、make等编译链工具 $ apt-get update \u0026amp;\u0026amp; apt-get install -y build-essential # 安装编译curl依赖的那些软件包 $ apt-get build-dep curl # 创建编译目录 $ mkdir -p ~/build \u0026amp;\u0026amp; cd ~/build # 下载构建curl deb包的源码 $ apt source curl # 切换到构建目录 $ cd curl-7.52.1/ # 执行dpkg-buildpackage命令构建deb包，该条命令执行完毕后，在上一层目录下就会生成deb包 $ dpkg-buildpackage -us -uc -b 对于apt源里有source包的软件包，基本上像上面这样构建就差不多了。\n但有些软件厂商并没有提供apt源或apt源里没有相应CPU架构的包，只提供了软件的deb包，比如mysql。这个时候需要手工下载source deb包，并从source deb包构建出相应cpu架构的deb包，过程简述如下：\n# 安装gcc、make等编译链工具 $ apt-get update \u0026amp;\u0026amp; apt-get install -y build-essential # 下载mysql-community的source包 $ curl -O -L \u0026#39;https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-community-source_5.7.27-1debian9_amd64.deb\u0026#39; # 安装source deb包 $ dpkg -i mysql-community-source_5.7.27-1debian9_amd64.deb # 创建编译目录 $ mkdir -p ~/build \u0026amp;\u0026amp; cd ~/build # 将source deb包里的内容拷贝到构建目录 $ cp /usr/src/mysql/* ~/build/ # 解压mysql原始的源码 $ tar -xf mysql-community_5.7.27.orig.tar.gz # 解压构建deb包的debian目录 $ tar -xf mysql-community_5.7.27-1debian9.debian.tar.xz -C mysql-5.7.27/ # 切换到deb包构建目录 $ cd mysql-5.7.27/ # 检查编译deb包依赖的那些软件包是否都安装好了，如果没有安装好，先用apt-get install安装一下 $ grep \u0026#39;Build-Depends\u0026#39; debian/control # 执行dpkg-buildpackage命令构建deb包，该条命令执行完毕后，在上一层目录下就会生成deb包 $ dpkg-buildpackage -us -uc -b 可以看到过程其实跟x86下构建deb包类似，只是需要手工下载下source deb包、手工准备下deb包的构建目录、手工安装编译时的依赖包。\n构建出arm64v8的deb包后，再将之安装到arm64v8的base docker镜像里，一个arm64v8平台下可使用的docker镜像就生成好了，参考的Dockerfile如下：\nFROMarm64v8/debian:9COPY curl_7.52.1-5+deb9u9_aarm64.deb /tmp/curl_7.52.1-5+deb9u9_aarm64.debRUN dpkg -i /tmp/curl_7.52.1-5+deb9u9_aarm64.deb...构建CentOS/RHEL rpm包 在x86上构建CentOS/RHEL的rpm包也比较简单，过程简述如下：\n# 下面假设要编译curl的rpm包 # 安装gcc、make等编译链工具 $ yum groupinstall -y \u0026#39;Development tools\u0026#39; # 安装编译curl依赖的那些软件包 $ yum-builddep -y curl # 下载构建curl rpm包的源码包 $ yumdownloader --source curl # 安装构建curl rpm包的源码包 $ rpm -ivh curl-7.29.0-54.el7.src.rpm # 使用rpmbuild命令构建rpm包，该条命令执行完毕后，在~/rpmbuild/RPMS/x86_64目录下就生成了rpm包 $ cd ~/rpmbuild/SPECS/ $ rpmbuild -bb ./curl.spec 对于yum源里有source包的软件包，基本上像上面这样构建就差不多了。\n但有些软件厂商并没有提供yum源或yum源里没有相应CPU架构的包，只提供了软件的rpm包，比如mysql。这个时候需要手工下载source rpm包，并从source rpm包构建出相应cpu架构的rpm包，过程简述如下：\n# 安装gcc、make等编译链工具 $ yum groupinstall -y \u0026#39;Development tools\u0026#39; $ curl -O -L \u0026#39;https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-community-5.7.27-1.el7.src.rpm\u0026#39; # 安装编译mysql-community依赖的那些软件包 $ yum-builddep -y ./mysql-community-5.7.27-1.el7.src.rpm # 安装构建curl rpm包的源码包 $ rpm -ivh ./mysql-community-5.7.27-1.el7.src.rpm # 使用rpmbuild命令构建rpm包，该条命令执行完毕后，在~/rpmbuild/RPMS/x86_64目录下就生成了rpm包 $ cd ~/rpmbuild/SPECS/ $ rpmbuild -bb ./mysql.spec 可以看到过程其实跟x86下构建yum包类似，只是需要手工下载下source rpm包、根据下载的source rpm包安装编译时的依赖包。\n构建出arm64v8的rpm包后，再将之安装到arm64v8的base docker镜像里，一个arm64v8平台下可使用的docker镜像就生成好了，参考的Dockerfile如下：\nFROMarm64v8/centos:7COPY curl-7.29.0-54.el7.aarm64.rpm /tmp/curl-7.29.0-54.el7.aarm64.rpmRUN dpkg -i /tmp/curl-7.29.0-54.el7.aarm64.rpm...编译障碍 arm64v8平台现在还不是很流行，在编译过程中可能会遇到各种各样的编译报错，这时拿着编译报错信息到google上搜索一下，一般都可以找到解决方案，一般是改改源码使编译通过，或者改改编译参数使之通过，这里涉及工作细节，就不细述了。\n参考  https://www.debian.org/doc/manuals/maint-guide/build.zh-cn.html https://www.debian.org/doc/debian-policy/ch-source.html https://askubuntu.com/questions/324845/whats-the-difference-between-apt-get-install-and-apt-get-build-dep https://dev.mysql.com/downloads/mysql/ https://blog.packagecloud.io/eng/2015/04/20/working-with-source-rpms/  ","permalink":"https://jeremyxu2010.github.io/2019/09/%E7%BC%96%E8%AF%91arm64%E5%B9%B3%E5%8F%B0%E7%9A%84%E8%BD%AF%E4%BB%B6%E5%8C%85/","tags":["debian","centos","deb","rpm","arm64"],"title":"编译arm64平台的软件包"},{"categories":["容器编排"],"contents":"最近我在kubernetes中使用了ceph的rbd及cephfs存储卷，遇到了一些问题，并逐一解决了，在这里记录一下。\nceph rbd存储卷扩容失败 第一个问题是某应用程序使用了ceph rbd存储卷，但随着时间的推移，发现原来pvc申请的存储空间不够用了，需要进行扩容。这里参考官方指引，进行了一些配置。\nstorageclass设置allowVolumeExpansion: true：\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: rbd provisioner: ceph.com/rbd parameters: monitors: xx.xx.xx.xx:6789 pool: kube adminId: admin adminSecretNamespace: kube-system adminSecretName: ceph-admin-secret userId: kube userSecretNamespace: kube-system userSecretName: ceph-secret imageFormat: \u0026#34;2\u0026#34; imageFeatures: layering allowVolumeExpansion: true kube-controller-manager及kubelet均开启ExpandPersistentVolumes、PersistentVolumeClaimResize、ExpandInUsePersistentVolumes：\n--feature-gates ExpandPersistentVolumes=true,PersistentVolumeClaimResize=true,ExpandInUsePersistentVolumes=true 然后edit某个pvc，将spec.resources.requests.storage增大：\n$ kubectl edit pvc resize apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-provisioner: ceph.com/rbd name: resize spec: resources: requests: storage: 5Gi storageClassName: sata 然后查看pvc，但过了很久，pvc仍然没有进行FileSystemResizePending状态。\n难道遇到kubernetes的bug了？\n查阅kubernetes的代码后，发现kubernetes是调用rbd info及rbd resize等外部命令完成rbd存储卷的扩容的：\nhttps://github.com/kubernetes/kubernetes/blob/master/pkg/volume/rbd/rbd_util.go#L647\n// ExpandImage runs rbd resize command to resize the specified image. func (util *RBDUtil) ExpandImage(rbdExpander *rbdVolumeExpander, oldSize resource.Quantity, newSize resource.Quantity) (resource.Quantity, error) { var output []byte var err error // Convert to MB that rbd defaults on. \tsz := int(volumehelpers.RoundUpToMiB(newSize)) newVolSz := fmt.Sprintf(\u0026#34;%d\u0026#34;, sz) newSizeQuant := resource.MustParse(fmt.Sprintf(\u0026#34;%dMi\u0026#34;, sz)) // Check the current size of rbd image, if equals to or greater that the new request size, do nothing. \tcurSize, infoErr := util.rbdInfo(rbdExpander.rbdMounter) if infoErr != nil { return oldSize, fmt.Errorf(\u0026#34;rbd info failed, error: %v\u0026#34;, infoErr) } if curSize \u0026gt;= sz { return newSizeQuant, nil } // rbd resize. \tmon := util.kernelRBDMonitorsOpt(rbdExpander.rbdMounter.Mon) klog.V(4).Infof(\u0026#34;rbd: resize %s using mon %s, pool %s id %s key %s\u0026#34;, rbdExpander.rbdMounter.Image, mon, rbdExpander.rbdMounter.Pool, rbdExpander.rbdMounter.adminId, rbdExpander.rbdMounter.adminSecret) output, err = rbdExpander.exec.Run(\u0026#34;rbd\u0026#34;, \u0026#34;resize\u0026#34;, rbdExpander.rbdMounter.Image, \u0026#34;--size\u0026#34;, newVolSz, \u0026#34;--pool\u0026#34;, rbdExpander.rbdMounter.Pool, \u0026#34;--id\u0026#34;, rbdExpander.rbdMounter.adminId, \u0026#34;-m\u0026#34;, mon, \u0026#34;--key=\u0026#34;+rbdExpander.rbdMounter.adminSecret) if err == nil { return newSizeQuant, nil } klog.Errorf(\u0026#34;failed to resize rbd image: %v, command output: %s\u0026#34;, err, string(output)) return oldSize, err } 而执行这些外部命令后，代码还会解析这些命令的输出。而如果ceph的配置文件中启用了一些调试输出，则解析会发生错误。\n知道原因就很好办了，修改/etc/ceph/ceph.conf文件，注释掉调试输出的设置就好了。\ncephfs存储卷quota失效 项目里还有一些应用程序使用了cephfs的存储卷，但经过验证，发现pvc里设置的存储卷大小无效，应用程序可以随意往存储卷里写入大量数据，这就很危险了。\n$ cat test.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test spec: storageClassName: cephfs accessModes: - ReadWriteMany resources: requests: storage: 1Gi --- apiVersion: v1 kind: Pod metadata: name: test-pod spec: volumes: - name: data persistentVolumeClaim: claimName: test containers: - name: nginx-server image: nginx ports: - containerPort: 80 name: \u0026#34;http-server\u0026#34; volumeMounts: - mountPath: \u0026#34;/usr/share/nginx/html\u0026#34; name: data # pvc的容量只有1G，竟然可以写入2G的数据到存储卷 $ kubectl exec -ti test-pod -- dd if=/dev/zero of=/usr/share/nginx/html/testfile.dat count=2018 bs=1048576 这里使用的是cephfs-provisioner来供应cephfs存储卷的。\n浏览cephfs-provisioner的代码，发现它其实提供了一个enable-quota参数，用来启用pvc的quota功能。\nhttps://github.com/kubernetes-incubator/external-storage/blob/master/ceph/cephfs/cephfs-provisioner.go#L383\nenableQuota = flag.Bool(\u0026#34;enable-quota\u0026#34;, false, \u0026#34;Enable PVC quota\u0026#34;) 于是给cephfs-provisioner加上参数-enable-quota=true：\nkubectl edit deployment cephfs-provisioner spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: cephfs-provisioner strategy: type: Recreate template: metadata: creationTimestamp: null labels: app: cephfs-provisioner spec: containers: - args: ... - -enable-quota=true command: - /usr/local/bin/cephfs-provisioner 本以为会一切正常，但在创建cephfs存储卷时却报错了：\nE0831 12:27:01.347130 1 cephfs-provisioner.go:158] failed to provision share \u0026quot;kubernetes-dynamic-pvc-a1f36dc2-cbea-11e9-97cf-6639535e4727\u0026quot; for \u0026quot;kubernetes-dynamic-user-a1f36ec8-cbea-11e9-97cf-6639535e4727\u0026quot;, err: exit status 1, output: Traceback (most recent call last): File \u0026quot;/usr/local/bin/cephfs_provisioner\u0026quot;, line 364, in \u0026lt;module\u0026gt; main() File \u0026quot;/usr/local/bin/cephfs_provisioner\u0026quot;, line 358, in main print cephfs.create_share(share, user, size=size) File \u0026quot;/usr/local/bin/cephfs_provisioner\u0026quot;, line 228, in create_share volume = self.volume_client.create_volume(volume_path, size=size, namespace_isolated=not self.ceph_namespace_isolation_disabled) File \u0026quot;/lib/python2.7/site-packages/ceph_volume_client.py\u0026quot;, line 622, in create_volume self.fs.setxattr(path, 'ceph.quota.max_bytes', size.__str__(), 0) File \u0026quot;cephfs.pyx\u0026quot;, line 988, in cephfs.LibCephFS.setxattr (/home/jenkins-build/build/workspace/ceph-build/ARCH/x86_64/AVAILABLE_ARCH/x86_64/AVAILABLE_DIST/centos7/DIST/centos7/MACHINE_SIZE/huge/release/13.2.1/rpm/el7/BUILD/ceph-13.2.1/build/src/pybind/cephfs/pyrex/cephfs.c:10498) cephfs.OperationNotSupported: [Errno 95] error in setxattr 继续查原因，发现cephfs-provisioner实际上是调用libcephfs中的ceph_setxattr函数，以实现给创建的cephfs存储卷设置quota的。而直接在ceph集群上通过命令给某个目录设置quota是正常的。看来还是哪里不太正常。\n继续追查，发现cephfs-provisioner项目的Dockerfile里指示了安装的是mimic版的ceph-common和python-cephfs，而我部署的ceph集群是luminous版。会不会是ceph客户端与服务端不兼容？立即修改Dockerfile文件，改成安装luminous版ceph-common和python-cephfs，重新编译docker镜像，更新cephfs-provisioner所使用的镜像，这下创建pvc时终于不报错了。这里我们再检查下cephfs存储卷目录的quota是正常的。\n$ mkdir test $ ceph-fuse -c /etc/ceph/ceph.conf -m xx.xx.xx.xx:6789 -r /xxxx test $ getfattr -n ceph.quota.max_bytes test 1073741824 看起来一切很美好了，但经测试发现quota依然无效，应用程序还是无视quota随意往存储卷里写入大量数据。\n继续追查问题，发现CephFS的mount方式分为内核态mount和用户态mount，内核态使用mount命令挂载，用户态使用ceph-fuse。内核态只有在kernel 4.17 以上的版本才支持Quotas，用户态则没有限制。而我们的环境中内核明显没有这么高，而kubernetes的代码里会根据是否找得到ceph-fuse命令决定是否使用用户态挂载。\nhttps://github.com/kubernetes/kubernetes/blob/master/pkg/volume/cephfs/cephfs.go#L245\n// check whether it belongs to fuse, if not, default to use kernel mount. \tif cephfsVolume.checkFuseMount() { klog.V(4).Info(\u0026#34;CephFS fuse mount.\u0026#34;) err = cephfsVolume.execFuseMount(dir) // cleanup no matter if fuse mount fail. \tkeyringPath := cephfsVolume.GetKeyringPath() _, StatErr := os.Stat(keyringPath) if !os.IsNotExist(StatErr) { os.RemoveAll(keyringPath) } if err == nil { // cephfs fuse mount succeeded. \treturn nil } // if cephfs fuse mount failed, fallback to kernel mount. \tklog.V(2).Infof(\u0026#34;CephFS fuse mount failed: %v, fallback to kernel mount.\u0026#34;, err) } klog.V(4).Info(\u0026#34;CephFS kernel mount.\u0026#34;) err = cephfsVolume.execMount(dir) if err != nil { // cleanup upon failure. \tmount.CleanupMountPoint(dir, cephfsVolume.mounter, false) return err } 为了使用用户态挂载，则需要在node节点上安装ceph-fuse的软件包。验证一把，这下应用程序终于在受限的盒子里使用cephfs存储卷了。\n真正的用户场景还是涉及cephfs存储卷的扩容，在网上找了下，发现已经有人实现了，文章在这里。咨询过作者，大致了解了实现方法，需要修改kubernetes的代码：\n 修改k8s.io/kubernetes/cmd/kube-controller-manager/app/plugins.go， 将cephfs加入ExpandableVolumePlugins列表里  // ProbeExpandableVolumePlugins returns volume plugins which are expandable func ProbeExpandableVolumePlugins(config persistentvolumeconfig.VolumeConfiguration) []volume.VolumePlugin { allPlugins := []volume.VolumePlugin{} allPlugins = append(allPlugins, awsebs.ProbeVolumePlugins()...) allPlugins = append(allPlugins, gcepd.ProbeVolumePlugins()...) allPlugins = append(allPlugins, cinder.ProbeVolumePlugins()...) allPlugins = append(allPlugins, portworx.ProbeVolumePlugins()...) allPlugins = append(allPlugins, vsphere_volume.ProbeVolumePlugins()...) allPlugins = append(allPlugins, glusterfs.ProbeVolumePlugins()...) allPlugins = append(allPlugins, rbd.ProbeVolumePlugins()...) allPlugins = append(allPlugins, azure_dd.ProbeVolumePlugins()...) allPlugins = append(allPlugins, azure_file.ProbeVolumePlugins()...) allPlugins = append(allPlugins, scaleio.ProbeVolumePlugins()...) allPlugins = append(allPlugins, storageos.ProbeVolumePlugins()...) allPlugins = append(allPlugins, fc.ProbeVolumePlugins()...) return allPlugins } 修改k8s.io/kubernetes/pkg/volume/cephfs/cephfs.go  type ExpandableVolumePlugin interface { VolumePlugin ExpandVolumeDevice(spec *Spec, newSize resource.Quantity, oldSize resource.Quantity) (resource.Quantity, error) RequiresFSResize() bool } 在交流的过程中还结识了一个做kubernetes的同行-伊布。\n使用cephfs用户态挂载并不是完全没有缺陷的，在实际运营过程中，我们发现当重启了node节点上的kubelet，已经挂载的cephfs卷会失效，而使用这些cephfs卷的容器会出现Transport endpoint is not connected的报错。目前想到三种办法解决问题：\n  通过kubectl describe pod、docker inspect等命令找到需要挂载cephfs卷的目录，通过kubectl describe pv命令找到cephfs卷的连接信息，然后使用ceph-fuse命令将cephfs卷挂载起来，参考如下命令：\nceph-fuse -c /etc/ceph/ceph.conf -m xx.xx.xx.xx:6789 -r /xxxx /yyyy   使用kubectl delete pod删除pod，kubernetes重建pod时会重新将cephfs卷挂载上。\n  最后一招是一劳永逸的，修改kubernetes的代码，使用 systemd-run 来执行 ceph-fuse命令，这样重启kubelet后，这些ceph-fuse用户态进程不会随着kubelet进程的退出而退出，因此cephfs卷的挂载就不会失效，参考这个bug提交，抽空把这个bug改了，给社区做点贡献。\n  参考  https://kubernetes.io/blog/2018/07/12/resizing-persistent-volumes-using-kubernetes/ https://ieevee.com/tech/2019/04/14/pvc-resize.html https://docs.ceph.com/docs/master/cephfs/quota/ https://docs.ceph.com/docs/master/cephfs/kernel/ https://docs.ceph.com/docs/master/cephfs/fuse/ https://www.cnblogs.com/ltxdzh/p/9173706.html  ","permalink":"https://jeremyxu2010.github.io/2019/09/kubernetes%E4%BD%BF%E7%94%A8ceph%E5%AD%98%E5%82%A8%E5%8D%B7/","tags":["kubernetes","ceph"],"title":"kubernetes使用ceph存储卷"},{"categories":["容器编排"],"contents":"很多软件后端使用的存储都是mysql，当这些软件系统在生产环境部署时，都会面临一个严峻问题，需要在生产环境中部署一个高可用的mysql集群服务。刚好在最近一周的工作中，需要在kubernetes环境中搭建mysql高可用集群，这里记录一下。\n搭建MySQL集群 MySQL的主从半同步复制方案、Galera集群方案以前都也实践过，感觉都不是太友好，配置比较麻烦，而且发生故障转移时经常需要人工参与。所以这里还是采用MySQL官方推荐的Group Replication集群方案。关于MySQL Group Replication集群的架构设计可以看官方文档，懒得看英文的话，也可以看我之前整理出的资料。另外kubedb网页上也有介绍MySQL几种高可用方案的构架方案，也比较有意思。\n之前的博文也讲过在非容器环境搭建MySQL Group Replication集群，现在在Kubernetes的容器环境配合kubedb，搭建更方便了，命令如下：\n# 添加appscode的helm仓库 $ helm repo add appscode https://charts.appscode.com/stable/ $ helm repo update # 部署kubedb $ helm install appscode/kubedb --namespace kube-system --name kubedb --version 0.12.0 # 创建部署mysql集群的命名空间 $ kubectl create ns demo # 创建MySQL类型的自定义资源，kubedb作为Controller会负责自动将MySQL Group Replication集群部署好 $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - --- apiVersion: catalog.kubedb.com/v1alpha1 kind: MySQLVersion metadata: name: \u0026#34;5.7.25\u0026#34; labels: app: kubedb spec: version: \u0026#34;5.7.25\u0026#34; db: image: \u0026#34;kubedb/mysql:5.7.25\u0026#34; exporter: image: \u0026#34;kubedb/mysqld-exporter:v0.11.0\u0026#34; tools: image: \u0026#34;kubedb/mysql-tools:5.7.25\u0026#34; podSecurityPolicies: databasePolicyName: \u0026#34;mysql-db\u0026#34; snapshotterPolicyName: \u0026#34;mysql-snapshot\u0026#34; initContainer: image: \u0026#34;kubedb/mysql-tools:5.7.25\u0026#34; --- apiVersion: v1 kind: ConfigMap metadata: name: my-custom-config namespace: demo data: my-config.cnf: | [mysqld] max_connections = 2048 read_buffer_size = 4194304 skip-name-resolve innodb_lru_scan_depth = 256 character-set-server = utf8mb4 collation-server = utf8mb4_general_ci --- apiVersion: kubedb.com/v1alpha1 kind: MySQL metadata: name: mysql namespace: demo spec: version: \u0026#34;5.7.25\u0026#34; replicas: 3 topology: mode: GroupReplication group: name: \u0026#34;dc002fc3-c412-4d18-b1d4-66c1fbfbbc9b\u0026#34; baseServerID: 100 storageType: Durable configSource: configMap: name: my-custom-config storage: storageClassName: \u0026#34;standard\u0026#34; accessModes: - ReadWriteOnce resources: requests: storage: 1Gi terminationPolicy: WipeOut updateStrategy: type: RollingUpdate --- EOF 实现MySQL集群透明访问 MySQL集群搭建好了，如何访问呢？kubedb的文档上有说明：\n# 首先找到mysql的root密码 $ kubectl get secrets -n demo mysql-auth -o jsonpath=\u0026#39;{.data.\\username}\u0026#39; | base64 -d root $ kubectl get secrets -n demo mysql-auth -o jsonpath=\u0026#39;{.data.\\password}\u0026#39; | base64 -d dlNiQpjULZvEqo3B # 读数据的话，连接3个member中任何一个mysql实例都可以 $ kubectl exec -it -n demo mysql-0 -- mysql -u root --password=dlNiQpjULZvEqo3B --host=mysql-0.mysql-gvr.demo -e \u0026#34;select 1;\u0026#34; mysql: [Warning] Using a password on the command line interface can be insecure. +---+ | 1 | +---+ | 1 | +---+ # 写数据的话，得先找到当前是master的mysql实例地址 $ kubectl exec -it -n demo mysql-0 -- mysql -u root --password=dlNiQpjULZvEqo3B --host=mysql-0.mysql-gvr.demo -e \u0026#34;SELECT MEMBER_HOST, MEMBER_PORT FROM performance_schema.replication_group_members WHERE MEMBER_ID = (SELECT VARIABLE_VALUE FROM performance_schema.global_status WHERE VARIABLE_NAME = \u0026#39;group_replication_primary_member\u0026#39;);\u0026#34; +------------------------------+-------------+ | MEMBER_HOST | MEMBER_PORT | +------------------------------+-------------+ | mysql-0.mysql-gvr.demo | 3306 | +------------------------------+-------------+ # 然后连接mysql的master实例地址进行数据库写操作 $ kubectl exec -it -n demo mysql-0 -- mysql -u root --password=dlNiQpjULZvEqo3B --host=mysql-0.mysql-gvr.demo -P 3306 -e \u0026#34;create database test;\u0026#34; 特别对于写操作，上层应用还得先找mysql的master实例地址后，操作才能进行下去，这样太难受了。\n这里我们可以使用MySQL Router方案来处理，这个在之前的博文里也讲到。不过在MySQL官方的方案里MySQL Router一般是作为应用的sidecar进行部署的。我这里想更集中式地部署，于是采用了业界广泛实践的ProxySQL方案。\n部署ProxySQL的过程如下：\n# 在MGR集群中创建检查MGR节点状态的函数和视图 $ curl -O https://raw.githubusercontent.com/lefred/mysql_gr_routing_check/master/addition_to_sys.sql $ kubectl cp addition_to_sys.sql demo/mysql-0:/tmp/ $ kubectl -n demo exec -ti mysql-0 -- mysql -uroot -pdlNiQpjULZvEqo3B --host=mysql-0.mysql-gvr.demo -P 3306 -e \u0026#39;source /tmp/addition_to_sys.sql\u0026#39; # 在MGR集群中创建监控用户并授权 $ kubectl -n demo exec -ti mysql-0 -- mysql -uroot -pdlNiQpjULZvEqo3B --host=mysql-0.mysql-gvr.demo -P 3306 -e \u0026#34;grant SELECT on sys.* to \u0026#39;proxymonitor\u0026#39;@\u0026#39;%\u0026#39; identified by \u0026#39;proxymonitor\u0026#39;;flush privileges;\u0026#34; # 在MGR集群中创建访问业务库的用户并授权 $ kubectl -n demo exec -ti mysql-0 -- mysql -uroot -pdlNiQpjULZvEqo3B --host=mysql-0.mysql-gvr.demo -P 3306 -e \u0026#34;grant all privileges on biz_db.* to \u0026#39;biz_user\u0026#39;@\u0026#39;%\u0026#39; identified by \u0026#39;bizpassword\u0026#39;;flush privileges;\u0026#34; # 借助proxysql-cluster项目提供的helm charts部署proxysql 3实例集群 $ git clone https://github.com/jeremyxu2010/proxysql-cluster.git $ docker build --rm -t severalnines/proxysql:1.4.16 -f ./proxysql-cluster/docker/Dockerfile ./proxysql-cluster/docker $ cat \u0026lt;\u0026lt; EOF \u0026gt; proxysql-values.yaml # Default admin username proxysql: admin: username: admin password: admin clusterAdmin: username: cluster1 password: secret1pass # 在proxysql中初始化MGR集群的相关信息 # 1. 向mysql_servers表插入MGR各member的地址信息，其中当前的master示例放入hostgroup 1中，所有示例放入hostgroup 2中 # 2. 向mysql_group_replication_hostgroups表插入proxysql使用hostgroup的规则 # * proxysql会导流写请求到writer_hostgroup，即hostgrup 1 # * proxysql会导流读请求到reader_hostgroup，即hostgrup 2 # * backup_writer_hostgroup的id为3 # * offline_hostgroup的id为4 # * active表明这条规则是生效的 # * max_writers表明最多只有一个writer，如果监测到多个实例是可写的，则只会将一个实例移入writer_hostgroup，其它实例会被移入backup_writer_hostgroup # * writer_is_also_reader表明可写实例也会被作为reader # * max_transactions_behind表明后端最大允许的事务数 # 3. 插入允许连接的帐户信息，注意要与MGR集群中的访问用户信息一致 # 4. 插入proxysql读写分离规则 additionConfig: | mysql_servers = ( { address=\u0026#34;mysql-0.mysql-gvr\u0026#34;, port=3306 , hostgroup=1, max_connections=2048 }, { address=\u0026#34;mysql-0.mysql-gvr\u0026#34;, port=3306 , hostgroup=2, max_connections=2048 }, { address=\u0026#34;mysql-1.mysql-gvr\u0026#34;, port=3306 , hostgroup=2, max_connections=2048 }, { address=\u0026#34;mysql-2.mysql-gvr\u0026#34;, port=3306 , hostgroup=2, max_connections=2048 } ) mysql_group_replication_hostgroups = ( { writer_hostgroup=1 , backup_writer_hostgroup=3, reader_hostgroup=2, offline_hostgroup=4, active=1, max_writers=1, writer_is_also_reader=1, max_transactions_behind=100 } ) mysql_users = ( { username = \u0026#34;biz_user\u0026#34; , password = \u0026#34;bizpassword\u0026#34; , default_hostgroup = 1 , active = 1 } ) mysql_query_rules = ( { rule_id=100 active=1 match_pattern=\u0026#34;^SELECT .* FOR UPDATE\u0026#34; destination_hostgroup=1 apply=1 }, { rule_id=200 active=1 match_pattern=\u0026#34;^SELECT .*\u0026#34; destination_hostgroup=2 apply=1 }, { rule_id=300 active=1 match_pattern=\u0026#34;.*\u0026#34; destination_hostgroup=1 apply=1 } ) # MySQL Settings mysql: # This is the monitor user, just needs usage rights on the databases monitor: username: proxymonitor password: proxymonitor admin: username: root password: dlNiQpjULZvEqo3B EOF $ helm install --namespace demo --name proxysql ./proxysql-cluster/ -f proxysql-values.yaml 这里在部署时遇到了一些小波折，最开始是使用k8s-proxysql-cluster部署一套proxysql集群，并到其中手动初始化MGR集群相关信息的。但后来遇到了一系列问题：\n proxysql的配置信息未保存到pv中，这个导致某个proxysql实例重启后，proxysql集群中的MGR信息完全丢失。 某个proxysql实例pod被重新调度后，其ip地址发生变化，proxysql集群便会处于不健康状况。  为了解决上述问题，直接新写了一个部署proxysql集群的helm chart，其采用config file的方式初始化MGR集群信息，同时proxysql_servers中不再使用IP，而是使用固定的服务。经测试通过该方式部署的proxysql集群运行得十分稳定。\n业务访问MySQL 像上面那样部署了MySQL Group Replication集群和ProxySQL集群后，业务方访问MySQL服务就很轻松了：\n# 容器内 $ mysql -ubiz_user -pbizpassword -hproxysql-proxysql-cluste.demo -P3306 biz_db -e \u0026#34;select 1;\u0026#34; # 容器外，先得到k8s svc的clusterIP $ kubectl -n demo get svc proxysql-proxysql-cluste -o=jsonpath=\u0026#39;{.spec.clusterIP}\u0026#39; 10.68.63.23 # 然后也是直接连接 $ mysql -ubiz_user -pbizpassword -h10.68.63.23 -P3306 biz_db -e \u0026#34;select 1;\u0026#34; done\n参考  https://kubedb.com/docs/0.12.0/guides/mysql/clustering/overview/ https://kubedb.com/docs/0.12.0/guides/mysql/clustering/group_replication_single_primary/ https://kubedb.com/docs/0.12.0/concepts/databases/mysql/ https://www.xuejiayuan.net/blog/ea021ec24ac240db8665f0299dbb0667 https://blog.frognew.com/2017/08/proxysql-1.4-and-mysql-group-replication.html https://dev.mysql.com/doc/mysql-router/8.0/en/mysql-router-general-using-deploying.html https://github.com/sysown/proxysql/wiki/Configuring-ProxySQL http://fuxkdb.com/2018/08/25/%E5%A6%82%E4%BD%95%E7%A1%AE%E5%AE%9ASingle-Primary%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84MGR%E4%B8%BB%E8%8A%82%E7%82%B9/  ","permalink":"https://jeremyxu2010.github.io/2019/08/kubernetes%E4%B8%AD%E9%83%A8%E7%BD%B2mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/","tags":["kubernetes","mysql","proxysql","helm"],"title":"kubernetes中部署mysql高可用集群"},{"categories":["golang开发"],"contents":"偶然在github上看到paho.mqtt.golang项目，这是一个MQTT客户端库，进而花了些时间学习下时下十分火热的MQTT协议。\nMQTT简述  MQTT（Message Queuing Telemetry Transport，消息队列遥测传输协议），是一种基于发布/订阅（publish/subscribe）模式的\u0026quot;轻量级\u0026quot;通讯协议，该协议构建于TCP/IP协议上，由IBM在1999年发布。MQTT最大优点在于，可以以极少的代码和有限的带宽，为连接远程设备提供实时可靠的消息服务。作为一种低开销、低带宽占用的即时通讯协议，使其在物联网、小型设备、移动应用等方面有较广泛的应用。\nMQTT是一个基于客户端-服务器的消息发布/订阅传输协议。MQTT协议是轻量、简单、开放和易于实现的，这些特点使它适用范围非常广泛。在很多情况下，包括受限的环境中，如：机器与机器（M2M）通信和物联网（IoT）。其在，通过卫星链路通信传感器、偶尔拨号的医疗设备、智能家居、及一些小型化设备中已广泛使用。\n 从上面的架构图来看，MQTT其实跟传统的MQ很像，也是消息队列。但MQTT协议工作在低带宽、不可靠的网络的远程传感器和控制设备通讯而设计的协议，跟传统MQ相比，它具有以下主要的几项特性：\n （1）使用发布/订阅消息模式，提供一对多的消息发布，解除应用程序耦合。\n这一点很类似于XMPP，但是MQTT的信息冗余远小于XMPP，,因为XMPP使用XML格式文本来传递数据。\n（2）对负载内容屏蔽的消息传输。\n（3）使用TCP/IP提供网络连接。\n主流的MQTT是基于TCP连接进行数据推送的，但是同样有基于UDP的版本，叫做MQTT-SN。这两种版本由于基于不同的连接方式，优缺点自然也就各有不同了。\n（4）有三种消息发布服务质量：\n\u0026ldquo;至多一次\u0026rdquo;，消息发布完全依赖底层TCP/IP网络。会发生消息丢失或重复。这一级别可用于如下情况，环境传感器数据，丢失一次读记录无所谓，因为不久后还会有第二次发送。这一种方式主要普通APP的推送，倘若你的智能设备在消息推送时未联网，推送过去没收到，再次联网也就收不到了。\n\u0026ldquo;至少一次\u0026rdquo;，确保消息到达，但消息重复可能会发生。\n\u0026ldquo;只有一次\u0026rdquo;，确保消息到达一次。在一些要求比较严格的计费系统中，可以使用此级别。在计费系统中，消息重复或丢失会导致不正确的结果。这种最高质量的消息发布服务还可以用于即时通讯类的APP的推送，确保用户收到且只会收到一次。\n（5）小型传输，开销很小（固定长度的头部是2字节），协议交换最小化，以降低网络流量。\n这就是为什么在介绍里说它非常适合\u0026quot;在物联网领域，传感器与服务器的通信，信息的收集\u0026rdquo;，要知道嵌入式设备的运算能力和带宽都相对薄弱，使用这种协议来传递消息再适合不过了。\n（6）使用Last Will和Testament特性通知有关各方客户端异常中断的机制。\nLast Will：即遗言机制，用于通知同一主题下的其他设备发送遗言的设备已经断开了连接。\nTestament：遗嘱机制，功能类似于Last Will。\n 快速入门 可以运行以下命令快速体验一下MQTT的功能。\n克隆示例项目：\ngit clone https://github.com/jeremyxu2010/demo-mqtt.git cd demo-mqtt 运行MQTT Broker：\nmake start-mqtt-broker 运行MQTT Client：\nmake start-mqtt-client MQTT协议原理 本节内容基本是从https://www.runoob.com/w3cnote/mqtt-intro.html得来，这里将MQTT协议的核心概念介绍得比较清楚了，对照下https://github.com/jeremyxu2010/demo-mqtt/blob/master/cmd/simple.go的源码可以理解得更深刻。\nMQTT协议实现方式 实现MQTT协议需要客户端和服务器端通讯完成，在通讯过程中，MQTT协议中有三种身份：发布者（Publish）、代理（Broker）（服务器）、订阅者（Subscribe）。其中，消息的发布者和订阅者都是客户端，消息代理是服务器，消息发布者可以同时是订阅者。\nMQTT传输的消息分为：主题（Topic）和负载（payload）两部分：\n Topic，可以理解为消息的类型，订阅者订阅（Subscribe）后，就会收到该主题的消息内容（payload）； payload，可以理解为消息的内容，是指订阅者具体要使用的内容。  网络传输与应用消息 MQTT会构建底层网络传输：它将建立客户端到服务器的连接，提供两者之间的一个有序的、无损的、基于字节流的双向传输。\n当应用数据通过MQTT网络发送时，MQTT会把与之相关的服务质量（QoS）和主题名（Topic）相关连。\nMQTT客户端 一个使用MQTT协议的应用程序或者设备，它总是建立到服务器的网络连接。客户端可以：\n 发布其他客户端可能会订阅的信息； 订阅其它客户端发布的消息； 退订或删除应用程序的消息； 断开与服务器连接。  MQTT服务器 MQTT服务器以称为\u0026quot;消息代理\u0026rdquo;（Broker），可以是一个应用程序或一台设备。它是位于消息发布者和订阅者之间，它可以：\n 接受来自客户的网络连接； 接受客户发布的应用信息； 处理来自客户端的订阅和退订请求； 向订阅的客户转发应用程序消息。  MQTT协议中的订阅、主题、会话 订阅（Subscription） 订阅包含主题筛选器（Topic Filter）和最大服务质量（QoS）。订阅会与一个会话（Session）关联。一个会话可以包含多个订阅。每一个会话中的每个订阅都有一个不同的主题筛选器。\n会话（Session） 每个客户端与服务器建立连接后就是一个会话，客户端和服务器之间有状态交互。会话存在于一个网络之间，也可能在客户端和服务器之间跨越多个连续的网络连接。\n主题名（Topic Name） 连接到一个应用程序消息的标签，该标签与服务器的订阅相匹配。服务器会将消息发送给订阅所匹配标签的每个客户端。\n主题筛选器（Topic Filter） 一个对主题名通配符筛选器，在订阅表达式中使用，表示订阅所匹配到的多个主题。\n负载（Payload） 消息订阅者所具体接收的内容。\nMQTT协议中的方法 MQTT协议中定义了一些方法（也被称为动作），来于表示对确定资源所进行操作。这个资源可以代表预先存在的数据或动态生成数据，这取决于服务器的实现。通常来说，资源指服务器上的文件或输出。主要方法有：\n Connect。等待与服务器建立连接。 Disconnect。等待MQTT客户端完成所做的工作，并与服务器断开TCP/IP会话。 Subscribe。等待完成订阅。 UnSubscribe。等待服务器取消客户端的一个或多个topics订阅。 Publish。MQTT客户端发送消息请求，发送完成后返回应用程序线程。  MQTT协议数据包分析 要分析一个协议，最重要的是分析其协议数据包的格式及含义。MQTT控制报文格式已有很详细的文档。 MQTT协议也算是比较简单的，只有14种类型的控制报文，每种控制报文发挥的作用及细节参见这里。\n按我的经验，这样直接看协议文档很难理解清楚。更好的办法抓包，并结合文档分析理解。抓包分析方法如下：\n# 启动tcpdump进行抓包，程序运行完毕后按Ctrl+C结束抓包 make dump-mqtt-packet # 运行MQTT Broker make start-mqtt-broker # 运行MQTT Client： make start-mqtt-client 将得到的dmp文件，使用wireshark打开，再用mqtt协议过滤规则过滤一下，就可以很清楚地看到MQTT的数据包了，如下图：\n大概看了下各类型的数据包，果然是相当的精练，基本找不到信息冗余。\n看看英文原版的协议规范也挺好的。\nMQTT Broker的选择 对Go语言熟悉一点，同时为了学习下高性能网络编程的代码，本示例使用的是一款高性能的MQTT Brokervolantmq，其具备的功能已经很丰富了，应付大部分应用场景也够用。\n但面对大量设备的IoT场景，还是要考虑集群形式部署以进行水平扩展，参考wikipedia上的对比资料，还是比较推荐emqx和vernemq。这个两个都是利用Erlang/OTP实现的，看来在消息队列领域Erlang/OTP果真是利器啊！Erlang/OTP分布式编程的简要说明可以参考这里。\nMQTT Client Library的选择 本示例使用的是github上star数较多的MQTT Golang Client Library - paho.mqtt.golang，其使用还是很方便的，源码里还有一些高级用法的示例。\n这里还有一个比较全面的Client Library列表，可以根据所使用的语言，Client Library支持的特性、成熟度等因素找到一款适合的Client Library。\nMQTT使用上一些特殊玩法 MQTT基于主题(Topic)消息路由 MQTT协议基于主题(Topic)进行消息路由，主题(Topic)类似URL路径，例如:\nchat/room/1 sensor/10/temperature sensor/+/temperature $SYS/broker/metrics/packets/received $SYS/broker/metrics/# 主题(Topic)通过/分割层级，支持+, #通配符:\n`+`: 表示通配一个层级，例如a/+，匹配a/x, a/y `#`: 表示通配多个层级，例如a/#，匹配a/x, a/b/c/d 订阅者与发布者之间通过主题路由消息进行通信，例如采用mosquitto命令行发布订阅消息:\nmosquitto_sub -t a/b/+ -q 1 mosquitto_pub -t a/b/c -m hello -q 1 订阅者可以订阅含通配符主题，但发布者不允许向含通配符主题发布消息。\nMQTT消息QoS MQTT发布消息QoS保证不是端到端的，是客户端与服务器之间的。订阅者收到MQTT消息的QoS级别，最终取决于发布消息的QoS和主题订阅的QoS，简单说就是发布消息的QoS和主题订阅的QoS两者间的较小值。\nQos0消息发布订阅\nQos1消息发布订阅\nQos2消息发布订阅\n可以看到为了满足越来越高的QoS，消息传递过程增加了很多保障性的控制指令。\nMQTT会话自动销毁 MQTT客户端向服务器发起CONNECT请求时，可以通过Clean Session标志设置会话。\nClean Session设置为0，表示创建一个持久会话，在客户端断开连接时，会话仍然保持并保存离线消息，直到会话超时注销。\nClean Session设置为1，表示创建一个新的临时会话，在客户端断开时，会话自动销毁。\nMQTT遗愿消息 MQTT客户端向服务器端CONNECT请求时，可以设置是否发送遗愿消息(Will Message)标志，和遗愿消息主题(Topic)与内容(Payload)。\nMQTT客户端异常下线时(客户端断开前未向服务器发送DISCONNECT消息)，MQTT消息服务器会发布遗愿消息。\nMQTT保留消息 MQTT客户端向服务器发布(PUBLISH)消息时，可以设置保留消息(Retained Message)标志。保留消息(Retained Message)会驻留在消息服务器，后来的订阅者订阅主题时仍可以接收该消息。\n例如mosquitto命令行发布一条保留消息到主题’a/b/c’:\nmosquitto_pub -r -q 1 -t a/b/c -m 'hello' 之后连接上来的MQTT客户端订阅主题’a/b/c’时候，仍可收到该消息:\n$ mosquitto_sub -t a/b/c -q 1 hello 保留消息(Retained Message)有两种清除方式:\n 客户端向有保留消息的主题发布一个空消息:  mosquitto_pub -r -q 1 -t a/b/c -m ''  设置消息服务器的保留消息超期时间  参考  https://www.runoob.com/w3cnote/mqtt-intro.html https://legacy.gitbook.com/book/mcxiaoke/mqtt-cn http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html https://en.wikipedia.org/wiki/Comparison_of_MQTT_implementations https://docs.emqx.io/broker/v3/cn/cluster.html#erlang-otp http://erlang.org/doc/reference_manual/distributed.html https://kknews.cc/tech/ejya48q.html  ","permalink":"https://jeremyxu2010.github.io/2019/08/mqtt%E5%8D%8F%E8%AE%AE%E5%AD%A6%E4%B9%A0/","tags":["mqtt","golang","IoT"],"title":"MQTT协议学习"},{"categories":["容器编排"],"contents":"问题 使用kong的chart，在kubernetes集群默认安装出来kong的容器是监听8000和8443端口的，而为了让外部以80和443端口访问kong这个API网关，一般会使用kubernetes的service proxy技术或外部load balancer将流量反向代理到kong。能否直接让kong直接监听80和443端口，从而避免反向代理的网络开销，这里进行一些尝试。\n解决过程 修改kubernetes清单文件 原来用kong的chart安装出来的kong其关键yaml如下：\n--- apiVersion: v1 kind: Service metadata: name: kong-kong-proxy labels: app: kong chart: \u0026#34;kong-0.15.1\u0026#34; release: \u0026#34;kong\u0026#34; heritage: \u0026#34;Tiller\u0026#34; spec: type: NodePort ports: - name: kong-proxy port: 80 targetPort: 8000 protocol: TCP - name: kong-proxy-tls port: 443 targetPort: 8443 protocol: TCP selector: app: kong release: kong component: app --- apiVersion: apps/v1beta2 kind: Deployment metadata: name: \u0026#34;kong-kong\u0026#34; labels: app: \u0026#34;kong\u0026#34; chart: \u0026#34;kong-0.15.1\u0026#34; release: \u0026#34;kong\u0026#34; heritage: \u0026#34;Tiller\u0026#34; component: app spec: replicas: 1 selector: matchLabels: app: kong release: kong component: app template: metadata: labels: app: kong release: kong component: app spec: initContainers: - name: wait-for-db image: \u0026#34;kong:1.2.2\u0026#34; imagePullPolicy: IfNotPresent env: - name: KONG_PG_HOST value: kong-postgresql - name: KONG_PG_PORT value: \u0026#34;5432\u0026#34; - name: KONG_PG_PASSWORD valueFrom: secretKeyRef: name: kong-postgresql key: postgresql-password - name: KONG_ADMIN_ACCESS_LOG value: \u0026#34;/dev/stdout\u0026#34; - name: KONG_ADMIN_ERROR_LOG value: \u0026#34;/dev/stderr\u0026#34; - name: KONG_ADMIN_GUI_ACCESS_LOG value: \u0026#34;/dev/stdout\u0026#34; - name: KONG_ADMIN_GUI_ERROR_LOG value: \u0026#34;/dev/stderr\u0026#34; - name: KONG_DATABASE value: \u0026#34;postgres\u0026#34; - name: KONG_PORTAL_API_ACCESS_LOG value: \u0026#34;/dev/stdout\u0026#34; - name: KONG_PORTAL_API_ERROR_LOG value: \u0026#34;/dev/stderr\u0026#34; - name: KONG_PROXY_ACCESS_LOG value: \u0026#34;/dev/stdout\u0026#34; - name: KONG_PROXY_ERROR_LOG value: \u0026#34;/dev/stderr\u0026#34; command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;until kong start; do echo \u0026#39;waiting for db\u0026#39;; sleep 1; done; kong stop\u0026#34; ] containers: - name: kong image: \u0026#34;kong:1.2.2\u0026#34; imagePullPolicy: IfNotPresent env: - name: KONG_ADMIN_LISTEN value: \u0026#34;0.0.0.0:8444 ssl\u0026#34; - name: KONG_PROXY_LISTEN value: 0.0.0.0:8000,0.0.0.0:8443 ssl - name: KONG_NGINX_DAEMON value: \u0026#34;off\u0026#34; - name: KONG_NGINX_HTTP_INCLUDE value: /kong/servers.conf - name: KONG_PG_HOST value: kong-postgresql - name: KONG_PG_PORT value: \u0026#34;5432\u0026#34; - name: KONG_PG_PASSWORD valueFrom: secretKeyRef: name: kong-postgresql key: postgresql-password - name: KONG_ADMIN_ACCESS_LOG value: \u0026#34;/dev/stdout\u0026#34; - name: KONG_ADMIN_ERROR_LOG value: \u0026#34;/dev/stderr\u0026#34; - name: KONG_ADMIN_GUI_ACCESS_LOG value: \u0026#34;/dev/stdout\u0026#34; - name: KONG_ADMIN_GUI_ERROR_LOG value: \u0026#34;/dev/stderr\u0026#34; - name: KONG_DATABASE value: \u0026#34;postgres\u0026#34; - name: KONG_PORTAL_API_ACCESS_LOG value: \u0026#34;/dev/stdout\u0026#34; - name: KONG_PORTAL_API_ERROR_LOG value: \u0026#34;/dev/stderr\u0026#34; - name: KONG_PROXY_ACCESS_LOG value: \u0026#34;/dev/stdout\u0026#34; - name: KONG_PROXY_ERROR_LOG value: \u0026#34;/dev/stderr\u0026#34; ports: - name: admin containerPort: 8444 protocol: TCP - name: proxy containerPort: 8000 protocol: TCP - name: proxy-tls containerPort: 8443 protocol: TCP - name: metrics containerPort: 9542 protocol: TCP volumeMounts: - name: custom-nginx-template-volume mountPath: /kong readinessProbe: failureThreshold: 5 httpGet: path: /status port: admin scheme: HTTPS initialDelaySeconds: 30 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 livenessProbe: failureThreshold: 5 httpGet: path: /status port: admin scheme: HTTPS initialDelaySeconds: 30 periodSeconds: 30 successThreshold: 1 timeoutSeconds: 5 resources: {} tolerations: [] volumes: - name: custom-nginx-template-volume configMap: name: kong-kong-default-custom-server-blocks --- 可以看到定义了一个Deployment，并用Service将pod提供的服务暴露出来了。其中kong这个容器有一个环境变量KONG_PROXY_LISTEN，其值为0.0.0.0:8000,0.0.0.0:8443 ssl，说明容器会监听8000和8443端口。看到这里很自然想到直接修改KONG_PROXY_LISTEN这个环境变量，pod直接使用hostNetwork，这样就很可以很轻松地让kong监听node节点上的80和443端口，修改成的yaml文件如下：\n--- apiVersion: apps/v1 kind: DaemonSet metadata: name: \u0026#34;kong-kong\u0026#34; labels: app: \u0026#34;kong\u0026#34; chart: \u0026#34;kong-0.15.1\u0026#34; release: \u0026#34;kong\u0026#34; heritage: \u0026#34;Tiller\u0026#34; component: app spec: selector: matchLabels: app: kong release: kong component: app template: metadata: labels: app: kong release: kong component: app spec: initContainers: - name: wait-for-db image: \u0026#34;kong:1.2.2\u0026#34; imagePullPolicy: IfNotPresent env: - name: KONG_PG_HOST value: kong-postgresql - name: KONG_PG_PORT value: \u0026#34;5432\u0026#34; - name: KONG_PG_PASSWORD valueFrom: secretKeyRef: name: kong-postgresql key: postgresql-password - name: KONG_ADMIN_ACCESS_LOG value: \u0026#34;/dev/stdout\u0026#34; - name: KONG_ADMIN_ERROR_LOG value: \u0026#34;/dev/stderr\u0026#34; - name: KONG_ADMIN_GUI_ACCESS_LOG value: \u0026#34;/dev/stdout\u0026#34; - name: KONG_ADMIN_GUI_ERROR_LOG value: \u0026#34;/dev/stderr\u0026#34; - name: KONG_DATABASE value: \u0026#34;postgres\u0026#34; - name: KONG_PORTAL_API_ACCESS_LOG value: \u0026#34;/dev/stdout\u0026#34; - name: KONG_PORTAL_API_ERROR_LOG value: \u0026#34;/dev/stderr\u0026#34; - name: KONG_PROXY_ACCESS_LOG value: \u0026#34;/dev/stdout\u0026#34; - name: KONG_PROXY_ERROR_LOG value: \u0026#34;/dev/stderr\u0026#34; command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;until kong start; do echo \u0026#39;waiting for db\u0026#39;; sleep 1; done; kong stop\u0026#34; ] containers: - name: kong image: \u0026#34;kong:1.2.2\u0026#34; imagePullPolicy: IfNotPresent env: - name: KONG_ADMIN_LISTEN value: \u0026#34;0.0.0.0:8444 ssl\u0026#34; - name: KONG_PROXY_LISTEN value: 0.0.0.0:80,0.0.0.0:443 ssl - name: KONG_NGINX_DAEMON value: \u0026#34;off\u0026#34; - name: KONG_NGINX_HTTP_INCLUDE value: /kong/servers.conf - name: KONG_PG_HOST value: kong-postgresql - name: KONG_PG_PORT value: \u0026#34;5432\u0026#34; - name: KONG_PG_PASSWORD valueFrom: secretKeyRef: name: kong-postgresql key: postgresql-password - name: KONG_ADMIN_ACCESS_LOG value: \u0026#34;/dev/stdout\u0026#34; - name: KONG_ADMIN_ERROR_LOG value: \u0026#34;/dev/stderr\u0026#34; - name: KONG_ADMIN_GUI_ACCESS_LOG value: \u0026#34;/dev/stdout\u0026#34; - name: KONG_ADMIN_GUI_ERROR_LOG value: \u0026#34;/dev/stderr\u0026#34; - name: KONG_DATABASE value: \u0026#34;postgres\u0026#34; - name: KONG_PORTAL_API_ACCESS_LOG value: \u0026#34;/dev/stdout\u0026#34; - name: KONG_PORTAL_API_ERROR_LOG value: \u0026#34;/dev/stderr\u0026#34; - name: KONG_PROXY_ACCESS_LOG value: \u0026#34;/dev/stdout\u0026#34; - name: KONG_PROXY_ERROR_LOG value: \u0026#34;/dev/stderr\u0026#34; ports: - name: admin containerPort: 8444 protocol: TCP - name: proxy containerPort: 80 protocol: TCP - name: proxy-tls containerPort: 443 protocol: TCP - name: metrics containerPort: 9542 protocol: TCP volumeMounts: - name: custom-nginx-template-volume mountPath: /kong readinessProbe: failureThreshold: 5 httpGet: path: /status port: admin scheme: HTTPS initialDelaySeconds: 30 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 livenessProbe: failureThreshold: 5 httpGet: path: /status port: admin scheme: HTTPS initialDelaySeconds: 30 periodSeconds: 30 successThreshold: 1 timeoutSeconds: 5 resources: {} hostNetwork: true dnsPolicy: ClusterFirstWithHostNet tolerations: [] volumes: - name: custom-nginx-template-volume configMap: name: kong-kong-default-custom-server-blocks --- 主要做了以下改动：\n 将Deployment修改为了DaemonSet 删除了Service 将8000端口修改为了80，8443端口修改为了443 配置了hostNetwork为ture，dnsPolicy为ClusterFirstWithHostNet。因为kong这个pod还引用了kubernetes里的kong-postgresql服务名，以了让DaemonSet的pod能正确解析kong-postgresql服务名，必须设置使用了hostNetwork的pod其 dnsPolicy为ClusterFirstWithHostNet，见这里的说明。  kubernetes清单文件修改完毕后，我们将之部署进kubernetes测试一下，结果pod报错：\n2019/08/17 12:59:06 [emerg] 1#0: bind() to 0.0.0.0:80 failed (13: Permission denied) nginx: [emerg] bind() to 0.0.0.0:80 failed (13: Permission denied) 授予合适的Linux capabilities 从上面的报错来看，是说没有足够的权限监听80端口，应该是没有绑定1024以下特权端口的权限。查阅下Linux capabilities，得知得添加CAP_NET_BIND_SERVICE的capability，程序才能绑定1024以下特权端口。于是参考kubernetes的SecurityContext的文档，我给pod配置上合适的Linux capabilities。\n  CAP_NET_BIND_SERVICE Bind a socket to Internet domain privileged ports (port numbers less than 1024).   podSpec: ...... securityContext: capabilities: add: - NET_BIND_SERVICE 再将部署pod，发现问题依旧。\n通过kubernetes的SecurityContext还可以设置很多pod安全相关的设置，以后在工作中可以多实践下。\n分析kong的启动过程 已经添加了合适的Linux capabilities，竟然还不能正常监听80和443，看来问题并不是这儿。接下来我分析下kong镜像中kong进程的启动过程。\n找到kong镜像的Dockerfile\nFROMalpine:3.10LABEL maintainer=\u0026#34;Kong Core Team \u0026lt;team-core@konghq.com\u0026gt;\u0026#34;ENV KONG_VERSION 1.2.2ENV KONG_SHA256 76183d7e8ff084c86767b917da441001d0d779d35fa2464275b74226029a46bfRUN adduser -Su 1337 kong \\ \t\u0026amp;\u0026amp; mkdir -p \u0026#34;/usr/local/kong\u0026#34; \\ \t\u0026amp;\u0026amp; apk add --no-cache --virtual .build-deps wget tar ca-certificates \\ \t\u0026amp;\u0026amp; apk add --no-cache libgcc openssl pcre perl tzdata curl libcap su-exec zip \\ \t\u0026amp;\u0026amp; wget -O kong.tar.gz \u0026#34;https://bintray.com/kong/kong-alpine-tar/download_file?file_path=kong-$KONG_VERSION.apk.tar.gz\u0026#34; \\ \t\u0026amp;\u0026amp; echo \u0026#34;$KONG_SHA256*kong.tar.gz\u0026#34; | sha256sum -c - \\ \t\u0026amp;\u0026amp; tar -xzf kong.tar.gz -C /tmp \\ \t\u0026amp;\u0026amp; rm -f kong.tar.gz \\ \t\u0026amp;\u0026amp; cp -R /tmp/usr / \\ \t\u0026amp;\u0026amp; rm -rf /tmp/usr \\ \t\u0026amp;\u0026amp; cp -R /tmp/etc / \\ \t\u0026amp;\u0026amp; rm -rf /tmp/etc \\ \t\u0026amp;\u0026amp; apk del .build-deps \\ \t\u0026amp;\u0026amp; chown -R kong:0 /usr/local/kong \\ \t\u0026amp;\u0026amp; chmod -R g=u /usr/local/kongCOPY docker-entrypoint.sh /docker-entrypoint.shENTRYPOINT [\u0026#34;/docker-entrypoint.sh\u0026#34;]EXPOSE8000 8443 8001 8444STOPSIGNALSIGQUITCMD [\u0026#34;kong\u0026#34;, \u0026#34;docker-start\u0026#34;]这个镜像的构建过程很简单，逻辑如下：\n 创建kong用户 安装kong的程序 将docker-entrypoint.sh启动脚本拷贝到镜像里 设置ENTRYPOINT及CMD  再看一看docker-entrypoint.sh启动脚本\n#!/bin/sh set -e export KONG_NGINX_DAEMON=off has_transparent() { echo \u0026#34;$1\u0026#34; | grep -E \u0026#34;[^\\s,]+\\s+transparent\\b\u0026#34; \u0026gt;/dev/null } if [[ \u0026#34;$1\u0026#34; == \u0026#34;kong\u0026#34; ]]; then PREFIX=${KONG_PREFIX:=/usr/local/kong} if [[ \u0026#34;$2\u0026#34; == \u0026#34;docker-start\u0026#34; ]]; then shift 2 kong prepare -p \u0026#34;$PREFIX\u0026#34; \u0026#34;$@\u0026#34; # workaround for https://github.com/moby/moby/issues/31243 chmod o+w /proc/self/fd/1 || true chmod o+w /proc/self/fd/2 || true if [ \u0026#34;$(id -u)\u0026#34; != \u0026#34;0\u0026#34; ]; then exec /usr/local/openresty/nginx/sbin/nginx \\  -p \u0026#34;$PREFIX\u0026#34; \\  -c nginx.conf else if [ ! -z ${SET_CAP_NET_RAW} ] \\  || has_transparent \u0026#34;$KONG_STREAM_LISTEN\u0026#34; \\  || has_transparent \u0026#34;$KONG_PROXY_LISTEN\u0026#34; \\  || has_transparent \u0026#34;$KONG_ADMIN_LISTEN\u0026#34;; then setcap cap_net_raw=+ep /usr/local/openresty/nginx/sbin/nginx fi chown -R kong:0 /usr/local/kong exec su-exec kong /usr/local/openresty/nginx/sbin/nginx \\  -p \u0026#34;$PREFIX\u0026#34; \\  -c nginx.conf fi fi fi exec \u0026#34;$@\u0026#34; 分析上面的脚本，可以知道，如果以root启动镜像，脚本里最终会以kong用户启动/usr/local/openresty/nginx/sbin/nginx二进制程序。\nexec su-exec kong /usr/local/openresty/nginx/sbin/nginx \\ -p \u0026quot;$PREFIX\u0026quot; \\ -c nginx.conf 即然是普通用户kong运行的程序，自然无法正常监听1024以下的端口。\n使用setcap给二进制提权 这时我会问了，为啥安装了apache，以www用户运行apache的二进制程序，为啥又可以监听80端口呢？\n查阅文档，我们知道有两种办法让普通用户执行二进制程序时：\n 使用chmod设置setuid位，这样一个可执行文件启动时，它不会以启动它的用户的权限运行，而是以该文件所有者的权限运行，参见这里。 另一种方法是使用setcap给二进制文件添加必要的Linux capabilities，参见这里。  一般会采用方法2，这样二进制文件的权限更受控一点。于是我在docker-entrypoint.sh里使用setcap命令给二进制文件添加必要的Linux capabilities。\nsetcap cap_net_bind_service=+eip /usr/local/openresty/nginx/sbin/nginx 至此，使用kong的docker镜像，容器本身终于可以监听80和443端口了。\n更优雅的处理方案 问题终于解决了，偶然在kong的开源端点上发现有人为解决该问题，发了一个PR，看PR的代码，是通过判断一个环境变量来决定是否调用setcap命令的，而且还考虑了setcap作用被覆盖的场景，处理方案明显更优雅。\ncaps=\u0026#34;\u0026#34; if [ -n \u0026#34;${SET_CAP_NET_RAW}\u0026#34; ] \\  || has_transparent \u0026#34;$KONG_STREAM_LISTEN\u0026#34; \\  || has_transparent \u0026#34;$KONG_PROXY_LISTEN\u0026#34; \\  || has_transparent \u0026#34;$KONG_ADMIN_LISTEN\u0026#34;; then caps=\u0026#34;${caps:+\u0026#34;${caps}\u0026#34;,}cap_net_raw\u0026#34; fi if [ -n \u0026#34;${SET_CAP_NET_BIND_SERVICE}\u0026#34; ] ; then caps=\u0026#34;${caps:+\u0026#34;${caps}\u0026#34;,}cap_net_bind_service\u0026#34; fi if [ -n \u0026#34;${caps}\u0026#34; ] ; then setcap \u0026#34;${caps}=+ep\u0026#34; /usr/local/openresty/nginx/sbin/nginx fi 这个PR不知原因一直没有合进主干。\n看man文档的技巧 有时看到一个命令的用法，觉得很陌生，可以使用man命令快速学习一下。比如：\n$ man setcap NAME setcap - set file capabilities SYNOPSIS setcap [-q] [-v] (capabilities|-|-r) filename [ ... capabilitiesN fileN ] DESCRIPTION In the absence of the -v (verify) option setcap sets the capabilities of each specified filename to the capabilities specified. The -v option is used to verify that the specified capabilities are currently associated with the file. The capabilities are specified in the form described in cap_from_text(3). # 上述文档中说capabilities的格式在cap_from_text(3)里进行说明 # 找不到cap_from_text(3)的man文档 $ man 3 cap_from_text No manual entry for cap_from_text in section 3 # 安装man-pages、man-db $ yum install -y man-pages $ yum install -y man-db $ mandb # 还是找不到cap_from_text(3)的man文档 $ man 3 cap_from_text No manual entry for cap_from_text in section 3 # 搜索下cap_from_text的man文档在哪个rpm包里，下面的输出说明在libcap-devel这个rpm包里 $ yum whatprovides */cap_from_text.*.gz libcap-devel-2.22-9.el7.i686 : Development files for libcap Repo : os Matched from: Filename : /usr/share/man/man3/cap_from_text.3.gz libcap-devel-2.22-9.el7.x86_64 : Development files for libcap Repo : os Matched from: Filename : /usr/share/man/man3/cap_from_text.3.gz # 安装libcap-devel软件包 $ yum install -y libcap-devel # 这次终于可以查寻到cap_from_text(3)的man文档，看了man文档，总算知道setcap命令里+eip是什么意思了 $ man 3 cap_from_text ...... Each clause consists of a list of comma-separated capability names (or the word `all\u0026#39;), followed by an action-list. An action-list consists of a sequence of operator flag pairs. Legal operators are: `=\u0026#39;, \u0026#39;+\u0026#39;, and `-\u0026#39;. Legal flags are: `e\u0026#39;, `i\u0026#39;, and `p\u0026#39;. These flags are case- sensitive and specify the Effective, Inheritable and Permitted sets respectively. ...... 总结 这个权限问题虽说是个小问题，但在解决的过程中查阅了kubernetes的SecurityContext的相关资料、Linux capabilities的概念及用法、分析了kong的启动过程，整个过程学到了挺多东西，还挺有意思的。\n参考  https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy http://man7.org/linux/man-pages/man7/capabilities.7.html https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container https://linux.cn/article-9355-1.html https://www.cnblogs.com/nf01/articles/10418141.html https://github.com/Kong/docker-kong/pull/213 https://codingbee.net/centos/man-how-to-fix-the-no-manual-entry-for [https://luanlengli.github.io/2019/07/05/%E9%80%9A%E8%BF%87Linux%20capabilities%E6%9C%BA%E5%88%B6%E8%AE%A9kong%E7%9B%91%E5%90%AC80%E5%92%8C443%E7%AB%AF%E5%8F%A3.html](https://luanlengli.github.io/2019/07/05/通过Linux capabilities机制让kong监听80和443端口.html) https://docs.konghq.com/1.2.x/configuration/#proxy_listen  ","permalink":"https://jeremyxu2010.github.io/2019/08/%E8%AE%A9kong%E7%9B%91%E5%90%AC80%E5%92%8C443%E7%AB%AF%E5%8F%A3/","tags":["k8s","docker","kong"],"title":"让kong监听80和443端口"},{"categories":["golang开发"],"contents":"上一篇讲了搭建一个身份认证系统，可以看到借助dex搭建一个安全可靠的身份认证系统并不是太难。本篇再讲一下用casbin完成验证授权。\n什么是验证授权  授权（英语：Authorization）一般是指对信息安全或计算机安全相关的资源定义与授予访问权限，尤指访问控制。动词“授权”可指定义访问策略与接受访问。\n 授权作为名词，其代表的是在计算机系统中定义的资源访问权限。而验证授权就是验证计算机帐户是否有资源的访问权限。\n举个栗子，假设现在有一本书book1，其拥有read, write的操作，那么我们可以先定义以下授权：\n alice可以read书籍book1 bob可以write书籍book1 bob可以read书籍book1  现在来了一个用户alice她想write书籍book1，这时调用验证授权功能模块的接口，验证授权功能模块根据上述授权规则可以快速判断alice不可以write书籍book1；过一会儿又来了一个用户bob他想write书籍book1，这时调用验证授权系统的接口，验证授权系统根据上述授权规则可以快速判断bob可以write书籍book1。\n可以看到身份认证系统强调地是安全可靠地得到计算机用户的身份信息，而验证授权强调地是根据计算机的身份信息、访问的资源、对资源的操作等给出一个Yes/No的答复。\n常用授权模型 ACL ACL是Access Control List的缩写，称为访问控制列表. 定义了谁可以对某个数据进行何种操作. 关键数据模型有: 用户, 权限.\nACL规则简单, 也带来一些问题: 资源的权限需要在用户间切换的成本极大; 用户数或资源的数量增长, 都会加剧规则维护成本;\n典型应用  文件系统  文件系统的文件或文件夹定义某个账号(user)或某个群组(group)对文件(夹)的读(read)/写(write)/执行(execute)权限.\n网络访问  防火墙: 服务器限制不允许指定机器访问其指定端口, 或允许特定指定服务器访问其指定几个端口.\nRBAC RBAC是Role-based access control的缩写, 称为 基于角色的访问控制. 核心数据模型有: 用户, 角色, 权限.\n用户具有角色, 而角色具有权限, 从而表达用户具有权限.\n由于有角色作为中间纽带, 当新增用户时, 只需要为用户赋予角色, 用户即获得角色所包含的所有权限.\nRBAC存在多个扩展版本, RBAC0、RBAC1、RBAC2、RBAC3。这些版本的详细说明可以参数这里。我们在实际项目中经常使用的是RBAC1，即带有角色继承概念的RBAC模型。\nABAC ABAC是Attribute-based access control的缩写, 称为基于属性的访问控制.\n权限和资源当时的状态(属性)有关, 属性的值可以用于正向判断(符合某种条件则通过), 也可以用于反向判断(符合某种条件则拒绝):\n典型应用  论坛的评论权限, 当帖子是锁定状态时, 则不再允许继续评论; Github 私有仓库不允许其他人访问; 发帖者可以编辑/删除评论(如果是RBAC, 会为发帖者定义一个角色, 但是每个帖子都要新增一条用户/发帖角色的记录); 微信聊天消息超过2分钟则不再允许撤回; 12306 只有实名认证后的账号才能购票; 已过期的付费账号将不再允许使用付费功能;  实现权限验证 前面提到了多种不同的权限模型，要完全自研实现不同的权限模型还是挺麻烦的。幸好casbin出现，它将上述不同的模型抽象为自己的PERM metamodel，这个PERM metamodel只包括Policy, Effect, Request, Matchers，只通过这几个模型对象的组合即可实现上述提到的多种权限模型，如果业务上需要切换权限模型，也只需要配置一下PERM metamodel，并不需要大改权限模型相关的代码，这一点真的很赞！！！\n一个最简单的ACL权限模型即可像下面这样定义：\nacl_simple_model.conf\n# Request definition [request_definition] r = sub, obj, act # Policy definition [policy_definition] p = sub, obj, act # Policy effect [policy_effect] e = some(where (p.eft == allow)) # Matchers [matchers] m = r.sub == p.sub \u0026amp;\u0026amp; r.obj == p.obj \u0026amp;\u0026amp; r.act == p.act 相应的授权规则可以像下面这样定义：\nacl_simple_policy.csv\np, alice, data1, read p, bob, data2, write 这意味着alice可以read资源data1；bob可以write资源data2。\n写一个简单的程序就可以完成该权限验证：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/casbin/casbin/v2\u0026#34; ) func main() { e, _ := casbin.NewSyncedEnforcer(\u0026#34;acl_simple_model.conf\u0026#34;, \u0026#34;acl_simple_policy.csv\u0026#34;) sub := \u0026#34;alice\u0026#34; // the user that wants to access a resource. \tobj := \u0026#34;data1\u0026#34; // the resource that is going to be accessed. \tact := \u0026#34;read\u0026#34; // the operation that the user performs on the resource.  if passed, _ := e.Enforce(sub, obj, act); passed { // permit alice to read data1 \tfmt.Println(\u0026#34;Enforce policy passed.\u0026#34;) } else { // deny the request, show an error \tfmt.Println(\u0026#34;Enforce policy denied.\u0026#34;) } } casbin模型详解 casbin官方其实已经提供了多种模型的定义及示例policy定义，见这里。而且为了便于用户理解诊断模型及policy，还给了个在线的editor，修改模型或policy时可以利用此工具。\n从上面的示例可以看出基于casbin实现权限验证，代码很简单，但casbin的模型定义及policy定义初看还是挺复杂的，这样详细理解一下。\ncasbin的模型定义里会出现4个部分：[request_definition], [policy_definition], [policy_effect], [matchers]。\n其中[request_definition]描述的是访问请求的定义，如下面的定义将访问请求的三个参数分别映射为r.sub、r.obj、r.act（注意并不是所有的访问请求一定是3个参数）:\n[request_definition] r = sub, obj, act 同理[policy_definition]描述的是授权policy的定义，如下面的定义将每条授权policy分别映射为p.sub、p.obj、p.act（注意并不是所有的授权policy一定是3个参数，也不是必须只有一条授权policy定义）:\n[policy_definition] p = sub, obj, act [matchers]描述的是根据访问请求如何找到匹配的授权policy，如下面的定义将根据r.sub、r.obj、r.act、p.sub、p.obj、p.act找到完全匹配的授权policy：\n[matchers] m = r.sub == p.sub \u0026amp;\u0026amp; r.obj == p.obj \u0026amp;\u0026amp; r.act == p.act 在写[matchers]规则是还可以使用一些内置或自定义函数，参考这里的文档。\n最后[policy_effect]描述如果找到匹配的多条的授权policy，最终给出的验证授权结果，如下面的定义说明只要有一条匹配的授权策略其eft是allow，则最终给出的验证授权结果就是允许（注意每条授权policy默认的eft就是allow）。\n[policy_effect] e = some(where (p.eft == allow)) 如果使用RBAC权限模型，可能还会使用[role_definition]，这个[role_definition]算是最复杂的了，其可以描述user-role之间的映射关系或resource-role之间的映射关系。这么说比较抽象，还是举例说明：\n假设模型定义如下：\n[request_definition] r = sub, obj, act [policy_definition] p = sub, obj, act [role_definition] g = _, _ [policy_effect] e = some(where (p.eft == allow)) [matchers] m = g(r.sub, p.sub) \u0026amp;\u0026amp; r.obj == p.obj \u0026amp;\u0026amp; r.act == p.act 授权policy文件如下：\np, data2_admin, data2, read p, data2_admin, data2, write g, alice, data2_admin 现在收到了授权请求alice, data2, read，这时r.sub为alice，根据g = _, _及g(r.sub, p.sub)，我们可以得出对应的p.sub可以为data2_admin，接下来再根据r.obj == p.obj \u0026amp;\u0026amp; r.act == p.act，最终找到匹配的授权policy规则为p, data2_admin, data2, read，最后根据some(where (p.eft == allow))规则，此时验证授权的结果就应该是allow。\n这里casbin根据r.sub查找对应p.sub的过程还会考虑角色继承。考虑以下授权policy文件：\np, reader, data2, read p, writer, data2, write g, admin, reader g, admin, writer g, alice, admin 现在收到了授权请求alice, data2, read，这时r.sub为alice，根据g = _, _及g(r.sub, p.sub)，我们可以得出对应的p.sub可以为admin，reader，writer，接下来再根据r.obj == p.obj \u0026amp;\u0026amp; r.act == p.act，最终找到匹配的授权policy规则为p, reader, data2, read，最后根据some(where (p.eft == allow))规则，此时验证授权的结果就应该是allow。\n通过[role_definition]还可以定义resource-role之间的映射关系，见示例。\ncasbin的模型大概就是上面描述的了，很多概念理解起来可能比较费劲，结合示例及在editor上做些实验应该理解得更快一些。\ncasbin相关事项   casbin的模型定义及授权policy定义还可以选择保存在其它存储中，见Model Storage、Policy Storage、Adapters。\n  casbin的Enforcer对象还提供了一系列API接口管理授权policy规则，见Management API、RBAC API。\n  可以修改授权policy规则，那么当多个验证授权服务分布式部署时，必然要考虑某个服务修改了授权规则后，其它服务示例必须进行规则的同步。casbin也考虑到了这个需求，提供了Watchers机制，用于在观察到授权规则发生变更时进行必要的回调，见Watchers。\n  在多线程环境下使用Enforcer对象的接口，必须使用casbin.NewSyncedEnforcer创建Enforcer，另外还支持授权policyAutoLoad特性，见这里。\n  casbin默认是从授权policy文件中加载角色及角色的继承信息的，也可以从其它外部数据源获取这些信息，见这里。\n  casbin代码分析 casbin整体代码很简单，很多代码都是模型定义及授权policy定义加载的逻辑，关键代码只有一个方法Enforce，见下面：\nif !e.enabled { return true, nil } functions := make(map[string]govaluate.ExpressionFunction) for key, function := range e.fm { functions[key] = function } if _, ok := e.model[\u0026#34;g\u0026#34;]; ok { for key, ast := range e.model[\u0026#34;g\u0026#34;] { rm := ast.RM functions[key] = util.GenerateGFunction(rm) } } expString := e.model[\u0026#34;m\u0026#34;][\u0026#34;m\u0026#34;].Value expression, err := govaluate.NewEvaluableExpressionWithFunctions(expString, functions) if err != nil { return false, err } rTokens := make(map[string]int, len(e.model[\u0026#34;r\u0026#34;][\u0026#34;r\u0026#34;].Tokens)) for i, token := range e.model[\u0026#34;r\u0026#34;][\u0026#34;r\u0026#34;].Tokens { rTokens[token] = i } pTokens := make(map[string]int, len(e.model[\u0026#34;p\u0026#34;][\u0026#34;p\u0026#34;].Tokens)) for i, token := range e.model[\u0026#34;p\u0026#34;][\u0026#34;p\u0026#34;].Tokens { pTokens[token] = i } parameters := enforceParameters{ rTokens: rTokens, rVals: rvals, pTokens: pTokens, } if policyLen := len(e.model[\u0026#34;p\u0026#34;][\u0026#34;p\u0026#34;].Policy); policyLen != 0 { policyEffects = make([]effect.Effect, policyLen) matcherResults = make([]float64, policyLen) if len(e.model[\u0026#34;r\u0026#34;][\u0026#34;r\u0026#34;].Tokens) != len(rvals) { return false, errors.New( fmt.Sprintf( \u0026#34;invalid request size: expected %d, got %d, rvals: %v\u0026#34;, len(e.model[\u0026#34;r\u0026#34;][\u0026#34;r\u0026#34;].Tokens), len(rvals), rvals)) } for i, pvals := range e.model[\u0026#34;p\u0026#34;][\u0026#34;p\u0026#34;].Policy { // log.LogPrint(\u0026#34;Policy Rule: \u0026#34;, pvals) \tif len(e.model[\u0026#34;p\u0026#34;][\u0026#34;p\u0026#34;].Tokens) != len(pvals) { return false, errors.New( fmt.Sprintf( \u0026#34;invalid policy size: expected %d, got %d, pvals: %v\u0026#34;, len(e.model[\u0026#34;p\u0026#34;][\u0026#34;p\u0026#34;].Tokens), len(pvals), pvals)) } parameters.pVals = pvals result, err := expression.Eval(parameters) // log.LogPrint(\u0026#34;Result: \u0026#34;, result)  if err != nil { return false, err } switch result := result.(type) { case bool: if !result { policyEffects[i] = effect.Indeterminate continue } case float64: if result == 0 { policyEffects[i] = effect.Indeterminate continue } else { matcherResults[i] = result } default: return false, errors.New(\u0026#34;matcher result should be bool, int or float\u0026#34;) } if j, ok := parameters.pTokens[\u0026#34;p_eft\u0026#34;]; ok { eft := parameters.pVals[j] if eft == \u0026#34;allow\u0026#34; { policyEffects[i] = effect.Allow } else if eft == \u0026#34;deny\u0026#34; { policyEffects[i] = effect.Deny } else { policyEffects[i] = effect.Indeterminate } } else { policyEffects[i] = effect.Allow } if e.model[\u0026#34;e\u0026#34;][\u0026#34;e\u0026#34;].Value == \u0026#34;priority(p_eft) || deny\u0026#34; { break } } } 这个代码逻辑很清楚了，就是根据[matchers]、[request_definition]、[policy_definition]找到匹配的[policy_definition]，再根据[policy_effect]最后得出最终的验证授权结果。可以看到该处理逻辑里大量地遍历了e.model[\u0026quot;r\u0026quot;][\u0026quot;r\u0026quot;].Tokens、e.model[\u0026quot;p\u0026quot;][\u0026quot;p\u0026quot;].Tokens、e.model[\u0026quot;p\u0026quot;][\u0026quot;p\u0026quot;].Policy，当授权policy规则条数较多时，估计性能不会太好。但官方给了个性能测试报告，据说性能还可以，这个后面还须再验证下。\n为了优化性能，其实是可以将验证授权操作的结果进行缓存，官方也提供了CachedEnforcer，目测逻辑没问题，如果确实遇到性能瓶颈，可以考虑采用。\n其它外部支援 一些开源爱好者为casbin贡献了很多中间件组件，便于在多个编程语言中集成casbin进行权限验证。\n还有一些开源爱好者为casbin贡献了模型管理及授权策略管理的web前端，如果觉得手工修改授权策略文件不直观的话，可以考虑采用。\n还可以看到目前很多开源项目的权限验证部分都是采用了casbin来实现的，例如harbor里的rbac权限验证。\n还发现一个基于casbin实现的身份认证及验证授权服务，这个例子以后可以好好参考一下。\n自己研究casbin的示例项目。\n参考  https://github.com/isayme/blog/issues/34 https://www.jianshu.com/p/b078abe9534f https://casbin.org/docs/en/overview https://casbin.org/docs/en/supported-models https://casbin.org/docs/en/syntax-for-models https://casbin.org/docs/en/rbac https://casbin.org/docs/en/model-storage https://casbin.org/docs/en/policy-storage https://casbin.org/docs/en/adapters https://casbin.org/docs/en/management-api https://casbin.org/docs/en/rbac-api https://casbin.org/docs/en/watchers https://casbin.org/docs/en/role-managers https://github.com/casbin/casbin https://casbin.org/docs/en/benchmark https://casbin.org/docs/en/middlewares https://casbin.org/docs/en/admin-portal https://casbin.org/docs/en/adopters https://github.com/Soontao/go-simple-api-gateway  ","permalink":"https://jeremyxu2010.github.io/2019/08/%E4%BD%BF%E7%94%A8casbin%E5%AE%8C%E6%88%90%E9%AA%8C%E8%AF%81%E6%8E%88%E6%9D%83/","tags":["casbin","golang","rbac"],"title":"使用casbin完成验证授权"},{"categories":["golang开发"],"contents":"一个成熟的软件系统一般必须有一个可靠的身份认证与权限验证功能。这一块要自研快速实现还是需要花费挺多精力的，幸好开源领域目前已经有不错的解决方案，一般拿过来按项目的实际需求进行一些简单的定制基本就可以实现业务目标了。最近刚好在这方面进行了一些工作，这里将如何实现身份认证及权限验证分两篇博文大概梳理一下，这篇先讲身份认证。\n什么是身份认证  身份验证（英语：Authentication）又称“验证”，是指通过一定的手段，完成对用户身份的确认。\n身份验证的目的是确认当前所声称为某种身份的用户，确实是所声称的用户。在日常生活中，身份验证并不罕见；比如，通过检查对方的证件，我们一般可以确信对方的身份。虽然日常生活中的这种确认对方身份的做法也属于广义的“身份验证”，但“身份验证”一词更多地被用在计算机、通信等领域。\n 以上是维基百科的解释，说白了就是用某种方式确保用户是某种身份，这种确保需要保证其它用户没那么容易伪装其身份。一般只有经过身份认证得到可靠的用户身份后，才能基于该身份进行后续的权限验证流程。\n实现身份认证系统 一般来说业务系统会专注于业务逻辑的处理，而身份认证相关的功能会放入独立的身份认证系统进行开发维护。要纯自研地完成一套可靠、安全度高的身份认证系统还是比较花费精力的，幸好开源领域目前已经有不错的解决方案，一般拿过来按项目的实际需求进行一些简单的定制基本就可以实现业务目标了。我这里的示例使用dex实现了一个简单的用户认证系统。\n运行示例 前提是已经搭建好了go语言的开发环境，并设置好了GOPATH。\n然后按以下步骤运行本程序：\n# 编译dexserver $ make build-dexserver # 编译dexclient $ make build-dexclient # 运行dexserver $ make run-dexserver # 运行dexclient $ run-dexclient 然后用浏览器访问http://127.0.0.1:8080, 页面会自动跳转至dexserver的登录页面，输入用户名admin@example.com、密码password之后，会跳回dexclient的callback页面http://127.0.0.1:8080/callback。\n示例的技术细节 dexserver 这里使用的dexserver是由官方代码直接编译得出的，没有修改任何代码。只不过使用了自定义的配置文件dexserver-config.yaml，这里分析一下这个配置文件。\n# The base path of dex and the external name of the OpenID Connect service. # This is the canonical URL that all clients MUST use to refer to dex. If a # path is provided, dex\u0026#39;s HTTP service will listen at a non-root URL. issuer: http://127.0.0.1:5556/dex # The storage configuration determines where dex stores its state. Supported # options include SQL flavors and Kubernetes third party resources. # # See the storage document at Documentation/storage.md for further information. storage: type: sqlite3 config: file: config/dex.db # Configuration for the HTTP endpoints. web: http: 0.0.0.0:5556 # Uncomment for HTTPS options. # https: 127.0.0.1:5554 # tlsCert: /etc/dex/tls.crt # tlsKey: /etc/dex/tls.key # Configuration for telemetry telemetry: http: 0.0.0.0:5558 # Uncomment this block to enable the gRPC API. This values MUST be different # from the HTTP endpoints. # grpc: # addr: 127.0.0.1:5557 # tlsCert: examples/grpc-client/server.crt # tlsKey: examples/grpc-client/server.key # tlsClientCA: /etc/dex/client.crt # Uncomment this block to enable configuration for the expiration time durations. # expiry: # signingKeys: \u0026#34;6h\u0026#34; # idTokens: \u0026#34;24h\u0026#34; # Options for controlling the logger. # logger: # level: \u0026#34;debug\u0026#34; # format: \u0026#34;text\u0026#34; # can also be \u0026#34;json\u0026#34; # Uncomment this block to control which response types dex supports. For example # the following response types enable the implicit flow for web-only clients. # Defaults to [\u0026#34;code\u0026#34;], the code flow. # oauth2: # responseTypes: [\u0026#34;code\u0026#34;, \u0026#34;token\u0026#34;, \u0026#34;id_token\u0026#34;] oauth2: skipApprovalScreen: true # Instead of reading from an external storage, use this list of clients. # # If this option isn\u0026#39;t chosen clients may be added through the gRPC API. staticClients: - id: demo-dexclient redirectURIs: - \u0026#39;http://127.0.0.1:8080/callback\u0026#39; name: \u0026#39;Demo dex client\u0026#39; secret: ZXhhbXBsZS1hcHAtc2VjcmV0 connectors: [] # - type: mockCallback # id: mock # name: Example # - type: oidc # id: google # name: Google # config: # issuer: https://accounts.google.com # # Connector config values starting with a \u0026#34;$\u0026#34; will read from the environment. # clientID: $GOOGLE_CLIENT_ID # clientSecret: $GOOGLE_CLIENT_SECRET # redirectURI: http://127.0.0.1:5556/dex/callback # hostedDomains: # - $GOOGLE_HOSTED_DOMAIN # Let dex keep a list of passwords which can be used to login to dex. enablePasswordDB: true # A static list of passwords to login the end user. By identifying here, dex # won\u0026#39;t look in its underlying storage for passwords. # # If this option isn\u0026#39;t chosen users may be added through the gRPC API. # staticPasswords:  # - email: \u0026#34;admin@example.com\u0026#34; # # bcrypt hash of the string \u0026#34;password\u0026#34; # hash: \u0026#34;$2a$10$2b2cU8CPhOTaGrs1HRQuAueS7JTT5ZHsHSzYiFPm1leZck7Mc8T4W\u0026#34; # username: \u0026#34;admin\u0026#34; # userID: \u0026#34;08a8684b-db88-4b73-90a9-3cd1661f5466\u0026#34; web段配置的是dexserver的监听地址及HTTPS证书参数，issuer配置的是外部会访问到的系统URL，这两者一般要对应地设置。\ntelemetry段配置的是监控指标抓取地址，本例中dexserver启动完毕后，可访问http://127.0.0.1:5558/metrics抓取到该dexserver的监控指标。\nstorage段配置的是dexserver的存储设置。dexserver在运行时跟踪refresh_token、auth_code、keys、password等的状态，因此需要将这些状态保存下来。dex提供了多种存储方案，如etcd、CRDs、SQLite3、Postgres、MySQL、memory，总有一款能满足需求。如果要其它需求，还可以参考现有Storage及文档扩展一个。我这里使用的是比较简单的SQLite3Storage，提前插入了一条测试的用户数据：\nsqlite3 config/dex.db sqlite\u0026gt; insert info password values(\u0026#39;admin@example.com\u0026#39;, \u0026#39;$2a$10$2b2cU8CPhOTaGrs1HRQuAueS7JTT5ZHsHSzYiFPm1leZck7Mc8T4W\u0026#39;, \u0026#39;admin\u0026#39;, \u0026#39;08a8684b-db88-4b73-90a9-3cd1661f5466\u0026#39;); sqlite\u0026gt; .quit oauth2.skipApprovalScreen这个选项我设置成了true，这样就不会有提示用户同意的页面出现。\nstaticClients段配置的是该dexserver允许接入的dexclient信息，这个要跟dexclient那边的配置一致。\nconnectors段我并没有配置任何Connector。Connector是dex中一项重要特性，其可以将dex这个身份认证系统与其它身份认证系统串联起来。dex目前自带的Connector有LDAP、GitHub、SAML 2.0、GitLab、OpenID Connect、LinkedIn、Microsoft、AuthProxy、Bitbucket Cloud，基本上满足绝大部分需求，如果要扩展，参考某个现成的Connector实现即可。我这个示例里因为直接使用保存在DB里的帐户密码信息，因此只需要配置enablePasswordDB为true，就会自动使用上passwordDB这个Connector，passwordDB的实现代码见这里。\n最近由于登录页面是由dexserver提供了，这里还将dex自带的登录页面web端资源带上了，具体的项目中根据场景对页面进行一些定制就可以了。\ndexclient dexclient就很简单了，就两个go文件，重点是cmd/dexclient/main.go。\n首先是根据一系列参数构造出oidc.Provider及oidc.IDTokenVerifier，这个后面获取认证系统的跳转地址、获取id_token、校验id_token都会用到:\n... a.provider = provider a.verifier = provider.Verifier(\u0026amp;oidc.Config{ClientID: a.clientID}) 然后声明处理三个请求地址，并启动Web Server：\nhttp.HandleFunc(\u0026#34;/\u0026#34;, a.handleIndex) http.HandleFunc(\u0026#34;/login\u0026#34;, a.handleLogin) http.HandleFunc(u.Path, a.handleCallback) switch listenURL.Scheme { case \u0026#34;http\u0026#34;: log.Printf(\u0026#34;listening on %s\u0026#34;, listen) return http.ListenAndServe(listenURL.Host, nil) case \u0026#34;https\u0026#34;: log.Printf(\u0026#34;listening on %s\u0026#34;, listen) return http.ListenAndServeTLS(listenURL.Host, tlsCert, tlsKey, nil) default: return fmt.Errorf(\u0026#34;listen address %q is not using http or https\u0026#34;, listen) } 很明显handleIndex就是WEB应用的主页，这里一般逻辑应该是检查用户的登录身份信息是否合法，如果不合法\u0008则跳至dexserver的登录页面。\nvar indexTmpl = template.Must(template.New(\u0026#34;index.html\u0026#34;).Parse(`\u0026lt;html\u0026gt; \u0026lt;!-- TODO Redirect to login page if not logged --\u0026gt; \u0026lt;body\u0026gt; \u0026lt;form action=\u0026#34;/login\u0026#34; method=\u0026#34;post\u0026#34;\u0026gt; \u0026lt;p\u0026gt; Authenticate for:\u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;cross_client\u0026#34; placeholder=\u0026#34;list of client-ids\u0026#34;\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; Extra scopes:\u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;extra_scopes\u0026#34; placeholder=\u0026#34;list of scopes\u0026#34;\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; Connector ID:\u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;connector_id\u0026#34; placeholder=\u0026#34;connector id\u0026#34;\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; Request offline access:\u0026lt;input type=\u0026#34;checkbox\u0026#34; name=\u0026#34;offline_access\u0026#34; value=\u0026#34;yes\u0026#34; checked\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;Login\u0026#34; id=\u0026#34;submitBtn\u0026#34;\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; \u0026lt;!-- Redirect to login page --\u0026gt; document.getElementById(\u0026#34;submitBtn\u0026#34;).click(); \u0026lt;/script\u0026gt; \u0026lt;/html\u0026gt;`)) handleLogin根据浏览器发来的cross_client、extra_scopes、connector_id、offline_access参数构造出登录页跳转地址，并提示浏览器跳至该地址:\n... if r.FormValue(\u0026#34;offline_access\u0026#34;) != \u0026#34;yes\u0026#34; { authCodeURL = a.oauth2Config(scopes).AuthCodeURL(exampleAppState) } else if a.offlineAsScope { scopes = append(scopes, \u0026#34;offline_access\u0026#34;) authCodeURL = a.oauth2Config(scopes).AuthCodeURL(exampleAppState) } else { authCodeURL = a.oauth2Config(scopes).AuthCodeURL(exampleAppState, oauth2.AccessTypeOffline) } if connectorID != \u0026#34;\u0026#34; { authCodeURL = authCodeURL + \u0026#34;\u0026amp;connector_id=\u0026#34; + connectorID } http.Redirect(w, r, authCodeURL, http.StatusSeeOther) handleCallback处理登录成功后的回调请求，其根据回调请求中的code参数，调用dexserver的相关接口换取包含用户身份信息的Token：\ncode := r.FormValue(\u0026#34;code\u0026#34;) if code == \u0026#34;\u0026#34; { http.Error(w, fmt.Sprintf(\u0026#34;no code in request: %q\u0026#34;, r.Form), http.StatusBadRequest) return } if state := r.FormValue(\u0026#34;state\u0026#34;); state != exampleAppState { http.Error(w, fmt.Sprintf(\u0026#34;expected state %q got %q\u0026#34;, exampleAppState, state), http.StatusBadRequest) return } token, err = oauth2Config.Exchange(ctx, code) 一般来说，会将该Token中的id_token进行适当的编码发回到浏览器中保存（以Cookie或WebStorage等方式），这样浏览器中就保存了用户的身份信息。\n安全起见，dexserver签发的id_token有效期通常不会太长，这就需要dexclient凭借Token中的refresh_token隔段时间重新换取新的Token，并通过某种机制将新Token中的id_token重新发回浏览器端保存。以refresh_token重新换取新的Token的代码实现如下：\nt := \u0026amp;oauth2.Token{ RefreshToken: refresh, Expiry: time.Now().Add(-time.Hour), } token, err = oauth2Config.TokenSource(ctx, t).Token() 项目定制扩展 上面的示例基本说明得比较清楚了，在真实的生产项目需要进行必要的定制以满足项目需求，生产项目的代码必须保密，这里只简要说明下关键点：\n 生产系统，无论dexserver还是dexclient都必须配置HTTPS证书，通过HTTPS协议访问站点。 dexserver会根据项目情况配置一个更合适的Storage，用以安全可靠地保存refresh_token、auth_code、keys、password等的状态。要考虑这个Storage实现方案的性能、稳定性、高可用性等多个因素。 一般还需要对保存进Storage的帐户身份信息进行维护管理，不可能每次都命令行增删帐户身份信息。 示例中dexserver直接使用的passwordDB这个Connector，实际场景可能会扩展passwordDB，比如需要往返回的claims里植入Groups等其它信息。 示例中dexserver只使用了passwordDB这个Connector，实际场景为了接入其它身份认证系统，会配置一些Connect以对接其它身份认证系统。 示例中dexserver只使用了dex官方自带的登录页模板，这个放在实际项目中还是太简陋了，一般需要一些定制美化。 示例中dexclient并没有在浏览器中完成用户身份信息验证，以决定是否跳转登录页面，这个在实际项目中肯定是要做的。 示例中dexclient并没有根据用户的选择跳转至指定的Connector登录页，对于支持多种认证方式的业务系统来说，这个也是必须的。 示例中dexclient并没有实现将Token中的id_token进行适当的编码发回到浏览器中保存的逻辑，这个一般也是必须的。 示例中dexclient并没有凭借Token中的refresh_token隔段时间重新换取新的Token，并通过某种机制将新Token中的id_token重新发回浏览器端保存，这个一般也是必须的。  总之这个示例要最终在生产项目中落地，还是要不少工作要处理的，不过要了解其核心工作原理，看这个示例也就差不多了。\n总结 总体来说，采用dex实现一个身份认证系统相比纯自研，还是简单了不少。一旦掌握其原理，后续实现会很方便，其核心代码值得学习一下。\n参考  https://github.com/dexidp/dex https://github.com/dexidp/dex/blob/master/Documentation/getting-started.md https://github.com/dexidp/dex/blob/master/Documentation/using-dex.md https://github.com/dexidp/dex/blob/master/Documentation/storage.md https://github.com/dexidp/dex/blob/master/Documentation/kubernetes.md  ","permalink":"https://jeremyxu2010.github.io/2019/08/%E4%BD%BF%E7%94%A8dex%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%BA%AB%E4%BB%BD%E8%AE%A4%E8%AF%81%E7%B3%BB%E7%BB%9F/","tags":["dex","golang","idp"],"title":"使用dex搭建一个身份认证系统"},{"categories":["容器编排"],"contents":"工作中需要对kubernetes中workload使用的系统资源进行一些限制，本周花时间研究了一下，这里记录一下。\nkubernetes的系统资源限制机制 kuberentes里存在两种机制进行系统资源限制，一个是Resource Quotas，一个是Limit Ranges。\nResource Quotas 使用Resource Quotas可以限制某个命名空间使用的系统资源，使用方法如下：\nkubectl create namespace quota-object-example cat \u0026lt;\u0026lt; EOF | kubectl -n quota-object-example create -f - apiVersion: v1 kind: ResourceQuota metadata: name: object-quota-demo spec: hard: requests.cpu: \u0026#34;8\u0026#34; # 限制该命名空间使用的总cpu request requests.memory: \u0026#34;32Gi\u0026#34; # 限制该命名空间使用的总memory request limits.cpu: \u0026#34;16\u0026#34; # 限制该命名空间使用的总cpu limit limits.memory: \u0026#34;32Gi\u0026#34; # 限制该命名空间使用的总memory limit requests.nvidia.com/gpu: 4 # 限制该命名空间使用的扩展资源 requests.storage: \u0026#34;500Gi\u0026#34; # 限制该命名空间使用的总storage request limits.storage: \u0026#34;1000Gi\u0026#34; # 限制该命名空间使用的总storage limit foo.storageclass.storage.k8s.io/requests.storage: \u0026#34;100Gi\u0026#34; # 限制该命名空间经由某个storage class创建的总storage request foo.storageclass.storage.k8s.io/limits.storage: \u0026#34;200Gi\u0026#34; # 限制该命名空间经由某个storage class创建的总storage limit requests.ephemeral-storage: \u0026#34;5Gi\u0026#34; # 限制该命名空间使用的总ephemeral-storage request limits.ephemeral-storage: \u0026#34;50Gi\u0026#34; # 限制该命名空间使用的总ephemeral-storage limit count/services: 20 # 限制该命名空间创建的总service数目 count/services.nodeports: 3 # 限制该命名空间创建的总nodeport类型service数目 count/deployments.apps: 5 # 限制该命名空间创建的总deployment数目 count/widgets.example.com: 5 # 限制该命名空间创建的总自定义资源widgets.example.com数目 EOF 可配置的系统资源表达式参考Compute Resource Quota，Storage Resource Quota，Object Count Quota。\n另外还可以给不同的scope指定不同的系统资源限制，如下：\ncat \u0026lt;\u0026lt; EOF | kubectl -n quota-object-example create -f - apiVersion: v1 kind: List items: - apiVersion: v1 kind: ResourceQuota metadata: name: pods-high spec: hard: cpu: \u0026#34;1000\u0026#34; memory: 200Gi pods: \u0026#34;10\u0026#34; scopeSelector: matchExpressions: - operator : In scopeName: PriorityClass values: [\u0026#34;high\u0026#34;] - apiVersion: v1 kind: ResourceQuota metadata: name: pods-medium spec: hard: cpu: \u0026#34;10\u0026#34; memory: 20Gi pods: \u0026#34;10\u0026#34; scopeSelector: matchExpressions: - operator : In scopeName: PriorityClass values: [\u0026#34;medium\u0026#34;] - apiVersion: v1 kind: ResourceQuota metadata: name: pods-low spec: hard: cpu: \u0026#34;5\u0026#34; memory: 10Gi pods: \u0026#34;10\u0026#34; scopeSelector: matchExpressions: - operator : In scopeName: PriorityClass values: [\u0026#34;low\u0026#34;] EOF cat \u0026lt;\u0026lt; EOF | kubectl -n quota-object-example create -f - apiVersion: v1 kind: Pod metadata: name: high-priority spec: containers: - name: high-priority image: ubuntu command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do echo hello; sleep 10;done\u0026#34;] resources: requests: memory: \u0026#34;10Gi\u0026#34; cpu: \u0026#34;500m\u0026#34; limits: memory: \u0026#34;10Gi\u0026#34; cpu: \u0026#34;500m\u0026#34; priorityClassName: high EOF 这个例子创建了分别限制3个scope的ResourceQuota，下面创建的那个pod因为priorityClassName为high，因此它使用的系统资源只会遵守pods-high定义出的配额限制。\n最后还可以组合AdmissionConfiguration，确保某些priorityClassName的资源只会被创建在有相应ResourceQuota的命名空间中，如下：\ncat \u0026lt;\u0026lt; EOF | kubectl -n quota-object-example create -f - apiVersion: apiserver.k8s.io/v1alpha1 kind: AdmissionConfiguration plugins: - name: \u0026#34;ResourceQuota\u0026#34; configuration: apiVersion: resourcequota.admission.k8s.io/v1beta1 kind: Configuration limitedResources: - resource: pods matchScopes: - scopeName: PriorityClass operator: In values: [\u0026#34;cluster-services\u0026#34;] EOF cat \u0026lt;\u0026lt; EOF | kubectl -n quota-object-example create -f - apiVersion: v1 kind: ResourceQuota metadata: name: pods-high spec: hard: requests: memory: \u0026#34;10Gi\u0026#34; cpu: \u0026#34;500m\u0026#34; limits: memory: \u0026#34;10Gi\u0026#34; cpu: \u0026#34;500m\u0026#34; scopeSelector: matchExpressions: - scopeName: PriorityClass operator: In values: [\u0026#34;cluster-services\u0026#34;] EOF cat \u0026lt;\u0026lt; EOF | kubectl -n quota-object-example create -f - apiVersion: v1 kind: Pod metadata: name: test-service spec: containers: - name: test-service image: ubuntu command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do echo hello; sleep 10;done\u0026#34;] resources: requests: memory: \u0026#34;10Gi\u0026#34; cpu: \u0026#34;500m\u0026#34; limits: memory: \u0026#34;10Gi\u0026#34; cpu: \u0026#34;500m\u0026#34; priorityClassName: cluster-services EOF 如上的配置可保证该pod只会被创建在有相应ResourceQuota的命名空间中。\nLimit Ranges 除了限制整个命名空间的系统资源使用量外，还可以通过Limit Ranges限制容器或pod的系统资源使用量，如下：\nkubectl create namespace limitrange-demo cat \u0026lt;\u0026lt; EOF | kubectl -n limitrange-demo create -f - admin/resource/limit-mem-cpu-container.yaml apiVersion: v1 kind: LimitRange metadata: name: limit-mem-cpu-per-container spec: limits: - max: cpu: \u0026#34;800m\u0026#34; memory: \u0026#34;1Gi\u0026#34; min: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;99Mi\u0026#34; default: cpu: \u0026#34;700m\u0026#34; memory: \u0026#34;900Mi\u0026#34; defaultRequest: cpu: \u0026#34;110m\u0026#34; memory: \u0026#34;111Mi\u0026#34; maxLimitRequestRatio: cpu: 2 memory: 4 type: Container # 也可以设置为Pod EOF 这里跟ResourceQuotas的区别是这里设置的是最大最小值，只要申请的资源在范围内就算是合法的。同时还可以设置默认值，Container或Pod如果没有设置，就会使用默认值。\n参考  https://kubernetes.io/docs/concepts/policy/resource-quotas/ https://kubernetes.io/docs/tasks/administer-cluster/quota-api-object/ https://kubernetes.io/docs/concepts/policy/limit-range/  ","permalink":"https://jeremyxu2010.github.io/2019/07/%E9%99%90%E5%88%B6kubernetes%E9%87%8C%E7%9A%84%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%BA%90%E4%BD%BF%E7%94%A8/","tags":["kuberentes"],"title":"限制kubernetes里的系统资源使用"},{"categories":["容器编排"],"contents":"工作中需要以kubernetes原生的方式构建API接口服务，并将构建出的API接口直接聚合到kubernetes的apiserver服务上。本周花了不少时间研究这个，这里记录一下。\n好处 尽管可以使用gin, go-restful等go语言web框架轻易地构建出一个稳定的API接口服务，但以kubernetes原生的方式构建API接口服务还是有很多吸引人的好处的。官方文档中已经将这些好处列出了：\n User-Defined Types Consider adding a Custom Resource to Kubernetes if you want to define new controllers, application configuration objects or other declarative APIs, and to manage them using Kubernetes tools, such as kubectl.\nDo not use a Custom Resource as data storage for application, user, or monitoring data.\nFor more about Custom Resources, see the Custom Resources concept guide.\nCombining New APIs with Automation The combination of a custom resource API and a control loop is called the Operator pattern. The Operator pattern is used to manage specific, usually stateful, applications. These custom APIs and control loops can also be used to control other resources, such as storage or policies.\nChanging Built-in Resources When you extend the Kubernetes API by adding custom resources, the added resources always fall into a new API Groups. You cannot replace or change existing API groups. Adding an API does not directly let you affect the behavior of existing APIs (e.g. Pods), but API Access Extensions do.\nAPI Access Extensions When a request reaches the Kubernetes API Server, it is first Authenticated, then Authorized, then subject to various types of Admission Control. See Controlling Access to the Kubernetes API for more on this flow.\nEach of these steps offers extension points.\nKubernetes has several built-in authentication methods that it supports. It can also sit behind an authenticating proxy, and it can send a token from an Authorization header to a remote service for verification (a webhook). All of these methods are covered in the Authentication documentation.\nAuthentication Authentication maps headers or certificates in all requests to a username for the client making the request.\nKubernetes provides several built-in authentication methods, and an Authentication webhook method if those don’t meet your needs.\nAuthorization Authorization determines whether specific users can read, write, and do other operations on API resources. It just works at the level of whole resources – it doesn’t discriminate based on arbitrary object fields. If the built-in authorization options don’t meet your needs, and Authorization webhook allows calling out to user-provided code to make an authorization decision.\nDynamic Admission Control After a request is authorized, if it is a write operation, it also goes through Admission Control steps. In addition to the built-in steps, there are several extensions:\n The Image Policy webhook restricts what images can be run in containers. To make arbitrary admission control decisions, a general Admission webhook can be used. Admission Webhooks can reject creations or updates.   简单一句话就是构建出的API接口更加规范整齐，能利用kubernetes原生的认证、授权、准入机制。当然对个人来说，也能更了解kubernetes里那些API接口到底是如何实现的。\n实现方案 官方提供了两种方式以实现对标准kubernetes API接口的扩展：1）Aggregated APIServer 2）Custom Resource\n两种方式的区别是定义api-resource的方式不同。在Aggregated APIServer方式中，api-resource是通过代码向kubernetes注册资源类型的方式实现的，而Custom Resource是直接通过yaml文件创建自定义资源的方式实现的。\n最终达到的效果倒是比较类似，最终都可以通过访问/apis/myextension.mycompany.io/v1/…之类的API接口来存取api-resource。除此之外，在很多方面也存在一些区别，见这里。\n API Access Control Authentication  CR: All strategies supported. Configured by root apiserver. AA: Supporting all root apiserver's authenticating strategies but it has to be done via authentication token review apiexcept for authentication proxy which will cause an extra cost of network RTT.  Authorization  CR: All strategies supported. Configured by root apiserver. AA: Delegating authorization requests to root apiserver via SubjectAccessReview api. Note that this approach will also cost a network RTT.  Admission Control  CR: You could extend via dynamic admission control webhook (which is costing network RTT). AA: While You can develop and customize your own admission controller which is dedicated to your AA. While You can't reuse root-apiserver's built-in admission controllers nomore.  API Schema Note: CR's integration with OpenAPI schema is being enhanced in the future releases and it will have a stronger integration with OpenAPI mechanism.\nValidating  CR: (landed in 1.12) Defined via OpenAPIv3 Schema grammar. more AA: You can customize any validating flow you want.  Conversion  CR: (landed in 1.13) The CR conversioning (basically from storage version to requested version) could be done via conversioning webhook. AA: Develop any conversion you want.  SubResource  CR: Currently only status and scale sub-resource supported. AA: You can customize any sub-resouce you want.  OpenAPI Schema  CR: (landed in 1.13) The corresponding CRD's OpenAPI schema will be automatically synced to root-apiserver's openapi doc api. AA: OpenAPI doc has to be manually generated by code-generating tools.  Other    Functionalities AA (Aggregated APIServer) CR (Custom Resource)     SMP(Strategic Merge Patch) Supported Not yet. Will be replaced via server-side apply instead   Informative Kubectl Printing Not supported, unless you develop your own with server-side printing. By AdditionalPrinterColumns   Websocket/(Other non-HTTP transport) Supported No   metadata.GenerationAuto Increment Supported Nope, and this is designed   Use Another Backend/Secondary Storage Supported For now, ETCD3 only    More Comparision here\n 总的来看，AA这个方式相对复杂一点，但灵活度很高，基本后续业务上的所有需求都可以满足。最终我们选择使用AA方案来构建API接口服务。\n实现API接口服务 快速实现 虽然官方给了一个sample-apiserver，我们可以照着实现自己的Aggregated APIServer。但完全手工编写还是太费劲了，这里使用官方推荐的工具apiserver-builder帮助快速创建项目骨架。\napiserver-builder构建AA方案的API接口服务的原理还是比较清晰的，总之就是kubernetes里最常见的控制器模式，这里就不具体介绍了，官方文档既有文字又有图片讲得还是挺细致的，强烈推荐大家多看看，学习一下。\napiserver-builder的安装就不细说了，照着官方文档做就可以了。\n以下参考apiserver-builder的官方文档，得出的一些关键步骤：\n# 创建项目目录 mkdir $GOPATH/src/github.com/jeremyxu2010/demo-apiserver # 在项目目录下新建一个名为boilerplate.go.txt，里面是代码的头部版权声明 cd $GOPATH/src/github.com/jeremyxu2010/demo-apiserver curl -o boilerplate.go.txt https://github.com/kubernetes/kubernetes/blob/master/hack/boilerplate/boilerplate.go.txt # 初始化项目 apiserver-boot init repo --domain jeremyxu2010.me # 创建一个非命名空间范围的api-resource apiserver-boot create group version resource --group demo --version v1beta1 --non-namespaced=true --kind Foo # 创建Foo这个api-resource的子资源 apiserver-boot create subresource --subresource bar --group demo --version v1beta1 --kind Foo # 生成上述创建的api-resource类型的相关代码，包括deepcopy接口实现代码、versioned/unversioned类型转换代码、api-resource类型注册代码、api-resource类型的Controller代码、api-resource类型的AdmissionController代码 apiserver-boot build generated # 直接在本地将etcd, apiserver, controller运行起来 apiserver-boot run local 上述这样操作之后，就可以访问我们的APIServer了，如下面的命令：\ncurl -k https://127.0.0.1:9443/apis/demo.jeremyxu2010.me/v1beta1/foos 当然可以新建一个yaml文件，然后用kubectl命令直接对api-resource进行操作：\n# 创建Foo资源的yaml echo \u0026#39;apiVersion: demo.jeremyxu2010.me/v1beta1 kind: Foo metadata: name: foo-example namespace: test spec: {}\u0026#39; \u0026gt; sample/foo.yaml # 查看已经注册的api-resource类型 kubectl --kubeconfig api-resources # 列所有foos kubectl --kubeconfig kubeconfig get foos # 创建foo kubectl --kubeconfig kubeconfig create -f sample/foo.yaml # 再列所有foos kubectl --kubeconfig kubeconfig get foos # Get新创建的foo kubectl --kubeconfig kubeconfig get foos foo-example kubectl --kubeconfig kubeconfig get foos foo-example -o yaml # Delete新创建的foo kubectl --kubeconfig kubeconfig delete foos foo-example 如果在apiserver的main方法里补上一些代码，以开启swagger-ui，还能更方便地看到这些API接口：\nfunc main() { version := \u0026#34;v0\u0026#34; server.StartApiServer(\u0026#34;/registry/jeremyxu2010.me\u0026#34;, apis.GetAllApiBuilders(), openapi.GetOpenAPIDefinitions, \u0026#34;Api\u0026#34;, version, func(apiServerConfig *apiserver.Config) error { ... apiServerConfig.RecommendedConfig.EnableSwaggerUI = true apiServerConfig.RecommendedConfig.SwaggerConfig = genericapiserver.DefaultSwaggerConfig() return nil }) } 然后浏览器访问https://127.0.0.1:9443/swagger-ui/就可以在swagger的Web页面上看到创建出来的所有API接口。\n定制API接口 像上面这样创建的API接口，接口是都有了，但接口没有啥意义，一般要根据实际情况定义api-resource的spec、status等结构体。\ntype Foo struct { metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` metav1.ObjectMeta `json:\u0026#34;metadata,omitempty\u0026#34;` Spec FooSpec `json:\u0026#34;spec,omitempty\u0026#34;` Status FooStatus `json:\u0026#34;status,omitempty\u0026#34;` } // FooSpec defines the desired state of Foo type FooSpec struct { } // FooStatus defines the observed state of Foo type FooStatus struct { } 可参考这里。\n有时默认的增删改查操作并不满足业务需求，这时可以自定义api-resource或subresource的REST实现，默认实现是存取到etcd的，通过这种方式甚至可以将自定义资源存入后端数据库。自定义REST实现的方法参考adding_custom_rest，foo_rest.go，bar_foo_rest.go。另外kubernetes的代码里也有大量自定义REST实现可参考，见这里。\n为api-resource类型的默认值设置可参考这里，添加校验规则可参考这里。\n定制Controller 默认生成的api-resource的Reconcile逻辑如下：\n// Reconcile reads that state of the cluster for a Foo object and makes changes based on the state read // and what is in the Foo.Spec // TODO(user): Modify this Reconcile function to implement your Controller logic. The scaffolding writes // a Deployment as an example // +kubebuilder:rbac:groups=demo.jeremyxu2010.me,resources=foos,verbs=get;list;watch;create;update;patch;delete // +kubebuilder:rbac:groups=demo.jeremyxu2010.me,resources=foos/status,verbs=get;update;patch func (r *ReconcileFoo) Reconcile(request reconcile.Request) (reconcile.Result, error) { // Fetch the Foo instance \tinstance := \u0026amp;demov1beta1.Foo{} err := r.Get(context.TODO(), request.NamespacedName, instance) if err != nil { if errors.IsNotFound(err) { // Object not found, return. Created objects are automatically garbage collected. \t// For additional cleanup logic use finalizers. \treturn reconcile.Result{}, nil } // Error reading the object - requeue the request. \treturn reconcile.Result{}, err } return reconcile.Result{}, nil } 一般来说要按自己的业务逻辑进行定制，可参考这里。\napi-resource的admission controller编写可参考这里。\n打包部署 程序写好后，通过以下命令即可生成容器镜像及kubernetes的部署manifest文件：\n# 生成二进制文件 apiserver-boot build executables # 生成容器镜像 apiserver-boot build container --image demo/foo-apiserver:latest # 生成kubernetes的部署manifest文件，可直接在kubernetes里apply即完成部署 apiserver-boot build config --name fool-apiserver --namespace default --image demo/foo-apiserver:latest 观察生成的kubernetes部署manifest文件config/apiserver.yaml，可以发现最终会创建一个Deployment，一个Service和一个APIService类型的kubernetes资源，同时APIService的caBundle及apiserver的TLS证书也配置妥当了。这个跟官方文档中所说的第4、5、6、7、8、14点相符。\n生成文档 最终交付除了部署好的程序，还可以生成相应的API文档，操作如下：\ncurl -o docs/openapi-spec/swagger.json https://127.0.0.1:9443/openapi/v2 apiserver-build build docs --build-openapi=false --operations=true 使用浏览器打开docs/build/index.html即可访问生成的API文档，这文档的风格可kubernetes的reference文档风格是一致，相当专业！！！\n其它 在实现过程中还顺带改了apiserver-builder的一个小bug，也算为社区做了点贡献。\napiserver-builder在生成代码时使用了一些kubernetes项目本身使用的code generator，这些code generator也挺有趣的，有时间可以仔细研究下。\n总结 编写Aggregated APIServer风格的API接口服务这一工作，终于接触到了kubernetes里的一些内部设计，不得不说这套设计还是相当简洁稳定的，难怪kubernetes项目最终能成功。\n参考  https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/ https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/ https://github.com/kubernetes-incubator/apiserver-builder-alpha/blob/master/docs/compare_with_kubebuilder.md https://github.com/kubernetes/sample-apiserver/blob/master/README.md https://github.com/Kubernetes-incubator/apiserver-builder/blob/master/README.md https://github.com/kubernetes-incubator/apiserver-builder-alpha/blob/master/docs/installing.md https://github.com/kubernetes-incubator/apiserver-builder-alpha/blob/master/docs/concepts/api_building_overview.md https://github.com/kubernetes-incubator/apiserver-builder-alpha/blob/master/docs/tools_user_guide.md https://github.com/kubernetes-incubator/apiserver-builder-alpha/blob/master/docs/adding_non_namespaced_resources.md https://github.com/kubernetes-incubator/apiserver-builder-alpha/blob/master/docs/adding_defaulting.md https://github.com/kubernetes-incubator/apiserver-builder-alpha/blob/master/docs/adding_validation.md https://github.com/kubernetes-incubator/apiserver-builder-alpha/blob/master/docs/running_in_cluster.md https://github.com/kubernetes-incubator/apiserver-builder-alpha/blob/master/docs/building_docs.md https://github.com/kubernetes/code-generator https://kubernetes.io/docs/tasks/access-kubernetes-api/setup-extension-api-server/  ","permalink":"https://jeremyxu2010.github.io/2019/07/%E4%BD%BF%E7%94%A8aggregated-apiserver%E7%9A%84%E6%96%B9%E5%BC%8F%E6%9E%84%E5%BB%BAapi%E6%9C%8D%E5%8A%A1/","tags":["k8s","devops","api"],"title":"使用Aggregated APIServer的方式构建API服务"},{"categories":["容器编排"],"contents":"上周解决pvc无法mount的问题，其实留了一个尾巴，当时只是知道由于未知的原因，AttachDetachController执行detach操作失败了。这周这个问题又出现了，这次追查了一下根源，这里记录下。\n问题复现 在某个测试环境，删除大量pod，待kubernetes重建大量pod时，该问题就复现了。\n问题根源 检查kubelet的日志，可以看到以下的报错\nE0708 09:50:14.407804 26508 nestedpendingoperations.go:267] Operation for \u0026quot;\\\u0026quot;kubernetes.io/rbd/k8s:kubernetes-dynamic-pvc-998825ec-9c79-11e9-a3b2-fa163ed7c802\\\u0026quot;\u0026quot; failed. No retries permitted until 2019-07-08 09:50:14.907753132 +0800 CST m=+247869.135080115 (durationBeforeRetry 500ms). Error: \u0026quot;UnmountDevice failed for volume \\\u0026quot;pvc-9984bdab-9c79-11e9-b8ba-fa163ed7c802\\\u0026quot; (UniqueName: \\\u0026quot;kubernetes.io/rbd/k8s:kubernetes-dynamic-pvc-998825ec-9c79-11e9-a3b2-fa163ed7c802\\\u0026quot;) on node \\\u0026quot;10.125.54.133\\\u0026quot; : rbd: failed to unmap device /dev/rbd3, error exit status 16, rbd output: [114 98 100 58 32 115 121 115 102 115 32 119 114 105 116 101 32 102 97 105 108 101 100 10 114 98 100 58 32 117 110 109 97 112 32 102 97 105 108 101 100 58 32 40 49 54 41 32 68 101 118 105 99 101 32 111 114 32 114 101 115 111 117 114 99 101 32 98 117 115 121 10]\u0026quot; 上面的rbd output用ascii-to-text工具解码为字符串为rbd: sysfs write failed rbd: unmap failed: (16) Device or resource busy，应该是块设备被占用了。直接用rbd unmapdetach volome一次，发现也报这个错，所以该块设备一直被占用着，接下来查一下到底是什么进程占用着该块设备。\n先用lsof查一下：\n[root@node133 ~]# lsof 2\u0026gt;/dev/null | grep rbd3 rbd3-task 6561 root cwd DIR 253,1 4096 2 / rbd3-task 6561 root rtd DIR 253,1 4096 2 / rbd3-task 6561 root txt unknown /proc/6561/exe jbd2/rbd3 6589 root cwd DIR 253,1 4096 2 / jbd2/rbd3 6589 root rtd DIR 253,1 4096 2 / jbd2/rbd3 6589 root txt unknown /proc/6589/exe 可以看到只有两个进程占用着该块设备。网上google了下，找到这篇文章，最开始以为是内核的bug，于是将内核升级到较高的版本4.4.184-1.el7.elrepo.x86_64，但问题依然会重现。\n一愁莫展，就疯狂在网上搜寻答案，偶然看到这篇帖子里的一段话:\n I really expect, if the FS is marked as dirty (mounted), and JBD is still running\u0026hellip; that's because it's still mounted. E.g. what does /proc/self/mountinfo show? I wonder if you can pinky-swear you're not using mount namespaces\u0026hellip; see here for suggestions on how to find and inspect other mount namespaces. It assumes you are in the initial PID namespace, i.e. you're not running inside a sandbox where you can't see all the processes on the system. – sourcejedi May 18 \u0026lsquo;18 at 8:55\n 从这段话我知道了lsof查不到占用块设备文件的用户进程，很有可能是该进程在某个PID命名空间，在根PID命名空间通过普通命令查不到而已。\n于是我换了一种方式查，果然就查到关键的用户进程：\n[root@node133 ~]# grep \u0026#39;rbd3\u0026#39; /proc/*/task/*/mountinfo /proc/21839/task/28447/mountinfo:2892 2729 250:0 / /host/data/kubelet/plugins/kubernetes.io/rbd/mounts/k8s-image-kubernetes-dynamic-pvc-c838258f-9c7a-11e9-a3b2-fa163ed7c802 rw,relatime - ext4 /dev/rbd3 rw,stripe=1024,data=ordered [root@node133 ~]# ps -ef|grep 21839 nfsnobo+ 21839 21795 0 01:26 ? 00:00:08 /bin/node_exporter --path.procfs=/host/proc --path.sysfs=/host/sys --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|tmpfs)$ --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|data/docker/.+|etc/.+|run/secrets)($|/) --path.rootfs=/host 根源分析 竟然是prometheus的node_exporter进程占用着该块设备文件，真的是想不到啊。将该进程杀掉，rbd unmap终于可以正常detach rbd volume了。\n继续搜寻了一下，发现也有人遇到这个问题，这个评论里说明的原因是node_exporter因为要查探磁盘的使用率等数值，会\u0008将根文件系统挂载到容器里，而kubernetes默认的Mount propagation策略是None，即容器一旦启动，它从宿主机中读到的挂载信息就不变了，即使在宿主机里unmount了某个目录，容器里对此一无所知，它仍然会将块设备挂载到容器里的某个目录。\n继续看prometheus-node-exporter这个chart的变动，发现也有人给helm官方报告这个问题，并且还发了PR，在挂载卷时通过使用mountPropagation: HostToContainer选项以解决该问题。加上了该选项后，如果宿主机的挂载信息发生变动后，挂载信息将能传播到容器里。容器内也会unmount相应的目录，从而最终释放对块设备的占用。\n这里摘录下kubernetes提供的Mount propagation策略：\n Mount propagation allows for sharing volumes mounted by a Container to other Containers in the same Pod, or even to other Pods on the same node.\nMount propagation of a volume is controlled by mountPropagation field in Container.volumeMounts. Its values are:\nNone - This volume mount will not receive any subsequent mounts that are mounted to this volume or any of its subdirectories by the host. In similar fashion, no mounts created by the Container will be visible on the host. This is the default mode. This mode is equal to private mount propagation as described in the Linux kernel documentation\nHostToContainer - This volume mount will receive all subsequent mounts that are mounted to this volume or any of its subdirectories. In other words, if the host mounts anything inside the volume mount, the Container will see it mounted there.\nSimilarly, if any Pod with Bidirectional mount propagation to the same volume mounts anything there, the Container with HostToContainer mount propagation will see it.\nThis mode is equal to rslave mount propagation as described in the Linux kernel documentation\nBidirectional - This volume mount behaves the same the HostToContainer mount. In addition, all volume mounts created by the Container will be propagated back to the host and to all Containers of all Pods that use the same volume. A typical use case for this mode is a Pod with a Flexvolume or CSI driver or a Pod that needs to mount something on the host using a hostPath volume.\nThis mode is equal to rshared mount propagation as described in the Linux kernel documentation\nCaution: Bidirectional mount propagation can be dangerous. It can damage the host operating system and therefore it is allowed only in privileged Containers. Familiarity with Linux kernel behavior is strongly recommended. In addition, any volume mounts created by Containers in Pods must be destroyed (unmounted) by the Containers on termination.\n 问题解决 经验证prometheus-node-exporter挂载volume时加了mountPropagation: HostToContainer选项后，prometheus-node-exporter确实不会占用块设备了。\n但问题还没完，毕竟是采用Mount propagation方式来释放块设备的，既然是传播就存在一个时延。在这个时延范围内，如果detach volume还是会失败的。理论上kubernetes在detach volume失败后，会尝试重试的，但这个重试逻辑有些问题，会导致无法重试rbd unmap:\n// rbd volume will stuck when DetachDisk failed as Unmount will always returen not mounted  func (detacher *rbdDetacher) UnmountDevice(deviceMountPath string) error { if pathExists, pathErr := volutil.PathExists(deviceMountPath); pathErr != nil { return fmt.Errorf(\u0026#34;Error checking if path exists: %v\u0026#34;, pathErr) } else if !pathExists { klog.Warningf(\u0026#34;Warning: Unmount skipped because path does not exist: %v\u0026#34;, deviceMountPath) return nil } devicePath, _, err := mount.GetDeviceNameFromMount(detacher.mounter, deviceMountPath) if err != nil { return err } // Unmount the device from the device mount point.  klog.V(4).Infof(\u0026#34;rbd: unmouting device mountpoint %s\u0026#34;, deviceMountPath) if err = detacher.mounter.Unmount(deviceMountPath); err != nil { return err } klog.V(3).Infof(\u0026#34;rbd: successfully umount device mountpath %s\u0026#34;, deviceMountPath) klog.V(4).Infof(\u0026#34;rbd: detaching device %s\u0026#34;, devicePath) err = detacher.manager.DetachDisk(detacher.plugin, deviceMountPath, devicePath) if err != nil { return err } klog.V(3).Infof(\u0026#34;rbd: successfully detach device %s\u0026#34;, devicePath) err = os.Remove(deviceMountPath) if err != nil { return err } klog.V(3).Infof(\u0026#34;rbd: successfully remove device mount point %s\u0026#34;, deviceMountPath) return nil } 为此，我最终还是修改了kubernetes的代码，并给官方发了PR\n临时解决方案 等官方合入PR毕竟时间较长，在官方未修复该问题时，我们可以通过定时脚本规避该问题：\n/usr/local/bin/unmap_not_used_rbd.sh\n#!/bin/bash for v in $(find /dev -name \u0026#39;rbd*\u0026#39;); do # 如果设备没被占用，就正常unmap; 如果设备当前被使用，则unmap失败 /usr/bin/rbd unmap $v done # 每分钟定时执行下上述脚本 chmod +x /usr/local/bin/unmap_not_used_rbd.sh echo \u0026#39;* * * * * /usr/local/bin/unmap_not_used_rbd.sh \u0026gt;/dev/null 2\u0026gt;\u0026amp;1\u0026#39; \u0026gt;\u0026gt; /var/spool/cron/root 总结 有时候问题的根源总在一个想不到的地方，需要仔细追查。\n参考  https://www.fclose.com/linux-kernels/488425/jbd2-fix-use-after-free-in-kjournald2-linux-3-18-107/ https://www.browserling.com/tools/ascii-to-text https://unix.stackexchange.com/questions/437025/how-to-stop-jbd2-on-unmounted-filesystem https://github.com/kubernetes/kubernetes/issues/54214#issuecomment-341357733 https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation https://github.com/kubernetes/kubernetes/pull/79940  ","permalink":"https://jeremyxu2010.github.io/2019/07/%E5%BD%BB%E5%BA%95%E8%A7%A3%E5%86%B3pvc%E6%97%A0%E6%B3%95mount%E7%9A%84%E9%97%AE%E9%A2%98/","tags":["k8s","devops","ceph"],"title":"彻底解决pvc无法mount的问题"},{"categories":["容器编排"],"contents":"这周遇到了两个因pvc无法attach导致pod一直没法正常启动的问题，这里记录一下解决的过程。\n问题一 问题描述 一个deployment，在其spec中指定使用了某一个pvc，在很偶然的情况下，出现这一个deployment对应的pod被调度到了另外一个node节点，但pod在另外的node由于无法正常attach pv volume一直没法正常运行。\n问题解决 首先检查一下有问题的pod：\n$ kubectl describe pod xxxx Warning FailedAttachVolume 43m attachdetach-controller Multi-Attach error for volume \u0026#34;pvc-0a5eb91b-3720-11e8-8d2b-000c29f8a512\u0026#34; Volume is already exclusively attached to one node and can\u0026#39;t be attached to another 可以看到attachdetach-controller报告说这个volume已经被attach到一个node节点上了，因此不能被attach到其它的node节点。\n然后检查一下这个volume被哪个node节点attach住了：\n$ kubectl describe pv pvc-0a5eb91b-3720-11e8-8d2b-000c29f8a512 Source: Type: RBD (a Rados Block Device mount on the host that shares a pod\u0026#39;s lifetime) ... RBDImage: kubernetes-dynamic-pvc-0a5eb91b-3720-11e8-8d2b-000c29f8a512 ... RBDPool: k8s ... 这里可以看到这个volume对应的rbd镜像是kubernetes-dynamic-pvc-0a5eb91b-3720-11e8-8d2b-000c29f8a512，rbd池是k8s。\n接下来可以用ceph的相关命令查一下该rbd镜像现在被哪个node节点使用了：\n$ rbd info k8s/kubernetes-dynamic-pvc-0a5eb91b-3720-11e8-8d2b-000c29f8a512 rbd image \u0026#39;kubernetes-dynamic-pvc-0a5eb91b-3720-11e8-8d2b-000c29f8a512\u0026#39;: ... block_name_prefix: rbd_data.115f79643c9869 ... # 这里将上面的block_name_prefix属性值拿出来，将rbd_data修改为rbd_header即可 $ rados listwatchers -p k8s rbd_header.115f79643c9869 watcher=10.125.53.47:0/3423687629 client.1138525 cookie=18446462598732840965 上述命令就看到了这个rbd镜像是被10.125.53.47这个node节点使用了。\n接下来登录到10.125.53.47这个node节点，消除其对rbd镜像的使用。\n# 这里的路径为/dev/rbd/${rbdPool}/${rbdImage} $ ls -l /dev/rbd/k8s/kubernetes-dynamic-pvc-0a5eb91b-3720-11e8-8d2b-000c29f8a512 lrwxrwxrwx 1 root root 10 7月 2 11:30 /dev/rbd/k8s/kubernetes-dynamic-pvc-0a5eb91b-3720-11e8-8d2b-000c29f8a512 -\u0026gt; ../../rbd4 # 直接使用rbd unmap命令将rbd镜像unmap $ rbd unmap /dev/rbd4 到此问题解决。\n根源分析 首先回顾一下k8s里volume的挂载过程：\n provision，卷分配成功，这个操作由PVController完成 attach，卷挂载在对应worker node，这个操作为AttachDetachController完成 mount，卷挂载为文件系统并且映射给对应Pod，这个操作为VolumeManager完成  k8s里volume的卸载过程跟上述场景完全相反：\n umount，卷已经和对应worker node解除映射，且已经从文件系统umount detach，卷已经从worker node卸载 recycle，卷被回收  在我这个场景里，pod的迁移会导致原来的pod从其node节点删除，这时AttachDetachController没有成为将rbd从原来的node节点detach。后面多次尝试却无法重现问题，猜测是当时由于某些原因AttachDetachController执行detach操作失败了，可能是强制删除pod导致的，所以删除pod时还是要慎用—force —grace-period=0选项。\n问题二 问题描述 还是上述那个场景，这次对deployment作了一次滚动更新，这时k8s会新创建一个pod，尝试挂载volume，但这次原来那个node节点上pod仍处于Running状态，因而其使用volume是正常的。\n问题解决 这次很容易解决，直接删除旧的pod就可以了：\n$ kubectl delete pod xxxx 根源分析 很明显，滚动更新时产生多了一个pod，为什么会这样了，我们看一下deployment里的滚动更新策略：\n$ kubectl get deployment xxxx -o yaml ... deploySpec: replicas: 1 ... strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: volumes: - name: data persistentVolumeClaim: claimName: data-vol ... $ kubectl get pvc data-vol -o yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: data-vol spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: rbd 可以看到这里默认配置的滚动更新参数为maxSurge=1，也即允许比desired的pod数目多1个。而pvc又是ReadWriteOnce的访问模式，因此滚动更新时会产生多一个pod，而ReadWriteOnce的访问模式又不允许两个pod挂载同一个volume。\n因此这里有几个的解决方案：\n 使用ReadWriteMany访问模式的pvc 将maxSurge设置为0，避免在更新过程中产生多余的pod 将deployment改为statefulset，statefulset对应的pod与pvc是一一绑定的，在更新过程中不会产生多余的pod  总结 kuberentes里使用存储自有其逻辑，按照它的逻辑去分析问题，很多问题都可以迎刃而解。\n参考  http://newto.me/k8s-storage-architecture/ https://juejin.im/entry/5bc8be2ce51d450e8377e21d https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment  ","permalink":"https://jeremyxu2010.github.io/2019/07/%E8%A7%A3%E5%86%B3pvc%E6%97%A0%E6%B3%95mount%E7%9A%84%E9%97%AE%E9%A2%98/","tags":["k8s","devops","ceph"],"title":"解决pvc无法mount的问题"},{"categories":["容器编排"],"contents":"上两周，为了优化k8s的网络性能，最终选择了macvlan+ptp方案，最终性能也达到标准了。但其实存在一个问题，macvlan的pod的IP其实不太好分配。\n原来ip分配的问题 原来的方案直接使用官方的host-local进行IP分配，虽然很稳定，但不同的node节点需要配置一个不重叠的网段，最终需要底层网络预先分配一个比较大的网段作为macvlan的地址范围。而在很多私有部署场景，一般只会给一个24位前缀的网段，如果采用host-local进行IP分配，每个node节点将得不到足够大的cidr。\n集中式的ip分配 比较理想的方案是使用一个集中式的ip分配策略，各node节点从一个网段范围内按需申请pod的ip。集中式的ip分配方案比较多，官方本身就是dhcp的cni插件，另外也可以找一个集中存储（如consul, etcd），基于这个做集中式的cni插件。\n本以为这个方案很简单，理论上业界上应该已经有现成方案了，但实际上在网上找了一圈，只找到cni-ipam-consul，而且代码都是3年前的，连编译都不成功。看来只能自行开发。\n快速开发 为了减少依赖，最终计划开发一个cni-ipam-etcd，其直接采用kubernetes底层使用的etcd作为集中存储，存储ipam的ip分配信息。\n其实host-local这个cni插件源代码架构比较好，它默认是使用本地文件存储ip分配信息的，只需要将这些逻辑修改为读写etcd就可以了。\n参考上述思路，我快速完成了此cni插件的开发，源代码地址为cni-ipam-etcd。这里对etcd的读写代码参考这里。\n这个cni插件使用也比较简单，可配合常用的underlay网络cni插件使用，如下：\n{ \u0026#34;name\u0026#34;: \u0026#34;mymacvlan\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;macvlan\u0026#34;, \u0026#34;master\u0026#34;: \u0026#34;enp5s0f0\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;myetcd-ipam\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;etcd\u0026#34;, \u0026#34;etcdConfig\u0026#34;: { \u0026#34;etcdURL\u0026#34;: \u0026#34;https://10.10.20.152:2379\u0026#34;, \u0026#34;etcdCertFile\u0026#34;: \u0026#34;/etc/etcd/ssl/etcd.pem\u0026#34;, \u0026#34;etcdKeyFile\u0026#34;: \u0026#34;/etc/etcd/ssl/etcd-key.pem\u0026#34;, \u0026#34;etcdTrustedCAFileFile\u0026#34;: \u0026#34;/etc/kubernetes/ssl/ca.pem\u0026#34; }, \u0026#34;subnet\u0026#34;: \u0026#34;10.10.20.0/24\u0026#34;, \u0026#34;rangeStart\u0026#34;: \u0026#34;10.10.20.50\u0026#34;, \u0026#34;rangeEnd\u0026#34;: \u0026#34;10.10.20.70\u0026#34;, \u0026#34;gateway\u0026#34;: \u0026#34;10.10.20.254\u0026#34;, \u0026#34;routes\u0026#34;: [{ \u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34; }] } } 简化部署 当然手工登录到每个node节点部署上述配置很是简单的，但在k8s里我们可以使用daemonset快速完成cni插件的部署，deamonset的配置可参考如下yaml：\nkind: ConfigMap apiVersion: v1 metadata: name: kube-macvlan-cfg namespace: kube-system labels: tier: node app: macvlan data: etcd.pem: | -----BEGIN CERTIFICATE----- MIID5DCCAsygAwIBAgIUB9cmzTIGNK2SINBAl9Cx284K6TowDQYJKoZIhvcNAQEL BQAwYjELMAkGA1UEBhMCQ04xEjAQBgNVBAgTCUd1YW5nRG9uZzELMAkGA1UEBxMC U1oxDDAKBgNVBAoTA2s4czEPMA0GA1UECxMGU3lzdGVtMRMwEQYDVQQDEwprdWJl cm5ldGVzMB4XDTE5MDQxNjIzMjYwMFoXDTI5MDQxMzIzMjYwMFowXDELMAkGA1UE BhMCQ04xEjAQBgNVBAgTCUd1YW5nRG9uZzELMAkGA1UEBxMCU1oxDDAKBgNVBAoT A2s4czEPMA0GA1UECxMGU3lzdGVtMQ0wCwYDVQQDEwRldGNkMIIBIjANBgkqhkiG 9w0BAQEFAAOCAQ8AMIIBCgKCAQEAz0avwoL3gTbLIjGURQi/8r+Np1A4ALLSR+KS ig4MA8nUYwO5WoU6+71nF83kpO9KnSr0YrsgXIYgI2u57AxR7WFMvPphGy9C+9Z0 BHDk6LCciiYiZphoE6792WfUchHrRjBbiAJDvvpb2qEu6qY53c1KQkX7jLVjkHt5 bMOBhY/Y33J4uCsokmPmFZ1GxtwV8wsXq/flWCbQ7dC9sMMO3JpNrG7/tiv+lQmv uPojcMUt/ZVnbRU+OXnlqljJDYLu2OqY0PPxXR9t9WpSpdesNQblwopTH+MzH0Ga yXY8BhvwuRkFHTpVIeYUraTFSokopL0XSSF9DDBUJ8E+QS1MEQIDAQABo4GXMIGU MA4GA1UdDwEB/wQEAwIFoDAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUHAwIw DAYDVR0TAQH/BAIwADAdBgNVHQ4EFgQUJ9k4rVtGuz+bip/YgGpAftPH2I8wHwYD VR0jBBgwFoAUUHIr7PnnjCO6GEc0+wln7Pr2HnowFQYDVR0RBA4wDIcEfwAAAYcE CU0L7DANBgkqhkiG9w0BAQsFAAOCAQEAhSjef8eG8X6z8xZEPNHEACKRgsPP8Hmv AzxJ2TLAYjGQaM7TklRseHSxn7cAx4bwvHZtVPRrIgK5ylkm4QHGQfMYejWQnUzJ m0F5/3oXfDTA10muw4tGQu7njUKMsfHnvgokxMjk3xdPpy+WrBsFtmO/TRgypzle 3MTXbWIFDPaxVRx1oBtIuTkYZjc1CHUMyuXQhWU8mZCoGqFpcYwfRjUtA6hWe8xJ 7aDEnxpXkA4/ehuWTrl3QCSrg4NBXqufSy7V0Y+mErxC9996rYP80bR8gdGKUo65 bWmgDT/c49TA96MGbepsZPbDdFVnr1g5jiJnXQSYDw1pdCxb5MsggQ== -----END CERTIFICATE----- etcd-key.pem: | -----BEGIN RSA PRIVATE KEY----- MIIEpAIBAAKCAQEAz0avwoL3gTbLIjGURQi/8r+Np1A4ALLSR+KSig4MA8nUYwO5 WoU6+71nF83kpO9KnSr0YrsgXIYgI2u57AxR7WFMvPphGy9C+9Z0BHDk6LCciiYi ZphoE6792WfUchHrRjBbiAJDvvpb2qEu6qY53c1KQkX7jLVjkHt5bMOBhY/Y33J4 uCsokmPmFZ1GxtwV8wsXq/flWCbQ7dC9sMMO3JpNrG7/tiv+lQmvuPojcMUt/ZVn bRU+OXnlqljJDYLu2OqY0PPxXR9t9WpSpdesNQblwopTH+MzH0GayXY8BhvwuRkF HTpVIeYUraTFSokopL0XSSF9DDBUJ8E+QS1MEQIDAQABAoIBAQDEuAqhadjrGozJ xBI7PqWmBqSzMZAlIZIvRVrciZ5fjhLzchpdTer/9u88CV3CJ5VB+v18IqsBBQ7F bz1CSSMMTvcct+ine0BwcUUk3dxy9wNqneyyQF0uqTslNcTMCjOoJscIG1Yej8/T fHxhmSd8WZTrty2ZiqGXA4jnb9miXoEtpHW65kWq50qK/ElxRhqHrMn3TO06nr+w tB7kSnT2E6Bx5eXCzvKL+2DlUWlBjme9dSessg566i+3Ua5Zmc2/SY3aS69Wp+9a DHsdLAtpVh/sfO3GXLEzoGo0wcPEjtbeV9snSGfQDluzt2rf1lht9vPHpbOpKZIj F61E6H0tAoGBAOUPQURoW37W22S8rH3u6iO53CgfWfiyIOoOk3hVWmmjuGx53i5d v8+dHNNSc94kj7EH5EIcksKpdc6fh9mRrHwnsTs3Pa6OgvcqR2LUe0UlECJ714HX SSQGNnYwZ+NKyOk9BkRt6PdAyvS/om3VSn7w6tXqb9SJBjBSbyo8C/czAoGBAOen jdd7Zt8yREE/GRfYLGANBfwbNcFAz5vXxRg+943OGmiJMr5q/o+rg+ubB119maZI fHzFryN77ZLo8gcudaiu6fNE0AxP68N3m0dUcOSSoX15MjWX3MQRtv7kYklg3lCr 5FpcXo/IIWTk5W8MgBUxrntPWjIeADkkbGGEwB+rAoGAajlA0zBx/cg1xemZNG1v N1IhvpmLZ8FzcheAW/V5EDRUejmpx2bCZM0/aOB7yzC5UieOuvn1NUDQ2RkyLrtX edwOXJ+pgyGjqmt432QaJl6htNwpfJUR3hrjdrvL8aPkuAUMuv8dYkwx0n5sHPMk sOmYfctSQQWqUQ5pbvSZt/ECgYEAydJPoGFxkZkQoCuh6AU9O/18rlTic1jMx0Co BWSudowOs+58GCvNVkwepcCuHQSVPaq/UlFEMc0BgVGTszAF8A1b48aa328tv2FQ Fkf6Bxm8uj1BwjFpdCTe4pkFDFrptSzcyODavbelaGqHfUVNvalIE0RiF3HNzfru tdNbMvsCgYACKftFa4UkHZjO8wEaTLpUOotJYRRaWcDxkuHSc+zOr2/X+Q6GI3k/ r1hi51otQSHissiNcTBskpsVO8JJUye77q5igNI/6rnmT5lmqbZrYQK5J322jYxt IVlXiSSyuY6ZL8k4yc27XG5Petk0SZiEdItRLh2AC5z/8Pg8gHFlsQ== -----END RSA PRIVATE KEY----- etcd-ca.pem: | -----BEGIN CERTIFICATE----- MIIDuDCCAqCgAwIBAgIUK4+VweLZgPFnIBGYCxBAzDJ17pwwDQYJKoZIhvcNAQEL BQAwYjELMAkGA1UEBhMCQ04xEjAQBgNVBAgTCUd1YW5nRG9uZzELMAkGA1UEBxMC U1oxDDAKBgNVBAoTA2s4czEPMA0GA1UECxMGU3lzdGVtMRMwEQYDVQQDEwprdWJl cm5ldGVzMB4XDTE5MDIyMDA2NDEwMFoXDTI0MDIxOTA2NDEwMFowYjELMAkGA1UE BhMCQ04xEjAQBgNVBAgTCUd1YW5nRG9uZzELMAkGA1UEBxMCU1oxDDAKBgNVBAoT A2s4czEPMA0GA1UECxMGU3lzdGVtMRMwEQYDVQQDEwprdWJlcm5ldGVzMIIBIjAN BgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAttiDwQyPK41fI8NgRjdF3AIr9P1U hBS0JFlEJ2UVDhQDfS+ns42r4IHGayWaydldlcFX7Xhg0qQAONLK+2p6fXojCDM6 ZEARNnOuQGFaOfrihsukQsj6Zn3VSi8PgwcMJUzjiY932cOTcQM49J9LOz0QCp8E tQ+so6qE7Bl0om+ifRuq1+O5bGmCBb2EEQfgHeuluZ8LoyHhQEbI3qq9e3XtdyKm KO1BRk7Z/m24amoC72gxljKP2KV1oiCZj+VcpFwEPpmYaNp1We0jk7WgDXZUXBkE NqhC2/Y2sUQ5VWm0dOnCA6pE3Fq2C7krVxhOelKIxFzuV56AeCVxe8qKMQIDAQAB o2YwZDAOBgNVHQ8BAf8EBAMCAQYwEgYDVR0TAQH/BAgwBgEB/wIBAjAdBgNVHQ4E FgQUUHIr7PnnjCO6GEc0+wln7Pr2HnowHwYDVR0jBBgwFoAUUHIr7PnnjCO6GEc0 +wln7Pr2HnowDQYJKoZIhvcNAQELBQADggEBAEwf+/Cy62uvp3DJIGubFP4AzKQL pbxJwjn+dsSJFJkIwSCJJJPVwtU8qPdwBS2UKJ1evFBvk+2uLyCKW1F6I5Ryc658 E/dOXAPUP0cpA1vwdkNv7yFqBY9S7pm9q2BZEOfwbS5sTkRztwqP75DKEf8t9eq/ 9JnTEhs3tCUtv2uSKLbwGrT/lmPwT32Xn90tYbrasfr9bzS8qhAml8KnHyIs0/DB h+p5N0gSRODK+u/JwSdnlwwDiYzeKfo87RypNCJt7UsX6J3RK8GNJwSMy5s70XhS 1C0s4RRsDT7dhRssPBCvsH6HHjrLUAM7hd1XBAdkjMDA4B8qDnjb+2fAKhs= -----END CERTIFICATE----- cni-conf.json: | { \u0026#34;name\u0026#34;: \u0026#34;cni0\u0026#34;, \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;mymacvlan\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;macvlan\u0026#34;, \u0026#34;master\u0026#34;: \u0026#34;enp5s0f0\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;myetcd-ipam\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;ipam-etcd\u0026#34;, \u0026#34;etcdConfig\u0026#34;: { \u0026#34;etcdURL\u0026#34;: \u0026#34;https://10.10.20.152:2379\u0026#34;, \u0026#34;etcdCertFile\u0026#34;: \u0026#34;/etc/cni/net.d/etcd.pem\u0026#34;, \u0026#34;etcdKeyFile\u0026#34;: \u0026#34;/etc/cni/net.d/etcd-key.pem\u0026#34;, \u0026#34;etcdTrustedCAFileFile\u0026#34;: \u0026#34;/etc/cni/net.d/etcd-ca.pem\u0026#34; }, \u0026#34;subnet\u0026#34;: \u0026#34;10.10.20.0/24\u0026#34;, \u0026#34;rangeStart\u0026#34;: \u0026#34;10.10.20.50\u0026#34;, \u0026#34;rangeEnd\u0026#34;: \u0026#34;10.10.20.70\u0026#34;, \u0026#34;gateway\u0026#34;: \u0026#34;10.10.20.254\u0026#34;, \u0026#34;routes\u0026#34;: [{ \u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34; }] } }, { \u0026#34;name\u0026#34;: \u0026#34;ptp\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;veth-to-host\u0026#34;, \u0026#34;hostInterface\u0026#34;: \u0026#34;enp5s0f0\u0026#34;, \u0026#34;containerInterface\u0026#34;: \u0026#34;veth0\u0026#34;, \u0026#34;ipMasq\u0026#34;: true } ] } --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: kube-macvlan-ds namespace: kube-system labels: tier: node app: macvlan spec: template: metadata: labels: tier: node app: macvlan spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: amd64 tolerations: - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule initContainers: - name: install-cni-binaries image: k8s-network:v1.0.0 command: - sh args: - -c - cp -r /opt/cni/bin/* /host/opt/cni/bin/ volumeMounts: - name: host-cni-bin mountPath: /host/opt/cni/bin - name: install-cni-cfg image: k8s-network:v1.0.0 command: - cp args: - -f - /etc/kube-macvlan/cni-conf.json - /etc/cni/net.d/00-macvlan.conflist volumeMounts: - name: host-cni-cfg mountPath: /etc/cni/net.d - name: macvlan-cfg mountPath: /etc/kube-macvlan/ - name: install-etcd-certs image: k8s-network:v1.0.0 command: - sh args: - -c - cp -f /etc/kube-macvlan/etcd*.pem /etc/cni/net.d/ volumeMounts: - name: host-cni-cfg mountPath: /etc/cni/net.d - name: macvlan-cfg mountPath: /etc/kube-macvlan/ containers: - name: kube-macvlan image: k8s-network:v1.0.0 resources: requests: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;50Mi\u0026#34; limits: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;100Mi\u0026#34; securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: macvlan-cfg mountPath: /etc/kube-macvlan/ volumes: - name: run hostPath: path: /run - name: host-cni-cfg hostPath: path: /etc/cni/net.d - name: macvlan-cfg configMap: name: kube-macvlan-cfg - name: host-cni-bin hostPath: path: /opt/cni/bin 假设提前将名为macvlan、ipam-etcd的cni插件二进制文件放入了k8s-network:v1.0.0 这个docker镜像中。\n这样部署就很方便了，命令如下：\nkubectl apply -f macvlan-dpl.yaml 总结 通过这两周的实践，基本完成了开发cni网络插件的一整套流程，算是又开启了一门技能。\n参考  https://github.com/logingood/cni-ipam-consul https://github.com/containernetworking/plugins/tree/master/plugins/ipam/host-local https://enpsl.top/2019/01/05/2019-01-05-golang-etcd/  ","permalink":"https://jeremyxu2010.github.io/2019/07/%E5%AE%8C%E5%96%84cni%E7%9A%84ipam%E6%96%B9%E6%A1%88/","tags":["k8s","devops","cni"],"title":"完善cni的ipam方案"},{"categories":["容器编排"],"contents":"k8s中使用underlay网络的障碍 上一篇说到在k8s里使用underlay网络有一个弊端，使用了underlay网络的pod无法访问serviceIP，这一点可能通过修改修改业务应用的chart来解决，主要解决方法是：\n 使kube-dns服务通过underlay网络直接可达，可以将coredns的pod设置为使用hostNetwork，然后修改kubelet里配置的--cluster-dns参数。 修改业务应用的chart，避免使用serviceIP，可以通过其它服务发现机制直接找到目标pod的podIP。  上述两个方案虽然可行，但需要进行一系列调整，成本确实比较高，如果业务应用的chart比较复杂，改造起来就更费劲了。\n使用修改后的ptp解决上述问题 在网上偶然看到一篇文章，在这篇文章里讲到可以向已经使用了macvlan等underlay网络的pod中再插入一个ptp的网络接口，设置必要的路由规则后，即可实现在这种pod中也可以正常访问serviceIP。当然依旧是使用cni插件的方式实现的。Bingo, 这正是我需要的。\n于是我参考unnumbered-ptp.go, 我实现了一个veth-to-host的cni插件，该插件的原理如下：\n 创建⼀对veth pair，⼀端挂⼊容器内，⼀端挂⼊宿主机内。 在容器内设置路由规则，当目标地址是宿主机IP或非underlay网络，则使用veth将数据包转出，nexthop地址是宿主机IP。 在宿主机内设置路由规则，当目标地址是容器的underlay网络IP时，使用veth将数据包转出, nexthop地址是容器的underlay网络IP。  然后原来的macvlan配合使用该cni插件，即可在使用了underlay网络的pod也可以正常访问serviceIP，cni的配置如下：\n/etc/cni/net.d/10-cnichain.conflist\n{ \u0026#34;name\u0026#34;: \u0026#34;cni0\u0026#34;, \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;mymacvlan\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;macvlan\u0026#34;, \u0026#34;master\u0026#34;: \u0026#34;enp5s0f0\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, \u0026#34;subnet\u0026#34;: \u0026#34;10.10.20.0/24\u0026#34;, \u0026#34;rangeStart\u0026#34;: \u0026#34;10.10.20.50\u0026#34;, \u0026#34;rangeEnd\u0026#34;: \u0026#34;10.10.20.60\u0026#34;, \u0026#34;gateway\u0026#34;: \u0026#34;10.10.20.254\u0026#34;, \u0026#34;routes\u0026#34;: [{ \u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34; }] } }, { \u0026#34;name\u0026#34;: \u0026#34;myptp\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;veth-to-host\u0026#34;, \u0026#34;hostInterface\u0026#34;: \u0026#34;enp5s0f0\u0026#34;, \u0026#34;containerInterface\u0026#34;: \u0026#34;veth0\u0026#34;, \u0026#34;ipMasq\u0026#34;: true } ] } 这里有几点要注意一下：\n veth-to-host cni插件的二进制文件需要放入CNI_BIN_PATH目录中 上述的cni配置是一个链式配置，一定要注意顺序，macvlan等underlay网络的cni配置要放在前面  应用了上述cni配置后，一般来说可以正常工作了。如果我们对实现原理比较好奇的话，可以简单创建一个pod，执行以下命令观察创建的网络接口和各种路由规则：\n# 检查pod容器里的网络接口 [root@node-1 ~]# kubectl -n demo exec -ti redis-predixy-67d989bdd9-p7fbf sh / # ip link show 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: eth0@if3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue state UNKNOWN link/ether 2e:31:a0:bc:39:20 brd ff:ff:ff:ff:ff:ff 4: veth0@if139: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue state UP link/ether 46:4a:fd:d5:3a:bf brd ff:ff:ff:ff:ff:ff # 检查pod容器里的路由规则 / # ip route show default via 10.10.20.152 dev veth0 10.10.20.0/24 dev eth0 scope link src 10.10.20.53 10.10.20.152 dev veth0 scope link # 退出pod容器 / # exit # 检查宿主机里的网络接口 [root@node-1 ~]# ip link show ... 139: vetha0e043dd@if4: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 12:78:cc:ea:9b:26 brd ff:ff:ff:ff:ff:ff link-netnsid 3 ... # 检查宿主机主表里的路由规则 [root@node-1 ~]# ip route show ... 10.10.20.53 dev vetha0e043dd scope link ... # 检查宿主机的自定义路由表 [root@node-1 ~]# ip rule show ... 1024:\tfrom all iif vetha0e043dd lookup 596 ... # 检查宿主机上自定义路由表中的路由规则 [root@node-1 ~]# ip route show table 596 default via 10.10.20.53 dev vetha0e043dd 从上述输出可以看出确实与该CNI插件的原理是一致的。\n开发cni插件的一般过程 上面实际的开发了一个崭新的cni插件，下面总结一下开发cni插件的过程：\n  首先是画好网络拓扑图，示例如下。在这个拓扑图中要将同宿主机上的pod之间、pod及宿主机、跨主机上的pod之间网络流量如何流转描述清楚，这里需要了解较多的网络知识，如网络的二三层、路由表、nat、bridge、openvswith、sdn、policy route等。\n  然后到cni官方插件这里找一找，看看有没有满足需求的现成cni插件。除非需求很特殊，一般是可以找到现成插件的。\n  如果找不到现在插件，可以参考cni插件示例编写新的cni插件，主要就是按网络拓扑图里设想的方案实现cmdAdd、cmdDel等方法，这些方法的主要逻辑就是按设想创建网络接口、设置路由规则、设置NAT规则等。\n  编写好cni插件后，编译出cni插件的二进制文件，可以本地先测试一下：\n# 提前将二进制文件放入CNI_PATH目录下，配置好/etc/cni/net.d目录下的cni配置文件 # 下面会创建一个网络命名空间，并调用cni插件尝试为该网络命名空间配置好网络 ip netns add test CNI_PATH=/usr/local/bin NETCONFPATH=/etc/cni/net.d /usr/local/bin/cnitool add cni0 /var/run/netns/test # 进入网络命名空间，检查网络接口、路由表信息 ip netns exec test sh .... # 在宿主机检查网络接口、路由表信息 ip link show ip route show ip rule show iptables -t nat -L -n iptables -L -n 检查结果如果没有什么大的问题，跟设想一致的话，这个cni插件就差不多了。\n  接下来可以创建两个跨主机的pod再检查一下。\n  如果可以的话，写点单元测试就更好了，参考这里。\n  最后记得为新写的cni插件写个文档，参考这里。\n  总结 总的来说，开发一个cni网络插件过程还是挺清晰的，也比较简单，本周也算是又掌握一门技能。\n参考  https://github.com/containernetworking/cni https://juejin.im/post/5c926709f265da60e86e0ca6#heading-3 https://github.com/kubernetes/dns/issues/176 https://github.com/lyft/cni-ipvlan-vpc-k8s/blob/master/plugin/unnumbered-ptp/unnumbered-ptp.go https://github.com/containernetworking/plugins  ","permalink":"https://jeremyxu2010.github.io/2019/06/%E6%B8%85%E9%99%A4k8s%E4%BD%BF%E7%94%A8underlay%E7%BD%91%E7%BB%9C%E7%9A%84%E9%9A%9C%E7%A2%8D/","tags":["macvlan","underlay","kubernetes"],"title":"清除k8s使用underlay网络的障碍"},{"categories":["容器编排"],"contents":"上周在kubernetes里发布了一个redis PaaS服务，不过其它同学简单测了一下，虽说功能上没啥问题，但性能相比物理上运行的Redis集群差太远，而且随着redis的分片数增加，性能并不能很好地线性增长，增长到一定程度就停止了，这个是需求方不能接受的，于是本周接了活，对部署在kubernetes中的redis服务进行性能优化。\n基准测试 按照之前此类工作的工作方法，首先进行基准测试，得到目前的性能状况指标，也便于后面总结本次优化的成果。\n我这里使用多个redis-benchmark进程对一个3分片的redis进行压测，最后得到的性能指标如下：\n   序号 场景 总QPS     1 3个物理机，每个物理机部署一个Redis分片 45w   2 同样在3个物理机上部署kuberntes集群，在其中部署3个pod，每个pod均部署一个redis分片 10w    逐步优化 优化内核参数 首先参考performance-tips-for-redis-cache-server优化几个十分影响redis集群性能的内核参数，由于redis是部署在kubernetes的pod中，因此优化方法跟文章中提到的办法有一点点不一样，如下：\n# sysctl.conf中配置fs.file-max、net.core.somaxconn两个属性 $ cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/sysctl.conf fs.file-max=655350 net.core.somaxconn=20480 EOF sysctl -p # limits.conf中配置文件句柄数及进程数的硬限制和软限制 $ cat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; \u0026gt;\u0026gt; /etc/security/limits.conf *\thard\tnofile\t655350 *\tsoft\tnofile\t655350 *\thard\tnproc\t655350 *\tsoft\tnproc\t655350 EOF # 关闭内存transparent_hugepage特性 $ cat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; \u0026gt;\u0026gt; /etc/rc.local echo never \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled EOF $ echo never \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled # kubelet中允许修改pod的net.core.somaxconn内核参数 $ cat /etc/systemd/system/kubelet.service ... ExecStart=/usr/local/bin/kubelet \\  ... --allowed-unsafe-sysctls=net.core.somaxconn \\  ... # 修改pod的net.core.somaxconn内核参数 $ kubectl -n demo get statefulsets redis-redis-cluster -o yaml ... podSpec: securityContext: sysctls: - name: net.core.somaxconn value: \u0026#34;20480\u0026#34; ... CPU绑核 压测时发现虽然服务的cpu核数较多，但任务数也有些多，cpu的争抢有些严重，因此这里进行CPU绑核操作。\n首先对redis的pod进行cpu绑核，这里参考kubernetes的官方文档-控制节点上的CPU管理策略。\n# 启用kubelet的静态绑核开关 $ cat /etc/systemd/system/kubelet.service ... ExecStart=/usr/local/bin/kubelet \\  ... --feature-gates=CPUManager=true \\  --cpu-manager-policy=static \\  --system-reserved=cpu=2,memory=500Mi,ephemeral-storage=1Gi \\  ... # pod的resources.limits.cpu及resources.requests.cpu设置为相同的整数 $ kubectl -n demo get statefulsets redis-redis-cluster -o yaml ... podSpec: resources: limits: cpu: \u0026#34;1\u0026#34; ... requests: cpu: \u0026#34;1\u0026#34; ... ... 为了减少网卡软中断CPU上下文切换的开销，这里对之进行绑定CPU，这里参考网上的一篇网卡软中断优化的文档。\n# 绑定网卡软中断至CPU0-CPU7 $ cat scripts/bind_nic_softirq.sh #!/bin/bash set -e -u systemctl stop irqbalance.service nic_name=enp5s0f0 irq_nos=$(grep \u0026#34;${nic_name}-TxRx\u0026#34; /proc/interrupts | awk \u0026#39;{print $1, $NF}\u0026#39; | awk -F \u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39;) dec_value=1 for irq_no in ${irq_nos[*]}; do cpu_smp_affinity=$(printf \u0026#39;%x\u0026#39; ${dec_value}) echo ${cpu_smp_affinity} \u0026gt; /proc/irq/${irq_no}/smp_affinity dec_value=$((2*${dec_value})) done bash scripts/bind_nic_softirq.sh iptables切换为ipvs 在压测过程中发现直接压测podIP性能会好不少，但压测serviceIP性能打一个折扣。而由kubernetes Service的实现原理可知，serviceIP是由iptables或ipvs实现的。社区里也谈到ipvs确实比iptables有更好的性能，从kubernetes 1.12开始就默认使用ipvs了。而我这里用的是kubernetes 1.11版本，因此手动配置一下以启动ipvs。\n# 所有node节点安装ipset $ yum install -y ipset # 配置启动时加载ipvs相关内核模块 $ cat /etc/sysconfig/modules/ipvs.modules #!/bin/bash ipvs_modules=(ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4) for kernel_module in ${ipvs_modules[*]}; do /sbin/modinfo -F filename ${kernel_module} \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 if [ $? -eq 0 ]; then /sbin/modprobe ${kernel_module} fi done $ chmod +x /etc/sysconfig/modules/ipvs.modules # kube-proxy启用ipvs模式 $ cat /etc/systemd/system/kube-proxy.service ... ExecStart=/usr/local/bin/kube-proxy \\  ... --proxy-mode=ipvs \\  --ipvs-min-sync-period=5s \\  --ipvs-sync-period=5s \\  --ipvs-scheduler=rr \\  --masquerade-all \\  ... ... 由overlay网络切换为underlay网络 继续压测，发现已可以达到26wQPS了，但和在物理机上部署的redis集群性能还有差距。咨询了专门搞kubernetes容器网络的同学，他建议使用underlay网络。\n这里提一下两个概念：\nUnderlay网络：Underlay网络由底层网络驱动将接口暴露给虚机或容器，比较常用的方案有bridge, macvlan, ipvlan, sriov等。\nOverlay网络：Overlay网络无需改造网络架构，只需三层可达即可，将二层报文封装在IP报文中。这样能利用成熟的IP路由协议进行数据分发，采用隔离标识能够突破VLAN的数量限制，必要时把广播流量转化为组网流量避免广播数据泛滥。比较常见的方案有vxlan, gre等。\n可以看出在Underlay网络下，容器里看到的是底层实际的网络接口，直接读写这种网络接口自然比Overlay网络下那种虚拟出来的网络性能好得多。\n由于不方便升级内核，因此这里就采用最简单的macvlan CNI网络方案了，切换方法就不详述了，参考这篇文章就可以了。\n不过切换为macvlan之间遇到了几个问题。\n  容器内无法ping通本主机主接口ip。\n这个会导致kubernetes无法对pod进行正常的健康检测。这里在网上找到一个解决方案：\n$ ip link add link ens160 mac0 type macvlan mode bridge # 下面的命令一定要放在一起执行，否则中间会失去连接 $ ip addr del 192.168.179.9/16 dev ens160 \u0026amp;\u0026amp; \\  ip addr add 192.168.179.9/16 dev mac0 \u0026amp;\u0026amp; \\  ip link set dev mac0 up \u0026amp;\u0026amp; \\  ip route flush dev ens160 \u0026amp;\u0026amp; \\  ip route flush dev mac0 \u0026amp;\u0026amp; \\  ip route add 192.168.0.0/16 dev mac0 metric 0 \u0026amp;\u0026amp; \\  ip route add default via 192.168.1.1 dev mac0 其实就是建立一个macvlan bridge，将主机主接口桥接到这上面，将主机主接口的ip挪到该bridge上的一个mac0网络接口上。\n  使用macvlan ip的pod无法访问kubernetes里的serviceIP。\nkubernetes里的serviceIP实现原理参见clusterip的实现机制，说白了serviceIP是由iptables或ipvs机制模拟出的虚拟IP，它的流量分发是由iptables或ipvs进行必要的NAT操作实现的。而macvlan之类的UnderLay网络方案属于外部网络，并且拥有独立的网络空间namespace，所以并不会经过node的网络空间的内核协议栈，进而造成并不会经过iptables/ipvs的配置，因此使用了macvlan的pod，自然无法正常访问servicrIP。kubernetes的clusterip机制调研及macvlan网络下的clusterip坑解决方案这篇文章也谈到了两个解决方案：\n  部分Node标记master，采用cluster network，例如flannel/calico/weave，貌似最近weave比较火，可以借机熟悉一下。然后部署管理Pod的时候，指定部署到master上去。\n  基于multus-cni插件做双网卡，然后配置默认路由走macvlan的网卡，内部网络走cluster network那块网卡。\n  我这里采用的方案一，给node打标签，区别出两种不同的node，采用不同的cni网络方案，一个是overlay网络，一个是underlay网络。利用节点亲和性规则，将一般应用的pod都调度到overlay网络的node上，将对网络性能有要求的pod调度到underlay网络的node上。同时调度到underlay网络的pod中要避免使用Service。\n节点亲和性规则如下编写：\n# 给某些node节点打label，标记为该node节点上运行管理类pod，该node节点上使用flannel的CNI网络方案，其它node节点上使用macvlan的CNI网络方案 $ kubectl label node 10.10.20.151 managed_node=true # 业务类的pod使用nodeAffinity，使之被调度到没有打了label的node节点 $ kubectl -n demo get statefulsets redis-redis-cluster -o yaml ... podSpec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: managed_node operator: NotIn values: - \u0026#34;true\u0026#34; 当然另一种方案后面也可以尝试一下，可以参考这里的文章。\n  性能回归测试 还是基准测试里的场景，重新进行压测，得到以下数据：\n   序号 场景 总QPS     1 3个物理机，每个物理机部署一个Redis分片 45w   2 3个物理机上部署kuberntes集群，在其中部署3个pod，每个pod均部署一个redis分片 44w    对比在物理机上直接部署的redis集群，两者的性能基本相近了，达到调优的目标。\n总结 相比功能开发，性能调优是一个很有趣的工作，其需要对运行的平台、软件架构、硬软件基础有比较深入的了解才行，在调优的过程中也可以将之前了解的一些概念性理论在实际场景进行验证，从而理解得更深刻。因此调优的工作还是相当难得和具有挑战的。\n参考   https://www.techandme.se/performance-tips-for-redis-cache-server/\n  https://kubernetes.io/zh/docs/tasks/administer-cluster/sysctl-cluster/\n  https://kubernetes.io/zh/docs/tasks/administer-cluster/cpu-management-policies/\n  http://www.simlinux.com/2017/02/28/net-softirq.html\n  https://blog.csdn.net/fanren224/article/details/86548398\n  http://lotleaf.com/linux/docker-network.html\n  https://blog.csdn.net/cloudvtech/article/details/79830887\n  https://www.yangcs.net/posts/macvlan-in-action/\n  https://zhuanlan.zhihu.com/p/67384482\n  https://juejin.im/post/5c926709f265da60e86e0ca6#heading-3\n  ","permalink":"https://jeremyxu2010.github.io/2019/06/kuberntes%E4%B8%AD%E7%9A%84redis%E9%9B%86%E7%BE%A4%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/","tags":["redis","kubernetes"],"title":"kuberntes中的redis集群性能调优"},{"categories":["容器编排"],"contents":"工作中需要向外部提供一些诸如MySQL、Redis、MongoDB、Kafka之类的基础PaaS服务。以前每做一个PaaS都要自己去实现工作节点管理、实例调度、实例运维、实例监控等功能模块，实在是太累。这次花了些时间想了下，感觉基于Kubernetes做这个会简单很多。下面概要性地梳理下基于Kubernetes构建基础PaaS服务的过程。\n构建基础PaaS服务 将基础PaaS服务部署进kubernetes 假设现在一套生产可用的Kubernetes集群就绪了，第一步要做的是将目标基础服务部署进kubernetes。大体有三种方案：\n 使用kubernetes的yaml描述文件部署 使用helm chart部署 使用operator部署  第一种方法太琐碎，长长的yaml描述文件也不方便维护。\n第三种方法是现在社区最推荐的，但实际执行后，发现问题还比较多：\n1）目前网上的operator质量参差不齐，很多是半成品，有一些官方维护的operator质量虽说还可以，但跟官方推荐的商业版软件绑定太死\n2）operator这种方案定制扩展性不太好，一旦operator定义自定义资源不能完全覆盖需求，需要定制时就需要改operator的源码，而且多个operator的代码风格也非常不一致，维护起来困难\n第二种方法目前只能解决基础软件的部署、升级、卸载，其它运维相关的功能如备份还原等需要另外开发。\n综合考虑，最终选择了第二种方法。第二种方法所要用到的chart在网上很方便地就可以搜索，可能参考官方的helm chart仓库。\n例如借助redis-ha这个chart，我们可以很方便地将redis主从集群部署进kubernetes集群，参考命令如下：\nhelm install stable/redis-ha 当然如果有一些特殊需求，需要把官方提供的chart进行一些定制。\n屏蔽底层集群 为了保证PaaS服务的高可用，上面我们部署redis时，使用的是redis-ha这个chart，其部署出的redis是高可用的主从集群。但PaaS服务的使用方以非集群模式的方便访问redis是最方便的。为了方便使用方，这里我们可以部署redis智能代理，以屏蔽底层的集群细节，让使用方像用单节点redis实例一样使用我们提供的redis服务。在网上搜索了一下，最后选择了predixy这款redis智能代理，这款redis智能代理的优点是性能好，支持主从哨兵集群和分片集群，配置简单方便。如何将predixy打包成docker镜像就不具体说了，这样列一下其代理redis主从集群的核心配置：\npredixy.conf\nBind 0.0.0.0:7617 WorkerThreads 4 Authority { Auth \u0026quot;123456\u0026quot; { Mode write } Auth \u0026quot;#a complex password#\u0026quot; { Mode admin } } SentinelServerPool { Databases 16 Hash crc16 HashTag \u0026quot;{}\u0026quot; Distribution modula MasterReadPriority 60 StaticSlaveReadPriority 50 DynamicSlaveReadPriority 50 RefreshInterval 1 ServerTimeout 1 ServerFailureLimit 10 ServerRetryTimeout 1 KeepAlive 120 Sentinels { + instance01-redis-ha:26379 } Group mymaster { } } 使kubernetes集群外能访问PaaS服务 PaaS服务已在kubernetes里部署好了，也可以以一种简单的方式向使用方提供服务了，接下来需要将PaaS服务暴露出来。我们知道如果是简单的http服务，要将服务暴露出来，直接使用kubernetes里的Ingress就可以了，但绝大部分基础PaaS服务都是TCP或UDP对外提供服务的，而很可惜我们所用的Ingress Controller竟然不支持TCP代理，于是只能另想办法处理这个问题。\n还是继续上面的例子，假设上述的redis-ha及predixy部署在kubernetes工作节点，而高可用kubernetes集群的vip只是在几个master节点间漂移，外部用户也肯定是通过vip来访问PaaS服务的。因此我们需要一种方式将外部用户的流量从master节点引向工作节点的方案。又是一番寻寻觅觅，我找到proxy-to-service，通过这个pod，我们可以很方便地完成这一功能。proxy-to-service关键性pod配置如下：\nnodeSelector: node-role.kubernetes.io/master: true containers: - name: proxy-tcp image: k8s.gcr.io/proxy-to-service:v2 args: [ \u0026#34;tcp\u0026#34;, \u0026#34;7617\u0026#34;, \u0026#34;predixy-svc.demo\u0026#34; ] ports: - name: tcp protocol: TCP containerPort: 7617 最后，我们创建proxy-to-service对应的Service，其Type设置为NodePort就可以了：\napiVersion: v1 kind: Service metadata: name: proxy-to-service-nodeport-service selector: app: proxy-to-service spec: type: NodePort ports: - name: tcp-redis port: 6379 targetPort: 7617 nodePort: 36379 protocol: TCP 此时外部用户已可以通过vip及nodePort访问到redis PaaS服务了：\nredis-cli -h ${kubernetes_master_vip} -p 36379 设置访问白名单 这种对外提供的PaaS服务，安全起见，至少还是应该提供访问白名单功能，以限制访问服务的客户端，避免潜在的安全风险。我们这里可以使用kubernetes的NetworkPolicy功能实现该功能。\n首先我们要选择一个支持NetworkPolicy的CNI网络方案，默认的flannel是不支持的，为此我们换用了calico。\n另外为了取得正确的客户端源IP地址，以进行访问白名单检查，我们需要将Service的externalTrafficPolicy设置为Local，官方文档中将如此设置后流量路径也解释得比较清楚，参考这里就可以了。\napiVersion: v1 kind: Service metadata: name: proxy-to-service-nodeport-service selector: app: proxy-to-service spec: type: NodePort externalTrafficPolicy: Local ports: - name: tcp-redis port: 6379 targetPort: 7617 nodePort: 36379 protocol: TCP 这里要注意，采用这种设置后，Service只会将流量代理到本节点的endpoint，如果本节点没有对应的endpoint，进入的流量就会被丢弃。这显然不是用户希望看到的，这里我们可以使用daemonset配合nodeSelector，将proxy-to-service的pod调度到每个master节点上，以解决该问题。\n最后应用一个NetworkPolicy就可以了，如下：\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: proxy-to-service-network-policy spec: podSelector: matchLabels: role: redis-proxy-to-service policyTypes: - Ingress ingress: - from: - ipBlock: cidr: 192.168.22.0/24 ports: - protocol: TCP port: 6379 上面的NetworkPolicy指示仅允许IP地址在192.168.22.0/24这个范围的客户端访问本PaaS服务。\n至此，一个基本可用的基础PaaS服务就可以交付用户使用了。\n组合起来 上面所说的是基于kubernetes构建基础PaaS服务的大概过程，为了简化用户使用，可以将上面的多步操作封装成一个大chart，最终只需要部署这个大的chart就可以快速搭建一个基本可用的PaaS服务了。封装大chart的方法参考helm官方文档，这里就不细讲了。\n实例监控 对于这种基础服务，一般能找到现成的prometheus exporter，如redis_exporter，再配合prometheus及grafana，就可以很方便地实现对基础PaaS服务示例的监控。prometheus的配置方法可以参考以前的博文。\n运维操作 升级操作 还可以通过helm完成基础PaaS服务的升级操作，参考命令如下：\nhelm upgrade ${special_release_name} ${big_chart_name} -f custom_values.yaml 其它运维操作 其它运维操作，如备份还原等，这些光用chart就无能为力了，这里可以参考mysql-operator的方案，定义一些备份或还原任务的自定义资源，基于这些自定义资源，配合kubernetes的job完成备份或还原操作。这属于比较进阶的，就不细讲了。\n总结 经实践，基于Kubernetes构建基础PaaS服务确实比以前要快很多，交付效率得到很大的提升，很多基础性的工作，kubernetes本身也已经实现了，而且稳定可靠，可以很方便地与现有的很多开源解决方案整合。而且这个方案很容易复制到其它基础PaaS服务的构建过程中，基本模式都很类似。\n但也不是全无代价的，kubernetes本身引入了较多的网络栈开销，另外为了确保pod能在node节点间漂移，使用kubernetes必然会引入分布式存储，这两者综合起来，还是对性能产生了不小的影响。因此最好在使用前进行一些的性能测试，得到一些性能对比数据，权衡下性能损耗，如果能接受，个人还是十分推荐使用该方案构建基础PaaS服务的。\n参考  http://www.ruanyifeng.com/blog/2017/07/iaas-paas-saas.html https://github.com/helm/charts https://github.com/joyieldInc/predixy/ https://github.com/kubernetes-retired/contrib/tree/master/for-demos/proxy-to-service https://docs.projectcalico.org/v3.5/usage/ https://kubernetes.io/docs/concepts/services-networking/network-policies/ https://github.com/oracle/mysql-operator https://github.com/oliver006/redis_exporter  ","permalink":"https://jeremyxu2010.github.io/2019/06/%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8Ekubernetes%E7%9A%84paas%E6%9C%8D%E5%8A%A1/","tags":["kubernetes","redis","helm","operator","networkpolicy"],"title":"构建基于kubernetes的PaaS服务"},{"categories":["容器编排"],"contents":"工作中需要将原本部署在物理机或虚拟机上的一些基础服务搬到kubernetes中，在搬的过程中遇到了不少坑，这里记录一下。\n异常网络引起的问题 之前使用redis-operator在kubernetes中部署了一套Redis集群，可测试的同事使用redis-benchmark随便一压测，这个集群就会出问题。经过艰苦的问题查找过程，终于发现了问题，原来是两个虚拟机之间的网络存在异常。\n经验教训，在测试前可用iperf3先测试下node节点之间，pod节点之间的网络状况，方法如下：\n# 在某台node节点上启动iperf3服务端 $ iperf3 --server # 在另一台node节点上启动iperf3客户端 $ iperf3 --client ${node_ip} --length 150 --parallel 100 -t 60 # 在kuberntes中部署iperf3的服务端与客户端 $ kubectl apply -f https://raw.githubusercontent.com/Pharb/kubernetes-iperf3/master/iperf3.yaml # 查看iperf3相关pod的podIP $ kubectl get pod -o wide # 在某个iperf3 client的pod中执行iperf3命令，以测试其到iperf3 server pod的网络状况 $ kubectl exec -ti iperf3-clients-5b5ll -- iperf3 --client ${iperf3_server_pod_ip} --length 150 --parallel 100 -t 60 mysql低版本引起的集群脑裂 之前使用mysql-operator在kubernetes中部署了一套3节点MySQL InnoDB集群，测试反馈压测一段时间后，这个集群会变得不可访问。检查出问题时mysql集群中mysql容器的日志，发现以下问题：\n$ kubectl logs mysql-0 -c mysql 2018-04-22T15:24:36.984054Z 0 [ERROR] [MY-000000] [InnoDB] InnoDB: Assertion failure: log0write.cc:1799:time_elapsed \u0026gt;= 0 InnoDB: thread 139746458191616 InnoDB: We intentionally generate a memory trap. InnoDB: Submit a detailed bug report to http://bugs.mysql.com. InnoDB: If you get repeated assertion failures or crashes, even InnoDB: immediately after the mysqld startup, there may be InnoDB: corruption in the InnoDB tablespace. Please refer to InnoDB: http://dev.mysql.com/doc/refman/8.0/en/forcing-innodb-recovery.html InnoDB: about forcing recovery. 15:24:36 UTC - mysqld got signal 6 ; This could be because you hit a bug. It is also possible that this binary or one of the libraries it was linked against is corrupt, improperly built, or misconfigured. This error can also be caused by malfunctioning hardware. Attempting to collect some information that could help diagnose the problem. As this is a crash and something is definitely wrong, the information collection process might fail. key_buffer_size=8388608 read_buffer_size=131072 max_used_connections=1 max_threads=151 thread_count=2 connection_count=1 It is possible that mysqld could use up to key_buffer_size + (read_buffer_size + sort_buffer_size)*max_threads = 67841 K bytes of memory Hope that\u0026#39;s ok; if not, decrease some variables in the equation. Thread pointer: 0x0 Attempting backtrace. You can use the following information to find out where mysqld died. If you see no messages after this, something went terribly wrong... stack_bottom = 0 thread_stack 0x46000 /home/mdcallag/b/orig811/bin/mysqld(my_print_stacktrace(unsigned char*, unsigned long)+0x3d) [0x1b1461d] /home/mdcallag/b/orig811/bin/mysqld(handle_fatal_signal+0x4c1) [0xd58441] /lib/x86_64-linux-gnu/libpthread.so.0(+0x11390) [0x7f1cae617390] /lib/x86_64-linux-gnu/libc.so.6(gsignal+0x38) [0x7f1cacb0a428] /lib/x86_64-linux-gnu/libc.so.6(abort+0x16a) [0x7f1cacb0c02a] /home/mdcallag/b/orig811/bin/mysqld(ut_dbg_assertion_failed(char const*, char const*, unsigned long)+0xea) [0xb25e13] /home/mdcallag/b/orig811/bin/mysqld() [0x1ce5408] /home/mdcallag/b/orig811/bin/mysqld(log_flusher(log_t*)+0x2fb) [0x1ce5fab] /home/mdcallag/b/orig811/bin/mysqld(std:🧵:_Impl\u0026lt;std::_Bind_simple\u0026lt;Runnable (void (*)(log_t*), log_t*)\u0026gt; \u0026gt;::_M_run()+0x68) [0x1ccbe18] /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb8c80) [0x7f1cad476c80] /lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba) [0x7f1cae60d6ba] /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d) [0x7f1cacbdc41d] The manual page at http://dev.mysql.com/doc/mysql/en/crashing.html contains 在mysql的bug跟踪系统里搜索了一下，果然发现了这个bug，官方提示这个bug在8.0.12之前都存在，推荐升级到8.0.13之后的版本。\n还好mysql-operator支持安装指定版本的MySQL，这里通过指定版本为最新稳定版8.0.16解决问题。\napiVersion: mysql.oracle.com/v1alpha1 kind: Cluster metadata: name: mysql spec: members: 3 version: \u0026#34;8.0.16\u0026#34; 超额使用ephemeral-storage空间引起集群故障 MySQL InnoDB集群方案中依赖于MySQL Group Replication在主从节点间同步数据，这种同步本质上是依赖于MySQL的binlog的，因此如果是压测场景，会在短时间内产生大量binlog日志，而这些binlog日志十分占用存储空间。\n而如果使用使用mysql-operator创建MySQL集群，如果在yaml文件中不声明volumeClaimTemplate，则pod会使用ephemeral-storage空间，虽然kubernetes官方提供了办法来设置ephemeral-storage空间的配额，但mysql-operator本身并没有提供参数让用户指定ephemeral-storage空间的配额。这样当MySQL集群长时间压测后，产生的大量binlog会超额使用ephemeral-storage空间，最终kubernetes为了保证容器平台的稳定，会将该pod杀掉，当3节点MySQL集群中有2个pod被杀掉时，整个集群就处于不法自动恢复的状态了。\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Warning Evicted 39m kubelet, 9.77.34.64 The node was low on resource: ephemeral-storage. Container mysql was using 256Ki, which exceeds its request of 0. Container mysql-agent was using 11572Ki, which exceeds its request of 0. Normal Killing 39m kubelet, 9.77.34.64 Killing container with id docker://mysql-agent:Need to kill Pod Normal Killing 39m kubelet, 9.77.34.64 Killing container with id docker://mysql:Need to kill Pod 解决办法也很简单，一是参考示例在yaml文件中声明volumeClaimTemplate，另外还可以在mysql的配置文件中指定binlog_expire_logs_seconds参数，在保证在压测场景下，能快速删除binlog，方法如下：\napiVersion: v1 data: my.cnf: | [mysqld] default_authentication_plugin=mysql_native_password skip-name-resolve binlog_expire_logs_seconds=300 kind: ConfigMap metadata: name: mycnf --- apiVersion: mysql.oracle.com/v1alpha1 kind: Cluster metadata: name: mysql spec: members: 3 version: \u0026#34;8.0.16\u0026#34; config: name: mycnf volumeClaimTemplate: metadata: name: data spec: storageClassName: default accessModes: - ReadWriteMany resources: requests: storage: 1Gi backupVolumeClaimTemplate: metadata: name: backup-data spec: storageClassName: default accessModes: - ReadWriteMany resources: requests: storage: 1Gi 至此，Redis集群、MySQL集群终于可以稳定地在kubernetes中运行了。\n","permalink":"https://jeremyxu2010.github.io/2019/05/kubernetes%E4%B8%AD%E5%9F%BA%E7%A1%80%E6%9C%8D%E5%8A%A1%E6%8E%92%E9%9A%9C%E8%AE%B0/","tags":["kubernetes","mysql","redis","iperf3"],"title":"kubernetes中基础服务排障记"},{"categories":["devops"],"contents":"对于MySQL的高可用集群方案，之前在项目实战中使用过简单的主从半同步复制方案、基于Galera的MySQL高可用集群，但总感觉配置太复杂，集群目前的状况不太清晰明确，发生故障转移时经常需要人工参与。这周使用mysql-operator，发现这里已经使用了MySQL官方推出的一套完整的、高可用的MySQL解决方案-MySQL InnoDB Cluster，这绝对是MySQL运维工程师的福音，这里将一些研究过程中查阅的资料记录一下。\nMySQL InnoDB Cluster简介 MySQL InnoDB Cluster 是最新GA的MySQL高可用方案，利用MySQL Group Replication和MySQL Shell、MySQL Router可以轻松搭建强壮的高可用方案。\nMySQL Shell 是新的mysql 客户端工具支持x protocol和mysql protocol，具备JavaScript和python可编程能力，作为搭建InnoDB Cluster管理工具。\nMySQL Router 是访问路由转发中间件，提供应用程序访问的failover能力。\n MySQL InnoDB cluster provides a complete high availability solution for MySQL. MySQL Shell includes AdminAPI which enables you to easily configure and administer a group of at least three MySQL server instances to function as an InnoDB cluster. Each MySQL server instance runs MySQL Group Replication, which provides the mechanism to replicate data within InnoDB clusters, with built-in failover. AdminAPI removes the need to work directly with Group Replication in InnoDB clusters, but for more information see Chapter 17, Group Replication which explains the details. MySQL Router can automatically configure itself based on the cluster you deploy, connecting client applications transparently to the server instances. In the event of an unexpected failure of a server instance the cluster reconfigures automatically. In the default single-primary mode, an InnoDB cluster has a single read-write server instance - the primary. Multiple secondary server instances are replicas of the primary. If the primary fails, a secondary is automatically promoted to the role of primary. MySQL Router detects this and forwards client applications to the new primary. Advanced users can also configure a cluster to have multiple-primaries.\n 上面这张图看着比较清楚，通过MySQL Shell可以配置出一个高可用自动进行故障转移的MySQL InnoDB Cluster，在后续运维过程中也可以通过MySQL Shell对集群进行状态监控及管理维护。通过MySQL Router向应用层屏蔽底层集群的细节，以应用层将普通的MySQL协议访问集群。\nMySQL Group Replication 是最新GA的同步复制方式，具有以下特点：\n 支持单主和多主模式 基于Paxos算法，实现数据复制的一致性 插件化设计，支持插件检测，新增节点小于集群当前节点主版本号，拒绝加入集群，大于则加入，但无法作为主节点 没有第三方组件依赖 支持全链路SSL通讯 支持IP白名单 不依赖网络多播  搭建MySQL InnoDB Cluster 这里准备了3台虚拟机mysql-host1、mysql-host2、mysql-host3，IP分别为192.168.33.21、192.168.33.22、192.168.33.23。\n安装软件包 第一步是在三台虚拟机上均安装mysql-community-server、mysql-shell、mysql-router软件包。\n# 配置mysql的yum源 $ yum install -y https://dev.mysql.com/get/mysql80-community-release-el7-3.noarch.rpm # 安装 $ yum install -y mysql-community-server mysql-shell mysql-router 配置主机名称映射 为保证三台虚拟机上可正常通过名称解析到对方的IP，这里将主机名称映射写入hosts文件中\n$ cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/hosts 192.168.33.21 mysql-host1 192.168.33.22 mysql-host2 192.168.33.23 mysql-host3 EOF 修改root密码 为了后续操作方便，这里修改三台虚拟机上MySQL的root密码\n# 首先得到初始的root密码 $ systemctl start mysqld $ ORIGINAL_ROOT_PASSWORD=$(awk \u0026#39;/temporary password/{print $NF}\u0026#39; /var/log/mysqld.log) # 这里将mysql的root密码修改为R00T@mysql，这个密码符合复杂度要求 $ MYSQL_PWD=\u0026#34;$ORIGINAL_ROOT_PASSWORD\u0026#34; mysql --connect-expired-password -e \u0026#34;ALTER USER \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED WITH mysql_native_password BY \u0026#39;R00T@mysql\u0026#39;;\u0026#34; # 顺便允许mysql可在其它主机登录过来 $ MYSQL_PWD=\u0026#34;$ORIGINAL_ROOT_PASSWORD\u0026#34; mysql --connect-expired-password -e \u0026#34;CREATE USER \u0026#39;root\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED WITH mysql_native_password BY \u0026#39;R00T@mysql\u0026#39;;\u0026#34; $ MYSQL_PWD=\u0026#34;$ORIGINAL_ROOT_PASSWORD\u0026#34; mysql --connect-expired-password -e \u0026#34;GRANT ALL ON *.* TO \u0026#39;root\u0026#39;@\u0026#39;%\u0026#39; WITH GRANT OPTION; FLUSH PRIVILEGES;\u0026#34; 配置本地实例 MySQL InnoDB Cluster底层依赖Group Replication模式，而配置Group Replication模式首先要通过dba.configureLocalInstance设置每台虚拟机上的本地实例必要参数并持久化配置。\n# 通过mysqlsh即可轻松完成本机实例的配置 $ cat \u0026lt;\u0026lt; EOF \u0026gt; config_local_instance.js dba.configureLocalInstance(\u0026#39;root@localhost:3306\u0026#39;, {\u0026#39;password\u0026#39;: \u0026#39;R00T@mysql\u0026#39;, \u0026#39;interactive\u0026#39;: false}) EOF $ mysqlsh --no-password --js --file=config_local_instance.js # 重启后才能生效 $ systemctl restart mysqld # 再检查一下本地实例配置的状况 $ cat \u0026lt;\u0026lt; EOF \u0026gt; config_local_instance.js dba.checkLocalInstance(\u0026#39;root@localhost:3306\u0026#39;, {\u0026#39;password\u0026#39;: \u0026#39;R00T@mysql\u0026#39;, \u0026#39;interactive\u0026#39;: false}) EOF $ mysqlsh --no-password --js --file=check_local_instance.js 初始化MySQL InnoDB Cluster 只需在mysql-host1这台虚拟机上进行以下操作就可以了。\n$ cat \u0026lt;\u0026lt; EOF \u0026gt; init_cluster.js shell.connect(\u0026#39;root@localhost:3306\u0026#39;, \u0026#39;R00T@mysql\u0026#39;) dba.createCluster(\u0026#39;mycluster\u0026#39;, {\u0026#39;localAddress\u0026#39;: \u0026#39;192.168.33.21\u0026#39;}) var cluster=dba.getCluster(\u0026#39;mycluster\u0026#39;) cluster.addInstance(\u0026#39;root@192.168.33.22:3306\u0026#39;, {\u0026#39;localAddress\u0026#39;: \u0026#39;192.168.33.22\u0026#39;, \u0026#39;password\u0026#39;: \u0026#39;R00T@mysql\u0026#39;}) cluster.addInstance(\u0026#39;root@192.168.33.23:3306\u0026#39;, {\u0026#39;localAddress\u0026#39;: \u0026#39;192.168.33.23\u0026#39;, \u0026#39;password\u0026#39;: \u0026#39;R00T@mysql\u0026#39;}) EOF $ mysqlsh --no-password --js --file=init_cluster.js 初始化mysql-router 为了向应用层屏蔽底层集群的细节，我们还可以在三台虚拟机上均部署mysql-router。\n# 以当前集群信息创建mysql-router的配置信息，注意这里密码R00T@mysql被编码为R00T%40mysql $ mysqlrouter --bootstrap root:R00T%40mysql@192.168.33.21:3306 --user=mysqlrouter # 重启mysqlrouter服务 $ systemctl restart mysqlrouter 部署验证 至此整套MySQL InnoDB Cluster就部署好了，我们在其它节点以MySQL协议即可访问该MySQL集群。\n$ mysql -h192.168.33.21 -P3306 -uroot -pR00T@mysql 因为三台虚拟机上均安装了mysql-router，因此这里的IP三台虚拟机的IP均可，更好的办法用haproxy或nginx再做一下4层代理，由vip或LVS保证负载均衡器无单点故障，这个就是常规方案，按下不表了。\n这里还以Vagrant及Ansible脚本的方式，整理了上述部署方案，参见这里。\n运维中可能遇到的问题 同样在运维MySQL InnoDB Cluster时还是会遇到一些需要手工处理的场景，这里简要列举一下。\n重启节点后需要手动重新加入集群 mysql-js\u0026gt; var cluster = dba.getCluster(\u0026#34;mycluster\u0026#34;) mysql-js\u0026gt; cluster.status() { ... \u0026#34;192.168.33.23:3306\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;192.168.33.23:3306\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;R/O\u0026#34;, \u0026#34;readReplicas\u0026#34;: {}, \u0026#34;role\u0026#34;: \u0026#34;HA\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;(MISSING)\u0026#34; } } } } mysql-js\u0026gt; cluster.rejoinInstance(\u0026#39;root@192.168.33.23:3306\u0026#39;) mysql-js\u0026gt; cluster.status() { ... \u0026#34;192.168.33.23:3306\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;192.168.33.23:3306\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;R/O\u0026#34;, \u0026#34;readReplicas\u0026#34;: {}, \u0026#34;role\u0026#34;: \u0026#34;HA\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;ONLINE\u0026#34; } } } } 集群所有节点发生重启 当集群的所有节点都offline，直接获取集群信息失败，如何重新恢复集群\nmysql-js\u0026gt; var cluster=dba.getCluster(\u0026#39;mycluster\u0026#39;) Dba.getCluster: This function is not available through a session to a standalone instance (RuntimeError) 执行rebootClusterFromCompleteOutage命令，可恢复集群\nmysql-js\u0026gt; dba.rebootClusterFromCompleteOutage(\u0026#39;mycluster\u0026#39;) Reconfiguring the cluster \u0026#39;mycluster\u0026#39; from complete outage... The instance \u0026#39;192.168.33.22:3306\u0026#39; was part of the cluster configuration. Would you like to rejoin it to the cluster? [y|N]: y The instance \u0026#39;192.168.33.23:3306\u0026#39; was part of the cluster configuration. Would you like to rejoin it to the cluster? [y|N]: y The cluster was successfully rebooted. 脑裂场景 当集群中有部分节点出现UNREACHABLE状态，此时集群无法做出决策，，会出现以下局面，此时只剩下一个活跃节点，此节点只能提供查询，无法写入，执行写入操作会hang住。\nmysql-js\u0026gt; cluster.status() { \u0026#34;clusterName\u0026#34;: \u0026#34;mycluster\u0026#34;, \u0026#34;defaultReplicaSet\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;primary\u0026#34;: \u0026#34;192.168.33.21:3306\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;NO_QUORUM\u0026#34;, \u0026#34;statusText\u0026#34;: \u0026#34;Cluster has no quorum as visible from \u0026#39;192.168.33.21:3306\u0026#39; and cannot process write transactions. 2 members are not active\u0026#34;, \u0026#34;topology\u0026#34;: { \u0026#34;192.168.33.21:3306\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;192.168.33.21:3306\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;R/W\u0026#34;, \u0026#34;readReplicas\u0026#34;: {}, \u0026#34;role\u0026#34;: \u0026#34;HA\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;ONLINE\u0026#34; }, \u0026#34;192.168.33.22:3306\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;192.168.33.22:3306\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;R/O\u0026#34;, \u0026#34;readReplicas\u0026#34;: {}, \u0026#34;role\u0026#34;: \u0026#34;HA\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;UNREACHABLE\u0026#34; }, \u0026#34;192.168.33.23:3306\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;192.168.33.23:3306\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;R/O\u0026#34;, \u0026#34;readReplicas\u0026#34;: {}, \u0026#34;role\u0026#34;: \u0026#34;HA\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;(MISSING)\u0026#34; } } } } 修复这种状态，需要执行forceQuorumUsingPartitionOf指定当前活跃节点(如果是多个则选择primary node)，此时活跃节点可以提供读写操作，然后将其他节点加入此集群。\nmysql-js\u0026gt; cluster.forceQuorumUsingPartitionOf(\u0026#39;root@192.168.33.21:3306\u0026#39;) Restoring replicaset \u0026#39;default\u0026#39; from loss of quorum, by using the partition composed of [192.168.33.21:3306] Please provide the password for \u0026#39;root@192.168.33.21:3306\u0026#39;: Restoring the InnoDB cluster ... The InnoDB cluster was successfully restored using the partition from the instance \u0026#39;root@10.186.23.94:3306\u0026#39;. WARNING: To avoid a split-brain scenario, ensure that all other members of the replicaset are removed or joined back to the group that was restored. mysql-js\u0026gt; cluster.status() { \u0026#34;clusterName\u0026#34;: \u0026#34;mycluster\u0026#34;, \u0026#34;defaultReplicaSet\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;primary\u0026#34;: \u0026#34;192.168.33.21:3306\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;OK_NO_TOLERANCE\u0026#34;, \u0026#34;statusText\u0026#34;: \u0026#34;Cluster is NOT tolerant to any failures. 2 members are not active\u0026#34;, \u0026#34;topology\u0026#34;: { \u0026#34;192.168.33.21:3306\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;192.168.33.21:3306\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;R/W\u0026#34;, \u0026#34;readReplicas\u0026#34;: {}, \u0026#34;role\u0026#34;: \u0026#34;HA\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;ONLINE\u0026#34; }, \u0026#34;192.168.33.22:3306\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;192.168.33.22:3306\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;R/O\u0026#34;, \u0026#34;readReplicas\u0026#34;: {}, \u0026#34;role\u0026#34;: \u0026#34;HA\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;(MISSING)\u0026#34; }, \u0026#34;192.168.33.23:3306\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;192.168.33.23:3306\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;R/O\u0026#34;, \u0026#34;readReplicas\u0026#34;: {}, \u0026#34;role\u0026#34;: \u0026#34;HA\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;(MISSING)\u0026#34; } } } } mysql-js\u0026gt; cluster.rejoinInstance(\u0026#39;root@192.168.33.22:3306\u0026#39;) mysql-js\u0026gt; cluster.rejoinInstance(\u0026#39;root@192.168.33.23:3306\u0026#39;) 节点有哪状态\n ONLINE - 节点状态正常。 OFFLINE - 实例在运行，但没有加入任何Cluster。 RECOVERING - 实例已加入Cluster，正在同步数据。 ERROR - 同步数据发生异常。 UNREACHABLE - 与其他节点通讯中断，可能是网络问题，可能是节点crash。 MISSING 节点已加入集群，但未启动group replication  集群有哪些状态\n OK – 所有节点处于online状态，有冗余节点。 OK_PARTIAL – 有节点不可用，但仍有冗余节点。 OK_NO_TOLERANCE – 有足够的online节点，但没有冗余，例如：两个节点的Cluster，其中一个挂了，集群就不可用了。 NO_QUORUM – 有节点处于online状态，但达不到法定节点数，此状态下Cluster无法写入，只能读取。 UNKNOWN – 不是online或recovering状态，尝试连接其他实例查看状态。 UNAVAILABLE – 组内节点全是offline状态，但实例在运行，可能实例刚重启还没加入Cluster。  总结 总的来说，MySQL InnoDB Cluster相对于之前的集群方案还是要方便不少的，不过手工部署还是挺费时间的，看官们如果对手工部署感兴趣，也可以参考我整理出的anisble脚本，在Kubernetes环境快速部署MySQL InnoDB Cluster还是推荐直接使用mysql-operator。\n参考  https://github.com/oracle/mysql-operator https://dev.mysql.com/doc/refman/5.7/en/mysql-innodb-cluster-introduction.html https://zhuanlan.zhihu.com/p/44259581  ","permalink":"https://jeremyxu2010.github.io/2019/05/mysql-innodb-cluster%E5%AE%9E%E6%88%98/","tags":["mysql","vagrant","ansible"],"title":"MySQL InnoDB Cluster实战"},{"categories":["工具"],"contents":"本周的工作中需要对一套部署好的redis集群进行性能测试，在这个过程中用到了几个工具，这里对这些工具的用法记录一下。\nsar 我们拿到一台虚拟机，在使用之前可以先用sar看一下该虚拟机目前的性能概况。使用方法如下：\n$ sudo yum install -y sysstat # 安装sysstat包 $ sar -u 5 # Report CPU utilization every 5 seconds $ sar -r 5 # Report Memory utilization every 5 seconds $ sar -B 5 # Report paging statistics every 5 seconds $ sar -d -p 5 # Report activity for each block device every 5 seconds $ sar -d -p 5 # Report activity for each block device every 5 seconds $ sar -n ALL 5 # Report network statistics every 5 seconds 后面如果出现性能问题，还可以随时执行以上命令查看系统状况。sar的完整命令行参数可参考这里。\niperf3 如果工作涉及多台服务器之间的通讯，可以在具体工作前使用iperf3这个工具对服务器之间的网络性能测量一下，心里对网络性能也有个底。使用方法如下：\n$ sudo yum install -y iperf3 $ iperf3 --server # 在服务端启动Server $ iperf3 --client $server_ip --bandwidth 10M --bytes 300 # 使用10Mbit/s的带宽，发送的报文长度为300 Bytes $ iperf3 --client $server_ip --udp # 测试UDP传输的性能 有了iperf3的输出，我们对服务器之间的网络状况更加信心了。iperf3的完整命令行参数可参考这里。\nredis-benchmark redis本身带了一个叫redis-benchmark的工具，用它可以测量一下部署的redis的性能指标。使用方法如下：\n$ redis-benchmark -h ${redis_ip} -p ${redis_port} -c 50 -n 10000 -d 20 -r 10000 -P 30 # 50并发连接，共总10000个请求，set/get的数据大小为20，存取的键在10000间随时变动(避免热写某一个键值)，每30条命令作为一个pipeline执行 redis-benchmark的完整命令行参数可参考这里。\n参考  https://www.linuxtechi.com/generate-cpu-memory-io-report-sar-command/ https://blog.51cto.com/dingtongxue1990/1854124 http://ghoulich.xninja.org/2016/11/17/how-to-use-redis-benchmark-to-measure-performance/ https://www.computerhope.com/unix/usar.htm https://iperf.fr/iperf-doc.php https://redis.io/topics/benchmarks  ","permalink":"https://jeremyxu2010.github.io/2019/05/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7%E9%9B%86%E9%94%A6/","tags":["sysstat","iperf3","redis"],"title":"性能测试工具集锦"},{"categories":["容器编排"],"contents":"这两天搭建了一套新的kubernetes环境，由于这套环境会被用于演示，所以持续观察了好几天这套环境，发现不少容器平台稳定性的问题，这里记录一下以备忘。\n环境 我们这套环境的操作系统是4.4.131-20181130.kylin.server aarm64 GNU/Linux，64核128G内存。\n各类问题 docker的问题 最开始用的docker版本为18.03.1-ce，但在运行四五十个容器后，出现了docker info不响应的问题。因为情况紧急，没有查明原因，临时将docker版本降回ubuntu-ports xenial源里的docker.io了，版本为17.03.2，降级后，docker info终于一直有响应了。\n过了一晚上，第二天再来看，发现有部分pod的健康检查失败，同时exec进pod再exit会卡住，现象如下：\n$ kubectl describe pod some-pod ... Liveness probe failed. # 健康检查失败 Readiness probe failed. # 健康检查失败 ... $ kubectl exec -ti some-pod -- sh \u0026gt; exit # 这里会卡住 这次查了下原因，发现是低版本docker的bug，见bug记录。\n After some time a container becomes unhealthy (because of \u0026ldquo;containerd: container not found\u0026rdquo;). The container is running and accessible but it's not possible to \u0026ldquo;exec\u0026rdquo; into it. The container is also marked \u0026ldquo;unhealthy\u0026rdquo; because the healhcheck cannot be executed inside the container (same message).\nSteps to reproduce the issue:\n Run a container (success) Stop and remove container (success) Build new image (success) Start the image (success) The container is accessible but is unhealty and it's not possible to \u0026ldquo;exec\u0026rdquo; into it   官方建议将docker升级至17.12之后，于是将docker升级至17.12.1-ce，问题终于解决。\nkubelet的问题 今天下午又发现kubelet调度pod异常缓慢，kubelet的日志里疯狂打印以下的报错：\nE0521 16:45:28.353927 58235 kubelet_volumes.go:140] Orphaned pod \u0026#34;xxxxxx\u0026#34; found, but volume paths are still present on disk : There were a total of 1 errors similar to this. Turn up verbosity to see them. 查了下发现是kubernetes的bug，见bug记录，官方还没有将这个bug的PR合进主干代码，不过还好找到阿里容器服务提供的一个Workaround方法，简单改了下这个脚本，将其加入cron定时任务即可。\netcd的问题 之前发现创建了一个deployment，但一直没有pod产生出来，查到etcd服务日志中有大量下面的报错：\n2018-10-22 11:22:16.364641 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;xxx\\\u0026#34; range_end:\\\u0026#34;xxx\\\u0026#34; \u0026#34; with result \u0026#34;range_response_c ount:xx size:xxx\u0026#34; took too long (117.026891ms) to execute 查到了相关的bug记录，说是可能是磁盘性能太差导致的。又查了etcd的文档，其中有如下说明 ：\n Usually this issue is caused by a slow disk. The disk could be experiencing contention among etcd and other applications, or the disk is too simply slow (e.g., a shared virtualized disk). To rule out a slow disk from causing this warning, monitor backend_commit_duration_seconds (p99 duration should be less than 25ms)\nThe second most common cause is CPU starvation. If monitoring of the machine’s CPU usage shows heavy utilization, there may not be enough compute capacity for etcd. Moving etcd to dedicated machine, increasing process resource isolation cgroups, or renicing the etcd server process into a higher priority can usually solve the problem.\n 再查了下系统的iowait，果然很高，最后挂了一个独立的磁盘，专门用于etcd服务的数据存储，问题得到好转。\nflannel的问题 kubernetes在运行时，偶然发现flannel的pod因为OOM被Killed掉了，在该pod重启的过程中此node节点上的服务其它节点均不可访问。\n查了下，发现官方的bug记录。解决办法就是增加pod使用的资源限制：\nresources: limits: cpu: 100m memory: 256Mi # 默认为50Mi，增大这个数据 requests: cpu: 100m memory: 50Mi 总结 虽说现在容器平台蒸蒸日上，但还是有不少坑待我们去趟的，我辈须努力啊。\n参考  https://github.com/moby/moby/issues/35091 https://xigang.github.io/2018/12/31/Orphaned-pod/ https://github.com/kubernetes/kubernetes/issues/60987 https://coreos.com/etcd/docs/latest/faq.html#performance https://bingohuang.com/etcd-operation-3/ https://github.com/coreos/flannel/issues/963  ","permalink":"https://jeremyxu2010.github.io/2019/05/%E4%BF%9D%E6%8C%81kubernetes%E5%AE%B9%E5%99%A8%E5%B9%B3%E5%8F%B0%E7%A8%B3%E5%AE%9A%E6%80%A7/","tags":["kubernetes","docker"],"title":"保持Kubernetes容器平台稳定性"},{"categories":["工具"],"contents":"这几天的工作频繁地操作大量docker镜像，这里总结一些过程中的小技巧。\n小技巧 列出registry中的镜像 官方的docker registry虽然提供了一系列操作镜像的Restful API，但查看镜像列表并不直观，于是可以使用以下脚本查看registry中的镜像列表：\nDOCKER_REGISTRY_ADDR=127.0.0.1:5000 for img in `curl -s ${DOCKER_REGISTRY_ADDR}/v2/_catalog | python -m json.tool | jq \u0026#34;.repositories[]\u0026#34; | tr -d \u0026#39;\u0026#34;\u0026#39;`; do for tag in `curl -s 10.10.30.21:5000/v2/${img}/tags/list|jq \u0026#34;.tags[]\u0026#34; | tr -d \u0026#39;\u0026#34;\u0026#39;`; do echo $img:$tag done done 删除registry中的某个镜像 docker registry上的镜像默认是不允许删除的，如要删除，需要在启动docker registry时指定环境变量REGISTRY_STORAGE_DELETE_ENABLED=true，然后可以利用以下脚本删除镜像：\nDOCKER_REGISTRY_ADDR=127.0.0.1:5000 function delete_image { imgFullName=$1 img=`echo $imgFullName | awk -F \u0026#39;:\u0026#39; \u0026#39;{print $1}\u0026#39;` tag=`echo $imgFullName | awk -F \u0026#39;:\u0026#39; \u0026#39;{print $2}\u0026#39;` # delete image\u0026#39;s blobs for digest in `curl -H \u0026#39;Accept: application/vnd.docker.distribution.manifest.v2+json\u0026#39; -s ${DOCKER_REGISTRY_ADDR}/v2/${img}/manifests/${tag} | jq \u0026#34;.layers[].digest\u0026#34; | tr -d \u0026#39;\u0026#34;\u0026#39;`; do curl -X DELETE ${DOCKER_REGISTRY_ADDR}/v2/${img}/blobs/${digest} done # delete image\u0026#39;s manifest imgDigest=`curl -H \u0026#39;Accept: application/vnd.docker.distribution.manifest.v2+json\u0026#39; -s ${DOCKER_REGISTRY_ADDR}/v2/${img}/manifests/${tag} | jq \u0026#34;.config.digest\u0026#34; | tr -d \u0026#39;\u0026#34;\u0026#39;` curl -X DELETE ${DOCKER_REGISTRY_ADDR}/v2/${img}//manifests/${imgDigest} } 修改镜像的名字 一般手工用docker tag命令改镜像名字，一两个镜像这么做还行，如要批量操作，还是需要用脚本：\npulledImageName=127.0.0.1:5000/test/testdb/db:v1.0.0 # 只取不包含127.0.0.1:5000的镜像名 shortImageName=${pulledImageName#*/} # 将repository的前缀修改为test2 changedShortImageName=${shortImageName/#test/test2} # 将tag修改为1.0.0 changed2ShortImageName=${changedShortImageName/%v1.0.0/1.0.0} docker tag $pulledImageName $changed2ShortImageName 列出kubernetes中目前使用到的镜像 有时需要列出Kubernetes中目前使用到的所有镜像，可以用以下脚本：\nfor img in `kubectl get pod -o yaml --all-namespaces | grep \u0026#39;image:\u0026#39; | cut -c14- | sort | uniq`; do echo $img # save image to tar file save_dir=${img%/*} mkdir -p $save_dir docker save -o ${img}.tar ${img} done 建个远程隧道 在家工作，需要建个隧道连到远程服务器上去，可使用以下命令：\n# Make PubkeyAuthentication enabled ssh-copy-id root@${remote_ip} # make tunnel with autossh autossh -f -M 34567 -ND 7070 root@${remote_ip} # use web browser with Socks 5 Proxy Server 127.0.0.1:7070 参考  https://docs.docker.com/registry/spec/api/ https://www.jianshu.com/p/6a7b80122602 https://www.cnblogs.com/chengmo/archive/2010/10/02/1841355.html https://www.cnblogs.com/eshizhan/archive/2012/07/16/2592902.html  ","permalink":"https://jeremyxu2010.github.io/2019/05/%E6%89%B9%E9%87%8F%E5%A4%84%E7%90%86docker%E9%95%9C%E5%83%8F/","tags":["bash","docker"],"title":"批量处理docker镜像"},{"categories":["golang开发"],"contents":"这周的工作主要是验证几个Kubernetes Operator：\n mysql-operator redis-operator Redis-Operator percona-server-mongodb-operator  这些operator基本上都是用来部署、管理、维护一些基础服务的。在验证这些operator的过程中，也顺便研究了下如何写Kubernetes Operator，这里记录一下。\nOperator Operator 是 CoreOS 推出的旨在简化复杂有状态应用管理的框架，它是一个感知应用状态的控制器，通过扩展 Kubernetes API 来自动创建、管理和配置应用实例。\n你可以在 OperatorHub.io 上查看 Kubernetes 社区推荐的一些 Operator 范例。\nOperator 原理 Operator 基于 Third Party Resources 扩展了新的应用资源，并通过控制器来保证应用处于预期状态。比如 etcd operator 通过下面的三个步骤模拟了管理 etcd 集群的行为：\n 通过 Kubernetes API 观察集群的当前状态； 分析当前状态与期望状态的差别； 调用 etcd 集群管理 API 或 Kubernetes API 消除这些差别。  Operator 是一个感知应用状态的控制器，所以实现一个 Operator 最关键的就是把管理应用状态的所有操作封装到配置资源和控制器中。通常来说 Operator 需要包括以下功能：\n Operator 自身以 deployment 的方式部署 Operator 自动创建一个 Third Party Resources 资源类型，用户可以用该类型创建应用实例 Operator 应该利用 Kubernetes 内置的 Serivce/ReplicaSet 等管理应用 Operator 应该向后兼容，并且在 Operator 自身退出或删除时不影响应用的状态 Operator 应该支持应用版本更新 Operator 应该测试 Pod 失效、配置错误、网络错误等异常情况  实例分析 上面这样说的一些概念可能比较抽象，这里以mysql-operator这个operator为例，我们具体分析一下一个Kubernetes Operator具体是如何实现的。\n首先分析其入口main函数，这个没有太多说的，就是解析参数，并执行app.Run函数。\nhttps://github.com/oracle/mysql-operator/blob/master/cmd/mysql-operator/main.go\n... if err := app.Run(opts); err != nil { fmt.Fprintf(os.Stderr, \u0026#34;%v\\n\u0026#34;, err) os.Exit(1) } ...\t然后看看app.Run函数\nhttps://github.com/oracle/mysql-operator/blob/master/cmd/mysql-operator/app/mysql_operator.go#L56:6\n// Run starts the mysql-operator controllers. This should never exit. func Run(s *operatoropts.MySQLOperatorOpts) error { // 构造kubeconfig以便连接kubernetes的APIServer \tkubeconfig, err := clientcmd.BuildConfigFromFlags(s.Master, s.KubeConfig) if err != nil { return err } ... // 构造kubeClient、 mysqlopClient, 以便操作Kubernetes里的一些资源 \tkubeClient := kubernetes.NewForConfigOrDie(kubeconfig) mysqlopClient := clientset.NewForConfigOrDie(kubeconfig) // 构造一些共享的informer，以便监听自定义对象及kubernetes里的一些核心资源 \t// Shared informers (non namespace specific). \toperatorInformerFactory := informers.NewFilteredSharedInformerFactory(mysqlopClient, resyncPeriod(s)(), s.Namespace, nil) kubeInformerFactory := kubeinformers.NewFilteredSharedInformerFactory(kubeClient, resyncPeriod(s)(), s.Namespace, nil) var wg sync.WaitGroup // 构造自定义类型mysqlcluster的控制器 \tclusterController := cluster.NewController( *s, mysqlopClient, kubeClient, operatorInformerFactory.MySQL().V1alpha1().Clusters(), kubeInformerFactory.Apps().V1beta1().StatefulSets(), kubeInformerFactory.Core().V1().Pods(), kubeInformerFactory.Core().V1().Services(), 30*time.Second, s.Namespace, ) wg.Add(1) go func() { defer wg.Done() clusterController.Run(ctx, 5) }() // 下面分别为每个自定义类型构造了相应的控制器 \t... Kubernetes Operator的核心逻辑就在自定义类型的控制器里面。\nhttps://github.com/oracle/mysql-operator/blob/master/pkg/controllers/cluster/controller.go#L142\n// NewController creates a new MySQLController. func NewController( ... ) *MySQLController { // 构造MySQLController  m := MySQLController{ ... } // 监控自定义类型mysqlcluster的变化(增加、更新、删除)，这里看一看m.enqueueCluster函数可以发现都只是把发生变化的自定义对象的名称放入工作队列中 \tclusterInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: m.enqueueCluster, UpdateFunc: func(old, new interface{}) { m.enqueueCluster(new) }, DeleteFunc: func(obj interface{}) { cluster, ok := obj.(*v1alpha1.Cluster) if ok { m.onClusterDeleted(cluster.Name) } }, }) https://github.com/oracle/mysql-operator/blob/master/pkg/controllers/cluster/controller.go#L231\n// Run函数里会启动工作协程处理上述放入工作队列的自定义对象的名称 func (m *MySQLController) Run(ctx context.Context, threadiness int) { ... // Launch two workers to process Foo resources \tfor i := 0; i \u0026lt; threadiness; i++ { go wait.Until(m.runWorker, time.Second, ctx.Done()) } ... } 从runWorker函数一步步跟踪过程，发现真正干活的是syncHandler函数\nhttps://github.com/oracle/mysql-operator/blob/master/pkg/controllers/cluster/controller.go#L301\nfunc (m *MySQLController) syncHandler(key string) error { ... nsName := types.NamespacedName{Namespace: namespace, Name: name} // Get the Cluster resource with this namespace/name. \tcluster, err := m.clusterLister.Clusters(namespace).Get(name) if err != nil { // 如果自定义资源对象已不存在，则不用处理 \t// The Cluster resource may no longer exist, in which case we stop processing. \tif apierrors.IsNotFound(err) { utilruntime.HandleError(fmt.Errorf(\u0026#34;mysqlcluster \u0026#39;%s\u0026#39; in work queue no longer exists\u0026#34;, key)) return nil } return err } cluster.EnsureDefaults() // 校验自定义资源对象 \tif err = cluster.Validate(); err != nil { return errors.Wrap(err, \u0026#34;validating Cluster\u0026#34;) } // 给自定义资源对象设置一些默认属性 \tif cluster.Spec.Repository == \u0026#34;\u0026#34; { cluster.Spec.Repository = m.opConfig.Images.DefaultMySQLServerImage } ... svc, err := m.serviceLister.Services(cluster.Namespace).Get(cluster.Name) // If the resource doesn\u0026#39;t exist, we\u0026#39;ll create it \t// 如果该自定义资源对象存在，则应该要创建相应的Serivce，如Serivce不存在，则创建 \tif apierrors.IsNotFound(err) { glog.V(2).Infof(\u0026#34;Creating a new Service for cluster %q\u0026#34;, nsName) svc = services.NewForCluster(cluster) err = m.serviceControl.CreateService(svc) } // If an error occurs during Get/Create, we\u0026#39;ll requeue the item so we can \t// attempt processing again later. This could have been caused by a \t// temporary network failure, or any other transient reason. \tif err != nil { return err } // If the Service is not controlled by this Cluster resource, we should \t// log a warning to the event recorder and return. \tif !metav1.IsControlledBy(svc, cluster) { msg := fmt.Sprintf(MessageResourceExists, \u0026#34;Service\u0026#34;, svc.Namespace, svc.Name) m.recorder.Event(cluster, corev1.EventTypeWarning, ErrResourceExists, msg) return errors.New(msg) } ss, err := m.statefulSetLister.StatefulSets(cluster.Namespace).Get(cluster.Name) // If the resource doesn\u0026#39;t exist, we\u0026#39;ll create it \t// 如果该自定义资源对象存在，则应该要创建相应的StatefulSet，如StatefulSet不存在，则创建 \tif apierrors.IsNotFound(err) { glog.V(2).Infof(\u0026#34;Creating a new StatefulSet for cluster %q\u0026#34;, nsName) ss = statefulsets.NewForCluster(cluster, m.opConfig.Images, svc.Name) err = m.statefulSetControl.CreateStatefulSet(ss) } // If an error occurs during Get/Create, we\u0026#39;ll requeue the item so we can \t// attempt processing again later. This could have been caused by a \t// temporary network failure, or any other transient reason. \tif err != nil { return err } // If the StatefulSet is not controlled by this Cluster resource, we \t// should log a warning to the event recorder and return. \tif !metav1.IsControlledBy(ss, cluster) { msg := fmt.Sprintf(MessageResourceExists, \u0026#34;StatefulSet\u0026#34;, ss.Namespace, ss.Name) m.recorder.Event(cluster, corev1.EventTypeWarning, ErrResourceExists, msg) return fmt.Errorf(msg) } // Upgrade the required component resources the current MySQLOperator version. \t// 确保StatefulSet上的BuildVersion与自定义资源对象上的一致，如不一致，则修改得一致 \tif err := m.ensureMySQLOperatorVersion(cluster, ss, buildversion.GetBuildVersion()); err != nil { return errors.Wrap(err, \u0026#34;ensuring MySQL Operator version\u0026#34;) } // Upgrade the MySQL server version if required. \tif err := m.ensureMySQLVersion(cluster, ss); err != nil { return errors.Wrap(err, \u0026#34;ensuring MySQL version\u0026#34;) } // If this number of the members on the Cluster does not equal the \t// current desired replicas on the StatefulSet, we should update the \t// StatefulSet resource. \t// 如果StatefulSet的Replicas值与自定义资源对象上配置不一致，则更新StatefulSet \tif cluster.Spec.Members != *ss.Spec.Replicas { glog.V(4).Infof(\u0026#34;Updating %q: clusterMembers=%d statefulSetReplicas=%d\u0026#34;, nsName, cluster.Spec.Members, ss.Spec.Replicas) old := ss.DeepCopy() ss = statefulsets.NewForCluster(cluster, m.opConfig.Images, svc.Name) if err := m.statefulSetControl.Patch(old, ss); err != nil { // Requeue the item so we can attempt processing again later. \t// This could have been caused by a temporary network failure etc. \treturn err } } // Finally, we update the status block of the Cluster resource to \t// reflect the current state of the world. \t// 最后更新自定义资源对象的状态 \terr = m.updateClusterStatus(cluster, ss) if err != nil { return err } m.recorder.Event(cluster, corev1.EventTypeNormal, SuccessSynced, MessageResourceSynced) return nil } 这个方法比较长，上面已在关键代码处加上中文注释了，结合代码应该看得比较清楚了。基本上大部分Controller都是这么个逻辑。\n这里有个地址要注意下，为了保证那些依据自定义资源对象创建出的核心资源生命周期一致，比如随着自定义资源对象一起删除，在构建核心资源时需要设置OwnerReferences\nhttps://github.com/oracle/mysql-operator/blob/master/pkg/resources/statefulsets/statefulset.go#L390\nOwnerReferences: []metav1.OwnerReference{ *metav1.NewControllerRef(cluster, schema.GroupVersionKind{ Group: v1alpha1.SchemeGroupVersion.Group, Version: v1alpha1.SchemeGroupVersion.Version, Kind: v1alpha1.ClusterCRDResourceKind, }), }, 整个Operator大概就是这样了。\n简易方法 上面这样写Operator还是太麻烦了点，其实官方已经给出了operator-sdk，参考其教程，只需要重点编写Reconcile函数的逻辑就可以了。\nhttps://github.com/operator-framework/operator-sdk-samples/blob/master/memcached-operator/pkg/controller/memcached/memcached_controller.go#L84\n// Reconcile reads that state of the cluster for a Memcached object and makes changes based on the state read // and what is in the Memcached.Spec // TODO(user): Modify this Reconcile function to implement your Controller logic. This example creates // a Memcached Deployment for each Memcached CR // Note: // The Controller will requeue the Request to be processed again if the returned error is non-nil or // Result.Requeue is true, otherwise upon completion it will remove the work from the queue. func (r *ReconcileMemcached) Reconcile(request reconcile.Request) (reconcile.Result, error) { ... } 看一下该函数的注释，其功能已经很清楚，就是完成通过 Kubernetes API 观察集群的当前状态；分析当前状态与期望状态的差别；调用 Kubernetes API 消除这些差别，也就是上面syncHandler的逻辑。\n当然用operator-sdk生成Operator的骨架，只需填充核心逻辑，这种方法无疑更好，整个代码结构更加标准。\nOperator的现状 官方是希望通过Operator封装大部分基础服务软件的运维操作的，但目前很多Operator并不完善。比如虽然形式上给Operator划分了5个成熟度等级，但实际上大部分Operator仅只能完成安装部署而已。\n还有很多Operator明确说明目前只是alpha状态，目前不建议投入生产。\n总结 本文概述了Kubernetes Operator的实现原理，并以mysql-operator的核心代码为示例，大致走读了一遍核心逻辑。\n参考  https://kubernetes.feisky.xyz/fu-wu-zhi-li/index/operator https://github.com/operator-framework/operator-sdk https://github.com/oracle/mysql-operator/ https://operatorhub.io/  ","permalink":"https://jeremyxu2010.github.io/2019/05/%E7%BC%96%E5%86%99kubernetes-operator/","tags":["kubernetes","operator","mysql"],"title":"编写Kubernetes Operator"},{"categories":["工作杂记"],"contents":"最近的工作比较杂，因此一直没有整理一篇博文。刚好五一假期了，想着不能再拖下去了，即使写出的东西太琐碎，也稍微记录下，作个备忘也挺好的。\nspring boot应用中使用redis缓存 如子标题，有需求要在spring boot应用中使用redis缓存，这个还是比较简单的，如下：\n添加maven依赖\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 添加配置\nspring: redis: # Redis服务器连接密码（默认为空） password: # Redis数据库索引（默认为0） database: 0 # Redis服务器连接端口 port: 6379 pool: # 连接池中的最大空闲连接 max-idle: 8 # 连接池中的最小空闲连接 min-idle: 0 # 连接池最大连接数（使用负值表示没有限制） max-active: 8 # 连接池最大阻塞等待时间（使用负值表示没有限制） max-wait: -1 # Redis服务器地址 host: localhost # 连接超时时间（毫秒） timeout: 0 然后就可以使用了StringRedisTemplate等Bean了\n@Autowired private StringRedisTemplate stringRedisTemplate; @Test public void test() throws Exception { // 保存字符串 \tstringRedisTemplate.opsForValue().set(\u0026#34;aaa\u0026#34;, \u0026#34;111\u0026#34;); Assert.assertEquals(\u0026#34;111\u0026#34;, stringRedisTemplate.opsForValue().get(\u0026#34;aaa\u0026#34;)); } 上面的代码通过自动配置的StringRedisTemplate对象进行Redis的读写操作，该对象从命名中就可注意到支持的是String类型。StringRedisTemplate就相当于RedisTemplate\u0026lt;String, String\u0026gt;的实现。除了String类型，实战中我们还经常会在Redis中存储对象，这时可以自己实现RedisSerializer\u0026lt;T\u0026gt;接口来对传入对象进行序列化和反序列化，进而将该对象写入Redis缓存。\npublic class User implements Serializable { private static final long serialVersionUID = -1L; private String username; ... } public class RedisObjectSerializer implements RedisSerializer\u0026lt;Object\u0026gt; { private Converter\u0026lt;Object, byte[]\u0026gt; serializer = new SerializingConverter(); private Converter\u0026lt;byte[], Object\u0026gt; deserializer = new DeserializingConverter(); static final byte[] EMPTY_ARRAY = new byte[0]; public Object deserialize(byte[] bytes) { if (isEmpty(bytes)) { return null; } try { return deserializer.convert(bytes); } catch (Exception ex) { throw new SerializationException(\u0026#34;Cannot deserialize\u0026#34;, ex); } } public byte[] serialize(Object object) { if (object == null) { return EMPTY_ARRAY; } try { return serializer.convert(object); } catch (Exception ex) { return EMPTY_ARRAY; } } private boolean isEmpty(byte[] data) { return (data == null || data.length == 0); } } @Configuration public class RedisConfig { @Bean public RedisTemplate\u0026lt;String, User\u0026gt; userRedisTemplate(RedisConnectionFactory factory) { RedisTemplate\u0026lt;String, User\u0026gt; template = new RedisTemplate\u0026lt;String, User\u0026gt;(); template.setConnectionFactory(factory); template.setKeySerializer(new StringRedisSerializer()); template.setValueSerializer(new RedisObjectSerializer()); return template; } } @Autowired private RedisTemplate\u0026lt;String, User\u0026gt; redisTemplate; @Test public void test() throws Exception { // 保存对象 \tUser user = new User(\u0026#34;超人\u0026#34;, 20); redisTemplate.opsForValue().set(user.getUsername(), user); user = new User(\u0026#34;蝙蝠侠\u0026#34;, 30); redisTemplate.opsForValue().set(user.getUsername(), user); user = new User(\u0026#34;蜘蛛侠\u0026#34;, 40); redisTemplate.opsForValue().set(user.getUsername(), user); Assert.assertEquals(20, redisTemplate.opsForValue().get(\u0026#34;超人\u0026#34;).getAge().longValue()); Assert.assertEquals(30, redisTemplate.opsForValue().get(\u0026#34;蝙蝠侠\u0026#34;).getAge().longValue()); Assert.assertEquals(40, redisTemplate.opsForValue().get(\u0026#34;蜘蛛侠\u0026#34;).getAge().longValue()); } RedisTemplate接口的方法很多，基本上涵盖了Redis了绝大部分操作，使用时参考其API文档就可以了。\nspring boot应用中使用rabbitmq 如子标题，有需求要在spring boot应用中使用redis缓存，这个还是比较简单的，如下：\n添加maven依赖\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-amqp\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 添加配置项\nspring: rabbitmq: password: 123456 port: 5672 host: localhost username: spring 配置队列、交换器、路由等高级信息，一般会用到org.springframework.amqp.core.Queue、org.springframework.amqp.core.QueueBuilder等类、org.springframework.amqp.core.Exchange的实现类，使用时参考其API文档。\n@Configuration public class RabbitConfig { @Bean public Queue helloQueue() { return new Queue(\u0026#34;hello\u0026#34;); } } 使用AmqpTemplate发送MQ消息，基本使用AmqpTemplate就可以进行MQ消息的绝大部分操作，使用时参考其API文档就可以了。\n@Autowired private AmqpTemplate rabbitTemplate; public void send() { String context = \u0026#34;hello \u0026#34; + new Date(); System.out.println(\u0026#34;Sender : \u0026#34; + context); this.rabbitTemplate.convertAndSend(\u0026#34;hello\u0026#34;, context); } 使用RabbitListener配合RabbitHandler接收MQ消息，一般会用到org.springframework.amqp.rabbit.annotation.RabbitListener、org.springframework.amqp.rabbit.annotation.RabbitHandler、org.springframework.amqp.rabbit.annotation.Queue、org.springframework.amqp.rabbit.annotation.Exchange等类，使用时参考其API文档。\n@Component @RabbitListener(queues = \u0026#34;hello\u0026#34;) public class Receiver { @RabbitHandler public void process(String hello) { System.out.println(\u0026#34;Receiver : \u0026#34; + hello); } } spring boot应用打包成docker镜像 spring boot应用的构建工具已经很完善了，要完成子标题所述的任务已经有很成熟的maven plugin - docker-maven-plugin。\n简单使用方法\n\u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; ... \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;com.spotify\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;docker-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;VERSION GOES HERE\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;imageName\u0026gt;example\u0026lt;/imageName\u0026gt; \u0026lt;baseImage\u0026gt;java\u0026lt;/baseImage\u0026gt; \u0026lt;entryPoint\u0026gt;[\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;/${project.build.finalName}.jar\u0026#34;]\u0026lt;/entryPoint\u0026gt; \u0026lt;!--copy the service\u0026#39;s jar file from target into the root directory of the image --\u0026gt; \u0026lt;resources\u0026gt; \u0026lt;resource\u0026gt; \u0026lt;targetPath\u0026gt;/\u0026lt;/targetPath\u0026gt; \u0026lt;directory\u0026gt;${project.build.directory}\u0026lt;/directory\u0026gt; \u0026lt;include\u0026gt;${project.build.finalName}.jar\u0026lt;/include\u0026gt; \u0026lt;/resource\u0026gt; \u0026lt;/resources\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; ... \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; 如果要使用自定义的Dockerfile，则如下配置，在dockerDirectory指定的目录下放入Dockerfile文件就可以了：\n\u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; ... \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;com.spotify\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;docker-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;VERSION GOES HERE\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;imageName\u0026gt;example\u0026lt;/imageName\u0026gt; \u0026lt;dockerDirectory\u0026gt;docker\u0026lt;/dockerDirectory\u0026gt; \u0026lt;resources\u0026gt; \u0026lt;resource\u0026gt; \u0026lt;targetPath\u0026gt;/\u0026lt;/targetPath\u0026gt; \u0026lt;directory\u0026gt;${project.build.directory}\u0026lt;/directory\u0026gt; \u0026lt;include\u0026gt;${project.build.finalName}.jar\u0026lt;/include\u0026gt; \u0026lt;/resource\u0026gt; \u0026lt;/resources\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; ... \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; 其它还可以在package时自动打docker镜像，在deploy时自动将docker镜像推入registry，这些高级功能参考官方文档。\ncentos7下手动设置DNS服务器 centos7下网络默认是由NetworkManager管理的，如果直接修改/etc/resolv.conf设置的DNS服务器很容易被冲掉，因此找到了一个办法解决这个问题。\n 修改 /etc/NetworkManager/NetworkManager.conf 文件，在main部分添加 “dns=none” 选项：  [main] plugins=ifcfg-rh dns=none  NetworkManager重新装载上面修改的配置  # systemctl restart NetworkManager.service  手工修改 /etc/resolv.conf  nameserver 114.114.114.114 nameserver 8.8.8.8 专业的bash脚本 最近看了istio-sidecar相关的bash脚本，发现一个专业的bash脚本最好还是不要像写流水帐一样书写脚本逻辑，是很有必要加入必要的注释、输入参数解析、脚本使用说明、定义主函数及各分支函数。\n  在脚本开关需要用英文书写必要的注释详细说明脚本的用途，这一点参考一些专业的脚本都可以看到。\n  建议使用Linux风格的输入参数风格解析，可以使用bash的内置命令getopts和外部命令getopt，这两种方法的使用方法可参考shell脚本之shift和getopts、shell中的getopt与getopts。\n  脚本使用说明可使用usage函数完成，如下:\nusage() { echo \u0026#34;bla bla bla ...\u0026#34; } # 解析参数时，当发现-h或--help参数，立即执行usage，输出脚本使用说明 -h|--help) usage ;;   为了避免bash脚本成为流水帐，建议整个脚本按以下函数组织\n# 解析参数 parse_args() { ... } # 校验参数 validate_args() { ... } # 脚本所做工作第一步 do_work_step1() { ... } # 脚本所做工作第二步 do_work_step2() { ... } # 脚本所做工作第三步 do_work_step3() { ... } main() { parse_args validate_args do_work_step1 do_work_step2 do_work_step3 ... } # 脚本入口函数 main swagger文档的妙用 很多后端的项目都以swagger文档的方式向外暴露API文档，最近在工作确实体会到这种方式的好处。\n  前端拿到swagger API文档后，可使用swagger-editor轻松生成nodejs-server版的server stub，在此基础上即可开发简易的mock server了，这样前端的开发即可不再依赖后端了。\n  后端可使用swagger-editor生成对应语言的server stub，生成的代码很有参考价值，可直接在此基础上改造或将部分代码拷贝到已有后端项目中。\n  如果是微服务架构的应用，可使用swagger-editor生成相应语言的客户端代码，这样服务间的调用直接用客户端代码组合形成的SDK即可，不再需要手动发送HTTP请求及解析HTTP响应了。\n  微服务架构的应用，每个微服务都以swagger方式暴露API，这时可以将这些API文档聚合起来，请团队中的成员在统一的文档中心查看各微服务的文档，如下：\ndocker run -d --name swagger-docs -p 8888:8080 -e \u0026#39;URLS=[{ url: \u0026#34;http://petstore.swagger.io/v2/swagger.json\u0026#34;, name: \u0026#34;Petstore\u0026#34; }, { url: \u0026#34;http://generator.swagger.io/api/swagger.json\u0026#34;, name: \u0026#34;Generator\u0026#34; }]\u0026#39; swaggerapi/swagger-ui:latest 这里使用了swagger-ui的一个urls选项，这个选项在2017年初就已经存在了，不知道为什么网上讲swagger API文档聚合的方案基本都是让改造swagger-ui的代码，汗！\n    参考  http://blog.didispace.com/springbootredis/ http://blog.didispace.com/spring-boot-rabbitmq/ https://github.com/spotify/docker-maven-plugin http://www.pubyun.com/blog/announce/centos-7-%E4%B8%8B%EF%BC%8C%E5%A6%82%E4%BD%95%E8%AE%BE%E7%BD%AEdns%E6%9C%8D%E5%8A%A1%E5%99%A8/ http://www.361way.com/shell-shift-getopts/4973.html http://www.361way.com/shell-getopt/4981.html https://github.com/swagger-api/swagger-ui/pull/3261  ","permalink":"https://jeremyxu2010.github.io/2019/05/%E5%B7%A5%E4%BD%9C%E4%BB%BB%E5%8A%A1%E9%A1%B9%E6%9D%82%E8%AE%B0_201904/","tags":["spring boot","redis","rabbitmq","docker","dns","bash","swagger"],"title":"工作任务项杂记_201904"},{"categories":["容器编排"],"contents":"问题 工作中经常发现一些第三方写的docker容器运行有问题，这时我们会通过docker logs命令观察容器的运行日志。很可惜，有时容器中运行的程序仅从日志很难查明问题。这时我们会通过docker exec在目标容器中执行某些命令以探查问题，有时却发现一些镜像很精简，连基本的sh、bash、netstat等命令都没包含。这时就很尴尬了，诊断问题很困难。\n不太优雅的解决方案 为了避免上述问题，我们在开发过程中一般要求最终打出的docker镜像中包含一些基本的调试命令，如sh、bash、netstat、telnet等。但这个解决方案只能规范自己开发的docker镜像，对于第三方开发的docker镜像就没办法了，而且会导致最终打出的镜像变大了不少，影响镜像的分发效率。\n更优雅的方案 今天在github.com上闲逛时偶然发现一个工具docker-debug，发现这个工具可以很好的解决这个问题。\n这个工具的使用方法也很简单，参考以下命令：\n# Suppose the container below is a container which should be checked docker run -d --name dev -p 8000:80 nginx:latest # Enter a shell where we can access the above container\u0026#39;s namespaces (ipc, pid, network, etc, filesystem) docker-debug dev bash -l 更丰富的使用说明参考这个视频\ndocker-debug的实现原理 看了下文档，发现docker-debug的实现原理也挺简单的。\n  find image docker is has, not has pull the image. find container name is has, not has return error. from customize image runs a new container in the container's namespaces (ipc, pid, network, etc, filesystem) with the STDIN stay open. create and run a exec on new container. Debug in the debug container. then waits for the debug container to exit and do the cleanup.   简单说执行docker-debug命令也会使用一个包含了常用诊断命令的镜像启动一个诊断容器，该诊断容器将在目标容器相关的命名空间中运行，这样在这个容器中就可以访问目标容器的ipc, pid, network, etc, filesystem，然后使用docker exec命令在诊断容器运行命令，并将docker exec运行命令的输入输出pipe到docker-debug命令的输入输出上。\ndocker-debug的源码分析 在大量使用该工具前，简单分析下这个工具的源码。\n工具的主逻辑源码在这里\ncontainerID, err = cli.FindContainer(options.container) if err != nil { return err } containerID, err = cli.CreateContainer(containerID, options) if err != nil { return err } resp, err := cli.ExecCreate(options, containerID) if err != nil { return err } errCh := make(chan error, 1) go func() { defer close(errCh) errCh \u0026lt;- func() error { return cli.ExecStart(options, resp.ID) }() }() 其中有两处重点：\n一个是创建一个容器使用目标容器的ipc, pid, network, etc, filesystem，源码在这里\n// CreateContainer create new container and attach target container resource func (cli *DebugCli) CreateContainer(attachContainer string, options execOptions) (string, error) { var mounts []mount.Mount if cli.config.MountDir != \u0026#34;\u0026#34; { ctx, cancel := cli.withContent(cli.config.Timeout) info, err := cli.client.ContainerInspect(ctx, attachContainer) cancel() if err != nil { return \u0026#34;\u0026#34;, errors.WithStack(err) } mountDir, ok := info.GraphDriver.Data[\u0026#34;MergedDir\u0026#34;] mounts = []mount.Mount{} if ok { mounts = append(mounts, mount.Mount{ Type: \u0026#34;bind\u0026#34;, Source: mountDir, Target: cli.config.MountDir, }) } for _, i := range info.Mounts { var mountType = i.Type if i.Type == \u0026#34;volume\u0026#34; { mountType = \u0026#34;bind\u0026#34; } mounts = append(mounts, mount.Mount{ Type: mountType, Source: i.Source, Target: cli.config.MountDir + i.Destination, ReadOnly: !i.RW, }) } } if options.volumes != nil { // -v bind mount \tif mounts == nil { mounts = []mount.Mount{} } for _, m := range options.volumes { mountArgs := strings.Split(m, \u0026#34;:\u0026#34;) mountLen := len(mountArgs) if mountLen \u0026gt; 0 \u0026amp;\u0026amp; mountLen \u0026lt;= 3 { mountDefault := mount.Mount{ Type: \u0026#34;bind\u0026#34;, ReadOnly: false, } switch mountLen { case 1: mountDefault.Source = mountArgs[0] mountDefault.Target = mountArgs[0] case 2: if mountArgs[1] == \u0026#34;rw\u0026#34; || mountArgs[1] == \u0026#34;ro\u0026#34; { mountDefault.ReadOnly = mountArgs[1] != \u0026#34;rw\u0026#34; mountDefault.Source = mountArgs[0] mountDefault.Target = mountArgs[0] } else { mountDefault.Source = mountArgs[0] mountDefault.Target = mountArgs[1] } case 3: mountDefault.Source = mountArgs[0] mountDefault.Target = mountArgs[1] mountDefault.ReadOnly = mountArgs[2] != \u0026#34;rw\u0026#34; } mounts = append(mounts, mountDefault) } } } targetName := containerMode(attachContainer) conf := \u0026amp;container.Config{ Entrypoint: strslice.StrSlice([]string{\u0026#34;/usr/bin/env\u0026#34;, \u0026#34;sh\u0026#34;}), Image: cli.config.Image, Tty: true, OpenStdin: true, StdinOnce: true, } hostConfig := \u0026amp;container.HostConfig{ NetworkMode: container.NetworkMode(targetName), UsernsMode: container.UsernsMode(targetName), IpcMode: container.IpcMode(targetName), PidMode: container.PidMode(targetName), Mounts: mounts, //VolumesFrom: []string{attachContainer}, \t} ctx, cancel := cli.withContent(cli.config.Timeout) body, err := cli.client.ContainerCreate( ctx, conf, hostConfig, nil, \u0026#34;\u0026#34;, ) cancel() if err != nil { return \u0026#34;\u0026#34;, errors.WithStack(err) } ctx, cancel = cli.withContent(cli.config.Timeout) err = cli.client.ContainerStart( ctx, body.ID, types.ContainerStartOptions{}, ) cancel() return body.ID, errors.WithStack(err) } 一个是将docker exec运行命令的输入输出pipe到docker-debug命令的输入输出，源码在这里\n// ExecStart exec start func (cli *DebugCli) ExecStart(options execOptions, execID string) error { execConfig := types.ExecStartCheck{ Tty: true, } ctx, cancel := cli.withContent(cli.config.Timeout) response, err := cli.client.ContainerExecAttach(ctx, execID, execConfig) defer cancel() if err != nil { return errors.WithStack(err) } streamer := tty.HijackedIOStreamer{ Streams: cli, InputStream: cli.in, OutputStream: cli.out, ErrorStream: cli.err, Resp: response, TTY: true, } return streamer.Stream(context.Background()) } 整个实现逻辑还是比较清晰的。\n另外，还发现类似的工具kube-debug，以后诊断pod中的问题方便多了。\n参考  https://docs.docker.com/engine/api/latest https://github.com/zeromake/docker-debug https://github.com/aylei/kubectl-debug https://draveness.me/docker  ","permalink":"https://jeremyxu2010.github.io/2019/04/%E5%B7%A7%E5%A6%99%E8%B0%83%E8%AF%95docker%E5%AE%B9%E5%99%A8/","tags":["docker","bash"],"title":"巧妙调试docker容器"},{"categories":["容器编排"],"contents":"最近在工作中验证istio的网格扩展方案，其中涉及打通网络的需求，也即希望在外部虚拟机可以连通kubernetes集群内部的服务IP、Pod IP，在kubernetes的Pod中可以连通外部虚拟机的IP。\n显然kubernetes里的Pod连通外部虚拟机的IP不是问题，只要虚拟机的防火墙没有限制，这个本身就是连通的。关键是怎样让虚拟机可以直接连通kubernetes里的service IP和pod IP。\n这里试验了好几种方案，记录一下。\n配置路由规则 最先想到的是直接配置路由规则方案。由于kubernetes的宿主机上可以直接连通service IP和pod IP，而且kubernetes的宿主机上一般安装了docker，ip forward本身也是开启的。因此只需要在虚拟机上设置两条路由规则，就可以将从虚拟机发出的目标地址是service cidr和pod cidr范围里的数据包转发到kubernetes的宿主机，然后kubernetes的宿主机则可以将数据包再转发给service或pod，这基本是最简单的方案了。参考命令如下：\n# 在虚拟机上添加以下两条路由规则 sudo route add -net 10.96.0.0/12 gw 192.168.33.10 sudo route add -net 10.244.0.0/16 gw 192.168.33.10 其中10.96.0.0/12为service cidr, 10.244.0.0/16为pod cidr, 192.168.33.10为kubernetes的宿主机。\nstrongswan搭建VPN 配置路由规则虽然简单，但感觉还是太简易了，路由规则很容易被其它进程覆写了。偶然看到rancher推出的多kubernetes网络打通方案submariner，仔细读了下它的设计方案，发现它是使用strongswan建立的IPsec VPN。于是也尝试了直接使用strongswan搭建IPsec VPN。strongswan的文档写得比较好，将各种场景如何配置都举了个例子。经过仔细对比，发现Roadwarrior Case with Virtual IP这个场景应该是最适合我们的，于是参考这个场景的配置，验证了下我们这个场景，具体验证过程见这里。可惜最终没有成功，原因未知。\nsshuttle搭建VPN 虽然使用sshuttle搭建VPN失败了，但还是不甘心。前几天偶然发现了一个叫sshuttle的工具，其号称是穷人极简的VPN方案。只有能与对端建立SSH连接，就可以使用这个工具非常方便地建立一个VPN隧道，于是试了试。参考命令如下：\nsudo yum install -y sshuttle sleep 5 # 提前生成ssh密钥对，并做好ssh密钥认证登录 # ssh-keygen -t rsa -N \u0026#39;\u0026#39; # ssh-copy-id root@192.168.33.10 sudo sshuttle -r root@192.168.33.10 10.96.0.0/12 10.244.0.0/16 \u0026gt; sshuttle.log 2\u0026gt;\u0026amp;1 \u0026amp; 上面的命令意思是，如果发现数据包的目标IP地址是10.96.0.0/12、10.244.0.0/16这个范围内，则将数据包经由VPN隧道传送出去，真的是好方便。完整的验证过程见这里。\n总结 打通网络的方案很多，但基本都要求对网络及iptables知识有一定了解，幸好平时在这方面有一切储备。对于一般场景，设置路由规则或sshuttle建VPN基本就满足需求了。如果是对安全性要求非常高的场景，还是得采用一套成熟的VPN软件搞定。\n参考  https://istio.io/zh/docs/setup/kubernetes/additional-setup/mesh-expansion/ https://github.com/rancher/submariner https://github.com/strongswan/strongswan https://sshuttle.readthedocs.io/  ","permalink":"https://jeremyxu2010.github.io/2019/03/%E6%89%93%E9%80%9A%E5%88%B0kubernetes%E9%9B%86%E7%BE%A4%E7%9A%84%E7%BD%91%E7%BB%9C/","tags":["kubernetes","iptables","vpn","strongswan"],"title":"打通到kubernetes集群的网络"},{"categories":["工具"],"contents":"这周的工作需要在一个独立的kubernetes环境调试功能，自然而然地想到在本机装个虚拟机搭建这个环境。不过有同事推荐我试一下vagrant，久闻Vagrant大名，之前也经常在一些开源项目中看到它，今天花了些时间了解下这个新东西。\nvagrant的简介 Vagrant是hashicorp这家公司的产品，这家公司主要做云基础设施自动化的，其名下大名鼎鼎的产品有Consul、Vault、Nomad、Terraform，这前在做微服务框架时做过他们的Consul，还是挺靠谱的。他们的产品感觉都比较有创新，而且基本上都开源了，他们的开源地址是这里。\nVagrant是用来管理虚拟机的，如VirtualBox、VMware、AWS等，主要好处是可以提供一个可配置、可移植和复用的软件环境，可以使用shell、chef、puppet等工具部署。所以vagrant不能单独使用，如果你用它来管理自己的开发环境的话，必须在自己的电脑里安装了虚拟机软件，我使用的是virtualbox。\nVagrant提供一个命令行工具vagrant，通过这个命令行工具可以直接启动一个虚拟机，当然你需要提前定义一个Vagrantfile文件，这有点类似Dockerfile之于docker了。\n跟docker类比这来看vagrant就比较好理解了，vagrant也是用来提供一致性环境的，vagrant本身也提供一个镜像源，使用vagrant init hashicorp/precise64就可以初始化一个Ubuntu 12.04的镜像。\n安装vagrant 我本机是macOS系统，安装vagrant比较简单，命令如下：\n$ brew cask install virtualbox $ brew cask install vagrant 其它操作下安装也挺简单的，参见官方文档就可以了。\n使用vagrant 首先我这里创建第一个虚拟机，第一步是要将基础镜像拉回到本地缓存着，用以下命令：\n$ vagrant box add --provider virtualbox centos/7 # 如果box文件下载太慢，也可以通过其它工具将box文件下载到本地之后，用下面的命令添加到缓存 $ vagrant box add --name centos/7 --provider virtualbox $your_donwload_dir/centos_virtualbox.box 对box的一系列操作命令文档见这里。\n有了基础镜像box后，接下来在某一目录用box init即可创建一个初始的Vagrantfile文件:\n$ cd $your_working_dir $ vagrant init centos/7 vagrant init命令比较简单，参见官方文档就可以了。\n接下来就是修改Vagrantfile文件了，打开Vagrantfile文件，看一看里面的注释大概就知道怎么写了，主要是ruby的语法，我们用得最多的就是虚拟机配置config.vm和ssh配置config.ssh，这个官方文档里将每个配置项都详细描述了，按描述配置就可以了，当然对于虚拟机、ssh本身有哪些配置可以调整提前要了解。\n除了对虚拟机进行一些配置外，还可以通过各类Provisioner自动化地安装软件、调整配置。官方默认提供的Provisioner列表在这里。但我们平时用得比较多的主要有以下几个File、Shell、Ansible、Docker等，使用方法如下：\nVagrant.configure(\u0026#34;2\u0026#34;) do |config| # ... other configuration # Copy files from host to guest vm config.vm.provision \u0026#34;file\u0026#34;, source: \u0026#34;~/path/to/host/folder\u0026#34;, destination: \u0026#34;$HOME/remote/newfolder\u0026#34; # Execute shell script on guest vm config.vm.provision \u0026#34;shell\u0026#34;, path: \u0026#34;script.sh\u0026#34; # Run Ansible from the Vagrant Host config.vm.provision \u0026#34;ansible\u0026#34; do |ansible| ansible.playbook = \u0026#34;playbook.yml\u0026#34; end # Install Docker and pull Docker images on guest vm config.vm.provision \u0026#34;docker\u0026#34; do |d| d.pull_images \u0026#34;ubuntu\u0026#34; d.pull_images \u0026#34;vagrant\u0026#34; end end 上面的示例都比较简单，每个Provisioner都有一些参数用于满足一些特殊场景，这些参数的用法参考官方文档就可以了。\n除此之外还可以进行一些网络相关的配置，主要是映射一些端口到宿主机、设置私有网络、设置公开网络。如果是私有网络，则创建的虚拟机不对外公布，仅宿主机可访问。如果是公开网络，则创建的虚拟机会连接到局域网中的路由器上，如果能从路由器那里申请到IP，则其它主机也可以访问该虚拟机。\nvagrant还提供多种机制将宿主机上的一些目录同步到虚拟机中，平时用得比较多就是它的默认机制：\nVagrant.configure(\u0026#34;2\u0026#34;) do |config| # other config here config.vm.synced_folder \u0026#34;src/\u0026#34;, \u0026#34;/srv/website\u0026#34; end Vagrantfile文件写好后，就可以以此为基础操作虚拟机了：\n# 启动 vagrant up # 关机 vagrant halt # 重启 vagrant reload # 暂停 vagrant suspend # 恢复 vagrant resume # 给虚拟机打个快照 vagrant snapshot save ${snapshotName} # 将虚拟机还原到快照 vagrant snapshot restore ${snapshotName} # 列出虚拟机的快照 vagrant snapshot list # 删除虚拟机的某个快照 vagrant snapshot delete ${snapshotName} # 用ssh连接虚拟机 vagrant ssh # 输出虚拟机的SSH连接配置，其它SSH工具可参考这些配置连接虚拟机 vagrant ssh-config # 用RDP客户端连接虚拟机 vagrant rdp # 删除虚拟机 vagrant destroy 这样操作虚拟机真的是很方便啊。\n还有一些高级功能，比如定义操控多个虚拟机、发布自已的镜像等，这些参考官方文档就可以了。\n为什么用vagrant vagrant的功能介绍得差不多了，再来说一下为啥要用vagrant。原来我们搭建一个测试场景，会涉及很多台虚拟机，如果全部手工搭建，不仅很繁琐，而且不便于保存成果，下次遇到同样的需求又得重搭一次，而极容易出错，这些人肉操作也不便于修订管理。后面为了自动化实施，我们用了ansible之类工具，将操作步骤都写进ansible脚本中。ansbile方案确实解决了很大的问题，但失败率还是有些高，原因是待部署的虚拟机状态不统一。而vagrant直接将待部署的虚拟机也统一了，本身也支持用shell脚本、ansible脚本将操作步骤都记录下来。这样一来，只要拿到Vagrantfile，在任何主机上都只需要一条命令就可以将整套环境部署起来了。\n比如我写了一个搭建单节点kubernetes环境的Vagrantfile，别人只要在本机安装好了vagrant，将这个vagrantfile下载下来，在该目录执行vagrant up命令，一个单节点kubernetes环境就自动创建好了。\n总结 用正确的工具去做正确的事儿，真的是事半功倍。\n参考  https://www.vagrantup.com/docs https://jimmysong.io/posts/vagrant-intro/  ","permalink":"https://jeremyxu2010.github.io/2019/03/%E4%BD%BF%E7%94%A8vagrant%E6%90%AD%E5%BB%BA%E9%AA%8C%E8%AF%81%E7%8E%AF%E5%A2%83/","tags":["linux","vagrant","kubernetes"],"title":"使用vagrant搭建验证环境"},{"categories":["容器编排"],"contents":"工作中一个第三方软件使用了Posgresql数据库，而在我们的场景里，我们需要保证Posgresql数据库的高可用，网上查找了一下，发现stolon这个高可用，在使用前，先研究一下它的原理。\nStolon的架构  Stolon is composed of 3 main components\n keeper: it manages a PostgreSQL instance converging to the clusterview computed by the leader sentinel. sentinel: it discovers and monitors keepers and proxies and computes the optimal clusterview. proxy: the client's access point. It enforce connections to the right PostgreSQL master and forcibly closes connections to old masters.  For more details and requirements see Stolon Architecture and Requirements\n 上面是Stolon项目Readme中的说明，可以看到其本质与Redis Sentinel的方案比较类似，都是哨兵模式。\n由sentinel组件发现、观察keeper与proxy的信息，并计算出最优的集群视图。\n每个keeper组件管理一个posgresql实例，并根据sentinel计算出的最优集群视图，将posgresql集群中各实例加以配置，最实现集群的最优方案。\n除此之外，为了让客户端能透明地访问Posgresql集群，还提供了proxy组件处理客户端请求，最请求导向集群的master节点，这一点比redis sentinel方案更好了，就不用客户端驱动专门做sentinel模式支持了。\nStolon安装 官方文档中有写如何在kubernetes集群中部署Stolon集群，虽然也是用yaml文件分别3个组件，不过还是麻烦了些，幸好找到了对应的helm chart。\n这样部署Stolon集群就很简单了：\ngit clone https://github.com/lwolf/stolon-chart cd stolon-chart/stolon helm install --namespace test --name stolon . --set store.backend=kubernetes --set persistence.enabled=true --set persistence.storageClassName=defaultScName stolon-chart里使用storageclass的方式不太合规，顺便改了下，给它们发了个PR，不过貌似没有回应\n然后kubernetes集群内部的其它pod配置stolon-proxy的service FQDN地址就可以访问到它了，比如用上面的命令部署的stolon集群可以以下面的地址来访问它：\nstolon-stolon-proxy.test.svc.cluster.local:5432 验证高可用 既然是解决高可用性问题，光看看官方文档就这么部署了，还是不放心的，还是要模拟一下出问题的场景。官方文档里讲了验证的方法：\n In a single node setup we can kill the current master keeper pod but usually the statefulset controller will recreate a new pod before the sentinel declares it as failed. To avoid the restart we'll first remove the statefulset without removing the pod and then kill the master keeper pod. The persistent volume will be kept so we'll be able to recreate the statefulset and the missing pods will be recreated with the previous data.\nkubectl delete statefulset stolon-keeper --cascade=false kubectl delete pod stolon-keeper-0 You can take a look at the leader sentinel log and will see that after some seconds it'll declare the master keeper as not healthy and elect the other one as the new master:\nno keeper info available db=cb96f42d keeper=keeper0 no keeper info available db=cb96f42d keeper=keeper0 master db is failed db=cb96f42d keeper=keeper0 trying to find a standby to replace failed master electing db as the new master db=087ce88a keeper=keeper1 Now, inside the previous psql session you can redo the last select. The first time psql will report that the connection was closed and then it successfully reconnected:\npostgres=# select * from test; server closed the connection unexpectedly This probably means the server terminated abnormally before or while processing the request. The connection to the server was lost. Attempting reset: Succeeded. postgres=# select * from test; id | value ----+-------- 1 | value1 (1 row)  就是删掉一个master节点的stolon-keeper组件，然后看sentinel的日志，可以很明显地看到一个新的master节点被选举出来了，这时posgresql客户端用原来的地址连上新的master节点了，验证成功了。\n这里又学到一个小技巧，删除deployment、statefulset等时，加上--cascade=false可以保留住与这些资源对应的pod。\n参考  https://github.com/sorintlab/stolon/ https://github.com/sorintlab/stolon/blob/master/examples/kubernetes/README.md https://github.com/lwolf/stolon-chart/tree/master/stolon  ","permalink":"https://jeremyxu2010.github.io/2019/03/posgresql%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E5%AE%9E%E8%B7%B5/","tags":["posgresql","kubernetes","helm"],"title":"PosgreSQL高可用集群实践"},{"categories":["devops"],"contents":"很早以前就听说过redis社区推崇一种哨兵模式的高可用集群部署模式，今天花时间研究了一下，正好记录下来。\n哨兵模式 哨兵简介 哨兵模式是在Redis 2.8 版本开始引入的。哨兵的核心功能是主节点的自动故障转移。\n下面是 Redis 官方文档对于哨兵功能的描述：\n 监控（Monitoring）：哨兵会不断地检查主节点和从节点是否运作正常。   自动故障转移（Automatic failover）：当主节点不能正常工作时，哨兵会开始自动故障转移操作，它会将失效主节点的其中一个从节点升级为新的主节点，并让其他从节点改为复制新的主节点。 配置提供者（Configurationprovider）：客户端在初始化时，通过连接哨兵来获得当前 Redis 服务的主节点地址。 通知（Notification）：哨兵可以将故障转移的结果发送给客户端。  其中，监控和自动故障转移功能，使得哨兵可以及时发现主节点故障并完成转移；而配置提供者和通知功能，则需要在与客户端的交互中才能体现。\n哨兵的架构 典型的哨兵架构图如下所示：\n它由两部分组成，哨兵节点和数据节点：\n 哨兵节点：哨兵系统由一个或多个哨兵节点组成，哨兵节点是特殊的 Redis 节点，不存储数据。 数据节点：主节点和从节点都是数据节点。  哨兵模式的部署 参考官方文档手工部署一个哨兵模式的redis集群还是挺麻烦的，网上有不少这方面的操作指引，这里就不详细介绍了。\n云原生时代，这里还是介绍一个快速在kubernetes中部署哨兵模式redis集群的办法，云原生时代部署基础组件就是方便啊。\n因为在本机的k8s集群测试，k8s集群里只有一个节点，因此稍微修改部署时的values.yaml:\nredis-ha-values-custom.yaml\n## Node labels, affinity, and tolerations for pod assignment ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#taints-and-tolerations-beta-feature ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity # Just for local develop environment affinity: {} 然后直接部署redis-ha:\nhelm install --namespace test --name redis-ha stable/redis-ha -f ./redis-ha-values-custom.yaml 很快在test命名空间就会发现redis的pod都正常运行了：\n$ kubectl get pod -n test NAME READY STATUS RESTARTS AGE redis-ha-server-0 2/2 Running 0 1h redis-ha-server-1 2/2 Running 0 1h redis-ha-server-2 2/2 Running 0 1h $ kubectl -n test get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE redis-ha ClusterIP None \u0026lt;none\u0026gt; 6379/TCP,26379/TCP 2h redis-ha-announce-0 ClusterIP 10.103.179.132 \u0026lt;none\u0026gt; 6379/TCP,26379/TCP 2h redis-ha-announce-1 ClusterIP 10.98.58.246 \u0026lt;none\u0026gt; 6379/TCP,26379/TCP 2h redis-ha-announce-2 ClusterIP 10.97.66.79 \u0026lt;none\u0026gt; 6379/TCP,26379/TCP 2h 总共3个pod，每个pod里有两个容器，一个是redis，一个是sentinel。整个部署方案的相关参数默认都配置得比较合理，完整的参考列表见这里。\n这里有特别注意，使用哨兵模式的客户端应该要配置哨兵的访问地址，如redis-ha-announce-0.test.svc.cluster.local:26379。有3个哨兵的访问地址，一般客户端这边会将这3个哨兵都备份下来，这样将可以避免sentinel成为单点故障。\n使用哨兵模式redis集群 从架构上看，要想使用哨兵模式的redis集群，客户端必须与哨兵先通信，拿到可用redis主节点信息后，再连接redis主节点，所以对redis客户端有一些要求。\n好消息是有些redis连接库已经支持了sentinel模式，如go-redis。即使有些redis库并不支持，也可以轻松加上对sentinel模式的支持，如这里。\n最后看了下harbor的代码，发现它目前的代码还不支持sentinel模式，正好可以改明儿把这块功能做上，给官方发Pull Request。\nbootstrap.go\n// Load and run the worker pool func (bs *Bootstrap) loadAndRunRedisWorkerPool(ctx *env.Context, cfg *config.Configuration) (pool.Interface, error) { redisPool := \u0026amp;redis.Pool{ MaxActive: 6, MaxIdle: 6, Wait: true, Dial: func() (redis.Conn, error) { return redis.DialURL( cfg.PoolConfig.RedisPoolCfg.RedisURL, redis.DialConnectTimeout(dialConnectionTimeout), redis.DialReadTimeout(dialReadTimeout), redis.DialWriteTimeout(dialWriteTimeout), ) }, TestOnBorrow: func(c redis.Conn, t time.Time) error { if time.Since(t) \u0026lt; time.Minute { return nil } _, err := c.Do(\u0026#34;PING\u0026#34;) return err }, } ...... 遇到的坑 整个部署还是比较顺利的，只是部署完毕后，想用redis-cli连接试验一下，结果一直卡住：\n# 这里会直接卡住 kubectl -n test run redis-client -t -i --image=redis:5.0.3-alpine -- redis-cli -h redis-ha-announce-0.test.svc.cluster.local -p 26379 改成下面这样就可以了：\nkubectl -n test run redis-client -t -i --image=redis:5.0.3-alpine -- sh -c \u0026#34;sleep 3; redis-cli -h redis-ha-announce-0.test.svc.cluster.local -p 26379\u0026#34; 这个应该填kubernetes的bug吧。\n参考  http://developer.51cto.com/art/201809/583104.htm https://redis.io/topics/sentinel https://www.cnblogs.com/xishuai/p/redis-sentinel.html https://blog.csdn.net/lanyang123456/article/details/81153474 https://github.com/helm/charts/tree/master/stable/redis-ha  ","permalink":"https://jeremyxu2010.github.io/2019/03/%E4%BD%BF%E7%94%A8%E5%93%A8%E5%85%B5%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2redis%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/","tags":["redis","sentinel","kubernetes"],"title":"使用哨兵模式部署redis高可用集群"},{"categories":["devops"],"contents":"今天工作中，需要在本机启动consul、zipkin两个容器，参考docker和zipkin两个镜像的说明，很自然地敲出了以下命令：\ndocker run -d -p 8500:8500 --name=dev-consul -e CONSUL_BIND_INTERFACE=eth0 consul agent -dev -ui docker run -d -p 9411:9411 openzipkin/zipkin 然后用浏览器去访问http://127.0.0.1:8500和http://127.0.0.1:9411，结果发现竟然不能访问。\n研究了好半天终于找到原因了。\nconsul在docker容器里运行的正确姿势：\ndocker run -d -p 8500:8500 --name=dev-consul -e CONSUL_BIND_INTERFACE=eth0 consul agent -dev -ui -client 0.0.0.0 关键是要加一个-client参数，这个在官方文档上有说明的：\n -client - The address to which Consul will bind client interfaces, including the HTTP and DNS servers. By default, this is \u0026ldquo;127.0.0.1\u0026rdquo;, allowing only loopback connections. In Consul 1.0 and later this can be set to a space-separated list of addresses to bind to, or a go-sockaddr template that can potentially resolve to multiple addresses.\n 因为容器运行时是使用-p参数把容器命名空间里的端口映射出来的，因此在容器里运行的程序监听地址必须绑定到0.0.0.0，如果只绑定到127.0.0.1，这样的端口没法映射出来。\nzipkin在docker容器里运行的正确姿势：\ndocker run -d -p 9411:9411 openzipkin/zipkin:2.12.3 关键是要指定镜像的版本为2.12.3，最新的版本2.12.5或latest是前4天发布的，存在严重的bug，汗！！！\n","permalink":"https://jeremyxu2010.github.io/2019/03/%E8%BF%90%E8%A1%8Cdocker%E5%AE%B9%E5%99%A8%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/","tags":["docker","consul","zipkin"],"title":"运行docker容器遇到的坑"},{"categories":["工具"],"contents":"在网上看到某技术产品的文档，想把文档弄下来在电子书阅读器上离线看，可发现这个技术文档没有提供pdf等电子书格式，于是想办法自己做一个，这里记录一下。\n首先将文档全下载下来\nwget -k --recursive --no-parent https://rook.github.io/docs/rook/v0.9/ 然后写个脚本将所有html文件都转成pdf\n$ wget -k --recursive --no-parent https://rook.github.io/docs/rook/v0.9/ $ cd rook.github.io/docs/rook/ $ mkdir pdfs $ cat convert2pdf.sh # names of files (without extension) files=$(ls -1 v0.9 | sed -e \u0026#39;s/\\.html$//\u0026#39;) # convert files for file in $files do echo \u0026#34;converting ${file}.html to ${file}.pdf\u0026#34; wkhtmltopdf -p socks5://127.0.0.1:1086 --javascript-delay 1000 v0.9/${file}.html pdfs/${file}.pdf done $ bash ./convert2pdf.sh 注意由于这些html文件引用了外部css/js，必须设置代理才能下载回来。另外文档里用到了google字体，得设置一会儿延迟，生成的pdf中字体才比较正常。\n最后用常用的pdf编辑工具（如PDF Export）将这些pdf合并成一个pdf，可以参考这里。\n","permalink":"https://jeremyxu2010.github.io/2019/03/%E5%9C%A8%E7%BA%BF%E6%96%87%E6%A1%A3%E7%94%B5%E5%AD%90%E4%B9%A6%E5%8C%96/","tags":["linux","wkhtmltopdf","wget"],"title":"在线文档电子书化"},{"categories":["devops"],"contents":"今天又有朋友问我，这个博客是怎么搭建的。在回答后，顺便重新申视了下博客的构建部署方式，发现还是有一些改进空间的，刚好今天有点时间，就把它优化一下。\n现状 博客是基于hugo构建的，构建过程可参考参考，我基本每隔一段时间会写一篇markdown格式的博文。本来编译部署还是比较简单的，不过有段时间github在国内访问比较慢，于是想到做一个镜像站，因而编译部署过程稍微复杂一点了，我写了个脚本专门搞定这个事。\ndeploy.sh\n#!/bin/bash echo -e \u0026#34;\\033[0;32mDeploying updates to Server...\\033[0m\u0026#34; mkdir -p public_gitee hugo --baseURL https://jeremy-xu.oschina.io/ -d public_gitee cd public_gitee git init git add . msg=\u0026#34;rebuilding site `date`\u0026#34; git commit -m \u0026#34;$msg\u0026#34; git remote add origin git@gitee.com:jeremy-xu/jeremy-xu.git git push --force origin master:master cd .. mkdir -p public_github hugo --baseURL https://jeremyxu2010.github.io/ -d public_github cd public_github git init git add . msg=\u0026#34;rebuilding site `date`\u0026#34; git commit -m \u0026#34;$msg\u0026#34; git remote add origin git@github.com:jeremyxu2010/jeremyxu2010.github.io.git git push --force origin master:master cd .. 整个还是比较简单的，就是分别编译两个站点的静态页面文件，分别推送到不同的git仓库里去。\n但在用的过程发现一些问题：\n 换一台新电脑时，就是写个markdown文档，最好要部署，还得在本机安装hugo这类工具 换一台新电脑时，需要在重新配置该电脑到github.com、gitee.com的SSH Keys  改进 有了以上缺陷，于是就想着是不是可以在云上自动构建部署，现在这类专门作CI的解决方案还挺多的。看到github上有不少现在都是用TravisCI作自动构建的，于是我也选择了这个。\n在网上很轻松找到一个范例，我参考它就快速写了一个。\n.travis.yaml\n# https://docs.travis-ci.com/user/deployment/pages/ # https://docs.travis-ci.com/user/reference/xenial/ # https://docs.travis-ci.com/user/languages/go/ # https://docs.travis-ci.com/user/customizing-the-build/ dist: xenial language: go go: - 1.11.x env: - GO111MODULE=on cache: directories: - \u0026#34;$HOME/.cache/go-build\u0026#34; - \u0026#34;$HOME/gopath/pkg/mod\u0026#34; # before_install # install - install any dependencies required install: - go get github.com/gohugoio/hugo before_script: - mkdir -p public_github 2\u0026gt; /dev/null # script - run the build script script: - hugo --baseURL https://jeremyxu2010.github.io/ -d public_github deploy: - provider: pages skip-cleanup: true github-token: \u0026#34;$GITHUB_TOKEN\u0026#34; # Set in travis-ci.org dashboard, marked secure email: \u0026#34;$GITHUB_EMAIL\u0026#34; # Set in travis-ci.org dashboard, marked secure name: \u0026#34;$GITHUB_USERNAME\u0026#34; # Set in travis-ci.org dashboard, marked secure github-url: github.com repo: \u0026#34;$GITHUB_USERNAME/$GITHUB_USERNAME.github.io\u0026#34; target_branch: master verbose: true keep-history: false local-dir: public_github on: branch: master 这部署单个git站点确实没什么问题了，可我还想顺便把gitee也部署了。\n于是很自然的是像下面这样改一下。\n.travis.yaml\n# https://docs.travis-ci.com/user/deployment/pages/ # https://docs.travis-ci.com/user/reference/xenial/ # https://docs.travis-ci.com/user/languages/go/ # https://docs.travis-ci.com/user/customizing-the-build/ dist: xenial language: go go: - 1.11.x env: - GO111MODULE=on cache: directories: - \u0026#34;$HOME/.cache/go-build\u0026#34; - \u0026#34;$HOME/gopath/pkg/mod\u0026#34; # before_install # install - install any dependencies required install: - go get github.com/gohugoio/hugo before_script: - mkdir -p public_github 2\u0026gt; /dev/null - mkdir -p public_gitee 2\u0026gt; /dev/null # script - run the build script script: - hugo --baseURL https://jeremyxu2010.github.io/ -d public_github - hugo --baseURL https://jeremy-xu.github.io/ -d public_gitee deploy: - provider: pages skip-cleanup: true github-token: \u0026#34;$GITHUB_TOKEN\u0026#34; # Set in travis-ci.org dashboard, marked secure email: \u0026#34;$GITHUB_EMAIL\u0026#34; # Set in travis-ci.org dashboard, marked secure name: \u0026#34;$GITHUB_USERNAME\u0026#34; # Set in travis-ci.org dashboard, marked secure github-url: github.com repo: \u0026#34;$GITHUB_USERNAME/$GITHUB_USERNAME.github.io\u0026#34; target_branch: master verbose: true keep-history: false local-dir: public_github on: branch: master - provider: pages skip-cleanup: true github-token: \u0026#34;$GITEE_TOKEN\u0026#34; # Set in travis-ci.org dashboard, marked secure email: \u0026#34;$GITEE_EMAIL\u0026#34; # Set in travis-ci.org dashboard, marked secure name: \u0026#34;$GITEE_USERNAME\u0026#34; # Set in travis-ci.org dashboard, marked secure github-url: gitee.com repo: \u0026#34;$GITEE_USERNAME/$GITEE_USERNAME\u0026#34; target_branch: master verbose: true keep-history: false local-dir: public_gitee on: branch: master 可惜这样花报错，好像是github-pages的deploy provider与gitee的私人令牌兼容性不太好，只能作罢。\n想了下，还是决定gitee就用脚本部署吧，刚好travis有script的deploy provider。\n最终修改成下面的样子。\n.travis.yaml\n# https://docs.travis-ci.com/user/deployment/pages/ # https://docs.travis-ci.com/user/reference/xenial/ # https://docs.travis-ci.com/user/languages/go/ # https://docs.travis-ci.com/user/customizing-the-build/ dist: xenial language: go go: - 1.11.x env: - GO111MODULE=on cache: directories: - \u0026#34;$HOME/.cache/go-build\u0026#34; - \u0026#34;$HOME/gopath/pkg/mod\u0026#34; before_install: - openssl aes-256-cbc -K $encrypted_e5acc89010f1_key -iv $encrypted_e5acc89010f1_iv -in .travis/travis.key.enc -out ~/.ssh/id_rsa -d - chmod 600 ~/.ssh/id_rsa # install - install any dependencies required install: - go get github.com/gohugoio/hugo before_script: - mkdir -p public_github 2\u0026gt; /dev/null - mkdir -p public_gitee 2\u0026gt; /dev/null # script - run the build script script: - hugo --baseURL https://jeremyxu2010.github.io/ -d public_github - hugo --baseURL https://jeremy-xu.github.io/ -d public_gitee deploy: - provider: pages skip-cleanup: true github-token: \u0026#34;$GITHUB_TOKEN\u0026#34; # Set in travis-ci.org dashboard, marked secure email: \u0026#34;$GITHUB_EMAIL\u0026#34; # Set in travis-ci.org dashboard, marked secure name: \u0026#34;$GITHUB_USERNAME\u0026#34; # Set in travis-ci.org dashboard, marked secure repo: \u0026#34;$GITHUB_USERNAME/$GITHUB_USERNAME.github.io\u0026#34; target_branch: master verbose: true keep-history: false local-dir: public_github on: branch: master - provider: script skip-cleanup: true script: bash .travis/deploy_to_gitee.sh on: branch: master addons: ssh_known_hosts: - github.com - gitee.com deploy_to_gitee.sh\n#!/bin/bash cd public_gitee git config user.name \u0026#34;Jeremy Xu\u0026#34; git config user.email \u0026#34;xxxx@gmail.com\u0026#34; git init git add . msg=\u0026#34;rebuilding site `date`\u0026#34; git commit -m \u0026#34;$msg\u0026#34; git remote add origin git@gitee.com:jeremy-xu/jeremy-xu.git git push --force origin master:master cd .. 注意，这里为了保护gitee的SSH Keys，参考这里采用了Travis加密了SSH Key文件。\n成果 以后可以不再受工具、环境的限制的限制，随时写篇markdown文推到github上的blog_source项目就可以了。github本身提供了直接新建文件、编辑文件的功能，甚至都可以直接在github站点上编辑就可以了，太酷了。\n参考  https://axdlog.com/zh/2018/using-hugo-and-travis-ci-to-deploy-blog-to-github-pages-automatically/ https://docs.travis-ci.com/user/deployment https://docs.travis-ci.com/user/deployment/script https://docs.travis-ci.com/user/deployment/pages/ https://restic.net/blog/2018-09-02/travis-build-cache https://zzde.me/2018/auto-deploy-hexo-blog-with-traivs-ci/  ","permalink":"https://jeremyxu2010.github.io/2019/03/%E4%BD%BF%E7%94%A8travisci%E5%81%9A%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E7%9A%84%E8%87%AA%E5%8A%A8%E6%9E%84%E5%BB%BA/","tags":["linux","TravisCI"],"title":"使用TravisCI做个人博客的自动构建"},{"categories":["容器编排"],"contents":"工作中需要在容器里操作docker镜像，而且又不想污染宿主机上的docker镜像，找到了docker in docker(dind)的方案，这里记录一下。\n容器里用dind 首先直接用docker容器作试验，试验一下功能：\n# 启动docker in docker docker run --privileged -v `pwd`/ca.crt:/etc/docker/certs.d/myregistrydomain.com/ca.crt -d --name dockerd docker:stable-dind --registry-mirror=https://myregistrydomain.com # 在另一个容器里拉取镜像，从输出来看，拉取镜像是成功了的 docker run --rm --link dockerd:docker docker:stable docker pull busybox:latest # 在宿主机上检查，并没有看到拉取的镜像，说明没有污染宿主机的docker镜像 docker images | grep busybox 使用还是比较简单的。\n这里注意两点：\n  为了拉取镜像加速，我这里使用了自己架设的docker registry服务，因此dockerd加了参数--registry-mirror=https://myregistrydomain.com。\n  自己架设的docker registry服务使用的是自签名证书，因此参考官方文档，还设置了自签名证书对应的ca证书/etc/docker/certs.d/myregistrydomain.com/ca.crt。\n在看官方文档时，发现文档上说有些版本的docker需要设置在操作系统级别信任证书，不过我目前还没遇到这种情况，这里稍微关注一下。\n  k8s里使用dind 简单写个deployment的k8s描述文件：\napiVersion: extensions/v1beta1 kind: Deployment metadata: name: docker-test spec: replicas: 1 template: metadata: labels: app: docker-test spec: containers: - name: dockerd image: \u0026#39;docker:stable-dind\u0026#39; securityContext: privileged: true args: [\u0026#34;--registry-mirror=https://myregistrydomain.com\u0026#34;] volumeMounts: - name: cert-volume mountPath: /etc/docker/certs.d/myregistrydomain.com/ - name: docker-cli image: \u0026#39;docker:stable\u0026#39; env: - name: DOCKER_HOST value: tcp://127.0.0.1:2375 command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;docker info \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; while [ $? -ne 0 ] ; do sleep 3; docker info \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; done; docker pull library/busybox:latest; docker save -o busybox-latest.tar library/busybox:latest; docker rmi library/busybox:latest; while true; do sleep 86400; done\u0026#34;] volumes: - name: cert-volume configMap: name: registry-ca-cert 这里在pod里跑两个容器，其中一个dockerd，另外一个是使用docker命令的容器，这里注意两点：\n 同样因为使用了私有的registry服务，而且证书是自签名的，dockerd容器要作一些配置 因为两个container共享相同的网络空间，因此直接设置好DOCKER_HOST环境变量，docker-cli里就可以直接用docker命令了，因为不确定两个容器的启动顺序，这里用一段脚本做了个等待的处理docker info \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; while [ $? -ne 0 ] ; do sleep 3; docker info \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; done;  然后用kubectl apply -f docker-test.yaml把这个deployment部署起来，简单检查一下，一切正常。\ndocker in docker的原理 docker in docker的原理还是比较简单的，可以参考wrapdocker源码，其实就是挂载cgroup、tmpfs、securityfs、cgroup的SUBSYS、关掉不需要的文件描述符、最后启动dockerd。wrapdocker源码里注释写得比较清楚。\n用golang语言操作dockerd 运维时用docker命令来操作dockerd还是比较好的，但有时希望以编程的方式操作dockerd，这时docker的sdk就派上用场了，可以参考官方文档和示例。\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/docker/docker/client\u0026#34; \u0026#34;github.com/docker/docker/api/types\u0026#34; \u0026#34;github.com/docker/docker/api/types/container\u0026#34; \u0026#34;github.com/docker/docker/pkg/stdcopy\u0026#34; ) func main() { ctx := context.Background() cli, err := client.NewClientWithOpts(client.FromEnv) if err != nil { panic(err) } cli.NegotiateAPIVersion(ctx) reader, err := cli.ImagePull(ctx, \u0026#34;docker.io/library/alpine\u0026#34;, types.ImagePullOptions{}) if err != nil { panic(err) } io.Copy(os.Stdout, reader) // sleep for a while before using pulled new image  time.Sleep(2 * time.Second) resp, err := cli.ContainerCreate(ctx, \u0026amp;container.Config{ Image: \u0026#34;alpine\u0026#34;, Cmd: []string{\u0026#34;echo\u0026#34;, \u0026#34;hello world\u0026#34;}, }, nil, nil, \u0026#34;\u0026#34;) if err != nil { panic(err) } if err := cli.ContainerStart(ctx, resp.ID, types.ContainerStartOptions{}); err != nil { panic(err) } statusCh, errCh := cli.ContainerWait(ctx, resp.ID, container.WaitConditionNotRunning) select { case err := \u0026lt;-errCh: if err != nil { panic(err) } case \u0026lt;-statusCh: } out, err := cli.ContainerLogs(ctx, resp.ID, types.ContainerLogsOptions{ShowStdout: true}) if err != nil { panic(err) } stdcopy.StdCopy(os.Stdout, os.Stderr, out) } 这里要注意，刚pull下来的镜像要稍微等一下才可以使用，否则会报错，开发中踩到这个坑了，花了些时间才搞清楚原来是这个原因。\n参考  https://hub.docker.com/_/docker/ https://docs.docker.com/registry/insecure/#use-self-signed-certificates https://github.com/jpetazzo/dind https://www.docker-cn.com/registry-mirror https://docs.docker.com/develop/sdk/ https://docs.docker.com/develop/sdk/examples/  ","permalink":"https://jeremyxu2010.github.io/2019/02/%E4%BD%BF%E7%94%A8docker-in-docker/","tags":["docker","golang"],"title":"使用docker in docker"},{"categories":["容器编排"],"contents":"今天在工作遇到一个docker日志丢失的问题，最终一步步查找到原因了，这里记录一下过程。\n问题 工作中把自己写的应用部署到kubernetes中后，用kubectl -n some-ns logs some-pod -c one-container命令查看不了该pod中container的日志，而用该命令去查看其它pod的日志，是可以正常显示的。\n跟踪原因 首先怀疑是应用本身的程序问题，将应用在本机运行了一下，是可以正常输出日志到标准输出的。\n接下来怀疑是kubernetes本身的问题，于是在kubernetes的某个节点上直接用docker命令运行起容器：\ndocker run --name some-name -d some-image docker logs some-name # 这里还是没有输出日志 看来跟kubernetes无关，直接用docker run跑容器都没有输出日志啊。\n不过一想应该docker本身应该不会存在这么大的问题，再做一个实验，同样的镜像，在我本机的docker环境中运行试一下：\ndocker run --name some-name -d some-image docker logs some-name # 这里输出了日志 看来是服务器上的docker环境有些毛病。\n然后看看docker的日志配置：\ncat /etc/sysconfig/docker ... OPTIONS=\u0026#39;--selinux-enabled --log-driver=journald --signature-verification=false --graph=/data/docker\u0026#39; ... docker的配置里是将日志发送到journald的，于是看下docker服务本身的日志：\njournalctl _SYSTEMD_UNIT=docker.service 观察日志，日志发送应该是没有什么问题的。\n接下来看看journald服务的日志：\njournalctl _SYSTEMD_UNIT=systemd-journald.service ... Jan 10 19:18:50 tshift-master systemd-journal[547]: Suppressed 1833 messages from /system.slice/docker.service Jan 10 19:19:20 tshift-master systemd-journal[547]: Suppressed 1849 messages from /system.slice/docker.service Jan 10 19:19:50 tshift-master systemd-journal[547]: Suppressed 1857 messages from /system.slice/docker.service Jan 10 19:20:20 tshift-master systemd-journal[547]: Suppressed 2010 messages from /system.slice/docker.service Jan 10 19:20:50 tshift-master systemd-journal[547]: Suppressed 1820 messages from /system.slice/docker.service ... 这里可以看到有大量的Suppressed xxx messages from /system.slice/docker.service的日志，从字面意思来理解，有很多日志被抑制了。\n为啥会出现这种问题呢？参阅journald.conf的官方文档，可以看到下面这段话：\n man journald.conf ... RateLimitInterval=, RateLimitBurst= Configures the rate limiting that is applied to all messages generated on the system. If, in the time interval defined by RateLimitInterval=, more messages than specified in RateLimitBurst= are logged by a service, all further messages within the interval are dropped until the interval is over. A message about the number of dropped messages is generated. This rate limiting is applied per-service, so that two services which log do not interfere with each other's limits. Defaults to 1000 messages in 30s. The time specification for RateLimitInterval= may be specified in the following units: \u0026quot;s\u0026quot;, \u0026quot;min\u0026quot;, \u0026quot;h\u0026quot;, \u0026quot;ms\u0026quot;, \u0026quot;us\u0026quot;. To turn off any kind of rate limiting, set either value to 0. ...   也就是说docker这个服务输出日志超出了每30秒1000条的速率限制，因此超出部分的日志被journald丢弃了。\n那么docker服务真的输出日志速率这么快吗？我这里用命令计算一下：\njournalctl _SYSTEMD_UNIT=docker.service --since \u0026#34;2019-01-11 23:00:00\u0026#34; --until \u0026#34;2019-01-11 23:05:00\u0026#34; | wc -l 10011 # 10011/(5*60) * 30 = 1000 这样看来，docker服务输出日志的速度果然超出限制了。\n解决办法 既然是docker服务输出日志的速率超限，自然有两种解法：降低docker日志的输出速率、增大journald的速率限制。\n降低docker日志的输出速率 使用以下命令可以看到docker服务输出的日志：\njournalctl _SYSTEMD_UNIT=docker.service --since \u0026#34;2019-01-11 23:00:00\u0026#34; --until \u0026#34;2019-01-11 23:05:00\u0026#34; 从命令的输出来看，基本都是一些容器的标准输出，而刚好出问题的服务器上运行了大量容器，而且有些容器确实在大量输出日志。当然改法自然是调整这些容器的程序，一般来说就是设置更合理的日志级别。\n增大journald的速率限制 如果服务器上运行了大量容器，每个容器输出一些日志，这些日志加起来也很容易超过journald的速率限制。因此还可以直接增大journald的速率限制。\n# 直接修改/etc/systemd/journald.conf，增大RateLimitBurst配置项的取值，修改完毕后重启journald服务 vim /etc/systemd/journald.conf systemctl restart systemd-journald 其它 journald配置文件的优化 在修改journald.conf里，发现其某些配置项并不合理，可以参考这里优化一下。\n增大journald的速率限制的其它办法 查看journald.conf配置文件的文档时，还发现对于较新版本的systemd来说，可以只修改某个服务对应的日志速率限制参数，这样不用修改journald.conf影响全局，可惜这个特性只有较新版本的systemd才有。\n RateLimitIntervalSec=, RateLimitBurst= Configures the rate limiting that is applied to all messages generated on the system. If, in the time interval defined by RateLimitIntervalSec=, more messages than specified in RateLimitBurst= are logged by a service, all further messages within the interval are dropped until the interval is over. A message about the number of dropped messages is generated. This rate limiting is applied per-service, so that two services which log do not interfere with each other's limits. Defaults to 10000 messages in 30s. The time specification for RateLimitIntervalSec= may be specified in the following units: \u0026ldquo;s\u0026rdquo;, \u0026ldquo;min\u0026rdquo;, \u0026ldquo;h\u0026rdquo;, \u0026ldquo;ms\u0026rdquo;, \u0026ldquo;us\u0026rdquo;. To turn off any kind of rate limiting, set either value to 0.\nIf a service provides rate limits for itself through LogRateLimitIntervalSec= and/or LogRateLimitBurst= in systemd.exec(5), those values will override the settings specified here.\n 总结 docker的日志输出逻辑还是比较清晰的，这里就不具体介绍了，参考官方文档就可以了，出了问题冷静一步步分析还是很靠谱的。\n参考  https://docs.docker.com/config/containers/logging/configure/ https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/content/160.html https://docs.lvrui.io/2017/02/19/%E6%9B%B4%E6%94%B9docker%E7%9A%84%E6%97%A5%E5%BF%97%E5%BC%95%E6%93%8E%E4%B8%BA-journald/ https://www.freedesktop.org/software/systemd/man/journald.conf.html https://www.freedesktop.org/software/systemd/man/systemd.exec.html https://bani.com.br/2015/06/systemd-journal-what-does-systemd-journal-suppressed-n-messages-from-system-slice-mean/ https://www.sulabs.net/?p=828 https://www.howtoing.com/how-to-use-journalctl-to-view-and-manipulate-systemd-logs  ","permalink":"https://jeremyxu2010.github.io/2019/01/docker%E6%97%A5%E5%BF%97%E4%B8%A2%E5%A4%B1%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/","tags":["docker"],"title":"docker日志丢失问题分析"},{"categories":["devops"],"contents":"工作中需要自行编译一个Python二进制程序，并尽量减少该程序依赖的库文件，使之在相同CPU架构上有更良好的可移植性。先找了下网上的资料，都不太详尽，经过探索最终还是成功了，这里记录一下过程以备忘。\n过程记录 查阅Python27源码中的setup.py文件，发现Python核心仅依赖glibc，c++等标准库，因此按以下默认的编译命令即可编译出依赖较少的Python二进制程序了。\n# 安装必要的编译工具链 sudo yum install -y gcc make gcc-c++ glibc-static libstdc++-static curl -O https://www.python.org/ftp/python/2.7.13/Python-2.7.13.tgz tar -xf Python-2.7.13.tgz \u0026amp;\u0026amp; cd Python-2.7.13 # 注意这里添加了选项静态链接libgcc和libstdc++ LDFLAGS=\u0026#34;-static-libgcc -static-libstdc++\u0026#34; ./configure --prefix=/usr/local/python27 --with-cxx-main=/usr/bin/g++ make -j4 \u0026gt; make.log make install 我用ldd命令检查下Python二进制程序依赖的库文件：\n[root@centos-linux-7 deps]# ldd /usr/local/python27/bin/python linux-vdso.so.1 =\u0026gt; (0x00007fff78de8000) libpthread.so.0 =\u0026gt; /lib64/libpthread.so.0 (0x00007f8b2253a000) libdl.so.2 =\u0026gt; /lib64/libdl.so.2 (0x00007f8b22336000) libutil.so.1 =\u0026gt; /lib64/libutil.so.1 (0x00007f8b22133000) libm.so.6 =\u0026gt; /lib64/libm.so.6 (0x00007f8b21b2a000) libc.so.6 =\u0026gt; /lib64/libc.so.6 (0x00007f8b21547000) /lib64/ld-linux-x86-64.so.2 (0x00007f8b22756000) 发现依赖的库文件确实都是系统核心库文件，大部分Linux系统上均有这些库文件，因此可以断定将编译好的python程序拷贝到其它Linux系统上是可以执行的。\n但我发现Python程序的执行并不是只使用了python这个二进制程序，在其加载某些python模块是会动态加载该模块对应的动态链接库文件。因此我用ldd命令检查下各python模块的动态库文件的依赖情况：\n[root@centos-linux-7 Python-2.7.13]# find . -name \u0026#39;*.so\u0026#39;|xargs ldd ./build/lib.linux-x86_64-2.7/_socket.so: linux-vdso.so.1 =\u0026gt; (0x00007ffdba579000) libm.so.6 =\u0026gt; /lib64/libm.so.6 (0x00007ff0d8ded000) libpthread.so.0 =\u0026gt; /lib64/libpthread.so.0 (0x00007ff0d8bd1000) libc.so.6 =\u0026gt; /lib64/libc.so.6 (0x00007ff0d8804000) /lib64/ld-linux-x86-64.so.2 (0x00007ff0d9304000) ...... ./build/lib.linux-x86_64-2.7/_curses.so: linux-vdso.so.1 =\u0026gt; (0x00007ffd61969000) libpthread.so.0 =\u0026gt; /lib64/libpthread.so.0 (0x00007f6a52b86000) libc.so.6 =\u0026gt; /lib64/libc.so.6 (0x00007f6a527b9000) /lib64/ld-linux-x86-64.so.2 (0x00007f6a52fe5000) 这么一看绝大部分python模块的动态库文件也是仅依赖系统核心库文件，一切都挺美好！\n但在我的场景里，python编译时还需要启用ssl、hashlib、readline等模块，而这些模块编译时会依赖系统非核心库文件，我分析Python源代码目录下的setup.py文件，发现依赖关系如下：\n ssl、hashlib依赖于libssl、libcrypto，而libssl、libcrypto又依赖libz。 readline依赖libreadline、libncurses。  于是这里先编译安装这些非核心库文件：\n# 注意由于这些库文件后面都需要链接进python模块对应的动态库文件，所以下面编译的非核心库均要使用-fPIC选项，并且都只编译出静态库文件 mkdir -p deps/src cd deps/src curl -O https://zlib.net/zlib-1.2.11.tar.gz tar -xf zlib-1.2.11.tar.gz \u0026amp;\u0026amp; cd zlib-1.2.11 CFLAGS=\u0026#39;-fPIC\u0026#39; ./configure --prefix=`pwd`/../../zlib --static make -j4 \u0026amp;\u0026amp; make install cd .. curl -O https://www.openssl.org/source/openssl-1.0.2q.tar.gz tar -xf openssl-1.0.2q.tar.gz \u0026amp;\u0026amp; cd openssl-1.0.2q ./Configure zlib --prefix=`pwd`/../../ssl --openssldir=`pwd`/../../ssl linux-x86_64 --with-zlib-lib=`pwd`/../../zlib/lib --with-zlib-include=`pwd`/../../zlib/include -fPIC make -j4 \u0026amp;\u0026amp; make install cd .. curl -O http://ftp.ntu.edu.tw/gnu/ncurses/ncurses-5.9.tar.gz tar -xf ncurses-5.9.tar.gz \u0026amp;\u0026amp; cd ncurses-5.9 CPPFLAGS=\u0026#34;-fPIC\u0026#34; ./configure --prefix=`pwd`/../../ncurses make -j4 \u0026amp;\u0026amp; make install cd .. curl -O http://ftp.ntu.edu.tw/gnu/readline/readline-6.2.tar.gz tar -xf readline-6.2.tar.gz \u0026amp;\u0026amp; cd readline-6.2 CPPFLAGS=\u0026#34;-fPIC\u0026#34; ./configure --prefix=`pwd`/../../readline --disable-shared make -j4 \u0026amp;\u0026amp; make install cd .. cd ../.. 最后重新编译Python：\n# 注意这里添加了选项静态链接libgcc和libstdc++，还指定了一些头文件目录及库文件目录 CPPFLAGS=\u0026#34;-I`pwd`/deps/zlib/include -I`pwd`/deps/ssl/include -I`pwd`/deps/readline/include -I`pwd`/deps/ncurses/include -I`pwd`/deps/ncurses/include/ncurses\u0026#34; LDFLAGS=\u0026#34;-static-libgcc -static-libstdc++ -L`pwd`/deps/zlib/lib -L`pwd`/deps/ssl/lib -L`pwd`/deps/readline/lib -L`pwd`/deps/ncurses/lib\u0026#34; ./configure --prefix=/usr/local/python27 --with-cxx-main=/usr/bin/g++ make -j4 \u0026amp;\u0026amp; make install 最后检查下编译出的python二进制程序文件及各模块的动态库文件，发现仅依赖系统核心库文件，效果很好：\n[root@centos-linux-7 python27]# ldd /usr/local/python27/bin/python linux-vdso.so.1 =\u0026gt; (0x00007ffc54fe8000) libpthread.so.0 =\u0026gt; /lib64/libpthread.so.0 (0x00007f9774b3d000) libdl.so.2 =\u0026gt; /lib64/libdl.so.2 (0x00007f9774939000) libutil.so.1 =\u0026gt; /lib64/libutil.so.1 (0x00007f9774736000) libm.so.6 =\u0026gt; /lib64/libm.so.6 (0x00007f9774434000) libc.so.6 =\u0026gt; /lib64/libc.so.6 (0x00007f9774067000) /lib64/ld-linux-x86-64.so.2 (0x00007f9774d59000) [root@centos-linux-7 python27]# find /usr/local/python27 -name \u0026#39;*.so\u0026#39;|xargs ldd /usr/local/python27/lib/python2.7/lib-dynload/nis.so: linux-vdso.so.1 =\u0026gt; (0x00007fff42b14000) libnsl.so.1 =\u0026gt; /lib64/libnsl.so.1 (0x00007fbc2ad4f000) libpthread.so.0 =\u0026gt; /lib64/libpthread.so.0 (0x00007fbc2ab33000) libc.so.6 =\u0026gt; /lib64/libc.so.6 (0x00007fbc2a766000) /lib64/ld-linux-x86-64.so.2 (0x00007fbc2b16d000) ...... /usr/local/python27/lib/python2.7/lib-dynload/_codecs_cn.so: linux-vdso.so.1 =\u0026gt; (0x00007fff695db000) libpthread.so.0 =\u0026gt; /lib64/libpthread.so.0 (0x00007f421bd72000) libc.so.6 =\u0026gt; /lib64/libc.so.6 (0x00007f421b9a5000) /lib64/ld-linux-x86-64.so.2 (0x00007f421c1b2000) 安装其它第三方python模块的方法与上面的过程很类似了，这里就不赘述了。\n总结 手工编译一个python不难，但如果要尽可能少地依赖系统中库文件，这时要考虑的问题就比较多了。通过本次探索，对GCC的编译过程有了更深刻的认识，也基本掌握了CFLAGS、CPPFLAGS、LDFLAGS等常见编译参数的用法。\n参考  https://devguide.python.org/setup/#unix https://stackoverflow.com/questions/4156055/static-linking-only-some-libraries https://www.cnblogs.com/taskiller/archive/2012/12/14/2817650.html https://www.oschina.net/question/994701_105246 https://blog.csdn.net/haibosdu/article/details/77094833  ","permalink":"https://jeremyxu2010.github.io/2018/12/%E7%BC%96%E8%AF%91%E8%87%AA%E5%AE%9A%E4%B9%89python%E4%BA%8C%E8%BF%9B%E5%88%B6%E7%A8%8B%E5%BA%8F/","tags":["python","gcc","make"],"title":"编译自定义Python二进制程序"},{"categories":["devops"],"contents":"最近工作中经常需要ssh登录到某台跳板机，再连接受限网络环境中的某台服务器。以前经常用SSH端口转发这一功能，但周围的同事好像对这个并不清楚，这里记录一下以备其它同事询问。\nSSH一共提供了 3 种端口转发，分别是本地转发（-L参数）、远程转发（-R参数）和动态转发（-D参数）。接下来我就一一介绍这几种不同的转发方式的使用。\n一些术语和约定 既然提到转发，就应该明白：这是三台主机之间要合作干的事。不然为何不两台主机直连，而要通过第三者转发？\n本地主机：形式为IP或域名，你当前正在使用的这台机器；\n远程主机：形式与本地主机一样。这里的远程并不是指实际的距离有多远，准确地说是另一台；\n本地转发 本地转发，顾名思义就是把本地主机端口通过待登录主机端口转发到远程主机端口上去。\n本地转发通过参数 -L 指定，格式：-L [本地主机:]本地主机端口:远程网络主机:远程网络主机端口。加上ssh待登录主机，这里就有了三台主机。\n举例：ssh -L 0.0.0.0:50000:host2:80 user@host1。这条命令将host2的80端口映射到本地的50000端口，前提是待登录主机host1上可以正常连接到host2的80端口。\n畅想一下这个功能的作用：\n 因为本地的mysql更顺手，想用本地的mysql客户端命令连接受限网络环境的mysql服务端。 本地安装了开发工具，想用这个开发工具连接受限网络环境中某个服务的远程调试端口。 \u0026hellip;\u0026hellip;  远程转发 远程转发是指把登录主机所在网络中某个端口通过本地主机端口转发到远程主机上。\n远程转发通过参数 -R 指定，格式：-R [登录主机:]登录主机端口:本地网络主机:本地网络主机端口。\n举例：ssh -R 0.0.0.0:8080:host2:80 user@host1。这条命令将host2的80端口映射到待登录主机host1的8080端口，前提是本地主机可以正常连接host2的80端口。\n畅想一下这个功能的作用：\n 本地网络中有一个http代理，通过这个代理可以上外网，因此通过这条命令将这个http代理映射到待登录主机的某个端口，这样受限网络环境中所有其它服务器即可使用这个http代理上外网了。 在本机开发了一个web应用，想拿给别人测试，但现在你却处在内网，外网是无法直接访问内网的主机的，怎么办！？很多人可能会说，找台有公网IP的主机，重新部署一下就行了。这样可行，但太麻烦。然而自从你了解了ssh的远程转发之后，一切都变得简单了。只需在本地主机上执行一下上面例子的命令即可实现外网访问内网的web应用。  注意：\n sshd_config里要打开AllowTcpForwarding选项，否则-R远程端口转发会失败。 默认转发到远程主机上的端口绑定的是127.0.0.1，如要绑定0.0.0.0需要打开sshd_config里的GatewayPorts选项。这个选项如果由于权限没法打开也有办法，可配合ssh -L将端口绑定到0.0.0.0，聪明的你应该能想到办法，呵呵。  动态转发 相对于本地转发和远程转发的单一端口转发模式而言，动态转发有点更加强劲的端口转发功能，即是无需固定指定被访问目标主机的端口号。这个端口号需要在本地通过协议指定，该协议就是简单、安全、实用的 SOCKS 协议。\n动态转发通过参数 -D 指定，格式：-D [本地主机:]本地主机端口。相对于前两个来说，动态转发无需再指定远程主机及其端口。它们由通过 SOCKS协议 连接到本地主机端口的那个主机。\n举例：ssh -D 50000 user@host1。这条命令创建了一个SOCKS代理，所以通过该SOCKS代理发出的数据包将经过host1转发出去。\n怎么使用？\n  用firefox浏览器，在浏览器里设置使用socks5代理127.0.0.1:50000，然后浏览器就可以访问host1所在网络内的任何IP了。\n  如果是普通命令行应用，使用proxychains-ng，参考命令如下：\nbrew install proxychains-ng vim /usr/local/etc/proxychains.conf # 在ProxyList配置段下添加配置 \u0026#34;socks5 127.0.0.1 50000\u0026#34; proxychains-ng wget http://host2 # 在其它命令行前添加proxychains-ng即可   如果是ssh，则用以下命令使用socks5代理：\nssh -o ProxyCommand=\u0026#39;/usr/bin/nc -X 5 -x 127.0.0.1:5000 %h %p\u0026#39; user@host2   畅想一下这个功能的作用：\n 想访问受限网络环境中的多种服务 FQ \u0026hellip;\u0026hellip;  SSH Over HTTP Tunnel 有一些HTTP代理服务器支持HTTP Tunnel，目前HTTP Tunnel的主要作用是辅助代理HTTPS请求，详情参见这里。今天在工作中竟发现有同事通过HTTP Tunnel连接ssh过去，猛然想起来HTTP Tunnel的原理里并没有限制连接的目标服务一定是HTTP或HTTPS服务，貌似只要是基于TCP的服务都可以。macOS下ssh走CONNECT tunnel稍微麻烦一点，参考命令如一下：\nbrew install corkscrew ssh -o ProxyCommand=\u0026#39;/usr/local/bin/corkscrew http_proxy_host http_proxy_port %h %p\u0026#39; user@host2 总结 都是些小技巧，不过掌握了作用还是挺大的，可以大大提高生产力的。\n","permalink":"https://jeremyxu2010.github.io/2018/12/ssh%E7%9A%84%E4%B8%89%E7%A7%8D%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91/","tags":["ssh","port-forward"],"title":"SSH的三种端口转发"},{"categories":["devops"],"contents":"自从中兴事件后，国家开始在政策上大力支持国产硬软件，甚至在某些项目投标要求上都加上了隐性条件，软件系统必须能在国产硬软件基础上运行。而国产硬软件一般也就是代指arm64硬件架构及在此基础上的linux定制发行版，最近一周刚好完成了一些支持arm64硬件架构的工作，这里总结一下。\narm64的软件源 国产操作系统一般基于比较成熟的ubuntu或centos，算是这些个发行版的arm64衍生版，所以操作上跟x86上的ubuntu或centos差不多，可能唯一区别是软件源有些不同。\n一些常用的arm64软件源地址如下：\ncentos的arm64 yum源地址是：https://mirrors.aliyun.com/centos-altarch/\nubuntu的arm64 apt源地址是： https://mirrors.aliyun.com/ubuntu-ports/\nepel的arm64 yum源地址是：https://mirrors.aliyun.com/epel\nyum源、apt源的配置方法参考网上的文档就可以了。\n其实很多常用软件都有arm64的软件源，看看https://opsx.alibaba.com/mirror，软件源里有aarch64之类的目录，就是支持arm64硬件架构的软件源。\nk8s支持arm64架构 其实k8s要支持arm64还算是比较简单，由于Go语言里进行跨平台交叉编译很简单，所以k8s核心的一些二进制文件及docker镜像均有arm64架构的，将正常部署的k8s集群中这些二进制文件都替换成arm64架构的，k8s也就可以在arm64上正常运行了。比如：\netcd：https://github.com/etcd-io/etcd/releases（二进制文件名中带有aarch64的就是arm64架构的二进制文件）\nkubernetes: https://kubernetes.io/docs/setup/release/notes/#client-binaries, https://kubernetes.io/docs/setup/release/notes/#server-binaries, https://kubernetes.io/docs/setup/release/notes/#node-binaries（二进制文件名中带有arm64的就是arm64架构的二进制文件）\ndocker： https://mirrors.aliyun.com/docker-ce/linux/（centos, ubuntu都有对应的docker arm64软件源）\ndefault cni plugin(flannel): https://github.com/containernetworking/cni/releases，https://github.com/coreos/flannel/releases(二进制文件名中带有arm64的就是arm64架构的二进制文件)\ncalico：https://github.com/projectcalico/cni-plugin/releases\npause镜像：gcr.io/google_containers/pause-arm64\nmetrics-server镜像：gcr.io/google_containers/metrics-server-arm64\ncoredns镜像：coredns/coredns:coredns-arm64\nkubernetes-dashboard镜像：gcr.io/google_containers/kubernetes-dashboard-arm64\nflannel镜像：gcr.io/google_containers/flannel-arm64\nkube-state-metrics镜像：gcr.io/google_containers/kube-state-metrics-arm64\n其它一些arm64镜像可参考这里：https://hub.docker.com/u/googlecontainer/，https://hub.docker.com/r/arm64v8/。\nc++程序支持arm64架构 系统中还有一些c++写的程序，需要在arm64架构的服务器上重新编译一下，编译方法也比较简单，就是用如下这些命令：\nsudo apt-get install xxxx-dev # 安装某些依赖库的开发包 cd $cpp_prog_dir ./configure \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install # 重新编译c++程序 在编译c++程序的过程中还接触了一个开源的构建系统blade，研究了下，发现功能还挺强大的，不过文档太简略了点，很多功能要拿示例与文档对照看才能想明白，下面将使用过程中一些要点记录一下。\nblade安装 很奇怪官方文档连怎么安装都没详细说明\u0026hellip;\nbrew install scons #安装scons git clone https://github.com/chen3feng/typhoon-blade.git cd typhoon-blade bash ./install source ~/.profile # source ~/.zshrc 一个c++构建项目 mkdir -p ~/workspace/proj1 cd ~/workspace/proj1 touch BLADE_ROOT # 必须在项目根目录创建一个BLADE_ROOT文件 vim module1/test.cpp # 编写一个简单的c++文件 创建该模块的编译文件\nvim module1/BUILD cc_binary( name=\u0026#39;module1\u0026#39;, srcs=[\u0026#39;./test.cpp\u0026#39;], deps=[\u0026#39;#pthread\u0026#39;] # 该c++程序运行时需要动态链接pthread ) blade build module1 # 编译module1 编译文件的书写方法参见这里，比较简单，只有deps的配置特殊一点：\ndeps的允许的格式：\n \u0026ldquo;//path/to/dir/:name\u0026rdquo; 其他目录下的target，path为从BLADE_ROOT出发的路径，name为被依赖的目标名。看见就知道在哪里。 \u0026ldquo;:name\u0026rdquo; 当前BUILD文件内的target， path可以省略。 \u0026ldquo;#pthread\u0026rdquo; 系统库。直接写#跟名字即可。  当自己的多个模块间存在依赖时，按照上述规则书写deps配置即可，blade自己会分析依赖关系，如下：\nvim module1/BUILD cc_binary( name=\u0026#39;module1\u0026#39;, srcs=[\u0026#39;./test.cpp\u0026#39;], deps=[\u0026#39;#pthread\u0026#39;, \u0026#39;/module2:module2\u0026#39;] # 该c++程序编译时会链接module2, 同时动态链接系统中的pthread库 ) vim module2/BUILD cc_binary( name=\u0026#39;module2\u0026#39;, srcs=[\u0026#39;./mod2.cpp\u0026#39;] ) blade build module1 # 编译module1 静态链接系统库 有时候希望编译出的二进制程序尽量少依赖系统中的动态链接库，这样可以保证编出的二进制有更好的可移植性，不会由于部署的目标系统上没有某个动态链接库导致程序执行失败，这时可以使用prebuilt特性。这个在官方文档中并没有详实的例子说明，只有文档中一句话带过。\n prebuilt=True 主要应用在thirdparty中从rpm包解来的库，使用这个参数表示不从源码构建。对应的二进制文件必须存在 lib{32,64}_{release,debug} 这样的子目录中。不区分debug/release时可以只有两个实际的目录。\n 用法如下：\nvim module1/BUILD cc_binary( name=\u0026#39;module1\u0026#39;, srcs=[\u0026#39;./test.cpp\u0026#39;], deps=[\u0026#39;/gflags:gflags\u0026#39;] # 该c++程序编译时会链接gflags ) vim gflags/BUILD cc_library( name = \u0026#39;gflags\u0026#39;, prebuilt = True ) mkdir gflags/{lib64_release, lib64_debug} # 在这两个目录中均放入从其实地方得到的gflags静态库文件 blade build module1 # 编译module1 编译出来的二进制文件可用otool -L或ldd命令查看其依赖的动态链接库等信息。\n除了c++代码，blade还可以编译protobuf、thrift、java、scala、python等，只需要写不同的编译文件即可，可参考这里。\n通过修改blade的配置文件可调整编译过程的一些参数，可参考这里。\n总结 整个arm64硬件架构支持的调整工作并不是太难，不过在编译c++程序时还是遇到了一些困难，这时才发现这一块过度依赖公司内部框架及编译工具，开发人员并没有深入理解框架及编译工具的实现原理，当发现要为其它平台做一些适配工作时，顿时处于无法掌控的境地，很是被动。这些当初不考虑实现原理，用得爽不知其所以然的模块都属于迟早要攻克的技术债务啊。\n参考  https://opsx.alibaba.com/mirror?lang=zh-CN https://kubernetes.io/docs/setup/ https://github.com/chen3feng/typhoon-blade/tree/master/doc https://github.com/chen3feng/typhoon-blade/tree/master/example https://www.jianshu.com/p/e3fd94617fb3  ","permalink":"https://jeremyxu2010.github.io/2018/12/arm64%E7%A1%AC%E4%BB%B6%E6%9E%B6%E6%9E%84%E6%94%AF%E6%8C%81%E6%80%BB%E7%BB%93/","tags":["docker","k8s","cpp","blade"],"title":"arm64硬件架构支持总结"},{"categories":["容器编排"],"contents":"最近在做k8s相关的开发工作，涉及不少k8s的相关知识，这里记录下。\n问题引出 遇到一个需求，要使用prometheus监控多个k8s集群。\n调研发现prometheus配合node_exporter、kube-state-metrics可以很方便地采集单个集群的监控指标。因此最初的构想是在每套k8s集群里部署prometheus，由它采集该集群的监控指标，再运用prometheus的联邦模式将多个prometheus中的监控数据聚合采集到一个中心prometheus里来，参考模型为Hierarchical federation。\n但甲方觉得上述方案中每个k8s集群都要部署prometheus，增加了每套k8s集群的资源开销，希望全局只部署一套prometheus，由它统一采集多个k8s集群的监控指标。尽管个人不太认可这种方案，中心prometheus今后很有可能成为性能瓶颈，但甲方要求的总得尽力满足，下面开始研究如何用一个prometheus采集多个k8s集群的监控指标。\nprometheus采集当前k8s监控数据 首先分析prometheus是如何采集单个k8s集群的监控指标。在k8s集群里用helm部署一套prometheus还是很简单的，命令如下：\n# 假设就部署在default命名空间 helm install --name prometheus --namespace default stable/prometheus 部署完毕之后，由于默认并没有创造任何ingress资源对象，创建的service的类型也仅仅是ClusterIP，所以从集群外部是没法访问到它的，不过可以简单地将端口映射出来，如下：\nkubectl -n default port-forward service/prometheus-server 30080:80 这里使用了kubectl port-forward命令，详细使用方法参考这里。\n然后用浏览器访问http://127.0.0.1:30080/graph，就可访问到prometheus的WebConsole了。\n访问http://127.0.0.1:30080/config可以看到当前prometheus的配置，其中抓取当前k8s集群监控指标的配置如下：\nscrape_configs: # 抓取当前prometheus的监控指标 - job_name: prometheus scrape_interval: 1m scrape_timeout: 10s metrics_path: /metrics scheme: http static_configs: - targets: - localhost:9090 # 通过kubernetes_sd_configs发现机制，通过apiserver的接口列出当前k8s集群中endpoints列表，匹配到apiserver的endpoint，从该endpoint抓取apiserver的监控指标 - job_name: kubernetes-apiservers scrape_interval: 1m scrape_timeout: 10s metrics_path: /metrics scheme: https kubernetes_sd_configs: - role: endpoints bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] separator: ; regex: default;kubernetes;https replacement: $1 action: keep # 通过kubernetes_sd_configs发现机制，通过apiserver的接口列出当前k8s集群中node列表，从node列表中每个node抓取node的监控指标(kubelet通过/metrics接口将node的监控指标export出来了) - job_name: kubernetes-nodes scrape_interval: 1m scrape_timeout: 10s metrics_path: /metrics scheme: https kubernetes_sd_configs: - role: node bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true relabel_configs: - separator: ; regex: __meta_kubernetes_node_label_(.+) replacement: $1 action: labelmap - separator: ; regex: (.*) target_label: __address__ replacement: kubernetes.default.svc:443 action: replace - source_labels: [__meta_kubernetes_node_name] separator: ; regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics action: replace # 通过kubernetes_sd_configs发现机制，通过apiserver的接口列出当前k8s集群中node列表，从node列表中每个node抓取cadvisor的监控指标(kubelet通过/metrics/cadvisor接口将cadvisor的监控指标export出来了) - job_name: kubernetes-nodes-cadvisor scrape_interval: 1m scrape_timeout: 10s metrics_path: /metrics scheme: https kubernetes_sd_configs: - role: node bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true relabel_configs: - separator: ; regex: __meta_kubernetes_node_label_(.+) replacement: $1 action: labelmap - separator: ; regex: (.*) target_label: __address__ replacement: kubernetes.default.svc:443 action: replace - source_labels: [__meta_kubernetes_node_name] separator: ; regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor action: replace # 通过kubernetes_sd_configs发现机制，通过apiserver的接口列出当前k8s集群中endpoints列表，匹配到打了prometheus_io_scrape: true annotation的endpoint，从匹配到的endpoint列表中每个endpoint抓取该endpoint暴露的监控指标 - job_name: kubernetes-service-endpoints scrape_interval: 1m scrape_timeout: 10s metrics_path: /metrics scheme: http kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] separator: ; regex: \u0026#34;true\u0026#34; replacement: $1 action: keep - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] separator: ; regex: (https?) target_label: __scheme__ replacement: $1 action: replace - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] separator: ; regex: (.+) target_label: __metrics_path__ replacement: $1 action: replace - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] separator: ; regex: ([^:]+)(?::\\d+)?;(\\d+) target_label: __address__ replacement: $1:$2 action: replace - separator: ; regex: __meta_kubernetes_service_label_(.+) replacement: $1 action: labelmap - source_labels: [__meta_kubernetes_namespace] separator: ; regex: (.*) target_label: kubernetes_namespace replacement: $1 action: replace - source_labels: [__meta_kubernetes_service_name] separator: ; regex: (.*) target_label: kubernetes_name replacement: $1 action: replace # 通过kubernetes_sd_configs发现机制，通过apiserver的接口列出当前k8s集群中service列表，匹配到打了prometheus_io_scrape: pushgateway annotation的service，从匹配到的service列表中每个service抓取该service暴露的监控指标 - job_name: prometheus-pushgateway honor_labels: true scrape_interval: 1m scrape_timeout: 10s metrics_path: /metrics scheme: http kubernetes_sd_configs: - role: service relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] separator: ; regex: pushgateway replacement: $1 action: keep # 通过kubernetes_sd_configs发现机制，通过apiserver的接口列出当前k8s集群中service列表，匹配到打了prometheus_io_scrape: true annotation的service，从匹配到的service列表中每个service抓取该service暴露的监控指标(这里通过blackbox这个服务来抓取，需要在prometheus所在的namespace部署blackbox服务) - job_name: kubernetes-services params: module: - http_2xx scrape_interval: 1m scrape_timeout: 10s metrics_path: /probe scheme: http kubernetes_sd_configs: - role: service relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] separator: ; regex: \u0026#34;true\u0026#34; replacement: $1 action: keep - source_labels: [__address__] separator: ; regex: (.*) target_label: __param_target replacement: $1 action: replace - separator: ; regex: (.*) target_label: __address__ replacement: blackbox action: replace - source_labels: [__param_target] separator: ; regex: (.*) target_label: instance replacement: $1 action: replace - separator: ; regex: __meta_kubernetes_service_label_(.+) replacement: $1 action: labelmap - source_labels: [__meta_kubernetes_namespace] separator: ; regex: (.*) target_label: kubernetes_namespace replacement: $1 action: replace - source_labels: [__meta_kubernetes_service_name] separator: ; regex: (.*) target_label: kubernetes_name replacement: $1 action: replace # 通过kubernetes_sd_configs发现机制，通过apiserver的接口列出当前k8s集群中pod列表，匹配到打了prometheus_io_scrape: true annotation的pod，从匹配到的pod列表中每个pod抓取该pod暴露的监控指标 - job_name: kubernetes-pods scrape_interval: 1m scrape_timeout: 10s metrics_path: /metrics scheme: http kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] separator: ; regex: \u0026#34;true\u0026#34; replacement: $1 action: keep - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] separator: ; regex: (.+) target_label: __metrics_path__ replacement: $1 action: replace - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] separator: ; regex: ([^:]+)(?::\\d+)?;(\\d+) target_label: __address__ replacement: $1:$2 action: replace - separator: ; regex: __meta_kubernetes_pod_label_(.+) replacement: $1 action: labelmap - source_labels: [__meta_kubernetes_namespace] separator: ; regex: (.*) target_label: kubernetes_namespace replacement: $1 action: replace - source_labels: [__meta_kubernetes_pod_name] separator: ; regex: (.*) target_label: kubernetes_pod_name replacement: $1 action: replace 这段配置好复杂，最开始看的时候也是晕晕的，不过慢慢研究终于还是看懂意思了。这里选取其中一小段详细解释一下：\n# 通过kubernetes_sd_configs发现机制，通过apiserver的接口列出当前k8s集群中endpoints列表，匹配到打了prometheus_io_scrape: true annotation的endpoint，从匹配到的endpoint列表中每个endpoint抓取该endpoint暴露的监控指标 - job_name: kubernetes-service-endpoints scrape_interval: 1m scrape_timeout: 10s metrics_path: /metrics scheme: http kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] separator: ; regex: \u0026#34;true\u0026#34; replacement: $1 action: keep - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] separator: ; regex: (https?) target_label: __scheme__ replacement: $1 action: replace - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] separator: ; regex: (.+) target_label: __metrics_path__ replacement: $1 action: replace - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] separator: ; regex: ([^:]+)(?::\\d+)?;(\\d+) target_label: __address__ replacement: $1:$2 action: replace - separator: ; regex: __meta_kubernetes_service_label_(.+) replacement: $1 action: labelmap - source_labels: [__meta_kubernetes_namespace] separator: ; regex: (.*) target_label: kubernetes_namespace replacement: $1 action: replace - source_labels: [__meta_kubernetes_service_name] separator: ; regex: (.*) target_label: kubernetes_name replacement: $1 action: replace 首先是一小段prometheus的抓取配置，官方解释在这里，这个比较简单，就不具体解释了\n- job_name: kubernetes-service-endpoints scrape_interval: 1m scrape_timeout: 10s metrics_path: /metrics scheme: http 然后说明是用k8s的发现机制去发现抓取地址的：\nkubernetes_sd_configs: - role: endpoints prometheus里k8s的发现机制配置见这里，注意这里没填api_server属性，因此用了默认值kubernetes.default.svc。\n然后是一段relabel_configs配置，其作用主要是用于匹配最终要抓取的endpoint，构造抓取的地址，甚至给最终的时序指标加上一些label。这里以apiserver发现的node_exporter endpoint信息为例，在没有relabel之前，其endpoint信息如下：\n__address__=\u0026quot;192.168.65.3:9100\u0026quot; __meta_kubernetes_endpoint_address_target_kind=\u0026quot;Pod\u0026quot; __meta_kubernetes_endpoint_address_target_name=\u0026quot;prometheus-node-exporter-89tcq\u0026quot; __meta_kubernetes_endpoint_port_name=\u0026quot;metrics\u0026quot; __meta_kubernetes_endpoint_port_protocol=\u0026quot;TCP\u0026quot; __meta_kubernetes_endpoint_ready=\u0026quot;true\u0026quot; __meta_kubernetes_endpoints_name=\u0026quot;prometheus-node-exporter\u0026quot; __meta_kubernetes_namespace=\u0026quot;default\u0026quot; __meta_kubernetes_pod_container_name=\u0026quot;prometheus-node-exporter\u0026quot; __meta_kubernetes_pod_container_port_name=\u0026quot;metrics\u0026quot; __meta_kubernetes_pod_container_port_number=\u0026quot;9100\u0026quot; __meta_kubernetes_pod_container_port_protocol=\u0026quot;TCP\u0026quot; __meta_kubernetes_pod_controller_kind=\u0026quot;DaemonSet\u0026quot; __meta_kubernetes_pod_controller_name=\u0026quot;prometheus-node-exporter\u0026quot; __meta_kubernetes_pod_host_ip=\u0026quot;192.168.65.3\u0026quot; __meta_kubernetes_pod_ip=\u0026quot;192.168.65.3\u0026quot; __meta_kubernetes_pod_label_app=\u0026quot;prometheus\u0026quot; __meta_kubernetes_pod_label_component=\u0026quot;node-exporter\u0026quot; __meta_kubernetes_pod_label_controller_revision_hash=\u0026quot;1203132889\u0026quot; __meta_kubernetes_pod_label_pod_template_generation=\u0026quot;1\u0026quot; __meta_kubernetes_pod_label_release=\u0026quot;prometheus\u0026quot; __meta_kubernetes_pod_name=\u0026quot;prometheus-node-exporter-89tcq\u0026quot; __meta_kubernetes_pod_node_name=\u0026quot;docker-for-desktop\u0026quot; __meta_kubernetes_pod_ready=\u0026quot;true\u0026quot; __meta_kubernetes_pod_uid=\u0026quot;62b53ba7-eae3-11e8-b0a6-025000000001\u0026quot; __meta_kubernetes_service_annotation_prometheus_io_scrape=\u0026quot;true\u0026quot; __meta_kubernetes_service_label_app=\u0026quot;prometheus\u0026quot; __meta_kubernetes_service_label_chart=\u0026quot;prometheus-7.4.1\u0026quot; __meta_kubernetes_service_label_component=\u0026quot;node-exporter\u0026quot; __meta_kubernetes_service_label_heritage=\u0026quot;Tiller\u0026quot; __meta_kubernetes_service_label_release=\u0026quot;prometheus\u0026quot; __meta_kubernetes_service_name=\u0026quot;prometheus-node-exporter\u0026quot; __metrics_path__=\u0026quot;/metrics\u0026quot; __scheme__=\u0026quot;http\u0026quot; job=\u0026quot;kubernetes-service-endpoints\u0026quot; 经过下面的relabel规则后：\nrelabel_configs: # 只匹配__meta_kubernetes_service_annotation_prometheus_io_scrape=true的endpoint - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] separator: ; regex: \u0026#34;true\u0026#34; replacement: $1 action: keep # 如果有__meta_kubernetes_service_annotation_prometheus_io_scheme，且正则匹配(https?)，则将__scheme__修改为__meta_kubernetes_service_annotation_prometheus_io_scheme指定的值 - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] separator: ; regex: (https?) target_label: __scheme__ replacement: $1 action: replace # 如果有__meta_kubernetes_service_annotation_prometheus_io_path，且正则匹配(.+)，则将__metrics_path__修改为__meta_kubernetes_service_annotation_prometheus_io_path指定的值 - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] separator: ; regex: (.+) target_label: __metrics_path__ replacement: $1 action: replace # 如果有__address__;__meta_kubernetes_service_annotation_prometheus_io_port，且正则匹配([^:]+)(?::\\d+)?;(\\d+)，则将__address__修改为$1:$2指定的值 - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] separator: ; regex: ([^:]+)(?::\\d+)?;(\\d+) target_label: __address__ replacement: $1:$2 action: replace # 如果label名称正则匹配__meta_kubernetes_service_label_(.+)，则设置相应的label - separator: ; regex: __meta_kubernetes_service_label_(.+) replacement: $1 action: labelmap # 设置kubernetes_namespace为__meta_kubernetes_namespace指定的值 - source_labels: [__meta_kubernetes_namespace] separator: ; regex: (.*) target_label: kubernetes_namespace replacement: $1 action: replace # 设置kubernetes_name为__meta_kubernetes_service_name指定的值 - source_labels: [__meta_kubernetes_service_name] separator: ; regex: (.*) target_label: kubernetes_name replacement: $1 action: replace 最终形成的抓取endpoint信息如下：\n__address__=\u0026#34;192.168.65.3:9100\u0026#34; app=\u0026#34;prometheus\u0026#34; chart=\u0026#34;prometheus-7.4.1\u0026#34; component=\u0026#34;node-exporter\u0026#34; heritage=\u0026#34;Tiller\u0026#34; release=\u0026#34;prometheus\u0026#34; kubernetes_namespace=\u0026#34;default\u0026#34; kubernetes_service_name=\u0026#34;prometheus-node-exporter\u0026#34; __metrics_path__=\u0026#34;/metrics\u0026#34; __scheme__=\u0026#34;http\u0026#34; job=\u0026#34;kubernetes-service-endpoints\u0026#34; 上述的endpoint信息交由prometheus，prometheus就可以得到抓取地址了，为__scheme__://__address____metrics_path__，也即http://192.168.65.3:9100/metrics，最终在prometheus的targets里会看到：\nhttp://192.168.65.3:9100/metrics这个地址在k8s集群外部是无法被访问的，但在k8s集群内部可被访问：\nkubectl -n default run test -ti --rm --image=busybox -- /bin/wget -O - http://192.168.65.3:9100/metrics 可以看该地址确实一系列抓取的监控指标数据。\nprometheus采集其它k8s监控数据 从上述分析来看，假设其它k8s部署了node_exporter和kube-state-metrics，用prometheus采集其它k8s集群的监控数据也是可行的，只需要解决两个问题：\n 设置好 kubernetes_sd_configs，让其可通过其它k8s集群的apiserver发现抓取的endpionts。 设置好relabel_configs，构造出访问其它k8s集群中的service, pod, node等endpoint URL。  构造apiserver连接信息 解决问题1还是比较简单的，设置 kubernetes_sd_configs时填入其它k8s信息的api_server、ca_file、bearer_token_file即可。\n得到其它apiserver的ca_file、bearer_token_file方法如下：\n# 创建一个叫admin的serviceaccount kubectl -n kube-system create serviceaccount admin # 给这个admin的serviceaccount绑上cluser-admin的clusterrole kubectl -n kube-system create clusterrolebinding sa-cluster-admin --serviceaccount kube-system/admin --clusterrole cluser-admin # 查询admin的secret kubectl -n kube-system get serviceaccounts admin -o yaml | yq r - secrets[0].name # 查询admin的secret详细信息，这里的admin-token-vtrt6是上面的命令查询出来的 kubectl -n kube-system get secret admin-token-vtrt6 -o yaml # 获取bearer_token的内容 kubectl -n kube-system get secret admin-token-vtrt6 -o yaml | yq r - data.token|base64 -D 上面这段关于ServiceAccount、ClusterRoleBinding、Secret的操作原理见Authenticating和Using RBAC Authorization。\n得到bearer_token内容后，将其保存进文件，就可以设置kubernetes_sd_configs了，如下：\nkubernetes_sd_configs: - role: endpoints api_server: https://9.77.11.236:8443 tls_config: insecure_skip_verify: true bearer_token: \u0026#39;eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJ....j5ASEVs6epJVeQ\u0026#39; 构造出访问其它k8s集群中的service, pod, node等endpoint URL 经调研，发现外部可通过k8s的apiserver proxy机制很轻松地访问其它k8s集群内部的service、pod、node，参见Manually constructing apiserver proxy URLs，因此在外部访问其它k8s集群内的地址构造成如下这样就可以了：\nhttps://${other_apiserver_address}/api/v1/nodes/node_name:[port_name]/proxy/metrics\nhttps://${other_apiserver_address}/api/v1/namespaces/service_namespace/services/http:service_name[:port_name]/proxy/metrics\nhttps://${other_apiserver_address}/api/v1/namespaces/pod_namespace/pods/http:pod_name[:port_name]/proxy/metrics\n最终整理出的relabel_configs配置如下：\n- job_name: \u0026#39;kubernetes-apiservers-other-cluster\u0026#39; kubernetes_sd_configs: - role: endpoints api_server: https://${other_apiserver_address} tls_config: insecure_skip_verify: true bearer_token: \u0026#39;eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJ....j5ASEVs6epJVeQ\u0026#39; tls_config: insecure_skip_verify: true bearer_token: \u0026#39;eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJ....j5ASEVs6epJVeQ\u0026#39; scheme: https relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - target_label: __address__ replacement: ${other_apiserver_address} - job_name: \u0026#39;kubernetes-nodes-other-cluster\u0026#39; kubernetes_sd_configs: - role: node api_server: https://${other_apiserver_address} tls_config: insecure_skip_verify: true bearer_token: \u0026#39;eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJ....j5ASEVs6epJVeQ\u0026#39; tls_config: insecure_skip_verify: true bearer_token: \u0026#39;eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJ....j5ASEVs6epJVeQ\u0026#39; scheme: https relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: ${other_apiserver_address} - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics - job_name: \u0026#39;kubernetes-nodes-cadvisor-other-cluster\u0026#39; kubernetes_sd_configs: - role: node api_server: https://${other_apiserver_address} tls_config: insecure_skip_verify: true bearer_token: \u0026#39;eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJ....j5ASEVs6epJVeQ\u0026#39; tls_config: insecure_skip_verify: true bearer_token: \u0026#39;eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJ....j5ASEVs6epJVeQ\u0026#39; scheme: https relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: ${other_apiserver_address} - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor - job_name: \u0026#39;kubernetes-kube-service-endpoints-other-cluster\u0026#39; kubernetes_sd_configs: - role: endpoints api_server: https://${other_apiserver_address} tls_config: insecure_skip_verify: true bearer_token: \u0026#39;eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJ....j5ASEVs6epJVeQ\u0026#39; tls_config: insecure_skip_verify: true bearer_token: \u0026#39;eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJ....j5ASEVs6epJVeQ\u0026#39; scheme: https relabel_configs: - source_labels: [__meta_kubernetes_service_label_component] action: keep regex: \u0026#39;^(node-exporter|kube-state-metrics)$\u0026#39; - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__address__] action: replace target_label: instance - target_label: __address__ replacement: ${other_apiserver_address} - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_pod_name, __meta_kubernetes_pod_container_port_number] regex: ([^;]+);([^;]+);([^;]+) target_label: __metrics_path__ replacement: /api/v1/namespaces/${1}/pods/http:${2}:${3}/proxy/metrics - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name 上述配置已经很复杂了，更变态的是prometheus竟然只支持一个配置文件，因此每加入一个k8s集群的监控，就得往prometheus的配置文件里加入一段上述配置，想想都让人捏一把汗。。。\n总结 虽然用一个prometheus监控多个k8s集群在架构上不太合理，但运用多种技术手段还是搞定了这个问题。解决问题的过程中查询了k8s和prometheus的各种资料，对K8S Apiserver、K8S RBAC Authorization、Prometheus Configration有了更深入的理解。另外最终的解决方案将prometheus的配置搞得很复杂，很有必要用模板技术生成该配置文件了。\n","permalink":"https://jeremyxu2010.github.io/2018/11/%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7%E5%A4%9Ak8s%E9%9B%86%E7%BE%A4/","tags":["prometheus","kubernetes"],"title":"使用prometheus监控多k8s集群"},{"categories":["devops"],"contents":"今天在工作中用到了一条iptables规则，虽然明白这条规则的意思，但结合之前对iptables的理解，想不明白为什么会这么工作，后来仔细研读iptables的官方文档，终于从字里行间找到原因了，这里记录下问题的追踪过程。\n现象 工作中用到了一条iptables规则，如下：\n$ iptables -t nat -I OUTPUT 1 -p tcp -j REDIRECT --to-port 9999 这条规则的意思是从本机发出的数据包都重定向到本地的9999端口。\n当前的防火墙规则如下：\n$ iptables -t nat -L -n --line-numbers Chain PREROUTING (policy ACCEPT) num target prot opt source destination Chain INPUT (policy ACCEPT) num target prot opt source destination Chain OUTPUT (policy ACCEPT) num target prot opt source destination 1 REDIRECT tcp -- 0.0.0.0/0 0.0.0.0/0 redir ports 9999 Chain POSTROUTING (policy ACCEPT) num target prot opt source destination $ iptables -t filter -L -n --line-numbers Chain INPUT (policy ACCEPT) num target prot opt source destination Chain FORWARD (policy ACCEPT) num target prot opt source destination Chain OUTPUT (policy ACCEPT) num target prot opt source destination 那么我提出了一个问题，如果在本机上跑一个程序监听8888端口，那么外部还可以正常连接该端口吗？\n$ python -m SimpleHTTPServer 8888 # 那么从其它机器还可以正常访问它的8888端口吗？ curl http://$host_ip:8888 我最开始的判断是不能，但经过实测发现是可以的。\n我的推导过程 数据包在netfilter中的流转可参考下图：\nTCP连接的建立过程参照下图：\n由上述两图可知，curl命令发送HTTP请求至服务端，首先得建立TCP连接，而建立TCP连接的过程，客户端先向服务器发送了一个SYN包，服务端要回一个SYN+ACK包，但这个回应数据包会经过NAT表的OUTPUT链：\n而NAT表的OUTPUT链中第一条规则就是将这个数据重定向到本机9999端口，而本机的9999端口现在并没有任何程序在监听，因而这个数据包就丢了，而客户端收不到SYN+ACK包，连TCP连接都建立不了，更谈不上进行HTTP协议的其它处理。\n现在现象并不是这样的，因而一定是哪儿推导出现问题了。\n日志诊断 再添加几条规则打印出日志分析看看：\niptables -t nat -I OUTPUT 1 -p tcp --sport 8888 -j LOG --log-prefix \u0026#39;nat-output-log \u0026#39; iptables -t filter -I OUTPUT 1 -p tcp --sport 8888 -j LOG --log-prefix \u0026#39;filter-output-log \u0026#39; 再发起curl请求，检查下打印的日志：\n$ cat /var/log/messages | grep \u0026#39;output-log\u0026#39; Oct 13 13:45:28 centos-linux kernel: filter-output-log IN= OUT=eth0 SRC=10.211.55.11 DST=10.211.55.2 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=8888 DPT=60181 WINDOW=28960 RES=0x00 ACK SYN URGP=0 Oct 13 13:45:28 centos-linux kernel: filter-output-log IN= OUT=eth0 SRC=10.211.55.11 DST=10.211.55.2 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=14590 DF PROTO=TCP SPT=8888 DPT=60181 WINDOW=227 RES=0x00 ACK URGP=0 Oct 13 13:45:28 centos-linux kernel: filter-output-log IN= OUT=eth0 SRC=10.211.55.11 DST=10.211.55.2 LEN=69 TOS=0x00 PREC=0x00 TTL=64 ID=14591 DF PROTO=TCP SPT=8888 DPT=60181 WINDOW=227 RES=0x00 ACK PSH URGP=0 Oct 13 13:45:28 centos-linux kernel: filter-output-log IN= OUT=eth0 SRC=10.211.55.11 DST=10.211.55.2 LEN=89 TOS=0x00 PREC=0x00 TTL=64 ID=14592 DF PROTO=TCP SPT=8888 DPT=60181 WINDOW=227 RES=0x00 ACK PSH URGP=0 Oct 13 13:45:28 centos-linux kernel: filter-output-log IN= OUT=eth0 SRC=10.211.55.11 DST=10.211.55.2 LEN=808 TOS=0x00 PREC=0x00 TTL=64 ID=14593 DF PROTO=TCP SPT=8888 DPT=60181 WINDOW=227 RES=0x00 ACK PSH URGP=0 Oct 13 13:45:28 centos-linux kernel: filter-output-log IN= OUT=eth0 SRC=10.211.55.11 DST=10.211.55.2 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=14594 DF PROTO=TCP SPT=8888 DPT=60181 WINDOW=227 RES=0x00 ACK FIN URGP=0 Oct 13 13:45:28 centos-linux kernel: filter-output-log IN= OUT=eth0 SRC=10.211.55.11 DST=10.211.55.2 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=2569 DF PROTO=TCP SPT=8888 DPT=60181 WINDOW=227 RES=0x00 ACK URGP=0 奇怪，竟然没有nat-output-log的日志，难道NAT表的OUTPUT链失效了？\n再在本机访问外部试试：\n$ iptables -t nat -I OUTPUT 1 -p tcp --dport 8888 -j LOG --log-prefix \u0026#39;nat-output-log \u0026#39; $ curl http://www.baidu.com:8888 这回检查日志，发现nat-output-log的日志又出现了：\n$ cat /var/log/messages | grep \u0026#39;nat-output-log\u0026#39; Oct 13 13:53:00 centos-linux kernel: nat-output-log IN= OUT=eth0 SRC=10.211.55.11 DST=14.215.177.39 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=30145 DF PROTO=TCP SPT=35530 DPT=8888 WINDOW=29200 RES=0x00 SYN URGP=0 Oct 13 13:53:00 centos-linux kernel: nat-output-log IN= OUT=eth0 SRC=10.211.55.11 DST=14.215.177.38 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=7296 DF PROTO=TCP SPT=48494 DPT=8888 WINDOW=29200 RES=0x00 SYN URGP=0 难道NAT表的OUTPUT链里的规则时灵时不灵？\n问题分析 仔细研读netfilter的官方文档发现了下面这句话：\n NAT This is the realm of the `nat\u0026rsquo; table, which is fed packets from two netfilter hooks: for non-local packets, the NF_IP_PRE_ROUTING and NF_IP_POST_ROUTING hooks are perfect for destination and source alterations respectively. If CONFIG_IP_NF_NAT_LOCAL is defined, the hooks NF_IP_LOCAL_OUT and NF_IP_LOCAL_IN are used for altering the destination of local packets.\nThis table is slightly different from the `filter\u0026rsquo; table, in that only the first packet of a new connection will traverse the table: the result of this traversal is then applied to all future packets in the same connection.\n 原来NAT表里的规则仅针对一条连接的第一个数据包有效。响应回的SYN+ACK包是第2个包，自然不受影响。\n为什么会如此了？\n想了下终于明白原因了，估计是因为执行效率。因为NAT表里的规则主要是用来进行网络地址转换的，而第一个包经过时已经进行了网络地址转换，连接的信息，包括地址由a-\u0026gt;b转换为a-\u0026gt;c这些信息，都已经被conntrack记录下来了。那么接下来如果再过来a-\u0026gt;b的数据包，只要查查conntrack的记录信息，就可以推出要转成a-c，根本不需要再走一遍NAT表里的规则。\n又搜索了下，更加证实了我的猜想：\n https://serverfault.com/questions/741104/iptables-redirect-works-only-for-first-packet   nat table rules always work only for first packet in connection. Subsequent packets of same connection never traverse nat rule list and only supported by conntrack code\nAs UDP is connectionless in nature, \u0026ldquo;connection\u0026rdquo; here is defined simply by addresses, ports and timeout. So, if second UDP packet with same source port and address and same destination port and address arrives within the timeout, Linux believes it belongs to established \u0026ldquo;connection\u0026rdquo; and doensn't evaluate nat rule table for it at all, reusing verdict issued for previous packet.\n https://blog.csdn.net/dog250/article/details/5692601   修改应用层协议控制包使用了ip_conntrack，iptables的REDIRECT target也使用了ip_conntrack，另外包括iptables的state模块也是如此，使用ip_conntrack，可见ip_conntrack的重要性，ip_conntrack的一个无比重要的作用是实现nat，可以说REDIRECT target和对诸如ftp的修改以实现server回连client最终都落实到了nat上，比如，所谓的REDIRECT就是内置一个nat规则，将符合matchs的包nat到本机的特定端口，这个和iptables的nat表原理是一样的，不同的是，nat表的配置是显式的nat，而REDIRECT和ip_nat_ftp是隐式的nat而已。它们都是nat，都依赖于原始的ip_conntrack，因此原始的链接流信息并没有丢失，还是可以得到的，事实上，内核就是通过原始的链接流来匹配nat规则的，如果丢弃了原始链接流信息，何谈匹配！如果一个原始链接是a-\u0026gt;b，而后不管是显式的nat还是隐式的REDIRECT以及nat_ftp，将a-\u0026gt;b改为了a-\u0026gt;c，a-\u0026gt;b还是可以得到的，内核正是从a-\u0026gt;b的流信息中取得了“要转换为a-\u0026gt;c”这个信息的。\n 至此所有迷题均解开了。\n复习iptables数据包流程图 最后再复习一遍iptables的数据包流程图。\n iptables 基本概念\n匹配（match）：符合指定的条件，比如指定的 IP 地址和端口。\n丢弃（drop）：当一个包到达时，简单地丢弃，不做其它任何处理。\n接受（accept）：和丢弃相反，接受这个包，让这个包通过。\n拒绝（reject）：和丢弃相似，但它还会向发送这个包的源主机发送错误消息。这个错误消息可以指定，也可以自动产生。\n目标（target）：指定的动作，说明如何处理一个包，比如：丢弃，接受，或拒绝。\n跳转（jump）：和目标类似，不过它指定的不是一个具体的动作，而是另一个链，表示要跳转到那个链上。\n规则（rule）：一个或多个匹配及其对应的目标。\n链（chain）：每条链都包含有一系列的规则，这些规则会被依次应用到每个遍历该链的数据包上。每个链都有各自专门的用途， 这一点我们下面会详细讨论。\n表（table）：每个表包含有若干个不同的链，比如 filter 表默认包含有 INPUT，FORWARD，OUTPUT 三个链。iptables有四个表，分别是：raw，nat，mangle和filter，每个表都有自己专门的用处，比如最常用filter表就是专门用来做包过滤的，而 nat 表是专门用来做NAT的。\n策略（police）：我们在这里提到的策略是指，对于 iptables 中某条链，当所有规则都匹配不成功时其默认的处理动作。\n连接跟踪（connection track）：又称为动态过滤，可以根据指定连接的状态进行一些适当的过滤，是一个很强大的功能，但同时也比较消耗内存资源。\n经过iptables的数据包的流程介绍\n一个数据包到达时,是怎么依次穿过各个链和表的。\n基本步骤如下：\n 数据包到达网络接口，比如 eth0。 进入 raw 表的 PREROUTING 链，这个链的作用是赶在连接跟踪之前处理数据包。 如果进行了连接跟踪，在此处理。 进入 mangle 表的 PREROUTING 链，在此可以修改数据包，比如 TOS 等。 进入 nat 表的 PREROUTING 链，可以在此做DNAT，但不要做过滤。 决定路由，看是交给本地主机还是转发给其它主机。  到了这里我们就得分两种不同的情况进行讨论了，一种情况就是数据包要转发给其它主机，这时候它会依次经过：\n进入 mangle 表的 FORWARD 链，这里也比较特殊，这是在第一次路由决定之后，在进行最后的路由决定之前，我们仍然可以对数据包进行某些修改。 进入 filter 表的 FORWARD 链，在这里我们可以对所有转发的数据包进行过滤。需要注意的是：经过这里的数据包是转发的，方向是双向的。 进入 mangle 表的 POSTROUTING 链，到这里已经做完了所有的路由决定，但数据包仍然在本地主机，我们还可以进行某些修改。 进入 nat 表的 POSTROUTING 链，在这里一般都是用来做 SNAT ，不要在这里进行过滤。 进入出去的网络接口。完毕。  另一种情况是，数据包就是发给本地主机的，那么它会依次穿过：\n进入 mangle 表的 INPUT 链，这里是在路由之后，交由本地主机之前，我们也可以进行一些相应的修改。 进入 filter 表的 INPUT 链，在这里我们可以对流入的所有数据包进行过滤，无论它来自哪个网络接口。 交给本地主机的应用程序进行处理。 处理完毕后进行路由决定，看该往那里发出。 进入 raw 表的 OUTPUT 链，这里是在连接跟踪处理本地的数据包之前。 连接跟踪对本地的数据包进行处理。 进入 mangle 表的 OUTPUT 链，在这里我们可以修改数据包，但不要做过滤。 进入 nat 表的 OUTPUT 链，可以对防火墙自己发出的数据做 NAT 。 再次进行路由决定。 进入 filter 表的 OUTPUT 链，可以对本地出去的数据包进行过滤。 进入 mangle 表的 POSTROUTING 链，同上一种情况的第9步。注意，这里不光对经过防火墙的数据包进行处理，还对防火墙自己产生的数据包进行处理。 进入 nat 表的 POSTROUTING 链，同上一种情况的第10步。 进入出去的网络接口。完毕。   总结 这次对iptables的数据包流程又有更深的理解，幸好遇到问题时没有直接忽略，狠狠地追查下根本原因。\n参考  https://netfilter.org/documentation/HOWTO/netfilter-hacking-HOWTO-3.html https://serverfault.com/questions/741104/iptables-redirect-works-only-for-first-packet https://blog.csdn.net/dog250/article/details/5692601 http://www.opsers.org/security/iptables-related-concepts-and-processes-the-packet-figure.html http://www.ha97.com/4093.html https://blog.csdn.net/guyuealian/article/details/52535294  ","permalink":"https://jeremyxu2010.github.io/2018/10/%E8%BF%BD%E6%9F%A5iptables%E8%A7%84%E5%88%99%E5%A4%B1%E6%95%88%E5%8E%9F%E5%9B%A0/","tags":["iptables","netfilter"],"title":"追查iptables规则失效原因"},{"categories":["容器编排"],"contents":"这几天在工作中使用docker发现了docker的两个bug，这里记录下以备忘。\ndocker容器生成僵尸进程 现象 公司开发服务器上使用docker跑了几个容器，这些容器都是长时间运行的。偶然发现服务器上有大量僵尸进程，大约有两三千个。简单跟踪了下，发现这些僵尸进程均是在容器的进程命名空间的。\nps aux | grep \u0026#39;Z\u0026#39; | grep -v grep ll /proc/${any_zombie_pid} 在容器里运行的程序是很正常的web server，怎么会这样呢？在网上搜索了下，终于找到了答案：\nDocker 和子进程“僵尸化”问题\n 初始进程的责任：“收割”“僵尸进程” Unix 的进程之间是树状结构的关系。每个进程都可以派生出子进程，而除了最顶端的进程之外，也都会有一个父进程。\n这个最顶端的进程就是初始进程，其在启动系统时被内核启动，并负责启动系统的其余功能部分。如：SSH 后台程序、Docker 后台程序、Apache/Nginx 和 GUI 桌面环境等等。这些程序又可能会派生出它们自己的子进程。\n这一部分并没有什么问题。但问题在于当一个进程终止时，会发生什么？假设上图中的 bash (5) 进程结束了，那么其会转变为「废弃进程」（defunct process），也被称作为“僵尸进程”（zombie process）。\n为什么会这样？因为 Unix 这样设计地目的，在于让父进程能够耐心“等待”子进程结束，从而获得其结束状态（exit status）。只有当父进程调用 waitpid() 之后，“僵尸进程”才会真正结束。手册里是这样描述地：\n 一个已经终止但并未被“等待”的进程，就成为了一个“僵尸”。内核会记录这些“僵尸进程”的基本信息（PID、终止状态、资源占用信息），以确保其父进程在之后的时间里可以通过“等待”来获取这个子进程的信息。\n 通常来说，人们会简单地认为“僵尸进程”就是那些会造成破坏的失控进程。但从 Unix 系统角度来分析，“僵尸进程”有着非常清晰地定义：进程已经终止，但尚未被其父进程“等待”。\n绝大多数情况下，这都不会产生什么问题。在一个子进程上调用 waitpid() 以消除其“僵尸”状态，被称为“收割”。多数应用程序都能够正确地“收割”其子进程。在上例中，操作系统会在 bash 进程终止时发送 SIGCHLD 信号以唤醒 sshd 进程，其在接收到信号后就“收割”掉了此子进程。\n但还有一种特殊情况——如果父进程终止了，无论是正常的（程序逻辑正常终止），还是用户操作导致的（比如用户杀死了该进程）——子进程会如何处理？它们不再拥有父进程，变成了「孤儿进程」（orphaned）（这是确切的技术术语）。\n此时初始进程（PID 1）就会因其被赋予地特殊任务而介入——「领养」（adopt）（同样的，这是确切的技术术语）「孤儿进程」。这就意味着初始进程会成为这些子进程的父进程，而无论其是否由初始进程创建。\n以 Nginx 为例，其默认就会作为后台程序运行。工作流程如下：Nginx 创建一个子进程后，自身进程结束，然后该子进程就被初始进程「领养」了。\n其中的要点是什么？操作系统内核自动处理了「领养」逻辑，因此内核其实是希望初始进程也自动完成对这些「孤儿进程」的“收割”逻辑。\n这在 Unix 操作系统中是一个非常重要的机制，大量的软件都是因而设计和实现。几乎所有的服务（daemon）程序都预期初始进程会「领养」和“收割”其守护子进程。\n尽管我们是以服务程序做例子，但系统并没有什么机制对此进行规约。任何一个进程在结束时，都会预期初始进程能够清理（「领养」和“收割”）其子进程。这一点，在《操作系统概述》和《Unix 系统高级编程》两书中描述地非常详细。\n“僵尸进程”的危害 “僵尸进程”都已经终止了，它们危害在哪里？它们原本占用的内存已经释放了吗？在 ps 中除了多了些条目，还有什么别的吗？\n是的，内存确实已经释放，但能够在 ps 中看到，说明它们还仍然占用着一些内核资源。对 Linux waitpid 的文档引用如下：\n 在“僵尸进程”在被父进程“等待”以彻底消除之前，其仍然会被记录在内核进程表中。而当该表被写满后，新的进程将无法被创建。\n 对 Docker 的影响 这个问题会如何对 Docker 产生怎样的影响？我们可以看到很多人只在他们的容器中跑一个进程，而且也认为只需要跑这么一个进程就足够了。但显而易见地，这些进程无法承担初始进程在前文中所述的任务逻辑。因此，为了能够正确地“收割”被「领养」的进程，我们需要另外的初始进程来完成这些工作。\n举一个相对复杂地例子，我们的容器是一个 web 服务器，需要去跑一段基于 bash 的 CGI 脚本，而该脚本又会去调用 grep 程序。假定 web 服务器发现了 CGI 脚本执行超时，也中止了其继续执行。但此时 grep 程序并不会受到影响仍然继续执行，当其执行结束时，就变成了一个“僵尸进程”并由初始进程（即 web 服务器）「收养」。但 web 服务器无法正确地“收割”这个 grep 进程，所以该“僵尸进程”就在系统中常驻了。\n这个问题同样也存在于其它场景中。我们能看到人们尝尝为第三方程序创建 Docker 容器——又如 PostgreSQL ——并将其作为容器中的主进程运行。当我们运行别人的代码时，我们如何确保这些程序并不会派生出子进程并因而堆积大量的“僵尸进程”？唯独仅有我们运行着自己的代码，同时还对所有的依赖包和依赖包的依赖包做严格地审查，才能杜绝这种问题。因此，通常来说，我们很有必要来执行一个合适的初始化系统（init system）来避免这些问题地发生。\n 解决方案  重新编译容器镜像，像baseimage-docker一样，往镜像中引入一套轻量的初始化系统my_init，并将这个my_init程序作为容器运行的初始进程。 将原来的CMD [\u0026quot;/path-to-your-app\u0026quot;]修改为CMD [\u0026quot;/bin/bash\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;set -e \u0026amp;\u0026amp; /path-to-your-app\u0026quot;] \u0026amp;\u0026amp; true，这是一个不完善方案，因为没有干净地终止应用进程，可能会造成文件损坏，有风险。  容器的目录被其它的进程使用 现象 在正常停止docker容器后，删除容器报错：\nError response from daemon: Driver devicemapper failed to remove root filesystem a5144c558eabbe647ee9a25072746935e03bb797f4dcaf44c275e0ea4ada463a: remove /var/lib/docker/devicemapper/mnt/25cb26493fd3c804d96e802a95d6c74d7cae68032bf50fc640f40ffe40cc4188: device or resource busy Error response from daemon: Driver devicemapper failed to remove root filesystem bdd60d5104076351611efb4cdb34c50c9d3f2136fdaea74c9752e2df9fd6f40f: remove /var/lib/docker/devicemapper/mnt/d2b5b784495ece1c9365bdea78b95076f035426356e6654c65ee1db87d8c03e7: device or resource busy Error response from daemon: Driver devicemapper failed to remove root filesystem 847b5bb74762a7356457cc331d948e5c47335bbd2e0d9d3847361c6f69e9c369: remove /var/lib/docker/devicemapper/mnt/71e7b20dca8fd9e163c3dfe90a3b31577ee202a03cd1bd5620786ebabdc4e52a: device or resource busy Error response from daemon: Driver devicemapper failed to remove root filesystem a85e44dfa07c060244163e19a545c76fd25282f2474faa205d462712866aac51: remove /var/lib/docker/devicemapper/mnt/8bcd524cc8bfb1b36506bf100090c52d7fbbf48ea00b87a53d69f32e537737b7: device or resource busy 快速解决方案 # 找到使用容器目录的进程 $ find /proc/*/mounts | xargs grep -E \u0026#34;526c823031c2065c6fb3b92f9aaded4477eccceb65f245391a1d8a6acae13d0e\u0026#34; /proc/27837/mounts:shm /var/lib/docker/containers/526c823031c2065c6fb3b92f9aaded4477eccceb65f245391a1d8a6acae13d0e/shm tmpfs rw,nosuid,nodev,noexec,relatime,size=65536k 0 0 $ ps aux|grep 27837 # 先停掉这些进程后，再就可以成功删除容器了 问题根源 https://github.com/moby/moby/issues/27381\n Core of the issue here is that container is either still running or some of its mount points have leaked into other some mount namespace.\nYou docker-pid and host both seem to be sharing same mount namespace. And that means docker daemon is running in host mount namespace. And that probably means that nginx started at some point after container start and it seems to be running in its own mount namespace. And at that time mount points leaked into nginx mount namespace and that's preventing deletion of container.\n 原来是老的内核存在bug，docker进程共享宿主机的mount命名空间，这样容器的挂载点被泄漏给其它进程的命名空间了。\n解决方案 升级内核至3.10.0-693.5.2.el7.x86_64以后，另外安装docker仓库里最新的docker-ce：\nsudo yum remove docker \\  docker-client \\  docker-client-latest \\  docker-common \\  docker-latest \\  docker-latest-logrotate \\  docker-logrotate \\  docker-selinux \\  docker-engine-selinux \\  docker-engine sudo yum install -y yum-utils \\  device-mapper-persistent-data \\  lvm2 sudo yum-config-manager \\  --add-repo \\  https://download.docker.com/linux/centos/docker-ce.repo sudo yum install docker-ce 感想 没想法docker容器化技术发展了这么多年，容器隔离性、基础镜像等这些还存在问题，真是让人想不到，呵呵。\n参考  https://gist.github.com/snakevil/0b47072fcb626b87f4bd4ab80f7d8946 https://www.lijiaocn.com/%E9%97%AE%E9%A2%98/2017/07/14/docker-unable-to-rm-filesystem.html https://github.com/moby/moby/issues/27381 https://docs.docker.com/install/linux/docker-ce/centos/#install-using-the-repository  ","permalink":"https://jeremyxu2010.github.io/2018/10/docker%E7%9A%84%E4%B8%A4%E4%B8%AAbug/","tags":["docker","zombie","leak"],"title":"docker的两个bug"},{"categories":["devops"],"contents":"最近的工作涉及搭建一套日志采集系统，采用了业界成熟的ELFK方案，这里将搭建过程记录一下。\n环境准备 操作系统信息 系统系统：centos7.2\n三台服务器：10.211.55.11/12/13\n安装包：\nhttps://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.2.rpm\nhttps://artifacts.elastic.co/downloads/kibana/kibana-6.3.2-x86_64.rpm\nhttps://artifacts.elastic.co/downloads/logstash/logstash-6.3.2.rpm\nhttps://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.3.2-x86_64.rpm\n服务器规划    服务器host11 服务器host12 服务器host13     elasticsearch(master,data,client) elasticsearch(master,data,client) elasticsearch(master,data,client)    kibana    logstash logstash logstash   filebeat filebeat filebeat    整个ELFK的部署架构图大致如下图：\n日志采集系统搭建 安装elasticsearch集群 照手把手教你搭建一个 Elasticsearch 集群文章所述，elasticsearch集群中节点有多种类型：\n  主节点：即 Master 节点。主节点的主要职责是和集群操作相关的内容，如创建或删除索引，跟踪哪些节点是群集的一部分，并决定哪些分片分配给相关的节点。稳定的主节点对集群的健康是非常重要的。默认情况下任何一个集群中的节点都有可能被选为主节点。索引数据和搜索查询等操作会占用大量的cpu，内存，io资源，为了确保一个集群的稳定，分离主节点和数据节点是一个比较好的选择。虽然主节点也可以协调节点，路由搜索和从客户端新增数据到数据节点，但最好不要使用这些专用的主节点。一个重要的原则是，尽可能做尽量少的工作。 数据节点：即 Data 节点。数据节点主要是存储索引数据的节点，主要对文档进行增删改查操作，聚合操作等。数据节点对 CPU、内存、IO 要求较高，在优化的时候需要监控数据节点的状态，当资源不够的时候，需要在集群中添加新的节点。 负载均衡节点：也称作 Client 节点，也称作客户端节点。当一个节点既不配置为主节点，也不配置为数据节点时，该节点只能处理路由请求，处理搜索，分发索引操作等，从本质上来说该客户节点表现为智能负载平衡器。独立的客户端节点在一个比较大的集群中是非常有用的，他协调主节点和数据节点，客户端节点加入集群可以得到集群的状态，根据集群的状态可以直接路由请求。 预处理节点：也称作 Ingest 节点，在索引数据之前可以先对数据做预处理操作，所有节点其实默认都是支持 Ingest 操作的，也可以专门将某个节点配置为 Ingest 节点。  以上就是节点几种类型，一个节点其实可以对应不同的类型，如一个节点可以同时成为主节点和数据节点和预处理节点，但如果一个节点既不是主节点也不是数据节点，那么它就是负载均衡节点。具体的类型可以通过具体的配置文件来设置。\n 我部署的环境服务器较少，只有三台，因此部署在每个节点上的elasticsearch实例只好扮演master、data、client三种角色了。\n在三台服务器上均执行以下命令关闭selinux：\nsetenforce 0 sed -i -e \u0026#39;s/^SELINUX=.*$/SELINUX=disabled/g\u0026#39; /etc/selinux/config 在三台服务器上均安装java：\nyum install -y java 在三台服务器上均安装elasticsearch的rpm包：\nyum install -y https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.2.rpm 在三台服务器上修改elasticsearch的配置文件：\ncat \u0026lt;\u0026lt; EOF \u0026gt; /etc/elasticsearch/elasticsearch.yml cluster.name: DemoESCluster # 注意不同节点的node.name要设置得不一样 node.name: demo-es-node-1 path.data: /var/lib/elasticsearch path.logs: /var/log/elasticsearch network.host: 0.0.0.0 http.port: 9200 discovery.zen.ping.unicast.hosts: [\u0026#34;10.211.55.11\u0026#34;, \u0026#34;10.211.55.12\u0026#34;, \u0026#34;10.211.55.13\u0026#34;] discovery.zen.minimum_master_nodes: 2 gateway.recover_after_nodes: 2 EOF 在三台服务器上启动elasticsearch:\nsystemctl daemon-reload systemctl enable elasticsearch systemctl start elasticsearch 在任意服务器上检查集群中的节点列表：\nyum install -y jq curl --silent -XGET \u0026#39;http://localhost:9200/_cluster/state?pretty\u0026#39;|jq \u0026#39;.nodes\u0026#39; 在上述命令的输出里可以看到集群的相关信息，同时 nodes 字段里面包含了每个节点的详细信息，这样一个基本的elasticsearch集群就部署好了。\n安装 Kibana 接下来我们需要安装一个 Kibana 来帮助可视化管理 Elasticsearch，在host12上安装kibana:\nyum install -y https://artifacts.elastic.co/downloads/kibana/kibana-6.3.2-x86_64.rpm 修改kibana的配置文件：\ncat \u0026lt;\u0026lt; EOF \u0026gt; /etc/kibana/kibana.yml server.port: 5601 server.host: \u0026#34;0.0.0.0\u0026#34; elasticsearch.url: \u0026#34;http://localhost:9200\u0026#34; EOF 注意这里配置的elasticsearch.url为本机的es实例，这样其实还是存在单点故障的，官方建议在本机部署一个Elasticsearch 协调（Coordinating only node） 的节点，这里配置成协调节点的地址。\n启动kibana:\nsystemctl daemon-reload systemctl enable kibana systemctl start kibana 配置认证需要升级License，我这里是在内网使用，就不进行这个配置了。如果须要配置访问认证可参考这里。\n另外还可以启用SSL，可参考这里进行配置。\n为了避免单点故障，kibana可部署多个，然后由nginx作反向代理，实现对kibana服务的负载均衡访问。\n安装logstash 在每台服务器上安装logstash:\nyum install -y https://artifacts.elastic.co/downloads/logstash/logstash-6.3.2.rpm 修改logstash的配置文件：\ncat \u0026lt;\u0026lt; EOF \u0026gt; /etc/logstash/logstash.yml path.data: /var/lib/logstash path.logs: /var/log/logstash xpack.monitoring.enabled: true xpack.monitoring.elasticsearch.url: [\u0026#34;http://10.211.55.11:9200\u0026#34;, \u0026#34;http://10.211.55.12:9200\u0026#34;, \u0026#34;http://10.211.55.13:9200\u0026#34;] EOF cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/logstash/conf.d/beat-elasticsearch.conf input { beats { port =\u0026gt; 5044 ssl =\u0026gt; false } } filter { } output { elasticsearch { hosts =\u0026gt; [\u0026#34;10.211.55.11:9200\u0026#34;,\u0026#34;10.211.55.12:9200\u0026#34;,\u0026#34;10.211.55.13:9200\u0026#34;] index =\u0026gt; \u0026#34;%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}\u0026#34; document_type =\u0026gt; \u0026#34;%{[@metadata][type]}\u0026#34; } } EOF 为了从原始日志中解析出一些有意义的field字段，可以启用一些filter，可用的filter列表在这里。\n启动logstash:\nsystemctl daemon-reload systemctl enable logstash systemctl start logstash 安装filebeat 在每台服务器上安装filebeat:\nyum install -y https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.3.2-x86_64.rpm 修改每台服务器上的filebeat配置文件：\n# 这里根据在采集的日志路径，编写合适的inputs规则 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/filebeat/filebeat.yml filebeat.inputs: - type: log enabled: true paths: - /var/log/*.log filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: false output.logstash: hosts: [\u0026#34;10.211.55.11:5044\u0026#34;, \u0026#34;10.211.55.12:5044\u0026#34;, \u0026#34;10.211.55.13:5044\u0026#34;] ssl.enabled: false index: \u0026#39;var_log\u0026#39; EOF filebeat配置文件选项比较多，完整的参考可查看这里。\n在每台服务器上启动filebeat:\nsystemctl daemon-reload systemctl enable filebeat systemctl start filebeat 其它安全设置 为保证数据安全，filebeat与logstash、filebeat与elasticsearch、logstash与elasticsearch、kibana与elasticsearch之间的通讯及kibana自身均能启用SSL加密，具体启用办法就是在配置文件中配一配SSL证书就可以了，这个比较简单，不再赘述。\nkibana登录认证需要升级License，这一点比较不爽，如果考虑成本，还是在前置机nginx上配个HTTP Basic认证处理好了。\n部署测试 至此一个较完整的ELFK日志采集系统就搭建好了，用浏览器访问http://10.211.55.12:5601/，在kibana的界面上简单设置下就可以查看到抓取的日志了：\n总结 分布式日志采集，ELFK这一套比较成熟了，部署也很方便，不过部署起来还是稍显麻烦。好在还有自动化部署的ansible脚本：ansible-beats、ansible-elasticsearch、ansible-role-logstash、ansible-role-kibana，所以如果有经常部署这一套，还是拿这些ansible脚本组建自动化部署工具集吧。\n参考  https://mp.weixin.qq.com/s/eyfApIiDeg3qv-BD9hBNvw https://www.elastic.co/guide/cn/kibana/current/production.html https://www.ibm.com/developerworks/cn/opensource/os-cn-elk-filebeat/index.html  ","permalink":"https://jeremyxu2010.github.io/2018/10/%E6%90%AD%E5%BB%BAelfk%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E7%B3%BB%E7%BB%9F/","tags":["elasticsearch","logstash","kibana","filebeat"],"title":"搭建ELFK日志采集系统"},{"categories":["devops"],"contents":"最近比较忙，而且国庆节回了趟老家，各种事情比较多，博客又有一个月没有更新了。这周末有一些时间，所以计划分几篇文章把近一个月技术上的一些实践记录一下，这第一篇记录一下mongodb的高可用集群部署。\n环境准备 操作系统信息 系统系统：centos7.2\n三台服务器：10.211.55.11/12/13\n安装包：https://repo.mongodb.org/yum/redhat/7/mongodb-org/4.0/x86_64/RPMS/\n服务器规划    服务器mongo11 服务器mongo12 服务器mongo13     mongos mongos mongos   mongo config server mongo config server mongo config server   shard server1 复制集节点1 shard server1 复制集节点2 shard server1 复制集节点3   shard server2 复制集节点1 shard server2 复制集节点2 shard server2 复制集节点3   shard server3 复制集节点1 shard server3 复制集节点2 shard server3 复制集节点3    端口分配 mongos：27088 config：27077 shard1：27017 shard2：27018 shard3：27019 集群搭建 安装mongodb 3台服务器上均安装mongodb的rpm包\nyum install -y https://repo.mongodb.org/yum/redhat/7/mongodb-org/4.0/x86_64/RPMS/mongodb-org-4.0.3-1.el7.x86_64.rpm \\  https://repo.mongodb.org/yum/redhat/7/mongodb-org/4.0/x86_64/RPMS/mongodb-org-mongos-4.0.3-1.el7.x86_64.rpm \\  https://repo.mongodb.org/yum/redhat/7/mongodb-org/4.0/x86_64/RPMS/mongodb-org-server-4.0.3-1.el7.x86_64.rpm \\  https://repo.mongodb.org/yum/redhat/7/mongodb-org/4.0/x86_64/RPMS/mongodb-org-shell-4.0.3-1.el7.x86_64.rpm \\  https://repo.mongodb.org/yum/redhat/7/mongodb-org/4.0/x86_64/RPMS/mongodb-org-tools-4.0.3-1.el7.x86_64.rpm 分别在每台机器建立conf、mongos、config、shard1、shard2、shard3六个目录，因为mongos不存储数据，只需要建立日志文件目录即可。\nmkdir -p /etc/mongodb/conf mkdir -p /var/lib/mongodb/mongos/log mkdir -p /var/lib/mongodb/config/data mkdir -p /var/lib/mongodb/config/log mkdir -p /var/lib/mongodb/shard1/data mkdir -p /var/lib/mongodb/shard1/log mkdir -p /var/lib/mongodb/shard2/data mkdir -p /var/lib/mongodb/shard2/log mkdir -p /var/lib/mongodb/shard3/data mkdir -p /var/lib/mongodb/shard3/log 关闭selinux\nsetenforce 0 sed -i -e \u0026#39;s/^SELINUX=.*$/SELINUX=disabled/g\u0026#39; /etc/selinux/config 初始化分片的复制集 首先初始化分片shard1的复制集，在每台服务器上创建其配置文件：\n$ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/mongodb/conf/shard1.conf systemLog: destination: file logAppend: true path: /var/lib/mongodb/shard1/log/mongod.log # Where and how to store data. storage: dbPath: /var/lib/mongodb/shard1/data journal: enabled: true # how the process runs processManagement: fork: true # fork and run in background pidFilePath: /var/lib/mongodb/shard1/log/mongod.pid # location of pidfile # network interfaces net: port: 27017 bindIp: 0.0.0.0 # Listen to local interface only, comment to listen on all interfaces. replication: replSetName: shard1 sharding: clusterRole: \u0026#34;shardsvr\u0026#34; EOF 在每台服务器上创建该分片复制节点的systemd启动脚本：\n$ cat \u0026lt;\u0026lt;EOF \u0026gt; /usr/lib/systemd/system/mongod-shard1.service [Unit] Description=MongoDB Database shard1 Service Wants=network.target After=network.target [Service] Type=forking PIDFile=/var/lib/mongodb/shard1/log/mongod.pid ExecStart=/usr/bin/mongod -f /etc/mongodb/conf/shard1.conf ExecReload=/bin/kill -HUP $MAINPID Restart=always StandardOutput=syslog StandardError=syslog [Install] WantedBy=multi-user.target EOF 在每台服务器上启动该分片的复制集各节点：\nsystemctl enable mongod-shard1 systemctl start mongod-shard1 在任意一台服务上初始化复制集配置：\n#连接 $ mongo --port 27017 \u0026gt; use admin switched to db admin \u0026gt; config = { ... _id : \u0026#34;shard1\u0026#34;, ... members : [ ...... {_id : 0, host : \u0026#34;10.211.55.11:27017\u0026#34; }, ...... {_id : 1, host : \u0026#34;10.211.55.12:27017\u0026#34; }, ...... {_id : 2, host : \u0026#34;10.211.55.13:27017\u0026#34; } ... ] ... } \u0026gt; rs.initiate(config) \u0026gt; rs.status() \u0026gt; exit 稍微等一会儿，复制集就初始化好了。\n以同样的方法安装配置分片shard2、shard3的复制集。\n初始化配置服务的复制集 这个过程跟初始化某一个分片的复制集类似，这里就直接贴配置命令了。\n在每台服务器上执行下面的命令：\ncat \u0026lt;\u0026lt;EOF \u0026gt; /etc/mongodb/conf/config.conf systemLog: destination: file logAppend: true path: /var/lib/mongodb/config/log/mongod.log # Where and how to store data. storage: dbPath: /var/lib/mongodb/config/data journal: enabled: true # how the process runs processManagement: fork: true # fork and run in background pidFilePath: /var/lib/mongodb/config/log/mongod.pid # location of pidfile # network interfaces net: port: 27077 bindIp: 0.0.0.0 # Listen to local interface only, comment to listen on all interfaces. replication: replSetName: config sharding: clusterRole: \u0026#34;configsvr\u0026#34; EOF cat \u0026lt;\u0026lt;EOF \u0026gt; /usr/lib/systemd/system/mongod-config.service [Unit] Description=MongoDB Database config Service Wants=network.target After=network.target [Service] Type=forking PIDFile=/var/lib/mongodb/config/log/mongod.pid ExecStart=/usr/bin/mongod -f /etc/mongodb/conf/config.conf ExecReload=/bin/kill -HUP $MAINPID Restart=always StandardOutput=syslog StandardError=syslog [Install] WantedBy=multi-user.target EOF systemctl enable mongod-config systemctl start mongod-config 在任意一台服务上初始化复制集配置：\n#连接 $ mongo --port 27077 \u0026gt; use admin switched to db admin \u0026gt; config = { ... _id : \u0026#34;config\u0026#34;, ... members : [ ...... {_id : 0, host : \u0026#34;10.211.55.11:27077\u0026#34; }, ...... {_id : 1, host : \u0026#34;10.211.55.12:27077\u0026#34; }, ...... {_id : 2, host : \u0026#34;10.211.55.13:27077\u0026#34; } ... ] ... } \u0026gt; rs.initiate(config) \u0026gt; rs.status() \u0026gt; exit 稍微等一会儿，复制集就初始化好了。\n配置路由服务器 最后配置路由服务器，比较简单，在每台服务器初始化其的配置文件并启动：\ncat \u0026lt;\u0026lt;EOF \u0026gt; /etc/mongodb/conf/mongos.conf systemLog: destination: file logAppend: true path: /var/lib/mongodb/mongos/log/mongod.log # how the process runs processManagement: fork: true # fork and run in background pidFilePath: /var/lib/mongodb/mongos/log/mongod.pid # location of pidfile # network interfaces net: port: 27088 bindIp: 0.0.0.0 # Listen to local interface only, comment to listen on all interfaces. sharding: configDB: \u0026#34;config/10.211.55.11:27077,10.211.55.12:27077,10.211.55.13:27077\u0026#34; EOF cat \u0026lt;\u0026lt;EOF \u0026gt; /usr/lib/systemd/system/mongos.service [Unit] Description=MongoDB Database mongos Service Wants=network.target After=network.target [Service] Type=forking PIDFile=/var/lib/mongodb/mongos/log/mongod.pid ExecStart=/usr/bin/mongos -f /etc/mongodb/conf/mongos.conf ExecReload=/bin/kill -HUP $MAINPID Restart=always StandardOutput=syslog StandardError=syslog [Install] WantedBy=multi-user.target EOF systemctl enable mongos systemctl start mongos 再在任意一台服务器上依次将3个分片加入到集群中：\n#连接 $ mongo --port 27088 \u0026gt; use admin switched to db admin \u0026gt; sh.addShard( \u0026#34;shard1/10.211.55.11:27017,10.211.55.12:27017,10.211.55.13:27017\u0026#34;) \u0026gt; sh.addShard( \u0026#34;shard2/10.211.55.11:27018,10.211.55.12:27018,10.211.55.13:27018\u0026#34;) \u0026gt; sh.addShard( \u0026#34;shard3/10.211.55.11:27019,10.211.55.12:27019,10.211.55.13:27019\u0026#34;) \u0026gt; sh.status() \u0026gt; exit 启用用户认证登录 创建一个超级管理员用户，在任意一台服务器上执行：\n$ mongo --port 27088 \u0026gt; use admin \u0026gt; db.createUser({ user: \u0026#34;superadmin\u0026#34;, pwd: \u0026#34;123456\u0026#34;, roles: [ { role: \u0026#34;userAdminAnyDatabase\u0026#34;, db: \u0026#34;admin\u0026#34; }, { role: \u0026#34;clusterManager\u0026#34;, db : \u0026#34;admin\u0026#34; } ], passwordDigestor: \u0026#34;server\u0026#34; }) \u0026gt; exit 创建mongod、mongos之间通信所使用的key文件，在任意一台服务器上执行：\nopenssl rand -base64 756 \u0026gt; /etc/mongodb/conf/mongo.key chmod 400 /etc/mongodb/conf/mongo.key # 将/etc/mongodb/conf/mongo.key文件拷贝到其它服务器上，保持文件权限不变 修改每台服务器上的/etc/mongodb/conf/shard*.conf、/etc/mongodb/conf/config.conf、/etc/mongodb/conf/mongos.conf，其中shard*.conf、config.conf文件中加入以下内容：\nsecurity: keyFile: \u0026#34;/etc/mongodb/conf/mongo.key\u0026#34; authorization: \u0026#34;enabled\u0026#34; mongos.conf文件中加入以下内容：\nsecurity: keyFile: \u0026#34;/etc/mongodb/conf/mongo.key\u0026#34; 在每台服务器上依次执行以下命令：\nsystemctl restart mognod-shard1 systemctl restart mognod-shard2 systemctl restart mognod-shard3 systemctl restart mognod-config systemctl restart mognos 至此，整个mongodb高可用集群就搭建好了。\n部署测试 先建一个database及user测试一下：\n# 创建一个database的访问用户 $ mongo --username superadmin --password 123456 --authenticationDatabase admin --port 27088 admin \u0026gt; use test \u0026gt; db.createUser({ user: \u0026#34;testadmin\u0026#34;, pwd: \u0026#34;123456\u0026#34;, roles: [ { role: \u0026#34;dbOwner\u0026#34;, db: \u0026#34;test\u0026#34; }, ], passwordDigestor: \u0026#34;server\u0026#34; }) \u0026gt; exit # 使用该用户访问database，并插入数据，创建索引 $ mongo --username testadmin --password 123456 --authenticationDatabase test --port 27088 test \u0026gt; db.col1.insert({\u0026#34;name\u0026#34;: \u0026#34;xxj\u0026#34;}) \u0026gt; db.col1.createIndex( { \u0026#34;name\u0026#34;: 1 } ) \u0026gt; exit # 启用该database的分片及对某个collection分片 $ mongo --username superadmin --password 123456 --authenticationDatabase admin --port 27088 admin \u0026gt; use admin \u0026gt; sh.enableSharding(\u0026#34;test\u0026#34;) \u0026gt; sh.shardCollection(\u0026#34;test.col1\u0026#34;, { \u0026#34;name\u0026#34; : 1 } ) \u0026gt; exit 总结 手工部署mongodb集群还是比较麻烦的，所以如果图省事儿，还是使用云厂商提供的PaaS服务好了，比如云数据库 MongoDB。如果一定要自己搭建，还是建议用现成的ansible-mongodb-cluster脚本好了。\n参考  https://zhuanlan.zhihu.com/p/28600032 https://gist.github.com/guileen/e2ebc1f7de2d2039fed2 https://gist.github.com/jwilm/5842956 https://docs.mongodb.com/manual/tutorial/enforce-keyfile-access-control-in-existing-replica-set/ https://docs.mongodb.com/manual/tutorial/deploy-shard-cluster/ https://docs.mongodb.com/manual/reference/method/js-collection/ https://docs.mongodb.com/manual/reference/method/js-sharding/ https://docs.mongodb.com/manual/reference/built-in-roles/#database-administration-roles  ","permalink":"https://jeremyxu2010.github.io/2018/10/mongodb%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/","tags":["mongodb","devop"],"title":"mongodb高可用集群部署"},{"categories":["容器编排"],"contents":"harbor基本上是目前企业级docker registry唯一的开源方案了，之前就有接触，不过一直是当成一个功能丰富的镜像registry来用，并没有深入了解其实现原理。最近认领了一个任务，会涉及harbor代码级开发，这里提前阅读一下其源代码，提前了解其实现原理及细节。\nharbor的架构 官方有一个框架图，如下：\n也简单说了下各组件完成的功能，如下：\n As depicted in the above diagram, Harbor comprises 6 components:\nProxy: Components of Harbor, such as registry, UI and token services, are all behind a reversed proxy. The proxy forwards requests from browsers and Docker clients to various backend services.\nRegistry: Responsible for storing Docker images and processing Docker push/pull commands. As Harbor needs to enforce access control to images, the Registry will direct clients to a token service to obtain a valid token for each pull or push request.\nCore services: Harbor’s core functions, which mainly provides the following services:\nUI: a graphical user interface to help users manage images on the Registry Webhook: Webhook is a mechanism configured in the Registry so that image status changes in the Registry can be populated to the Webhook endpoint of Harbor. Harbor uses webhook to update logs, initiate replications, and some other functions. Token service: Responsible for issuing a token for every docker push/pull command according to a user’s role of a project. If there is no token in a request sent from a Docker client, the Registry will redirect the request to the token service. Database: Database stores the meta data of projects, users, roles, replication policies and images.\nJob services: used for image replication, local images can be replicated(synchronized) to other Harbor instances.\nLog collector: Responsible for collecting logs of other modules in a single place.\n 大致浏览了下代码，上述说明基本也是对的，不过为了方便开发人员理解，下面我用更直接的说法描述一下：\nProxy：底层实际上就是跑了一个nginx的容器，向docker client及浏览器暴露端口，将这些客户端发来的请求反向代理到后端Core Services、Registry。\nRegistry：这个其实是就是官方的docker registry，其配置了webhook到Core Services，这样当镜像的状态发生变化时，可通知到Core Services了。\nCore Services：这个里面内容就比较多了，主要由多个http服务组成，完成的功能主要有以下几点：\n 监听Registry上镜像的变化，做相应处理，比如记录日志、发起复制等 充当Docker Authorization Service的角色，对镜像资源进行基于角色的鉴权 连接Database，提供存取projects、users、roles、replication policies和images元数据的API接口 提供UI界面  从目前的代码来看主要有这4个部分ui（这个感觉改名为controller好一点）、adminserver、registryctl、portal。\nJob Service：定时执行一些任务，提供API供外部提交任务及查询执行结果。\nLog collector：说白了就是一个rsyslog日志服务，其它组件可以将日志发送到这里，它负责集中存储。\nDatabase：就是mysql数据库服务，用于存储projects、users、roles、replication policies和images的元数据。\n编译harbor 直接参照官方给出的编译指南即可编译harbor。\n准备环境 由于我使用的是macOS系统，比较简单，就是安装docker、 docker-compose、 git、 golang。\nbrew install docker docker-compose git golang 编译代码 首先克隆代码：\nmkdir $GOPATH/src/github.com/goharbor/ git clone https://github.com/goharbor/harbor $GOPATH/src/github.com/goharbor/harbor 根据实际情况修改配置文件：\ncd $GOPATH/src/github.com/goharbor/harbor vi make/harbor.cfg 编译代码：\nmake install -e NOTARYFLAG=true CLAIRFLAG=true 因为我用的是macOS系统，官方的脚本在macOS系统执行会报一些，幸好找到有人给出了补丁，不过该补丁还没合到主干上，需要手工合并自己的工作区。\n第一次成功编译后，后面给make命令传入不同的环境变量及预定义target名称，即可完成各种CI任务了。为啥不接入标准的CI系统？ Makefile的使用说明参见这里。\n快速部署 这里使用官方提供的helm chart快速在k8s里进行部署。\n下载helm chart源代码：\ngit clone https://github.com/goharbor/harbor-helm cd harbor-helm 编辑values.yaml文件：\ncp values.yaml values_local.yaml vi values_local.yaml ... externalURL: https://harbor.k8s.local ... ingress: enabled: true hosts: core: harbor.k8s.local notary: notary.k8s.local ... tls: enabled: true # Fill the secretName if you want to use the certificate of # yourself when Harbor serves with HTTPS. A certificate will # be generated automatically by the chart if leave it empty secretName: \u0026#34;default-tls-secret\u0026#34; 仅修改了几处域名相关的配置项，同时HTTPS证书使用已经创建好了的default-tls-secret，创建default-tls-secret的办法可参考之前的文章。\n使用helm命令安装：\nhelm install . --namespace kube-system --name local-harbor -f values_local.yaml 然后就可以用浏览器访问https://harbor.k8s.local了。\n分析部署结构 看一下部署的结构：\n$ kubectl -n kube-system get deployment|grep local-harbor local-harbor-harbor-adminserver 1 1 1 1 9h local-harbor-harbor-chartmuseum 1 1 1 1 9h local-harbor-harbor-clair 1 1 1 1 9h local-harbor-harbor-jobservice 1 1 1 1 9h local-harbor-harbor-notary-server 1 1 1 1 9h local-harbor-harbor-notary-signer 1 1 1 1 9h local-harbor-harbor-portal 1 1 1 1 9h local-harbor-harbor-registry 1 1 1 1 9h local-harbor-harbor-ui 1 1 1 1 9h $ kubectl -n kube-system get statefulset|grep local-harbor local-harbor-harbor-database 1 1 9h local-harbor-redis-master 1 1 9h 可以看到总共有9个deployments和2个statefulsets，结构架构图及官方文档，总结这11个组件的作用如下：\n local-harbor-harbor-notary-server、local-harbor-harbor-notary-signer这两个deployments主要用于实现Docker Content Trust，界面上显示为给镜像签名。 local-harbor-harbor-database、local-harbor-redis-master这两个是存储组件，是数据库及redis缓存，helm chart也提供方案使用外部数据库及redis缓存，在values.yaml里简单配置一下就好。 local-harbor-harbor-jobservice就是架构图里的Job services了。 local-harbor-harbor-clair主要用于对镜像进行安全扫描。 local-harbor-harbor-registry就是架构图里的Registry了。 local-harbor-harbor-adminserver、local-harbor-harbor-chartmuseum、local-harbor-harbor-ui、local-harbor-harbor-portal在架构图里都属于Core Services，分别是用于系统配置管理、chart存储抽象层、harbor核心逻辑控制层、harbor web界面前端。  这11个组件的镜像封装脚本在$GOPATH/src/github.com/goharbor/harbor/make/photon目录里，以后要将业务应用封装成docker镜像，可以参考这些Dockerfile文件。\n这11个组件的k8s部署描述文件在$GOPATH/src/github.com/goharbor/harbor/make/kubernetes目录里，同样以后如果要将业务应用部署在k8s里，可以参考这里的描述文件。\n解读源码 主要代码位于$GOPATH/src/github.com/goharbor/harbor/src这个目录，这里将这几个目录逐个分析一下。\nui 这是个API聚合层，个人感觉改名为controller好一点。它是一个标准的API服务，主要完成以下功能：\n 监听Registry上镜像的变化，做相应处理，比如记录日志、发起复制等 充当Docker Authorization Service的角色，对镜像资源进行基于角色的鉴权 连接Database，提供存取projects、users、roles、replication policies和images元数据的API接口 提供UI界面、  首先从其入口方法$GOPATH/src/github.com/goharbor/harbor/src/ui/main.go看起，主要完成以下几步：\n  初始化beego框架的session、模板。\nbeego.BConfig.WebConfig.Session.SessionOn = true // TODO  redisURL := os.Getenv(\u0026#34;_REDIS_URL\u0026#34;) if len(redisURL) \u0026gt; 0 { gob.Register(models.User{}) beego.BConfig.WebConfig.Session.SessionProvider = \u0026#34;redis\u0026#34; beego.BConfig.WebConfig.Session.SessionProviderConfig = redisURL } beego.AddTemplateExt(\u0026#34;htm\u0026#34;)   初始化配置，注意配置是从adminserver得来，配置的管理由adminserver负责。\nlog.Info(\u0026#34;initializing configurations...\u0026#34;) if err := config.Init(); err != nil { log.Fatalf(\u0026#34;failed to initialize configurations: %v\u0026#34;, err) } log.Info(\u0026#34;configurations initialization completed\u0026#34;)   为多个服务初始化accessFilter，主要就是Notary和Registry。accessFilter就是对Registry和Notary的一些操作进行过滤处理，主要是根据角色进行一些权限约束，架构上参考Docker Authorization Service。详见这个$GOPATH/src/github.com/goharbor/harbor/src/ui/service/token/creator.go文件。\ntoken.InitCreators()   初始化数据库连接。\ndatabase, err := config.Database() if err != nil { log.Fatalf(\u0026#34;failed to get database configuration: %v\u0026#34;, err) } if err := dao.InitDatabase(database); err != nil { log.Fatalf(\u0026#34;failed to initialize database: %v\u0026#34;, err) }   从adminserver得到配置的管理员密码，更新到数据库。\npassword, err := config.InitialAdminPassword() if err != nil { log.Fatalf(\u0026#34;failed to get admin\u0026#39;s initia password: %v\u0026#34;, err) } if err := updateInitPassword(adminUserID, password); err != nil { log.Error(err) }   初始化一些controller对象，这里主要是chartcontroller。\n// Init API handler  if err := api.Init(); err != nil { log.Fatalf(\u0026#34;Failed to initialize API handlers with error: %s\u0026#34;, err.Error()) }   启动定时任务队列处理器。\n// Enable the policy scheduler here.  scheduler.DefaultScheduler.Start()   订阅一些Policy通知的topic，当决策发生变化时，作出相应处理。\n// Subscribe the policy change topic.  if err = notifier.Subscribe(notifier.ScanAllPolicyTopic, \u0026amp;notifier.ScanPolicyNotificationHandler{}); err != nil { log.Errorf(\u0026#34;failed to subscribe scan all policy change topic: %v\u0026#34;, err) }   如果启用了Clair，则初始化相关的数据库及触发定时扫描所有镜像的事件。Clair是用于扫描镜像风险扫描的解决方案，见这里。\nif config.WithClair() { clairDB, err := config.ClairDB() if err != nil { log.Fatalf(\u0026#34;failed to load clair database information: %v\u0026#34;, err) } if err := dao.InitClairDB(clairDB); err != nil { log.Fatalf(\u0026#34;failed to initialize clair database: %v\u0026#34;, err) } // Get policy configuration.  scanAllPolicy := config.ScanAllPolicy() if scanAllPolicy.Type == notifier.PolicyTypeDaily { dailyTime := 0 if t, ok := scanAllPolicy.Parm[\u0026#34;daily_time\u0026#34;]; ok { if reflect.TypeOf(t).Kind() == reflect.Int { dailyTime = t.(int) } } // Send notification to handle first policy change.  if err = notifier.Publish(notifier.ScanAllPolicyTopic, notifier.ScanPolicyNotification{Type: scanAllPolicy.Type, DailyTime: (int64)(dailyTime)}); err != nil { log.Errorf(\u0026#34;failed to publish scan all policy topic: %v\u0026#34;, err) } } }   初始化replication controller，通过replication controller可以操控Job Service完成镜像复制的功能。\nif err := core.Init(); err != nil { log.Errorf(\u0026#34;failed to initialize the replication controller: %v\u0026#34;, err) }   初始化一些过滤器，主要是一些安全相关的Filter。\nfilter.Init() beego.InsertFilter(\u0026#34;/*\u0026#34;, beego.BeforeRouter, filter.SecurityFilter) beego.InsertFilter(\u0026#34;/*\u0026#34;, beego.BeforeRouter, filter.ReadonlyFilter) beego.InsertFilter(\u0026#34;/api/*\u0026#34;, beego.BeforeRouter, filter.MediaTypeFilter(\u0026#34;application/json\u0026#34;, \u0026#34;multipart/form-data\u0026#34;, \u0026#34;application/octet-stream\u0026#34;))   初始化请求路由，请求路由见$GOPATH/src/github.com/goharbor/harbor/src/ui/router.go这个文件，其实大概扫一眼每个接口的名字，就知道其主要完成的功能。\ninitRouters()   将当前Registry里的镜像相关信息同步至数据库。\nsyncRegistry := os.Getenv(\u0026#34;SYNC_REGISTRY\u0026#34;) sync, err := strconv.ParseBool(syncRegistry) if err != nil { log.Errorf(\u0026#34;Failed to parse SYNC_REGISTRY: %v\u0026#34;, err) // if err set it default to false  sync = false } if sync { if err := api.SyncRegistry(config.GlobalProjectMgr); err != nil { log.Error(err) } } else { log.Infof(\u0026#34;Because SYNC_REGISTRY set false , no need to sync registry \\n\u0026#34;) }   初始化到Registry的反向代理，有官方Registry的基础上主要添加了安装相关的Handler，见$GOPATH/src/github.com/goharbor/harbor/src/ui/proxy/interceptors.go这个文件。\nlog.Info(\u0026#34;Init proxy\u0026#34;) proxy.Init()   启动beego http服务。\nbeego.Run()   这么一分析ui的逻辑还是比较清晰的，想了解哪一方面的功能，直接相关入口方法跟进去就可以了，大部分模块的代码就在2-3个go文件里实现了。\nadminserver adminserver模块比较简单，主要实现一些配置管理的API接口，从$GOPATH/src/github.com/goharbor/harbor/src/adminserver/handlers/router.go这个文件为入口跟踪一下代码就很清楚了。\nchartserver chartserver模块主要实现一些操作chart资源相关的API接口，由ui模块里的$GOPATH/src/github.com/goharbor/harbor/src/ui/api/base.go#Init调过来。\ncommon common模块里放了一些其它模块共用的代码，比如一些工具函数、一些通用的base结构体、一些DTO对象等。\njobservice jobservice主要提供一些执行任务的API接口，其它模块会调用它的接口调度定时任务。核心的入口代码里这里$GOPATH/src/github.com/goharbor/harbor/src/jobservice/runtime/bootstrap.go#LoadAndRun，这里大致解读一下这个方法的代码。\n  初始化job执行上下文。\n// Create the root context  ctx, cancel := context.WithCancel(context.Background()) defer cancel() rootContext := \u0026amp;env.Context{ SystemContext: ctx, WG: \u0026amp;sync.WaitGroup{}, ErrorChan: make(chan error, 1), // with 1 buffer  } // Build specified job context  if bs.jobConextInitializer != nil { if jobCtx, err := bs.jobConextInitializer(rootContext); err == nil { rootContext.JobContext = jobCtx } else { logger.Fatalf(\u0026#34;Failed to initialize job context: %s\\n\u0026#34;, err) } }   加载并运行任务工作池。\n// Start the pool  var ( backendPool pool.Interface wpErr error ) if config.DefaultConfig.PoolConfig.Backend == config.JobServicePoolBackendRedis { backendPool, wpErr = bs.loadAndRunRedisWorkerPool(rootContext, config.DefaultConfig) if wpErr != nil { logger.Fatalf(\u0026#34;Failed to load and run worker pool: %s\\n\u0026#34;, wpErr.Error()) } } else { logger.Fatalf(\u0026#34;Worker pool backend \u0026#39;%s\u0026#39; is not supported\u0026#34;, config.DefaultConfig.PoolConfig.Backend) } 可以看到其是将任务工作池的信息保存在redis里。    启动API接口HTTP服务。\n// Initialize controller  ctl := core.NewController(backendPool) // Start the API server  apiServer := bs.loadAndRunAPIServer(rootContext, config.DefaultConfig, ctl) logger.Infof(\u0026#34;Server is started at %s:%d with %s\u0026#34;, \u0026#34;\u0026#34;, config.DefaultConfig.Port, config.DefaultConfig.Protocol) 处理的API接口见$GOPATH/src/github.com/goharbor/harbor/src/jobservice/api/router.go#registerRoutes\n// registerRoutes adds routes to the server mux.  func (br *BaseRouter) registerRoutes() { subRouter := br.router.PathPrefix(fmt.Sprintf(\u0026#34;%s/%s\u0026#34;, baseRoute, apiVersion)).Subrouter() subRouter.HandleFunc(\u0026#34;/jobs\u0026#34;, br.handler.HandleLaunchJobReq).Methods(http.MethodPost) subRouter.HandleFunc(\u0026#34;/jobs/{job_id}\u0026#34;, br.handler.HandleGetJobReq).Methods(http.MethodGet) subRouter.HandleFunc(\u0026#34;/jobs/{job_id}\u0026#34;, br.handler.HandleJobActionReq).Methods(http.MethodPost) subRouter.HandleFunc(\u0026#34;/jobs/{job_id}/log\u0026#34;, br.handler.HandleJobLogReq).Methods(http.MethodGet) subRouter.HandleFunc(\u0026#34;/stats\u0026#34;, br.handler.HandleCheckStatusReq).Methods(http.MethodGet) } 可以看到，就是一些操纵job任务的API接口。    将一些老旧的日志文件删除。\n// Start outdated log files sweeper  logSweeper := logger.NewSweeper(ctx, config.GetLogBasePath(), config.GetLogArchivePeriod()) logSweeper.Start()   进程优雅退出处理。\n// To indicate if any errors occurred  var err error // Block here  sig := make(chan os.Signal, 1) signal.Notify(sig, os.Interrupt, syscall.SIGTERM, os.Kill) select { case \u0026lt;-sig: case err = \u0026lt;-rootContext.ErrorChan: } // Call cancel to send termination signal to other interested parts.  cancel() // Gracefully shutdown  apiServer.Stop() // In case stop is called before the server is ready  close := make(chan bool, 1) go func() { timer := time.NewTimer(10 * time.Second) defer timer.Stop() select { case \u0026lt;-timer.C: // Try again  apiServer.Stop() case \u0026lt;-close: return } }() rootContext.WG.Wait() close \u0026lt;- true if err != nil { logger.Fatalf(\u0026#34;Server exit with error: %s\\n\u0026#34;, err) } logger.Infof(\u0026#34;Server gracefully exit\u0026#34;)   registryctl registryctl主要提供一些操纵Registry的API接口，比较简单，从$GOPATH/src/github.com/goharbor/harbor/src/registryctl/handlers/router.go看起就可以了。\nfunc newRouter() http.Handler { r := mux.NewRouter() r.HandleFunc(\u0026#34;/api/registry/gc\u0026#34;, api.StartGC).Methods(\u0026#34;POST\u0026#34;) r.HandleFunc(\u0026#34;/api/health\u0026#34;, api.Health).Methods(\u0026#34;GET\u0026#34;) return r } 可以看到现在就实现了两个接口。\nreplication replication实现镜像复制的业务逻辑。从$GOPATH/src/github.com/goharbor/harbor/src/replication/core/controller.go这个文件查看代码，注意DefaultController这个结构体的方法，每个方法完成一个具体的任务，比如CreatePolicy方法会根据ReplicationPolicy决策，根据要进行的ReplicationTask写入数据库，并调用jobservice创建一个job任务。\nportal portal是用AngularJS写的前端界面，我这里比较关注后端，前端代码就不具体分析了。\n源码目录大概就这些内容了，还是比较清晰的。\n总结 整体来看harbor的代码还是比较清晰的，并没有像k8s一样采用各种设计模式封装代码，这个项目的代码涵盖了go语言Web应用开发、docker镜像制作、k8s部署、封装helm chart、Makefile编译脚本等一系列内容，作为一个开源项目，还是可以从中学到不少好东西的。\n参考  https://github.com/vmware/harbor/wiki/Architecture-Overview-of-Harbor https://github.com/goharbor/harbor/blob/master/docs/compile_guide.md https://github.com/goharbor/harbor  ","permalink":"https://jeremyxu2010.github.io/2018/09/harbor%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/","tags":["docker","go"],"title":"harbor源码解读"},{"categories":["容器编排"],"contents":"前几天写过一篇k8s加入TLS安全访问，其中说到用cfssl之类的工具手动生成TLS证书，这样就可以轻松搞定站点的https访问了。理想是很美好，但实际操作时却很痛苦，主要有以下几点缺陷：\n 如果k8s集群上部署的应用较多，要为每个应用的不同域名生成https证书，操作太麻烦。 上述这些手动操作没有跟k8s的deployment描述文件放在一起记录下来，很容易遗忘。 证书过期后，又得手动执行命令重新生成证书。  这样就迫切需要一个证书管理工具来完成以上需求。正好这几天浏览网站发现了cert-manager，一个k8s原生的证书管理控制器。\n cert-manager is a native Kubernetes certificate management controller. It can help with issuing certificates from a variety of sources, such as Let’s Encrypt, HashiCorp Vault, a simple signing keypair, or self signed.\nIt will ensure certificates are valid and up to date, and attempt to renew certificates at a configured time before expiry.\n 周末有一点时间正好研究一下。\ncert-manager的架构 上面是官方给出的架构图，可以看到cert-manager在k8s中定义了两个自定义类型资源：Issuer和Certificate。\n其中Issuer代表的是证书颁发者，可以定义各种提供者的证书颁发者，当前支持基于Letsencrypt、vault和CA的证书颁发者，还可以定义不同环境下的证书颁发者。\n而Certificate代表的是生成证书的请求，一般其中存入生成证书的元信息，如域名等等。\n一旦在k8s中定义了上述两类资源，部署的cert-manager则会根据Issuer和Certificate生成TLS证书，并将证书保存进k8s的Secret资源中，然后在Ingress资源中就可以引用到这些生成的Secret资源。对于已经生成的证书，还是定期检查证书的有效期，如即将超过有效期，还会自动续期。\n把玩一下 部署cert-manager 部署cert-manager还是比较简单的，直接用helm部署就可以了：\nhelm install \\  --name cert-manager \\  --namespace kube-system \\  stable/cert-manager 创建Issuer资源 由于我试验环境是个人电脑，不能被外网访问，因此无法试验Letsencrypt类型的证书颁发者，而vault貌似部署起来很是麻烦，所以还是创建一个简单的CA类型Issuer资源。\n首先将根CA的key及证书文件存入一个secret中：\nkubectl create secret tls ca-key-pair \\  --cert=ca.pem \\  --key=ca-key.pem \\  --namespace=kube-system 上述操作中的ca.pem, ca-key.pem文件还是用cfssl命令生成的。\n然后创建Issuer资源：\ncat \u0026lt;\u0026lt; EOF | kubectl create -f - apiVersion: certmanager.k8s.io/v1alpha1 kind: ClusterIssuer metadata: name: ca-issuer namespace: kube-system spec: ca: secretName: ca-key-pair EOF 注意这里创建的资源类型是ClusterIssuer，这样这个证书颁发者就可以为整个集群中任意命名空间颁发证书。\n关于ClusterIssuer与Issuer的区别可以查阅这里。\n创建Certificate资源 然后创建Certificate资源：\ncat \u0026lt;\u0026lt; EOF | kubectl create -f - apiVersion: certmanager.k8s.io/v1alpha1 kind: Certificate metadata: name: example-com namespace: kube-system spec: secretName: example-com-tls issuerRef: name: ca-issuer # We can reference ClusterIssuers by changing the kind here. # The default value is Issuer (i.e. a locally namespaced Issuer) kind: ClusterIssuer commonName: example.com dnsNames: - example.com - www.example.com EOF 稍等一会儿，就可以查询到cert-manager生成的证书secret：\n\u0026gt; kubectl -n kube-system describe secret example-com-tls Name: example-com-tls Namespace: kube-system Labels: certmanager.k8s.io/certificate-name=example-com Annotations: certmanager.k8s.io/alt-names=example.com,www.example.com certmanager.k8s.io/common-name=example.com certmanager.k8s.io/issuer-kind=ClusterIssuer certmanager.k8s.io/issuer-name=ca-issuer Type: kubernetes.io/tls Data ==== tls.crt: 2721 bytes tls.key: 1679 bytes 然后就可以在Ingress资源里引用该Secret了，如下：\napiVersion: extensions/v1beta1 kind: Ingress .... spec: tls: - hosts: - www.example.com secretName: example-com-tls rules: ... 使用建议 实际生产环境中使用cert-manager可以考虑以下建议：\n 将CA的Secret及Issuer放在某个独立的命名空间中，与其它业务的命名空间隔离起来。 如果是CA类型的Issuer，要记得定期更新根CA证书。 如果服务可被公网访问，同时又不想花钱买域名证书，可以采用Letsencrypt类型的Issuer，目前支持两种方式验证域名的所有权，基于DNS记录的验证方案和基于文件的HTTP验证方案。 cert-manager还提供ingress-shim方式，自动为Ingress资源生成证书，只需要在Ingress资源上打上一些标签即可，很方便有木有，详细可参考这里。  总结 cert-manager要完成的功能不复杂，但恰好解决了原来比较麻烦的手工操作，因此还是带来的挺大价值的，所以说做产品功能不需要搞太高深的技术，只要解决刚需问题即可。\n参考  https://cert-manager.readthedocs.io/en/latest/getting-started/2-installing.html https://cert-manager.readthedocs.io/en/latest/tutorials/ca/creating-ca-issuer.html  ","permalink":"https://jeremyxu2010.github.io/2018/08/k8s%E4%B8%AD%E4%BD%BF%E7%94%A8cert-manager%E7%8E%A9%E8%BD%AC%E8%AF%81%E4%B9%A6/","tags":["kubernetes","tls"],"title":"k8s中使用cert-manager玩转证书"},{"categories":["容器编排"],"contents":"最近准备阅读一下k8s的源码，为了辅助理解代码运行逻辑，顺手搭一个k8s的开发调试环境，后面就可以结合断点调试掌握代码的运行脉络。\n准备虚拟机 虽然本机有k8s环境，但不建议直接在此环境做实验，还是创建一个CentOS7的虚拟机，为其分配1核CPU和4G内存（编译k8s要使用大量内存）。\n修改yum源 cp -f /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak\rsed -i -e \u0026#39;s/^mirrorlist=/#mirrorlist=/g\u0026#39; /etc/yum.repos.d/CentOS-Base.repo\rsed -i -e \u0026#39;s/^#baseurl=http:\\/\\/[^\\/]*\\/$/baseurl=http:\\/\\/mirror-sng.oa.com\\//g\u0026#39; /etc/yum.repos.d/CentOS-Base.repo\ryum install -y http://mirror-sng.oa.com/epel/epel-release-latest-7.noarch.rpm\rcp -f /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel.repo.bak\rsed -i -e \u0026#39;s/^metalink/#metalink/g\u0026#39; /etc/yum.repos.d/epel.repo\rsed -i -e \u0026#39;s/^#baseurl=http:\\/\\/[^\\/]*\\/pub\\//baseurl=http:\\/\\/mirror-sng.oa.com\\//g\u0026#39; /etc/yum.repos.d/epel.repo\r安装必要软件 参考这里，在虚拟机中安装必要的软件\nyum install -y docker wget curl vim golang etcd openssl git\rsystemctl enable docker \u0026amp;\u0026amp; systemctl start docker\recho \u0026#34;export GOPATH=$HOME/go\rexport PATH=$GOPATH/bin:$PATH\u0026#34; \u0026gt;\u0026gt; ~/.bashrc\rsource ~/.bashrc\rgo get -u -v github.com/cloudflare/cfssl/cmd/...\r安装delve调试工具 go get -u -v github.com/derekparker/delve/cmd/dlv\r准备源码及编译出调试版本 clone代码 git clone https://github.com/kubernetes/kubernetes.git $GOPATH/src/k8s.io/kubernetes\rcd $GOPATH/src/k8s.io/kubernetes\rgit checkout -b v1.10.3 v1.10.3 # 切换到v1.10.3分支\r参考这里，直接执行hack/local-up-cluster.sh脚本即可启动一个轻量的k8s集群，但由于 local-up-cluster.sh 默认使用的是 hyperkube 镜像在容器中启动 k8s 服务，这样不方便使用dlv调试代码，这里修改local-up-cluster.sh脚本，直接编译出带调试信息的可执行文件，并在主机中启动k8s相关进程。\n修改单机集群启动脚本 cp hack/local-up-cluster.sh hack/local-up-cluster.sh.bak\rtouch hack/local-up-cluster.sh \u0026amp;\u0026amp; chmod +x hack/local-up-cluster.sh\rhack/local-up-cluster.sh的内容如下：\n#!/bin/bash\r\r# Copyright 2014 The Kubernetes Authors.\r#\r# Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;);\r# you may not use this file except in compliance with the License.\r# You may obtain a copy of the License at\r#\r# http://www.apache.org/licenses/LICENSE-2.0\r#\r# Unless required by applicable law or agreed to in writing, software\r# distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS,\r# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r# See the License for the specific language governing permissions and\r# limitations under the License.\rKUBE_ROOT=$(dirname \u0026#34;${BASH_SOURCE}\u0026#34;)/..\r# This command builds and runs a local kubernetes cluster.\r# You may need to run this as root to allow kubelet to open docker\u0026#39;s socket,\r# and to write the test CA in /var/run/kubernetes.\rDOCKER_OPTS=${DOCKER_OPTS:-\u0026#34;\u0026#34;}\rDOCKER=(docker ${DOCKER_OPTS})\rDOCKERIZE_KUBELET=${DOCKERIZE_KUBELET:-\u0026#34;\u0026#34;}\rALLOW_PRIVILEGED=${ALLOW_PRIVILEGED:-\u0026#34;\u0026#34;}\rDENY_SECURITY_CONTEXT_ADMISSION=${DENY_SECURITY_CONTEXT_ADMISSION:-\u0026#34;\u0026#34;}\rPSP_ADMISSION=${PSP_ADMISSION:-\u0026#34;\u0026#34;}\rNODE_ADMISSION=${NODE_ADMISSION:-\u0026#34;\u0026#34;}\rRUNTIME_CONFIG=${RUNTIME_CONFIG:-\u0026#34;\u0026#34;}\rKUBELET_AUTHORIZATION_WEBHOOK=${KUBELET_AUTHORIZATION_WEBHOOK:-\u0026#34;\u0026#34;}\rKUBELET_AUTHENTICATION_WEBHOOK=${KUBELET_AUTHENTICATION_WEBHOOK:-\u0026#34;\u0026#34;}\rPOD_MANIFEST_PATH=${POD_MANIFEST_PATH:-\u0026#34;/var/run/kubernetes/static-pods\u0026#34;}\rKUBELET_FLAGS=${KUBELET_FLAGS:-\u0026#34;\u0026#34;}\r# many dev environments run with swap on, so we don\u0026#39;t fail in this env\rFAIL_SWAP_ON=${FAIL_SWAP_ON:-\u0026#34;false\u0026#34;}\r# Name of the network plugin, eg: \u0026#34;kubenet\u0026#34;\rNET_PLUGIN=${NET_PLUGIN:-\u0026#34;\u0026#34;}\r# Place the config files and binaries required by NET_PLUGIN in these directory,\r# eg: \u0026#34;/etc/cni/net.d\u0026#34; for config files, and \u0026#34;/opt/cni/bin\u0026#34; for binaries.\rCNI_CONF_DIR=${CNI_CONF_DIR:-\u0026#34;\u0026#34;}\rCNI_BIN_DIR=${CNI_BIN_DIR:-\u0026#34;\u0026#34;}\rSERVICE_CLUSTER_IP_RANGE=${SERVICE_CLUSTER_IP_RANGE:-10.0.0.0/24}\rFIRST_SERVICE_CLUSTER_IP=${FIRST_SERVICE_CLUSTER_IP:-10.0.0.1}\r# if enabled, must set CGROUP_ROOT\rCGROUPS_PER_QOS=${CGROUPS_PER_QOS:-true}\r# name of the cgroup driver, i.e. cgroupfs or systemd\rCGROUP_DRIVER=${CGROUP_DRIVER:-\u0026#34;\u0026#34;}\r# owner of client certs, default to current user if not specified\rUSER=${USER:-$(whoami)}\r# enables testing eviction scenarios locally.\rEVICTION_HARD=${EVICTION_HARD:-\u0026#34;memory.available\u0026lt;100Mi,nodefs.available\u0026lt;10%,nodefs.inodesFree\u0026lt;5%\u0026#34;}\rEVICTION_SOFT=${EVICTION_SOFT:-\u0026#34;\u0026#34;}\rEVICTION_PRESSURE_TRANSITION_PERIOD=${EVICTION_PRESSURE_TRANSITION_PERIOD:-\u0026#34;1m\u0026#34;}\r# This script uses docker0 (or whatever container bridge docker is currently using)\r# and we don\u0026#39;t know the IP of the DNS pod to pass in as --cluster-dns.\r# To set this up by hand, set this flag and change DNS_SERVER_IP.\r# Note also that you need API_HOST (defined above) for correct DNS.\rKUBE_PROXY_MODE=${KUBE_PROXY_MODE:-\u0026#34;\u0026#34;}\rENABLE_CLUSTER_DNS=${KUBE_ENABLE_CLUSTER_DNS:-true}\rDNS_SERVER_IP=${KUBE_DNS_SERVER_IP:-10.0.0.10}\rDNS_DOMAIN=${KUBE_DNS_NAME:-\u0026#34;cluster.local\u0026#34;}\rKUBECTL=${KUBECTL:-cluster/kubectl.sh}\rWAIT_FOR_URL_API_SERVER=${WAIT_FOR_URL_API_SERVER:-60}\rENABLE_DAEMON=${ENABLE_DAEMON:-false}\rHOSTNAME_OVERRIDE=${HOSTNAME_OVERRIDE:-\u0026#34;127.0.0.1\u0026#34;}\rEXTERNAL_CLOUD_PROVIDER=${EXTERNAL_CLOUD_PROVIDER:-false}\rEXTERNAL_CLOUD_PROVIDER_BINARY=${EXTERNAL_CLOUD_PROVIDER_BINARY:-\u0026#34;\u0026#34;}\rCLOUD_PROVIDER=${CLOUD_PROVIDER:-\u0026#34;\u0026#34;}\rCLOUD_CONFIG=${CLOUD_CONFIG:-\u0026#34;\u0026#34;}\rFEATURE_GATES=${FEATURE_GATES:-\u0026#34;AllAlpha=false\u0026#34;}\rSTORAGE_BACKEND=${STORAGE_BACKEND:-\u0026#34;etcd3\u0026#34;}\r# enable swagger ui\rENABLE_SWAGGER_UI=${ENABLE_SWAGGER_UI:-false}\r# enable Pod priority and preemption\rENABLE_POD_PRIORITY_PREEMPTION=${ENABLE_POD_PRIORITY_PREEMPTION:-\u0026#34;\u0026#34;}\r# enable kubernetes dashboard\rENABLE_CLUSTER_DASHBOARD=${KUBE_ENABLE_CLUSTER_DASHBOARD:-false}\r# enable audit log\rENABLE_APISERVER_BASIC_AUDIT=${ENABLE_APISERVER_BASIC_AUDIT:-false}\r# RBAC Mode options\rAUTHORIZATION_MODE=${AUTHORIZATION_MODE:-\u0026#34;Node,RBAC\u0026#34;}\rKUBECONFIG_TOKEN=${KUBECONFIG_TOKEN:-\u0026#34;\u0026#34;}\rAUTH_ARGS=${AUTH_ARGS:-\u0026#34;\u0026#34;}\r# Install a default storage class (enabled by default)\rDEFAULT_STORAGE_CLASS=${KUBE_DEFAULT_STORAGE_CLASS:-true}\r# start the cache mutation detector by default so that cache mutators will be found\rKUBE_CACHE_MUTATION_DETECTOR=\u0026#34;${KUBE_CACHE_MUTATION_DETECTOR:-true}\u0026#34;\rexport KUBE_CACHE_MUTATION_DETECTOR\r# panic the server on watch decode errors since they are considered coder mistakes\rKUBE_PANIC_WATCH_DECODE_ERROR=\u0026#34;${KUBE_PANIC_WATCH_DECODE_ERROR:-true}\u0026#34;\rexport KUBE_PANIC_WATCH_DECODE_ERROR\rENABLE_ADMISSION_PLUGINS=${ENABLE_ADMISSION_PLUGINS:-\u0026#34;\u0026#34;}\rDISABLE_ADMISSION_PLUGINS=${DISABLE_ADMISSION_PLUGINS:-\u0026#34;\u0026#34;}\rADMISSION_CONTROL_CONFIG_FILE=${ADMISSION_CONTROL_CONFIG_FILE:-\u0026#34;\u0026#34;}\r# START_MODE can be \u0026#39;all\u0026#39;, \u0026#39;kubeletonly\u0026#39;, or \u0026#39;nokubelet\u0026#39;\rSTART_MODE=${START_MODE:-\u0026#34;all\u0026#34;}\r# A list of controllers to enable\rKUBE_CONTROLLERS=\u0026#34;${KUBE_CONTROLLERS:-\u0026#34;*\u0026#34;}\u0026#34;\r# sanity check for OpenStack provider\rif [ \u0026#34;${CLOUD_PROVIDER}\u0026#34; == \u0026#34;openstack\u0026#34; ]; then\rif [ \u0026#34;${CLOUD_CONFIG}\u0026#34; == \u0026#34;\u0026#34; ]; then\recho \u0026#34;Missing CLOUD_CONFIG env for OpenStack provider!\u0026#34;\rexit 1\rfi\rif [ ! -f \u0026#34;${CLOUD_CONFIG}\u0026#34; ]; then\recho \u0026#34;Cloud config ${CLOUD_CONFIG}doesn\u0026#39;t exist\u0026#34;\rexit 1\rfi\rfi\r# set feature gates if using ipvs mode\rif [ \u0026#34;${KUBE_PROXY_MODE}\u0026#34; == \u0026#34;ipvs\u0026#34; ]; then\r# If required kernel modules are not available, fall back to iptables.\rsudo modprobe -a ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh nf_conntrack_ipv4\rif [[ $? -eq 0 ]]; then\rFEATURE_GATES=\u0026#34;${FEATURE_GATES},SupportIPVSProxyMode=true\u0026#34;\relse\recho \u0026#34;Required kernel modules for ipvs not found. Falling back to iptables mode.\u0026#34;\rKUBE_PROXY_MODE=iptables\rfi\rfi\r# set feature gates if enable Pod priority and preemption\rif [ \u0026#34;${ENABLE_POD_PRIORITY_PREEMPTION}\u0026#34; == true ]; then\rFEATURE_GATES=\u0026#34;$FEATURE_GATES,PodPriority=true\u0026#34;\rfi\r# warn if users are running with swap allowed\rif [ \u0026#34;${FAIL_SWAP_ON}\u0026#34; == \u0026#34;false\u0026#34; ]; then\recho \u0026#34;WARNING : The kubelet is configured to not fail if swap is enabled; production deployments should disable swap.\u0026#34;\rfi\rif [ \u0026#34;$(id -u)\u0026#34; != \u0026#34;0\u0026#34; ]; then\recho \u0026#34;WARNING : This script MAY be run as root for docker socket / iptables functionality; if failures occur, retry as root.\u0026#34; 2\u0026gt;\u0026amp;1\rfi\r# Stop right away if the build fails\rset -e\rsource \u0026#34;${KUBE_ROOT}/hack/lib/init.sh\u0026#34;\rfunction usage {\recho \u0026#34;This script starts a local kube cluster. \u0026#34;\recho \u0026#34;Example 0: hack/local-up-cluster.sh -h (this \u0026#39;help\u0026#39; usage description)\u0026#34;\recho \u0026#34;Example 1: hack/local-up-cluster.sh -o _output/dockerized/bin/linux/amd64/ (run from docker output)\u0026#34;\recho \u0026#34;Example 2: hack/local-up-cluster.sh -O (auto-guess the bin path for your platform)\u0026#34;\recho \u0026#34;Example 3: hack/local-up-cluster.sh (build a local copy of the source)\u0026#34;\r}\r# This function guesses where the existing cached binary build is for the `-O`\r# flag\rfunction guess_built_binary_path {\r#local hyperkube_path=$(kube::util::find-binary \u0026#34;hyperkube\u0026#34;)\rlocal hyperkube_path=$(kube::util::find-binary \u0026#34;kube-apiserver\u0026#34;)\rif [[ -z \u0026#34;${hyperkube_path}\u0026#34; ]]; then\rreturn\rfi\recho -n \u0026#34;$(dirname \u0026#34;${hyperkube_path}\u0026#34;)\u0026#34;\r}\r### Allow user to supply the source directory.\rGO_OUT=${GO_OUT:-}\rwhile getopts \u0026#34;ho:O\u0026#34; OPTION\rdo\rcase $OPTION in\ro)\recho \u0026#34;skipping build\u0026#34;\rGO_OUT=\u0026#34;$OPTARG\u0026#34;\recho \u0026#34;using source $GO_OUT\u0026#34;\r;;\rO)\rGO_OUT=$(guess_built_binary_path)\rif [ \u0026#34;$GO_OUT\u0026#34; == \u0026#34;\u0026#34; ]; then\recho \u0026#34;Could not guess the correct output directory to use.\u0026#34;\rexit 1\rfi\r;;\rh)\rusage\rexit\r;;\r?)\rusage\rexit\r;;\resac\rdone\rif [ \u0026#34;x$GO_OUT\u0026#34; == \u0026#34;x\u0026#34; ]; then\r#make -C \u0026#34;${KUBE_ROOT}\u0026#34; WHAT=\u0026#34;cmd/kubectl cmd/hyperkube\u0026#34;\rmake -C \u0026#34;${KUBE_ROOT}\u0026#34; GOGCFLAGS=\u0026#34;-N -l\u0026#34; WHAT=\u0026#34;cmd/kubectl cmd/kube-proxy cmd/kube-apiserver cmd/kube-controller-manager cmd/cloud-controller-manager cmd/kube-scheduler cmd/kubelet\u0026#34;\relse\recho \u0026#34;skipped the build.\u0026#34;\rfi\rfunction test_rkt {\rif [[ -n \u0026#34;${RKT_PATH}\u0026#34; ]]; then\r${RKT_PATH} list 2\u0026gt; /dev/null 1\u0026gt; /dev/null\rif [ \u0026#34;$?\u0026#34; != \u0026#34;0\u0026#34; ]; then\recho \u0026#34;Failed to successfully run \u0026#39;rkt list\u0026#39;, please verify that ${RKT_PATH}is the path of rkt binary.\u0026#34;\rexit 1\rfi\relse\rrkt list 2\u0026gt; /dev/null 1\u0026gt; /dev/null\rif [ \u0026#34;$?\u0026#34; != \u0026#34;0\u0026#34; ]; then\recho \u0026#34;Failed to successfully run \u0026#39;rkt list\u0026#39;, please verify that rkt is in \\$PATH.\u0026#34;\rexit 1\rfi\rfi\r}\r# Shut down anyway if there\u0026#39;s an error.\rset +e\rAPI_PORT=${API_PORT:-8080}\rAPI_SECURE_PORT=${API_SECURE_PORT:-6443}\r# WARNING: For DNS to work on most setups you should export API_HOST as the docker0 ip address,\rAPI_HOST=${API_HOST:-localhost}\rAPI_HOST_IP=${API_HOST_IP:-\u0026#34;127.0.0.1\u0026#34;}\rADVERTISE_ADDRESS=${ADVERTISE_ADDRESS:-\u0026#34;\u0026#34;}\rAPI_BIND_ADDR=${API_BIND_ADDR:-\u0026#34;0.0.0.0\u0026#34;}\rEXTERNAL_HOSTNAME=${EXTERNAL_HOSTNAME:-localhost}\rKUBELET_HOST=${KUBELET_HOST:-\u0026#34;127.0.0.1\u0026#34;}\r# By default only allow CORS for requests on localhost\rAPI_CORS_ALLOWED_ORIGINS=${API_CORS_ALLOWED_ORIGINS:-/127.0.0.1(:[0-9]+)?$,/localhost(:[0-9]+)?$}\rKUBELET_PORT=${KUBELET_PORT:-10250}\rLOG_LEVEL=${LOG_LEVEL:-3}\r# Use to increase verbosity on particular files, e.g. LOG_SPEC=token_controller*=5,other_controller*=4\rLOG_SPEC=${LOG_SPEC:-\u0026#34;\u0026#34;}\rLOG_DIR=${LOG_DIR:-\u0026#34;/tmp\u0026#34;}\rCONTAINER_RUNTIME=${CONTAINER_RUNTIME:-\u0026#34;docker\u0026#34;}\rCONTAINER_RUNTIME_ENDPOINT=${CONTAINER_RUNTIME_ENDPOINT:-\u0026#34;\u0026#34;}\rIMAGE_SERVICE_ENDPOINT=${IMAGE_SERVICE_ENDPOINT:-\u0026#34;\u0026#34;}\rRKT_PATH=${RKT_PATH:-\u0026#34;\u0026#34;}\rRKT_STAGE1_IMAGE=${RKT_STAGE1_IMAGE:-\u0026#34;\u0026#34;}\rCHAOS_CHANCE=${CHAOS_CHANCE:-0.0}\rCPU_CFS_QUOTA=${CPU_CFS_QUOTA:-true}\rENABLE_HOSTPATH_PROVISIONER=${ENABLE_HOSTPATH_PROVISIONER:-\u0026#34;false\u0026#34;}\rCLAIM_BINDER_SYNC_PERIOD=${CLAIM_BINDER_SYNC_PERIOD:-\u0026#34;15s\u0026#34;} # current k8s default\rENABLE_CONTROLLER_ATTACH_DETACH=${ENABLE_CONTROLLER_ATTACH_DETACH:-\u0026#34;true\u0026#34;} # current default\rKEEP_TERMINATED_POD_VOLUMES=${KEEP_TERMINATED_POD_VOLUMES:-\u0026#34;true\u0026#34;}\r# This is the default dir and filename where the apiserver will generate a self-signed cert\r# which should be able to be used as the CA to verify itself\rCERT_DIR=${CERT_DIR:-\u0026#34;/var/run/kubernetes\u0026#34;}\rROOT_CA_FILE=${CERT_DIR}/server-ca.crt\rROOT_CA_KEY=${CERT_DIR}/server-ca.key\rCLUSTER_SIGNING_CERT_FILE=${CLUSTER_SIGNING_CERT_FILE:-\u0026#34;${ROOT_CA_FILE}\u0026#34;}\rCLUSTER_SIGNING_KEY_FILE=${CLUSTER_SIGNING_KEY_FILE:-\u0026#34;${ROOT_CA_KEY}\u0026#34;}\r# name of the cgroup driver, i.e. cgroupfs or systemd\rif [[ ${CONTAINER_RUNTIME} == \u0026#34;docker\u0026#34; ]]; then\r# default cgroup driver to match what is reported by docker to simplify local development\rif [[ -z ${CGROUP_DRIVER} ]]; then\r# match driver with docker runtime reported value (they must match)\rCGROUP_DRIVER=$(docker info | grep \u0026#34;Cgroup Driver:\u0026#34; | cut -f3- -d\u0026#39; \u0026#39;)\recho \u0026#34;Kubelet cgroup driver defaulted to use: ${CGROUP_DRIVER}\u0026#34;\rfi\rfi\r# Ensure CERT_DIR is created for auto-generated crt/key and kubeconfig\rmkdir -p \u0026#34;${CERT_DIR}\u0026#34; \u0026amp;\u0026gt;/dev/null || sudo mkdir -p \u0026#34;${CERT_DIR}\u0026#34;\rCONTROLPLANE_SUDO=$(test -w \u0026#34;${CERT_DIR}\u0026#34; || echo \u0026#34;sudo -E\u0026#34;)\rfunction test_apiserver_off {\r# For the common local scenario, fail fast if server is already running.\r# this can happen if you run local-up-cluster.sh twice and kill etcd in between.\rif [[ \u0026#34;${API_PORT}\u0026#34; -gt \u0026#34;0\u0026#34; ]]; then\rcurl --silent -g $API_HOST:$API_PORT\rif [ ! $? -eq 0 ]; then\recho \u0026#34;API SERVER insecure port is free, proceeding...\u0026#34;\relse\recho \u0026#34;ERROR starting API SERVER, exiting. Some process on $API_HOSTis serving already on $API_PORT\u0026#34;\rexit 1\rfi\rfi\rcurl --silent -k -g $API_HOST:$API_SECURE_PORT\rif [ ! $? -eq 0 ]; then\recho \u0026#34;API SERVER secure port is free, proceeding...\u0026#34;\relse\recho \u0026#34;ERROR starting API SERVER, exiting. Some process on $API_HOSTis serving already on $API_SECURE_PORT\u0026#34;\rexit 1\rfi\r}\rfunction detect_binary {\r# Detect the OS name/arch so that we can find our binary\rcase \u0026#34;$(uname -s)\u0026#34; in\rDarwin)\rhost_os=darwin\r;;\rLinux)\rhost_os=linux\r;;\r*)\recho \u0026#34;Unsupported host OS. Must be Linux or Mac OS X.\u0026#34; \u0026gt;\u0026amp;2\rexit 1\r;;\resac\rcase \u0026#34;$(uname -m)\u0026#34; in\rx86_64*)\rhost_arch=amd64\r;;\ri?86_64*)\rhost_arch=amd64\r;;\ramd64*)\rhost_arch=amd64\r;;\raarch64*)\rhost_arch=arm64\r;;\rarm64*)\rhost_arch=arm64\r;;\rarm*)\rhost_arch=arm\r;;\ri?86*)\rhost_arch=x86\r;;\rs390x*)\rhost_arch=s390x\r;;\rppc64le*)\rhost_arch=ppc64le\r;;\r*)\recho \u0026#34;Unsupported host arch. Must be x86_64, 386, arm, arm64, s390x or ppc64le.\u0026#34; \u0026gt;\u0026amp;2\rexit 1\r;;\resac\rGO_OUT=\u0026#34;${KUBE_ROOT}/_output/local/bin/${host_os}/${host_arch}\u0026#34;\r}\rcleanup_dockerized_kubelet()\r{\rif [[ -e $KUBELET_CIDFILE ]]; then\rdocker kill $(\u0026lt;$KUBELET_CIDFILE) \u0026gt; /dev/null\rrm -f $KUBELET_CIDFILE\rfi\r}\rcleanup()\r{\recho \u0026#34;Cleaning up...\u0026#34;\r# delete running images\r# if [[ \u0026#34;${ENABLE_CLUSTER_DNS}\u0026#34; == true ]]; then\r# Still need to figure why this commands throw an error: Error from server: client: etcd cluster is unavailable or misconfigured\r# ${KUBECTL} --namespace=kube-system delete service kube-dns\r# And this one hang forever:\r# ${KUBECTL} --namespace=kube-system delete rc kube-dns-v10\r# fi\r# Check if the API server is still running\r[[ -n \u0026#34;${APISERVER_PID-}\u0026#34; ]] \u0026amp;\u0026amp; APISERVER_PIDS=$(pgrep -P ${APISERVER_PID} ; ps -o pid= -p ${APISERVER_PID})\r[[ -n \u0026#34;${APISERVER_PIDS-}\u0026#34; ]] \u0026amp;\u0026amp; sudo kill ${APISERVER_PIDS}\r# Check if the controller-manager is still running\r[[ -n \u0026#34;${CTLRMGR_PID-}\u0026#34; ]] \u0026amp;\u0026amp; CTLRMGR_PIDS=$(pgrep -P ${CTLRMGR_PID} ; ps -o pid= -p ${CTLRMGR_PID})\r[[ -n \u0026#34;${CTLRMGR_PIDS-}\u0026#34; ]] \u0026amp;\u0026amp; sudo kill ${CTLRMGR_PIDS}\rif [[ -n \u0026#34;$DOCKERIZE_KUBELET\u0026#34; ]]; then\rcleanup_dockerized_kubelet\relse\r# Check if the kubelet is still running\r[[ -n \u0026#34;${KUBELET_PID-}\u0026#34; ]] \u0026amp;\u0026amp; KUBELET_PIDS=$(pgrep -P ${KUBELET_PID} ; ps -o pid= -p ${KUBELET_PID})\r[[ -n \u0026#34;${KUBELET_PIDS-}\u0026#34; ]] \u0026amp;\u0026amp; sudo kill ${KUBELET_PIDS}\rfi\r# Check if the proxy is still running\r[[ -n \u0026#34;${PROXY_PID-}\u0026#34; ]] \u0026amp;\u0026amp; PROXY_PIDS=$(pgrep -P ${PROXY_PID} ; ps -o pid= -p ${PROXY_PID})\r[[ -n \u0026#34;${PROXY_PIDS-}\u0026#34; ]] \u0026amp;\u0026amp; sudo kill ${PROXY_PIDS}\r# Check if the scheduler is still running\r[[ -n \u0026#34;${SCHEDULER_PID-}\u0026#34; ]] \u0026amp;\u0026amp; SCHEDULER_PIDS=$(pgrep -P ${SCHEDULER_PID} ; ps -o pid= -p ${SCHEDULER_PID})\r[[ -n \u0026#34;${SCHEDULER_PIDS-}\u0026#34; ]] \u0026amp;\u0026amp; sudo kill ${SCHEDULER_PIDS}\r# Check if the etcd is still running\r[[ -n \u0026#34;${ETCD_PID-}\u0026#34; ]] \u0026amp;\u0026amp; kube::etcd::stop\r[[ -n \u0026#34;${ETCD_DIR-}\u0026#34; ]] \u0026amp;\u0026amp; kube::etcd::clean_etcd_dir\rexit 0\r}\rfunction warning {\rmessage=$1\recho $(tput bold)$(tput setaf 1)\recho \u0026#34;WARNING: ${message}\u0026#34;\recho $(tput sgr0)\r}\rfunction start_etcd {\recho \u0026#34;Starting etcd\u0026#34;\rkube::etcd::start\r}\rfunction set_service_accounts {\rSERVICE_ACCOUNT_LOOKUP=${SERVICE_ACCOUNT_LOOKUP:-true}\rSERVICE_ACCOUNT_KEY=${SERVICE_ACCOUNT_KEY:-/tmp/kube-serviceaccount.key}\r# Generate ServiceAccount key if needed\rif [[ ! -f \u0026#34;${SERVICE_ACCOUNT_KEY}\u0026#34; ]]; then\rmkdir -p \u0026#34;$(dirname ${SERVICE_ACCOUNT_KEY})\u0026#34;\ropenssl genrsa -out \u0026#34;${SERVICE_ACCOUNT_KEY}\u0026#34; 2048 2\u0026gt;/dev/null\rfi\r}\rfunction start_apiserver {\rsecurity_admission=\u0026#34;\u0026#34;\rif [[ -n \u0026#34;${DENY_SECURITY_CONTEXT_ADMISSION}\u0026#34; ]]; then\rsecurity_admission=\u0026#34;,SecurityContextDeny\u0026#34;\rfi\rif [[ -n \u0026#34;${PSP_ADMISSION}\u0026#34; ]]; then\rsecurity_admission=\u0026#34;,PodSecurityPolicy\u0026#34;\rfi\rif [[ -n \u0026#34;${NODE_ADMISSION}\u0026#34; ]]; then\rsecurity_admission=\u0026#34;,NodeRestriction\u0026#34;\rfi\rif [ \u0026#34;${ENABLE_POD_PRIORITY_PREEMPTION}\u0026#34; == true ]; then\rsecurity_admission=\u0026#34;,Priority\u0026#34;\rif [[ -n \u0026#34;${RUNTIME_CONFIG}\u0026#34; ]]; then\rRUNTIME_CONFIG+=\u0026#34;,\u0026#34;\rfi\rRUNTIME_CONFIG+=\u0026#34;scheduling.k8s.io/v1alpha1=true\u0026#34;\rfi\r# Admission Controllers to invoke prior to persisting objects in cluster\r#\r# The order defined here dose not matter.\rENABLE_ADMISSION_PLUGINS=Initializers,LimitRanger,ServiceAccount${security_admission},DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,PodPreset,StorageObjectInUseProtection\raudit_arg=\u0026#34;\u0026#34;\rAPISERVER_BASIC_AUDIT_LOG=\u0026#34;\u0026#34;\rif [[ \u0026#34;${ENABLE_APISERVER_BASIC_AUDIT:-}\u0026#34; = true ]]; then\r# We currently only support enabling with a fixed path and with built-in log\r# rotation \u0026#34;disabled\u0026#34; (large value) so it behaves like kube-apiserver.log.\r# External log rotation should be set up the same as for kube-apiserver.log.\rAPISERVER_BASIC_AUDIT_LOG=/tmp/kube-apiserver-audit.log\raudit_arg=\u0026#34;--audit-log-path=${APISERVER_BASIC_AUDIT_LOG}\u0026#34;\raudit_arg+=\u0026#34; --audit-log-maxage=0\u0026#34;\raudit_arg+=\u0026#34; --audit-log-maxbackup=0\u0026#34;\r# Lumberjack doesn\u0026#39;t offer any way to disable size-based rotation. It also\r# has an in-memory counter that doesn\u0026#39;t notice if you truncate the file.\r# 2000000000 (in MiB) is a large number that fits in 31 bits. If the log\r# grows at 10MiB/s (~30K QPS), it will rotate after ~6 years if apiserver\r# never restarts. Please manually restart apiserver before this time.\raudit_arg+=\u0026#34; --audit-log-maxsize=2000000000\u0026#34;\rfi\rswagger_arg=\u0026#34;\u0026#34;\rif [[ \u0026#34;${ENABLE_SWAGGER_UI}\u0026#34; = true ]]; then\rswagger_arg=\u0026#34;--enable-swagger-ui=true \u0026#34;\rfi\rauthorizer_arg=\u0026#34;\u0026#34;\rif [[ -n \u0026#34;${AUTHORIZATION_MODE}\u0026#34; ]]; then\rauthorizer_arg=\u0026#34;--authorization-mode=${AUTHORIZATION_MODE}\u0026#34;\rfi\rpriv_arg=\u0026#34;\u0026#34;\rif [[ -n \u0026#34;${ALLOW_PRIVILEGED}\u0026#34; ]]; then\rpriv_arg=\u0026#34;--allow-privileged \u0026#34;\rfi\rif [[ ${ENABLE_ADMISSION_PLUGINS} == *\u0026#34;Initializers\u0026#34;* ]]; then\rif [[ -n \u0026#34;${RUNTIME_CONFIG}\u0026#34; ]]; then\rRUNTIME_CONFIG+=\u0026#34;,\u0026#34;\rfi\rRUNTIME_CONFIG+=\u0026#34;admissionregistration.k8s.io/v1alpha1\u0026#34;\rfi\rif [[ ${ENABLE_ADMISSION_PLUGINS} == *\u0026#34;PodPreset\u0026#34;* ]]; then\rif [[ -n \u0026#34;${RUNTIME_CONFIG}\u0026#34; ]]; then\rRUNTIME_CONFIG+=\u0026#34;,\u0026#34;\rfi\rRUNTIME_CONFIG+=\u0026#34;settings.k8s.io/v1alpha1\u0026#34;\rfi\rruntime_config=\u0026#34;\u0026#34;\rif [[ -n \u0026#34;${RUNTIME_CONFIG}\u0026#34; ]]; then\rruntime_config=\u0026#34;--runtime-config=${RUNTIME_CONFIG}\u0026#34;\rfi\r# Let the API server pick a default address when API_HOST_IP\r# is set to 127.0.0.1\radvertise_address=\u0026#34;\u0026#34;\rif [[ \u0026#34;${API_HOST_IP}\u0026#34; != \u0026#34;127.0.0.1\u0026#34; ]]; then\radvertise_address=\u0026#34;--advertise-address=${API_HOST_IP}\u0026#34;\rfi\rif [[ \u0026#34;${ADVERTISE_ADDRESS}\u0026#34; != \u0026#34;\u0026#34; ]] ; then\radvertise_address=\u0026#34;--advertise-address=${ADVERTISE_ADDRESS}\u0026#34;\rfi\r# Create CA signers\rif [[ \u0026#34;${ENABLE_SINGLE_CA_SIGNER:-}\u0026#34; = true ]]; then\rkube::util::create_signing_certkey \u0026#34;${CONTROLPLANE_SUDO}\u0026#34; \u0026#34;${CERT_DIR}\u0026#34; server \u0026#39;\u0026#34;client auth\u0026#34;,\u0026#34;server auth\u0026#34;\u0026#39;\rsudo cp \u0026#34;${CERT_DIR}/server-ca.key\u0026#34; \u0026#34;${CERT_DIR}/client-ca.key\u0026#34;\rsudo cp \u0026#34;${CERT_DIR}/server-ca.crt\u0026#34; \u0026#34;${CERT_DIR}/client-ca.crt\u0026#34;\rsudo cp \u0026#34;${CERT_DIR}/server-ca-config.json\u0026#34; \u0026#34;${CERT_DIR}/client-ca-config.json\u0026#34;\relse\rkube::util::create_signing_certkey \u0026#34;${CONTROLPLANE_SUDO}\u0026#34; \u0026#34;${CERT_DIR}\u0026#34; server \u0026#39;\u0026#34;server auth\u0026#34;\u0026#39;\rkube::util::create_signing_certkey \u0026#34;${CONTROLPLANE_SUDO}\u0026#34; \u0026#34;${CERT_DIR}\u0026#34; client \u0026#39;\u0026#34;client auth\u0026#34;\u0026#39;\rfi\r# Create auth proxy client ca\rkube::util::create_signing_certkey \u0026#34;${CONTROLPLANE_SUDO}\u0026#34; \u0026#34;${CERT_DIR}\u0026#34; request-header \u0026#39;\u0026#34;client auth\u0026#34;\u0026#39;\r# serving cert for kube-apiserver\rkube::util::create_serving_certkey \u0026#34;${CONTROLPLANE_SUDO}\u0026#34; \u0026#34;${CERT_DIR}\u0026#34; \u0026#34;server-ca\u0026#34; kube-apiserver kubernetes.default kubernetes.default.svc \u0026#34;localhost\u0026#34; ${API_HOST_IP} ${API_HOST} ${FIRST_SERVICE_CLUSTER_IP}\r# Create client certs signed with client-ca, given id, given CN and a number of groups\rkube::util::create_client_certkey \u0026#34;${CONTROLPLANE_SUDO}\u0026#34; \u0026#34;${CERT_DIR}\u0026#34; \u0026#39;client-ca\u0026#39; kubelet system:node:${HOSTNAME_OVERRIDE} system:nodes\rkube::util::create_client_certkey \u0026#34;${CONTROLPLANE_SUDO}\u0026#34; \u0026#34;${CERT_DIR}\u0026#34; \u0026#39;client-ca\u0026#39; kube-proxy system:kube-proxy system:nodes\rkube::util::create_client_certkey \u0026#34;${CONTROLPLANE_SUDO}\u0026#34; \u0026#34;${CERT_DIR}\u0026#34; \u0026#39;client-ca\u0026#39; controller system:kube-controller-manager\rkube::util::create_client_certkey \u0026#34;${CONTROLPLANE_SUDO}\u0026#34; \u0026#34;${CERT_DIR}\u0026#34; \u0026#39;client-ca\u0026#39; scheduler system:kube-scheduler\rkube::util::create_client_certkey \u0026#34;${CONTROLPLANE_SUDO}\u0026#34; \u0026#34;${CERT_DIR}\u0026#34; \u0026#39;client-ca\u0026#39; admin system:admin system:masters\r# Create matching certificates for kube-aggregator\rkube::util::create_serving_certkey \u0026#34;${CONTROLPLANE_SUDO}\u0026#34; \u0026#34;${CERT_DIR}\u0026#34; \u0026#34;server-ca\u0026#34; kube-aggregator api.kube-public.svc \u0026#34;localhost\u0026#34; ${API_HOST_IP}\rkube::util::create_client_certkey \u0026#34;${CONTROLPLANE_SUDO}\u0026#34; \u0026#34;${CERT_DIR}\u0026#34; request-header-ca auth-proxy system:auth-proxy\r# TODO remove masters and add rolebinding\rkube::util::create_client_certkey \u0026#34;${CONTROLPLANE_SUDO}\u0026#34; \u0026#34;${CERT_DIR}\u0026#34; \u0026#39;client-ca\u0026#39; kube-aggregator system:kube-aggregator system:masters\rkube::util::write_client_kubeconfig \u0026#34;${CONTROLPLANE_SUDO}\u0026#34; \u0026#34;${CERT_DIR}\u0026#34; \u0026#34;${ROOT_CA_FILE}\u0026#34; \u0026#34;${API_HOST}\u0026#34; \u0026#34;${API_SECURE_PORT}\u0026#34; kube-aggregator\rcloud_config_arg=\u0026#34;--cloud-provider=${CLOUD_PROVIDER}--cloud-config=${CLOUD_CONFIG}\u0026#34;\rif [[ \u0026#34;${EXTERNAL_CLOUD_PROVIDER:-}\u0026#34; == \u0026#34;true\u0026#34; ]]; then\rcloud_config_arg=\u0026#34;--cloud-provider=external\u0026#34;\rfi\rAPISERVER_LOG=${LOG_DIR}/kube-apiserver.log\r#${CONTROLPLANE_SUDO} \u0026#34;${GO_OUT}/hyperkube\u0026#34; apiserver ${swagger_arg} ${audit_arg} ${authorizer_arg} ${priv_arg} ${runtime_config} \\\r${CONTROLPLANE_SUDO} \u0026#34;${GO_OUT}/kube-apiserver\u0026#34; ${swagger_arg} ${audit_arg} ${authorizer_arg} ${priv_arg} ${runtime_config} \\\r ${cloud_config_arg} \\\r ${advertise_address} \\\r --v=${LOG_LEVEL} \\\r --vmodule=\u0026#34;${LOG_SPEC}\u0026#34; \\\r --cert-dir=\u0026#34;${CERT_DIR}\u0026#34; \\\r --client-ca-file=\u0026#34;${CERT_DIR}/client-ca.crt\u0026#34; \\\r --service-account-key-file=\u0026#34;${SERVICE_ACCOUNT_KEY}\u0026#34; \\\r --service-account-lookup=\u0026#34;${SERVICE_ACCOUNT_LOOKUP}\u0026#34; \\\r --enable-admission-plugins=\u0026#34;${ENABLE_ADMISSION_PLUGINS}\u0026#34; \\\r --disable-admission-plugins=\u0026#34;${DISABLE_ADMISSION_PLUGINS}\u0026#34; \\\r --admission-control-config-file=\u0026#34;${ADMISSION_CONTROL_CONFIG_FILE}\u0026#34; \\\r --bind-address=\u0026#34;${API_BIND_ADDR}\u0026#34; \\\r --secure-port=\u0026#34;${API_SECURE_PORT}\u0026#34; \\\r --tls-cert-file=\u0026#34;${CERT_DIR}/serving-kube-apiserver.crt\u0026#34; \\\r --tls-private-key-file=\u0026#34;${CERT_DIR}/serving-kube-apiserver.key\u0026#34; \\\r --insecure-bind-address=\u0026#34;${API_HOST_IP}\u0026#34; \\\r --insecure-port=\u0026#34;${API_PORT}\u0026#34; \\\r --storage-backend=${STORAGE_BACKEND} \\\r --etcd-servers=\u0026#34;http://${ETCD_HOST}:${ETCD_PORT}\u0026#34; \\\r --service-cluster-ip-range=\u0026#34;${SERVICE_CLUSTER_IP_RANGE}\u0026#34; \\\r --feature-gates=\u0026#34;${FEATURE_GATES}\u0026#34; \\\r --external-hostname=\u0026#34;${EXTERNAL_HOSTNAME}\u0026#34; \\\r --requestheader-username-headers=X-Remote-User \\\r --requestheader-group-headers=X-Remote-Group \\\r --requestheader-extra-headers-prefix=X-Remote-Extra- \\\r --requestheader-client-ca-file=\u0026#34;${CERT_DIR}/request-header-ca.crt\u0026#34; \\\r --requestheader-allowed-names=system:auth-proxy \\\r --proxy-client-cert-file=\u0026#34;${CERT_DIR}/client-auth-proxy.crt\u0026#34; \\\r --proxy-client-key-file=\u0026#34;${CERT_DIR}/client-auth-proxy.key\u0026#34; \\\r --cors-allowed-origins=\u0026#34;${API_CORS_ALLOWED_ORIGINS}\u0026#34; \u0026gt;\u0026#34;${APISERVER_LOG}\u0026#34; 2\u0026gt;\u0026amp;1 \u0026amp;\rAPISERVER_PID=$!\r# Wait for kube-apiserver to come up before launching the rest of the components.\recho \u0026#34;Waiting for apiserver to come up\u0026#34;\r# this uses the API port because if you don\u0026#39;t have any authenticator, you can\u0026#39;t seem to use the secure port at all.\r# this matches what happened with the combination in 1.4.\r# TODO change this conditionally based on whether API_PORT is on or off\rkube::util::wait_for_url \u0026#34;https://${API_HOST_IP}:${API_SECURE_PORT}/healthz\u0026#34; \u0026#34;apiserver: \u0026#34; 1 ${WAIT_FOR_URL_API_SERVER} \\\r || { echo \u0026#34;check apiserver logs: ${APISERVER_LOG}\u0026#34; ; exit 1 ; }\r# Create kubeconfigs for all components, using client certs\rkube::util::write_client_kubeconfig \u0026#34;${CONTROLPLANE_SUDO}\u0026#34; \u0026#34;${CERT_DIR}\u0026#34; \u0026#34;${ROOT_CA_FILE}\u0026#34; \u0026#34;${API_HOST}\u0026#34; \u0026#34;${API_SECURE_PORT}\u0026#34; admin\r${CONTROLPLANE_SUDO} chown \u0026#34;${USER}\u0026#34; \u0026#34;${CERT_DIR}/client-admin.key\u0026#34; # make readable for kubectl\rkube::util::write_client_kubeconfig \u0026#34;${CONTROLPLANE_SUDO}\u0026#34; \u0026#34;${CERT_DIR}\u0026#34; \u0026#34;${ROOT_CA_FILE}\u0026#34; \u0026#34;${API_HOST}\u0026#34; \u0026#34;${API_SECURE_PORT}\u0026#34; kubelet\rkube::util::write_client_kubeconfig \u0026#34;${CONTROLPLANE_SUDO}\u0026#34; \u0026#34;${CERT_DIR}\u0026#34; \u0026#34;${ROOT_CA_FILE}\u0026#34; \u0026#34;${API_HOST}\u0026#34; \u0026#34;${API_SECURE_PORT}\u0026#34; kube-proxy\rkube::util::write_client_kubeconfig \u0026#34;${CONTROLPLANE_SUDO}\u0026#34; \u0026#34;${CERT_DIR}\u0026#34; \u0026#34;${ROOT_CA_FILE}\u0026#34; \u0026#34;${API_HOST}\u0026#34; \u0026#34;${API_SECURE_PORT}\u0026#34; controller\rkube::util::write_client_kubeconfig \u0026#34;${CONTROLPLANE_SUDO}\u0026#34; \u0026#34;${CERT_DIR}\u0026#34; \u0026#34;${ROOT_CA_FILE}\u0026#34; \u0026#34;${API_HOST}\u0026#34; \u0026#34;${API_SECURE_PORT}\u0026#34; scheduler\rif [[ -z \u0026#34;${AUTH_ARGS}\u0026#34; ]]; then\rAUTH_ARGS=\u0026#34;--client-key=${CERT_DIR}/client-admin.key --client-certificate=${CERT_DIR}/client-admin.crt\u0026#34;\rfi\r${CONTROLPLANE_SUDO} cp \u0026#34;${CERT_DIR}/admin.kubeconfig\u0026#34; \u0026#34;${CERT_DIR}/admin-kube-aggregator.kubeconfig\u0026#34;\r${CONTROLPLANE_SUDO} chown $(whoami) \u0026#34;${CERT_DIR}/admin-kube-aggregator.kubeconfig\u0026#34;\r${KUBECTL} config set-cluster local-up-cluster --kubeconfig=\u0026#34;${CERT_DIR}/admin-kube-aggregator.kubeconfig\u0026#34; --server=\u0026#34;https://${API_HOST_IP}:31090\u0026#34;\recho \u0026#34;use \u0026#39;kubectl --kubeconfig=${CERT_DIR}/admin-kube-aggregator.kubeconfig\u0026#39; to use the aggregated API server\u0026#34;\r}\rfunction start_controller_manager {\rnode_cidr_args=\u0026#34;\u0026#34;\rif [[ \u0026#34;${NET_PLUGIN}\u0026#34; == \u0026#34;kubenet\u0026#34; ]]; then\rnode_cidr_args=\u0026#34;--allocate-node-cidrs=true --cluster-cidr=10.1.0.0/16 \u0026#34;\rfi\rcloud_config_arg=\u0026#34;--cloud-provider=${CLOUD_PROVIDER}--cloud-config=${CLOUD_CONFIG}\u0026#34;\rif [[ \u0026#34;${EXTERNAL_CLOUD_PROVIDER:-}\u0026#34; == \u0026#34;true\u0026#34; ]]; then\rcloud_config_arg=\u0026#34;--cloud-provider=external\u0026#34;\rcloud_config_arg+=\u0026#34;--external-cloud-volume-plugin=${CLOUD_PROVIDER}\u0026#34;\rcloud_config_arg+=\u0026#34;--cloud-config=${CLOUD_CONFIG}\u0026#34;\rfi\rCTLRMGR_LOG=${LOG_DIR}/kube-controller-manager.log\r#${CONTROLPLANE_SUDO} \u0026#34;${GO_OUT}/hyperkube\u0026#34; controller-manager \\\r${CONTROLPLANE_SUDO} \u0026#34;${GO_OUT}/kube-controller-manager\u0026#34; \\\r --v=${LOG_LEVEL} \\\r --vmodule=\u0026#34;${LOG_SPEC}\u0026#34; \\\r --service-account-private-key-file=\u0026#34;${SERVICE_ACCOUNT_KEY}\u0026#34; \\\r --root-ca-file=\u0026#34;${ROOT_CA_FILE}\u0026#34; \\\r --cluster-signing-cert-file=\u0026#34;${CLUSTER_SIGNING_CERT_FILE}\u0026#34; \\\r --cluster-signing-key-file=\u0026#34;${CLUSTER_SIGNING_KEY_FILE}\u0026#34; \\\r --enable-hostpath-provisioner=\u0026#34;${ENABLE_HOSTPATH_PROVISIONER}\u0026#34; \\\r ${node_cidr_args} \\\r --pvclaimbinder-sync-period=\u0026#34;${CLAIM_BINDER_SYNC_PERIOD}\u0026#34; \\\r --feature-gates=\u0026#34;${FEATURE_GATES}\u0026#34; \\\r ${cloud_config_arg} \\\r --kubeconfig \u0026#34;$CERT_DIR\u0026#34;/controller.kubeconfig \\\r --use-service-account-credentials \\\r --controllers=\u0026#34;${KUBE_CONTROLLERS}\u0026#34; \\\r --master=\u0026#34;https://${API_HOST}:${API_SECURE_PORT}\u0026#34; \u0026gt;\u0026#34;${CTLRMGR_LOG}\u0026#34; 2\u0026gt;\u0026amp;1 \u0026amp;\rCTLRMGR_PID=$!\r}\rfunction start_cloud_controller_manager {\rif [ -z \u0026#34;${CLOUD_CONFIG}\u0026#34; ]; then\recho \u0026#34;CLOUD_CONFIG cannot be empty!\u0026#34;\rexit 1\rfi\rif [ ! -f \u0026#34;${CLOUD_CONFIG}\u0026#34; ]; then\recho \u0026#34;Cloud config ${CLOUD_CONFIG}doesn\u0026#39;t exist\u0026#34;\rexit 1\rfi\rnode_cidr_args=\u0026#34;\u0026#34;\rif [[ \u0026#34;${NET_PLUGIN}\u0026#34; == \u0026#34;kubenet\u0026#34; ]]; then\rnode_cidr_args=\u0026#34;--allocate-node-cidrs=true --cluster-cidr=10.1.0.0/16 \u0026#34;\rfi\rCLOUD_CTLRMGR_LOG=${LOG_DIR}/cloud-controller-manager.log\r#${CONTROLPLANE_SUDO} ${EXTERNAL_CLOUD_PROVIDER_BINARY:-\u0026#34;${GO_OUT}/hyperkube\u0026#34; cloud-controller-manager} \\\r${CONTROLPLANE_SUDO} ${EXTERNAL_CLOUD_PROVIDER_BINARY:-\u0026#34;${GO_OUT}/cloud-controller-manager\u0026#34;} \\\r --v=${LOG_LEVEL} \\\r --vmodule=\u0026#34;${LOG_SPEC}\u0026#34; \\\r ${node_cidr_args} \\\r --feature-gates=\u0026#34;${FEATURE_GATES}\u0026#34; \\\r --cloud-provider=${CLOUD_PROVIDER} \\\r --cloud-config=${CLOUD_CONFIG} \\\r --kubeconfig \u0026#34;$CERT_DIR\u0026#34;/controller.kubeconfig \\\r --use-service-account-credentials \\\r --master=\u0026#34;https://${API_HOST}:${API_SECURE_PORT}\u0026#34; \u0026gt;\u0026#34;${CLOUD_CTLRMGR_LOG}\u0026#34; 2\u0026gt;\u0026amp;1 \u0026amp;\rCLOUD_CTLRMGR_PID=$!\r}\rfunction start_kubelet {\rKUBELET_LOG=${LOG_DIR}/kubelet.log\rmkdir -p \u0026#34;${POD_MANIFEST_PATH}\u0026#34; \u0026amp;\u0026gt;/dev/null || sudo mkdir -p \u0026#34;${POD_MANIFEST_PATH}\u0026#34;\rpriv_arg=\u0026#34;\u0026#34;\rif [[ -n \u0026#34;${ALLOW_PRIVILEGED}\u0026#34; ]]; then\rpriv_arg=\u0026#34;--allow-privileged \u0026#34;\rfi\rcloud_config_arg=\u0026#34;--cloud-provider=${CLOUD_PROVIDER}--cloud-config=${CLOUD_CONFIG}\u0026#34;\rif [[ \u0026#34;${EXTERNAL_CLOUD_PROVIDER:-}\u0026#34; == \u0026#34;true\u0026#34; ]]; then\rcloud_config_arg=\u0026#34;--cloud-provider=external\u0026#34;\rcloud_config_arg+=\u0026#34;--provider-id=$(hostname)\u0026#34;\rfi\rmkdir -p \u0026#34;/var/lib/kubelet\u0026#34; \u0026amp;\u0026gt;/dev/null || sudo mkdir -p \u0026#34;/var/lib/kubelet\u0026#34;\rif [[ -z \u0026#34;${DOCKERIZE_KUBELET}\u0026#34; ]]; then\r# Enable dns\rif [[ \u0026#34;${ENABLE_CLUSTER_DNS}\u0026#34; = true ]]; then\rdns_args=\u0026#34;--cluster-dns=${DNS_SERVER_IP}--cluster-domain=${DNS_DOMAIN}\u0026#34;\relse\r# To start a private DNS server set ENABLE_CLUSTER_DNS and\r# DNS_SERVER_IP/DOMAIN. This will at least provide a working\r# DNS server for real world hostnames.\rdns_args=\u0026#34;--cluster-dns=8.8.8.8\u0026#34;\rfi\rnet_plugin_args=\u0026#34;\u0026#34;\rif [[ -n \u0026#34;${NET_PLUGIN}\u0026#34; ]]; then\rnet_plugin_args=\u0026#34;--network-plugin=${NET_PLUGIN}\u0026#34;\rfi\rauth_args=\u0026#34;\u0026#34;\rif [[ -n \u0026#34;${KUBELET_AUTHORIZATION_WEBHOOK:-}\u0026#34; ]]; then\rauth_args=\u0026#34;${auth_args}--authorization-mode=Webhook\u0026#34;\rfi\rif [[ -n \u0026#34;${KUBELET_AUTHENTICATION_WEBHOOK:-}\u0026#34; ]]; then\rauth_args=\u0026#34;${auth_args}--authentication-token-webhook\u0026#34;\rfi\rif [[ -n \u0026#34;${CLIENT_CA_FILE:-}\u0026#34; ]]; then\rauth_args=\u0026#34;${auth_args}--client-ca-file=${CLIENT_CA_FILE}\u0026#34;\rfi\rcni_conf_dir_args=\u0026#34;\u0026#34;\rif [[ -n \u0026#34;${CNI_CONF_DIR}\u0026#34; ]]; then\rcni_conf_dir_args=\u0026#34;--cni-conf-dir=${CNI_CONF_DIR}\u0026#34;\rfi\rcni_bin_dir_args=\u0026#34;\u0026#34;\rif [[ -n \u0026#34;${CNI_BIN_DIR}\u0026#34; ]]; then\rcni_bin_dir_args=\u0026#34;--cni-bin-dir=${CNI_BIN_DIR}\u0026#34;\rfi\rcontainer_runtime_endpoint_args=\u0026#34;\u0026#34;\rif [[ -n \u0026#34;${CONTAINER_RUNTIME_ENDPOINT}\u0026#34; ]]; then\rcontainer_runtime_endpoint_args=\u0026#34;--container-runtime-endpoint=${CONTAINER_RUNTIME_ENDPOINT}\u0026#34;\rfi\rimage_service_endpoint_args=\u0026#34;\u0026#34;\rif [[ -n \u0026#34;${IMAGE_SERVICE_ENDPOINT}\u0026#34; ]]; then\rimage_service_endpoint_args=\u0026#34;--image-service-endpoint=${IMAGE_SERVICE_ENDPOINT}\u0026#34;\rfi\r#sudo -E \u0026#34;${GO_OUT}/hyperkube\u0026#34; kubelet ${priv_arg}\\\rsudo -E \u0026#34;${GO_OUT}/kubelet\u0026#34; ${priv_arg}\\\r --v=${LOG_LEVEL} \\\r --vmodule=\u0026#34;${LOG_SPEC}\u0026#34; \\\r --chaos-chance=\u0026#34;${CHAOS_CHANCE}\u0026#34; \\\r --container-runtime=\u0026#34;${CONTAINER_RUNTIME}\u0026#34; \\\r --rkt-path=\u0026#34;${RKT_PATH}\u0026#34; \\\r --rkt-stage1-image=\u0026#34;${RKT_STAGE1_IMAGE}\u0026#34; \\\r --hostname-override=\u0026#34;${HOSTNAME_OVERRIDE}\u0026#34; \\\r ${cloud_config_arg} \\\r --address=\u0026#34;${KUBELET_HOST}\u0026#34; \\\r --kubeconfig \u0026#34;$CERT_DIR\u0026#34;/kubelet.kubeconfig \\\r --feature-gates=\u0026#34;${FEATURE_GATES}\u0026#34; \\\r --cpu-cfs-quota=${CPU_CFS_QUOTA} \\\r --enable-controller-attach-detach=\u0026#34;${ENABLE_CONTROLLER_ATTACH_DETACH}\u0026#34; \\\r --cgroups-per-qos=${CGROUPS_PER_QOS} \\\r --cgroup-driver=${CGROUP_DRIVER} \\\r --keep-terminated-pod-volumes=${KEEP_TERMINATED_POD_VOLUMES} \\\r --eviction-hard=${EVICTION_HARD} \\\r --eviction-soft=${EVICTION_SOFT} \\\r --eviction-pressure-transition-period=${EVICTION_PRESSURE_TRANSITION_PERIOD} \\\r --pod-manifest-path=\u0026#34;${POD_MANIFEST_PATH}\u0026#34; \\\r --fail-swap-on=\u0026#34;${FAIL_SWAP_ON}\u0026#34; \\\r ${auth_args} \\\r ${dns_args} \\\r ${cni_conf_dir_args} \\\r ${cni_bin_dir_args} \\\r ${net_plugin_args} \\\r ${container_runtime_endpoint_args} \\\r ${image_service_endpoint_args} \\\r --port=\u0026#34;$KUBELET_PORT\u0026#34; \\\r ${KUBELET_FLAGS} \u0026gt;\u0026#34;${KUBELET_LOG}\u0026#34; 2\u0026gt;\u0026amp;1 \u0026amp;\rKUBELET_PID=$!\r# Quick check that kubelet is running.\rif ps -p $KUBELET_PID \u0026gt; /dev/null ; then\recho \u0026#34;kubelet ( $KUBELET_PID) is running.\u0026#34;\relse\rcat ${KUBELET_LOG} ; exit 1\rfi\relse\r# Docker won\u0026#39;t run a container with a cidfile (container id file)\r# unless that file does not already exist; clean up an existing\r# dockerized kubelet that might be running.\rcleanup_dockerized_kubelet\rcred_bind=\u0026#34;\u0026#34;\r# path to cloud credentials.\rcloud_cred=\u0026#34;\u0026#34;\rif [ \u0026#34;${CLOUD_PROVIDER}\u0026#34; == \u0026#34;aws\u0026#34; ]; then\rcloud_cred=\u0026#34;${HOME}/.aws/credentials\u0026#34;\rfi\rif [ \u0026#34;${CLOUD_PROVIDER}\u0026#34; == \u0026#34;gce\u0026#34; ]; then\rcloud_cred=\u0026#34;${HOME}/.config/gcloud\u0026#34;\rfi\rif [ \u0026#34;${CLOUD_PROVIDER}\u0026#34; == \u0026#34;openstack\u0026#34; ]; then\rcloud_cred=\u0026#34;${CLOUD_CONFIG}\u0026#34;\rfi\rif [[ -n \u0026#34;${cloud_cred}\u0026#34; ]]; then\rcred_bind=\u0026#34;--volume=${cloud_cred}:${cloud_cred}:ro\u0026#34;\rfi\rdocker run \\\r --volume=/:/rootfs:ro \\\r --volume=/var/run:/var/run:rw \\\r --volume=/sys:/sys:ro \\\r --volume=/var/lib/docker/:/var/lib/docker:ro \\\r --volume=/var/lib/kubelet/:/var/lib/kubelet:rw \\\r --volume=/dev:/dev \\\r --volume=/run/xtables.lock:/run/xtables.lock:rw \\\r ${cred_bind} \\\r --net=host \\\r --privileged=true \\\r -i \\\r --cidfile=$KUBELET_CIDFILE \\\r k8s.gcr.io/kubelet \\\r /kubelet --v=${LOG_LEVEL} --containerized ${priv_arg}--chaos-chance=\u0026#34;${CHAOS_CHANCE}\u0026#34; --pod-manifest-path=\u0026#34;${POD_MANIFEST_PATH}\u0026#34; --hostname-override=\u0026#34;${HOSTNAME_OVERRIDE}\u0026#34; ${cloud_config_arg} \\ --address=\u0026#34;127.0.0.1\u0026#34; --kubeconfig \u0026#34;$CERT_DIR\u0026#34;/kubelet.kubeconfig --port=\u0026#34;$KUBELET_PORT\u0026#34; --enable-controller-attach-detach=\u0026#34;${ENABLE_CONTROLLER_ATTACH_DETACH}\u0026#34; \u0026amp;\u0026gt; $KUBELET_LOG \u0026amp;\rfi\r}\rfunction start_kubeproxy {\rPROXY_LOG=${LOG_DIR}/kube-proxy.log\rcat \u0026lt;\u0026lt;EOF \u0026gt; /tmp/kube-proxy.yaml\rapiVersion: kubeproxy.config.k8s.io/v1alpha1\rkind: KubeProxyConfiguration\rclientConnection:\rkubeconfig: ${CERT_DIR}/kube-proxy.kubeconfig\rhostnameOverride: ${HOSTNAME_OVERRIDE}\rmode: ${KUBE_PROXY_MODE}\rEOF\r#sudo \u0026#34;${GO_OUT}/hyperkube\u0026#34; proxy \\\rsudo \u0026#34;${GO_OUT}/kube-proxy\u0026#34; \\\r --v=${LOG_LEVEL} \\\r --feature-gates=\u0026#34;${FEATURE_GATES}\u0026#34; \\\r --config=/tmp/kube-proxy.yaml \\\r --master=\u0026#34;https://${API_HOST}:${API_SECURE_PORT}\u0026#34; \u0026gt;\u0026#34;${PROXY_LOG}\u0026#34; 2\u0026gt;\u0026amp;1 \u0026amp;\rPROXY_PID=$!\rSCHEDULER_LOG=${LOG_DIR}/kube-scheduler.log\r#${CONTROLPLANE_SUDO} \u0026#34;${GO_OUT}/hyperkube\u0026#34; scheduler \\\r${CONTROLPLANE_SUDO} \u0026#34;${GO_OUT}/kube-scheduler\u0026#34; \\\r --v=${LOG_LEVEL} \\\r --kubeconfig \u0026#34;$CERT_DIR\u0026#34;/scheduler.kubeconfig \\\r --feature-gates=\u0026#34;${FEATURE_GATES}\u0026#34; \\\r --master=\u0026#34;https://${API_HOST}:${API_SECURE_PORT}\u0026#34; \u0026gt;\u0026#34;${SCHEDULER_LOG}\u0026#34; 2\u0026gt;\u0026amp;1 \u0026amp;\rSCHEDULER_PID=$!\r}\rfunction start_kubedns {\rif [[ \u0026#34;${ENABLE_CLUSTER_DNS}\u0026#34; = true ]]; then\rcp \u0026#34;${KUBE_ROOT}/cluster/addons/dns/kube-dns.yaml.in\u0026#34; kube-dns.yaml\rsed -i -e \u0026#34;s/{{ pillar\\[\u0026#39;dns_domain\u0026#39;\\] }}/${DNS_DOMAIN}/g\u0026#34; kube-dns.yaml\rsed -i -e \u0026#34;s/{{ pillar\\[\u0026#39;dns_server\u0026#39;\\] }}/${DNS_SERVER_IP}/g\u0026#34; kube-dns.yaml\r# TODO update to dns role once we have one.\r# use kubectl to create kubedns addon\r${KUBECTL} --kubeconfig=\u0026#34;${CERT_DIR}/admin.kubeconfig\u0026#34; --namespace=kube-system create -f kube-dns.yaml\recho \u0026#34;Kube-dns addon successfully deployed.\u0026#34;\rrm kube-dns.yaml\rfi\r}\rfunction start_kubedashboard {\rif [[ \u0026#34;${ENABLE_CLUSTER_DASHBOARD}\u0026#34; = true ]]; then\recho \u0026#34;Creating kubernetes-dashboard\u0026#34;\r# use kubectl to create the dashboard\r${KUBECTL} --kubeconfig=\u0026#34;${CERT_DIR}/admin.kubeconfig\u0026#34; apply -f ${KUBE_ROOT}/cluster/addons/dashboard/dashboard-secret.yaml\r${KUBECTL} --kubeconfig=\u0026#34;${CERT_DIR}/admin.kubeconfig\u0026#34; apply -f ${KUBE_ROOT}/cluster/addons/dashboard/dashboard-configmap.yaml\r${KUBECTL} --kubeconfig=\u0026#34;${CERT_DIR}/admin.kubeconfig\u0026#34; apply -f ${KUBE_ROOT}/cluster/addons/dashboard/dashboard-rbac.yaml\r${KUBECTL} --kubeconfig=\u0026#34;${CERT_DIR}/admin.kubeconfig\u0026#34; apply -f ${KUBE_ROOT}/cluster/addons/dashboard/dashboard-controller.yaml\r${KUBECTL} --kubeconfig=\u0026#34;${CERT_DIR}/admin.kubeconfig\u0026#34; apply -f ${KUBE_ROOT}/cluster/addons/dashboard/dashboard-service.yaml\recho \u0026#34;kubernetes-dashboard deployment and service successfully deployed.\u0026#34;\rfi\r}\rfunction create_psp_policy {\recho \u0026#34;Create podsecuritypolicy policies for RBAC.\u0026#34;\r${KUBECTL} --kubeconfig=\u0026#34;${CERT_DIR}/admin.kubeconfig\u0026#34; create -f ${KUBE_ROOT}/examples/podsecuritypolicy/rbac/policies.yaml\r${KUBECTL} --kubeconfig=\u0026#34;${CERT_DIR}/admin.kubeconfig\u0026#34; create -f ${KUBE_ROOT}/examples/podsecuritypolicy/rbac/roles.yaml\r${KUBECTL} --kubeconfig=\u0026#34;${CERT_DIR}/admin.kubeconfig\u0026#34; create -f ${KUBE_ROOT}/examples/podsecuritypolicy/rbac/bindings.yaml\r}\rfunction create_storage_class {\rif [ -z \u0026#34;$CLOUD_PROVIDER\u0026#34; ]; then\rCLASS_FILE=${KUBE_ROOT}/cluster/addons/storage-class/local/default.yaml\relse\rCLASS_FILE=${KUBE_ROOT}/cluster/addons/storage-class/${CLOUD_PROVIDER}/default.yaml\rfi\rif [ -e $CLASS_FILE ]; then\recho \u0026#34;Create default storage class for $CLOUD_PROVIDER\u0026#34;\r${KUBECTL} --kubeconfig=\u0026#34;${CERT_DIR}/admin.kubeconfig\u0026#34; create -f $CLASS_FILE\relse\recho \u0026#34;No storage class available for $CLOUD_PROVIDER.\u0026#34;\rfi\r}\rfunction print_success {\rif [[ \u0026#34;${START_MODE}\u0026#34; != \u0026#34;kubeletonly\u0026#34; ]]; then\rif [[ \u0026#34;${ENABLE_DAEMON}\u0026#34; = false ]]; then\recho \u0026#34;Local Kubernetes cluster is running. Press Ctrl-C to shut it down.\u0026#34;\relse\recho \u0026#34;Local Kubernetes cluster is running.\u0026#34;\rfi\rcat \u0026lt;\u0026lt;EOF\rLogs:\r${APISERVER_LOG:-}\r${CTLRMGR_LOG:-}\r${CLOUD_CTLRMGR_LOG:-}\r${PROXY_LOG:-}\r${SCHEDULER_LOG:-}\rEOF\rfi\rif [[ \u0026#34;${ENABLE_APISERVER_BASIC_AUDIT:-}\u0026#34; = true ]]; then\recho \u0026#34;${APISERVER_BASIC_AUDIT_LOG}\u0026#34;\rfi\rif [[ \u0026#34;${START_MODE}\u0026#34; == \u0026#34;all\u0026#34; ]]; then\recho \u0026#34;${KUBELET_LOG}\u0026#34;\relif [[ \u0026#34;${START_MODE}\u0026#34; == \u0026#34;nokubelet\u0026#34; ]]; then\recho\recho \u0026#34;No kubelet was started because you set START_MODE=nokubelet\u0026#34;\recho \u0026#34;Run this script again with START_MODE=kubeletonly to run a kubelet\u0026#34;\rfi\rif [[ \u0026#34;${START_MODE}\u0026#34; != \u0026#34;kubeletonly\u0026#34; ]]; then\recho\rif [[ \u0026#34;${ENABLE_DAEMON}\u0026#34; = false ]]; then\recho \u0026#34;To start using your cluster, you can open up another terminal/tab and run:\u0026#34;\relse\recho \u0026#34;To start using your cluster, run:\u0026#34;\rfi\rcat \u0026lt;\u0026lt;EOF\rexport KUBECONFIG=${CERT_DIR}/admin.kubeconfig\rcluster/kubectl.sh\rAlternatively, you can write to the default kubeconfig:\rexport KUBERNETES_PROVIDER=local\rcluster/kubectl.sh config set-cluster local --server=https://${API_HOST}:${API_SECURE_PORT} --certificate-authority=${ROOT_CA_FILE}\rcluster/kubectl.sh config set-credentials myself ${AUTH_ARGS}\rcluster/kubectl.sh config set-context local --cluster=local --user=myself\rcluster/kubectl.sh config use-context local\rcluster/kubectl.sh\rEOF\relse\rcat \u0026lt;\u0026lt;EOF\rThe kubelet was started.\rLogs:\r${KUBELET_LOG}\rEOF\rfi\r}\r# If we are running in the CI, we need a few more things before we can start\rif [[ \u0026#34;${KUBETEST_IN_DOCKER:-}\u0026#34; == \u0026#34;true\u0026#34; ]]; then\recho \u0026#34;Preparing to test ...\u0026#34;\r${KUBE_ROOT}/hack/install-etcd.sh\rexport PATH=\u0026#34;${KUBE_ROOT}/third_party/etcd:${PATH}\u0026#34;\rKUBE_FASTBUILD=true make ginkgo cross\rapt install -y sudo\rfi\r# validate that etcd is: not running, in path, and has minimum required version.\rif [[ \u0026#34;${START_MODE}\u0026#34; != \u0026#34;kubeletonly\u0026#34; ]]; then\rkube::etcd::validate\rfi\rif [ \u0026#34;${CONTAINER_RUNTIME}\u0026#34; == \u0026#34;docker\u0026#34; ] \u0026amp;\u0026amp; ! kube::util::ensure_docker_daemon_connectivity; then\rexit 1\rfi\rif [[ \u0026#34;${CONTAINER_RUNTIME}\u0026#34; == \u0026#34;rkt\u0026#34; ]]; then\rtest_rkt\rfi\rif [[ \u0026#34;${START_MODE}\u0026#34; != \u0026#34;kubeletonly\u0026#34; ]]; then\rtest_apiserver_off\rfi\rkube::util::test_openssl_installed\rkube::util::ensure-cfssl\r### IF the user didn\u0026#39;t supply an output/ for the build... Then we detect.\rif [ \u0026#34;$GO_OUT\u0026#34; == \u0026#34;\u0026#34; ]; then\rdetect_binary\rfi\recho \u0026#34;Detected host and ready to start services. Doing some housekeeping first...\u0026#34;\recho \u0026#34;Using GO_OUT $GO_OUT\u0026#34;\rKUBELET_CIDFILE=/tmp/kubelet.cid\rif [[ \u0026#34;${ENABLE_DAEMON}\u0026#34; = false ]]; then\rtrap cleanup EXIT\rfi\recho \u0026#34;Starting services now!\u0026#34;\rif [[ \u0026#34;${START_MODE}\u0026#34; != \u0026#34;kubeletonly\u0026#34; ]]; then\rstart_etcd\rset_service_accounts\rstart_apiserver\rstart_controller_manager\rif [[ \u0026#34;${EXTERNAL_CLOUD_PROVIDER:-}\u0026#34; == \u0026#34;true\u0026#34; ]]; then\rstart_cloud_controller_manager\rfi\rstart_kubeproxy\rstart_kubedns\rstart_kubedashboard\rfi\rif [[ \u0026#34;${START_MODE}\u0026#34; != \u0026#34;nokubelet\u0026#34; ]]; then\r## TODO remove this check if/when kubelet is supported on darwin\r# Detect the OS name/arch and display appropriate error.\rcase \u0026#34;$(uname -s)\u0026#34; in\rDarwin)\rwarning \u0026#34;kubelet is not currently supported in darwin, kubelet aborted.\u0026#34;\rKUBELET_LOG=\u0026#34;\u0026#34;\r;;\rLinux)\rstart_kubelet\r;;\r*)\rwarning \u0026#34;Unsupported host OS. Must be Linux or Mac OS X, kubelet aborted.\u0026#34;\r;;\resac\rfi\rif [[ -n \u0026#34;${PSP_ADMISSION}\u0026#34; \u0026amp;\u0026amp; \u0026#34;${AUTHORIZATION_MODE}\u0026#34; = *RBAC* ]]; then\rcreate_psp_policy\rfi\rif [[ \u0026#34;$DEFAULT_STORAGE_CLASS\u0026#34; = \u0026#34;true\u0026#34; ]]; then\rcreate_storage_class\rfi\rprint_success\rif [[ \u0026#34;${ENABLE_DAEMON}\u0026#34; = false ]]; then\rwhile true; do sleep 1; done\rfi\rif [[ \u0026#34;${KUBETEST_IN_DOCKER:-}\u0026#34; == \u0026#34;true\u0026#34; ]]; then\rcluster/kubectl.sh config set-cluster local --server=https://localhost:6443 --certificate-authority=/var/run/kubernetes/server-ca.crt\rcluster/kubectl.sh config set-credentials myself --client-key=/var/run/kubernetes/client-admin.key --client-certificate=/var/run/kubernetes/client-admin.crt\rcluster/kubectl.sh config set-context local --cluster=local --user=myself\rcluster/kubectl.sh config use-context local\rfi\r启动单机集群 第一次执行下面的命令编译并启动集群\n./hack/local-up-cluster.sh\r后面修改了某个模块的代码，执行以下代码即可单独编译该模块：\nmake GOGCFLAGS=\u0026#34;-N -l\u0026#34; WHAT=\u0026#34;cmd/kube-apiserver\u0026#34; # 假设只编译kube-apiserver这一个模块\r执行下面的命令直接启动集群（跳过编译）\n./hack/local-up-cluster.sh -O\r调试k8s代码 正常启动k8s单机集群后，ps看一下，可以到启动了7个进程：\nps -ef|grep kube|grep -v grep\r假设要调试kube-apiserver这个进程，则先查出kube-apiserver启动时的运行参数：\n$ ps -ef|grep kube-apiserver|grep -v grep\rroot 5228 4983 4 05:06 pts/0 00:00:47 /root/go/src/k8s.io/kubernetes/_output/bin/kube-apiserver --authorization-mode=Node,RBAC --runtime-config=admissionregistration.k8s.io/v1alpha1,settings.k8s.io/v1alpha1 --cloud-provider= --cloud-config= --v=3 --vmodule= --cert-dir=/var/run/kubernetes --client-ca-file=/var/run/kubernetes/client-ca.crt --service-account-key-file=/tmp/kube-serviceaccount.key --service-account-lookup=true --enable-admission-plugins=Initializers,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,PodPreset,StorageObjectInUseProtection --disable-admission-plugins= --admission-control-config-file= --bind-address=0.0.0.0 --secure-port=6443 --tls-cert-file=/var/run/kubernetes/serving-kube-apiserver.crt --tls-private-key-file=/var/run/kubernetes/serving-kube-apiserver.key --insecure-bind-address=127.0.0.1 --insecure-port=8080 --storage-backend=etcd3 --etcd-servers=http://127.0.0.1:2379 --service-cluster-ip-range=10.0.0.0/24 --feature-gates=AllAlpha=false --external-hostname=localhost --requestheader-username-headers=X-Remote-User --requestheader-group-headers=X-Remote-Group --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-client-ca-file=/var/run/kubernetes/request-header-ca.crt --requestheader-allowed-names=system:auth-proxy --proxy-client-cert-file=/var/run/kubernetes/client-auth-proxy.crt --proxy-client-key-file=/var/run/kubernetes/client-auth-proxy.key --cors-allowed-origins=/127.0.0.1(:[0-9]+)?$,/localhost(:[0-9]+)?$\r可以看到该进程的PID是5228，于是停掉该进程，并用dlv启动：\nkill -15 5228\rdlv --listen=:39999 --headless=true --api-version=2 exec /root/go/src/k8s.io/kubernetes/_output/bin/kube-apiserver -- --authorization-mode=Node,RBAC --runtime-config=admissionregistration.k8s.io/v1alpha1,settings.k8s.io/v1alpha1 --cloud-provider= --cloud-config= --v=3 --vmodule= --cert-dir=/var/run/kubernetes --client-ca-file=/var/run/kubernetes/client-ca.crt --service-account-key-file=/tmp/kube-serviceaccount.key --service-account-lookup=true --enable-admission-plugins=Initializers,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,PodPreset,StorageObjectInUseProtection --disable-admission-plugins= --admission-control-config-file= --bind-address=0.0.0.0 --secure-port=6443 --tls-cert-file=/var/run/kubernetes/serving-kube-apiserver.crt --tls-private-key-file=/var/run/kubernetes/serving-kube-apiserver.key --insecure-bind-address=127.0.0.1 --insecure-port=8080 --storage-backend=etcd3 --etcd-servers=http://127.0.0.1:2379 --service-cluster-ip-range=10.0.0.0/24 --feature-gates=AllAlpha=false --external-hostname=localhost --requestheader-username-headers=X-Remote-User --requestheader-group-headers=X-Remote-Group --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-client-ca-file=/var/run/kubernetes/request-header-ca.crt --requestheader-allowed-names=system:auth-proxy --proxy-client-cert-file=/var/run/kubernetes/client-auth-proxy.crt --proxy-client-key-file=/var/run/kubernetes/client-auth-proxy.key --cors-allowed-origins=\u0026#39;/127.0.0.1(:[0-9]+)?$,/localhost(:[0-9]+)?$\u0026#39;\r允许防火墙允许39999端口接入：\nfirewall-cmd --zone=public --add-port=39999/tcp --permanent\rfirewall-cmd --reload\r最后在goland里新建一个远程调试任务，这里的10.211.55.3为虚拟机的IP，39999为dlv的远程调试端口。在kubernetes/cmd/kube-apiserver/apiserver.go的main方法处打上断点后，启动该远程调试任务，即可正常进行断点调试了。\nOVER\n参考  https://github.com/kubernetes/kubernetes https://github.com/kubernetes/community/blob/master/contributors/devel/running-locally.md https://github.com/derekparker/delve/blob/master/Documentation/usage/dlv_exec.md https://segmentfault.com/a/1190000015807702 https://blog.csdn.net/u011846257/article/details/54707864 https://github.com/derekparker/delve/issues/178  ","permalink":"https://jeremyxu2010.github.io/2018/08/%E6%90%AD%E5%BB%BAk8s%E7%9A%84%E5%BC%80%E5%8F%91%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/","tags":["docker","k8s"],"title":"搭建k8s的开发调试环境"},{"categories":["golang开发"],"contents":"mongodb4.0也出来一段时间了，这个版本最为大众期待的特性就是支持了多文档事务（multi-document transaction），本文记录一下尝鲜该特性的过程。\nmongodb多文档事务  In MongoDB, an operation on a single document is atomic. Because you can use embedded documents and arrays to capture relationships between data in a single document structure instead of normalizing across multiple documents and collections, this single-document atomicity obviates the need for multi-document transactions for many practical use cases.\nHowever, for situations that require atomicity for updates to multiple documents or consistency between reads to multiple documents, MongoDB provides the ability to perform multi-document transactions against replica sets. Multi-document transactions can be used across multiple operations, collections, databases, and documents. Multi-document transactions provide an “all-or-nothing” proposition. When a transaction commits, all data changes made in the transaction are saved. If any operation in the transaction fails, the transaction aborts and all data changes made in the transaction are discarded without ever becoming visible. Until a transaction commits, no write operations in the transaction are visible outside the transaction.\n 在mongodb里，对于单个文档的操作本身是原子性的。而因为在mongodb里还可以采用嵌入式文档和数组来描述文档中的数据结构关系，所以这种单文档原子性基本消除了许多实际对多文档事务的需求。\n在mongodb4.0里，对于副本集中的多文档，现在也有了一个机制用来原子性地更新多个文档，以保证读取多个文档的一致性。\n看上去挺美好，不过官方文档也说了：\n IMPORTANT\nIn most cases, multi-document transaction incurs a greater performance cost over single document writes, and the availability of multi-document transaction should not be a replacement for effective schema design. For many scenarios, the denormalized data model (embedded documents and arrays) will continue to be optimal for your data and use cases. That is, for many scenarios, modeling your data appropriately will minimize the need for multi-document transactions.\nMulti-document transactions are available for replica sets only. Transactions for sharded clusters are scheduled for MongoDB 4.2\n 在大多数场景，多文档事务会产生较大的性能开销，所以合理的模式设计（嵌入式文档和数组）还是应该是最应该优先考虑的解决方案。\n另外4.0版本仅支持复制集中的多文档事务，分片集群中的多文档事务将计划在4.2版本中实现。\n虽然有以上这些限制，还再怎么说也多了多文档事务能力，比以前还是进步了的。\n尝鲜步骤 安装mongodb4.0 macOS系统比较简单：\nbrew install mongodb brew services start mongodb 设置复制集名称 参考mongodb的配置文件设置说明，修改其主配置文件\nvim /usr/local/etc/mongod.conf ...... # 这里添加复制集名称的选项 replication: replSetName: rs0 brew services start mongodb 检查特性兼容版本 因为多文档事务功能是4.0版本新加的，所以要保证特性兼容版本大于等于4.0\nmongo \u0026gt; db.adminCommand( { getParameter: 1, featureCompatibilityVersion: 1 } ) \u0026gt; // 如果特性兼容版本小于4.0，则要设置为4.0 \u0026gt; db.adminCommand( { setFeatureCompatibilityVersion: \u0026#34;4.0\u0026#34; } ) \u0026gt; exit 初始化复制集 使用复制集的方法初始化复制集\nmongo \u0026gt; rs.initiate() \u0026gt; rs.status() \u0026gt; exit 运行多文档事务的例子 从这里拷贝多文档事务的例子，保存为test.js\ntest.js\n// Runs the txnFunc and retries if TransientTransactionError encountered  function runTransactionWithRetry(txnFunc, session) { while (true) { try { txnFunc(session); // performs transaction  break; } catch (error) { // If transient error, retry the whole transaction  if ( error.hasOwnProperty(\u0026#34;errorLabels\u0026#34;) \u0026amp;\u0026amp; error.errorLabels.includes(\u0026#34;TransientTransactionError\u0026#34;) ) { print(\u0026#34;TransientTransactionError, retrying transaction ...\u0026#34;); continue; } else { throw error; } } } } // Retries commit if UnknownTransactionCommitResult encountered  function commitWithRetry(session) { while (true) { try { session.commitTransaction(); // Uses write concern set at transaction start.  print(\u0026#34;Transaction committed.\u0026#34;); break; } catch (error) { // Can retry commit  if (error.hasOwnProperty(\u0026#34;errorLabels\u0026#34;) \u0026amp;\u0026amp; error.errorLabels.includes(\u0026#34;UnknownTransactionCommitResult\u0026#34;) ) { print(\u0026#34;UnknownTransactionCommitResult, retrying commit operation ...\u0026#34;); continue; } else { print(\u0026#34;Error during commit ...\u0026#34;); throw error; } } } } // Updates two collections in a transactions  function updateEmployeeInfo(session) { employeesCollection = session.getDatabase(\u0026#34;hr\u0026#34;).employees; eventsCollection = session.getDatabase(\u0026#34;reporting\u0026#34;).events; session.startTransaction( { readConcern: { level: \u0026#34;snapshot\u0026#34; }, writeConcern: { w: \u0026#34;majority\u0026#34; } } ); try{ employeesCollection.updateOne( { employee: 3 }, { $set: { status: \u0026#34;Inactive\u0026#34; } } ); eventsCollection.insertOne( { employee: 3, status: { new: \u0026#34;Inactive\u0026#34;, old: \u0026#34;Active\u0026#34; } } ); } catch (error) { print(\u0026#34;Caught exception during transaction, aborting.\u0026#34;); session.abortTransaction(); throw error; } commitWithRetry(session); } // Start a session. session = db.getMongo().startSession( { mode: \u0026#34;primary\u0026#34; } ); try{ runTransactionWithRetry(updateEmployeeInfo, session); } catch (error) { // Do something with error } finally { session.endSession(); } 在运行上述脚本前先创建好脚本依赖的databases及collections：\nmongo \u0026gt; use hr \u0026gt; db.createCollection(\u0026#34;employees\u0026#34;); \u0026gt; use reporting \u0026gt; db.createCollection(\u0026#34;events\u0026#34;) \u0026gt; exit 最后运行示例脚本\nmongo \u0026gt; load(\u0026#34;test.js\u0026#34;) Transaction committed. true \u0026gt; exit 如果结果是上面那样，说明一切正常。\n官方示例虽然写得复杂了一点，不过是考虑了重试运行事务、重试提交事务场景的，应该说考虑还是比较周全的，可以作为其它语言实现的参考。\n多文档事务的限制  The following operations are not allowed in multi-document transactions:\n  Operations that affect the database catalog, such as creating or dropping a collection or an index. For example, a multi-document transaction cannot include an insert operation that would result in the creation of a new collection.\nThe listCollections and listIndexes commands and their helper methods are also excluded.\n  Non-CRUD and non-informational operations, such as createUser, getParameter, count, etc. and their helpers.\n   说白了就是只支持对现有collections的增删查改操作及一些基本的信息查询操作，一般数据结构定义操作是不支持了。另外连 listCollections ， listIndexes 都不支持，如果真有需求，必须在事务外先查询保存起来，这点就比较变态了。\n其它语言支持 java语言支持 mongodb的官方其实也提供了java语言的示例，不过在java领域还是spring框架用得比较多，spring-data要比较新的版本才支持mongodb事务特性，文档见这里，也有示例代码。\ngo语言支持 mongodb社区版go语言驱动目前还没有支持mongodb4.0的多文档事务特性，看其开发计划，短期是不太可能支持了。\n不过看mongodb官方go语言驱动的提交记录，好像前几天刚好实现了这个功能，赶紧模仿mongo-shell脚本写个go语言测试代码：\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/mongodb/mongo-go-driver/mongo\u0026#34; \u0026#34;github.com/mongodb/mongo-go-driver/core/command\u0026#34; \u0026#34;github.com/mongodb/mongo-go-driver/bson\u0026#34; ) func main() { client, err := mongo.Connect(context.Background(), \u0026#34;mongodb://localhost:27017\u0026#34;, nil) panicIfErr(err) sess, err := client.StartSession() panicIfErr(err) defer sess.EndSession(context.Background()) runTransactionWithRetry(updateEmployeeInfo, client, sess) } // Runs the txnFunc and retries if TransientTransactionError encountered  func runTransactionWithRetry(txnFunc func(*mongo.Client, *mongo.Session) error, client *mongo.Client, sess *mongo.Session) error { var err error for { err = txnFunc(client, sess) // performs transaction \tif err != nil { if v, ok := err.(command.Error); ok { if contains(v.Labels, command.TransientTransactionError) { fmt.Println(\u0026#34;TransientTransactionError, retrying transaction ...\u0026#34;) continue } } return err } break } return nil } // Retries commit if UnknownTransactionCommitResult encountered  func commitWithRetry(sess *mongo.Session) error { for { err := sess.CommitTransaction(context.Background()) // Uses write concern set at transaction start. \tif err != nil { if v, ok := err.(command.Error); ok { if contains(v.Labels, command.UnknownTransactionCommitResult) { fmt.Println(\u0026#34;UnknownTransactionCommitResult, retrying commit operation ...\u0026#34;) continue } } fmt.Println(\u0026#34;Error during commit ...\u0026#34;) return err } fmt.Println(\u0026#34;Transaction committed.\u0026#34;) break } return nil } // Updates two collections in a transactions  func updateEmployeeInfo(client *mongo.Client, sess *mongo.Session) error{ employeesCollection := client.Database(\u0026#34;hr\u0026#34;).Collection(\u0026#34;employees\u0026#34;) eventsCollection := client.Database(\u0026#34;reporting\u0026#34;).Collection(\u0026#34;events\u0026#34;) err := sess.StartTransaction() if err != nil { return err } _, err = employeesCollection.UpdateOne(context.Background(), bson.NewDocument( bson.EC.Int32(\u0026#34;employee\u0026#34;, 3), ),bson.NewDocument( bson.EC.SubDocumentFromElements(\u0026#34;$set\u0026#34;, bson.EC.String(\u0026#34;status\u0026#34;, \u0026#34;Inactive\u0026#34;), ), ), sess) if err != nil { fmt.Println(\u0026#34;Caught exception during transaction, aborting.\u0026#34;) sess.AbortTransaction(context.Background()) return err } _, err = eventsCollection.InsertOne(context.Background(), bson.NewDocument( bson.EC.Int32(\u0026#34;employee\u0026#34;, 3), bson.EC.SubDocumentFromElements(\u0026#34;status\u0026#34;, bson.EC.String(\u0026#34;new\u0026#34;, \u0026#34;Inactive\u0026#34;), bson.EC.String(\u0026#34;old\u0026#34;, \u0026#34;Active\u0026#34;), ), ), sess) if err != nil { fmt.Println(\u0026#34;Caught exception during transaction, aborting.\u0026#34;) sess.AbortTransaction(context.Background()) return err } return commitWithRetry(sess) } func panicIfErr(err error) { if err != nil { panic(err) } } func contains(slice []string, item string) bool { set := make(map[string]struct{}, len(slice)) for _, s := range slice { set[s] = struct{}{} } _, ok := set[item] return ok } 果然是好使的，不过mongodb官方go语言驱动目前还未发正式版，现在还是第11个alpha版本，能否直接用于生产就很难说了。\nOVER\n参考  https://docs.mongodb.com/master/core/transactions/ https://docs.mongodb.com/manual/reference/configuration-options/#replication-options https://docs.mongodb.com/manual/reference/method/js-replication/ https://docs.mongodb.com/master/core/transactions/ https://docs.mongodb.com/manual/tutorial/write-scripts-for-the-mongo-shell/ https://github.com/mongodb/mongo-go-driver/  ","permalink":"https://jeremyxu2010.github.io/2018/08/mongodb4.0%E5%A4%9A%E6%96%87%E6%A1%A3%E4%BA%8B%E5%8A%A1%E5%B0%9D%E9%B2%9C/","tags":["mongodb","golang","transactions"],"title":"mongodb4.0多文档事务尝鲜"},{"categories":["微服务"],"contents":"前面写过一篇servicecomb-saga开发实战，当时说后面有时间写一篇源码解读，不过工作一忙，就把这事儿忘了，今天终于得闲可以补上这个坑了。\n整个servicecomb-saga的代码量还是比较多的，这里着重解读下omega模块的源码，其实如果理解了omega模块的代码逻辑，alpha模块就比较清楚了。\nomega模块的功能 首先参考具体处理流程：\n成功场景 成功场景下，每个开始的事件都会有对应的结束事件。\n\n异常场景 异常场景下，omega会向alpha上报中断事件，然后alpha会向该全局事务的其它已完成的子事务发送补偿指令，确保最终所有的子事务要么都成功，要么都回滚。\n\n超时场景 超时场景下，已超时的事件会被alpha的定期扫描器检测出来，与此同时，该超时事务对应的全局事务也会被中断。\n\n从上述处理流程可以看出omega主要完成以下4大功能：\n 注入分布式事务ID（包括向当前服务注入分布式事务id、向调用的其它服务传递分布式事务id） 在整个分布式事务开始与结束时记录saga执行事件 在本地事务方法执行的前后记录saga执行事件 收到补偿事件后执行补偿方法，并记录saga补偿执行事件  后面在解读时会逐一说明上述4大功能在代码上是如何实现的。\nomega代码解读 参考添加saga的注解及相应的补偿方法，我们可以看到servicecomb-saga仅要求业务应用配置EnableOmega，@SagaStart，@Compensable这三个annotation，下面看下这三个annotation具体是如何工作的。\nincubator-servicecomb-saga/omega/omega-spring-starter/src/main/java/org/apache/servicecomb/saga/omega/spring/EnableOmega.java\n@Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Import({OmegaSpringConfig.class, TransactionAspectConfig.class}) /** * Indicates create the OmegaContext and inject it into the interceptors * to pass the transactions id across the application. * @see org.apache.servicecomb.saga.omega.context.OmegaContext */ public @interface EnableOmega { } 一看名字就知道，这是个spring特性开关annotation，当在业务应用中打上此annotation，则会导入一些配置类，这种写法在spring-boot里很常见。\n再来看OmegaSpringConfig，TransactionAspectConfig。\nincubator-servicecomb-saga/omega/omega-spring-starter/src/main/java/org/apache/servicecomb/saga/omega/spring/OmegaSpringConfig.java\n@Configuration class OmegaSpringConfig { @Bean(name = {\u0026#34;omegaUniqueIdGenerator\u0026#34;}) IdGenerator\u0026lt;String\u0026gt; idGenerator() { return new UniqueIdGenerator(); } @Bean OmegaContext omegaContext(@Qualifier(\u0026#34;omegaUniqueIdGenerator\u0026#34;) IdGenerator\u0026lt;String\u0026gt; idGenerator) { return new OmegaContext(idGenerator); } @Bean CompensationContext compensationContext(OmegaContext omegaContext) { return new CompensationContext(omegaContext); } @Bean ServiceConfig serviceConfig(@Value(\u0026#34;${spring.application.name}\u0026#34;) String serviceName) { return new ServiceConfig(serviceName); } @Bean MessageSender grpcMessageSender( @Value(\u0026#34;${alpha.cluster.address:localhost:8080}\u0026#34;) String[] addresses, @Value(\u0026#34;${alpha.cluster.ssl.enable:false}\u0026#34;) boolean enableSSL, @Value(\u0026#34;${alpha.cluster.ssl.mutualAuth:false}\u0026#34;) boolean mutualAuth, @Value(\u0026#34;${alpha.cluster.ssl.cert:client.crt}\u0026#34;) String cert, @Value(\u0026#34;${alpha.cluster.ssl.key:client.pem}\u0026#34;) String key, @Value(\u0026#34;${alpha.cluster.ssl.certChain:ca.crt}\u0026#34;) String certChain, @Value(\u0026#34;${omega.connection.reconnectDelay:3000}\u0026#34;) int reconnectDelay, ServiceConfig serviceConfig, @Lazy MessageHandler handler) { MessageFormat messageFormat = new KryoMessageFormat(); AlphaClusterConfig clusterConfig = new AlphaClusterConfig(Arrays.asList(addresses), enableSSL, mutualAuth, cert, key, certChain); final MessageSender sender = new LoadBalancedClusterMessageSender( clusterConfig, messageFormat, messageFormat, serviceConfig, handler, reconnectDelay); sender.onConnected(); Runtime.getRuntime().addShutdownHook(new Thread(new Runnable() { @Override public void run() { sender.onDisconnected(); sender.close(); } })); return sender; } } 这个配置类里声明了5个spring bean，功能如下：\n omegaUniqueIdGenerator：这是一个唯一ID生成器，用于给分布式事务生成唯一的全局事务ID及本地事务ID。 omegaContext：这个bean里保存了当前的事务上下文信息（主要就是全局事务ID及本地事务ID），同时也提供API，用于读取设置当前的事务上下文信息。 compensationContext：这个bean里保存了可被调用的补偿方法，同时也提供API供其它部分执行某个补偿方法。 serviceConfig：这个bean里保存了当前业务服务的唯一标识。 grpcMessageSender：这个bean维护与alpha的grpc连接，同时如其名称提供API供其它部分通过grpc发送saga事件至alpha。  incubator-servicecomb-saga/omega/omega-spring-tx/src/main/java/org/apache/servicecomb/saga/omega/transaction/spring/TransactionAspectConfig.java\n@Configuration @EnableAspectJAutoProxy public class TransactionAspectConfig { @Bean MessageHandler messageHandler(MessageSender sender, CompensationContext context, OmegaContext omegaContext) { return new CompensationMessageHandler(sender, context); } @Order(0) @Bean SagaStartAspect sagaStartAspect(MessageSender sender, OmegaContext context) { return new SagaStartAspect(sender, context); } @Order(1) @Bean TransactionAspect transactionAspect(MessageSender sender, OmegaContext context) { return new TransactionAspect(sender, context); } @Bean CompensableAnnotationProcessor compensableAnnotationProcessor(OmegaContext omegaContext, CompensationContext compensationContext) { return new CompensableAnnotationProcessor(omegaContext, compensationContext); } } 这个配置类里声明了4个spring bean，功能如下：\n messageHandler：这个bean处理从alpha接收到的补偿事件，主要逻辑就是收到补偿事件后执行补偿方法，并向alpha发送saga补偿执行完成事件。 sagaStartAspect：这个bean完成@SagaStart这个annotation的AOP拦截处理，主要逻辑就是在整个分布式事务开始与结束时记录saga执行事件。 transactionAspect：这个bean完成@Compensable这个annotation的AOP拦截处理，主要逻辑就是在本地事务方法执行的前后记录saga执行事件。 compensableAnnotationProcessor：这个bean完成两个功能：   完成@OmegaContextAware这个annotation的处理逻辑，主要逻辑是当spring bean的某个field是一个Executor，并且打上了@OmegaContextAware这个annotation，则让在这个Executor中执行的任务执行前设置上正确的事务上下文信息（主要就是全局事务ID及本地事务ID）。从代码上看目前这个功能仅在框架内部使用。 将打上@Compensable这个annotation的方法提前注册好，保存在compensationContext这个bean中。  其实上面那样将主要的spring bean功能解读一遍后，整个脉络就很清楚了。这里再复述一遍omega的主体功能的如何实现的。\n注入分布式事务ID 通过对@SagaStart这个annotation的AOP拦截处理，在分布式事务开始时给当前分布式事务ID分配全局唯一ID，代码如下：\nincubator-servicecomb-saga/omega/omega-transaction/src/main/java/org/apache/servicecomb/saga/omega/transaction/SagaStartAspect.java\n@Around(\u0026#34;execution(@org.apache.servicecomb.saga.omega.context.annotations.SagaStart * *(..)) \u0026amp;\u0026amp; @annotation(sagaStart)\u0026#34;) Object advise(ProceedingJoinPoint joinPoint, SagaStart sagaStart) throws Throwable { initializeOmegaContext(); ...... } private void initializeOmegaContext() { context.setLocalTxId(context.newGlobalTxId()); } 通过对@Compensable这个annotation的AOP拦截处理，在本地事务开始时给当前本地事务ID分配唯一ID，代码如下：\nincubator-servicecomb-saga/omega/omega-transaction/src/main/java/org/apache/servicecomb/saga/omega/transaction/TransactionAspect.java\n@Around(\u0026#34;execution(@org.apache.servicecomb.saga.omega.transaction.annotations.Compensable * *(..)) \u0026amp;\u0026amp; @annotation(compensable)\u0026#34;) Object advise(ProceedingJoinPoint joinPoint, Compensable compensable) throws Throwable { Method method = ((MethodSignature) joinPoint.getSignature()).getMethod(); String localTxId = context.localTxId(); context.newLocalTxId(); LOG.debug(\u0026#34;Updated context {} for compensable method {} \u0026#34;, context, method.toString()); ...... try { ...... } finally { context.setLocalTxId(localTxId); LOG.debug(\u0026#34;Restored context back to {}\u0026#34;, context); } } 通过不同RequestInterceptor将当前的分布式上下文信息通过请求头等方式传递给其它的服务，代码如下：\n框架实现了基于多种transport的分布式上下文信息传递方案，见incubator-servicecomb-saga/omega/omega-transport目录下的各类实现。下面的代码以resttemplate为例。\nincubator-servicecomb-saga/omega/omega-transport/omega-transport-resttemplate/src/main/java/org/apache/servicecomb/saga/omega/transport/resttemplate/TransactionClientHttpRequestInterceptor.java\nclass TransactionClientHttpRequestInterceptor implements ClientHttpRequestInterceptor { private static final Logger LOG = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass()); private final OmegaContext omegaContext; TransactionClientHttpRequestInterceptor(OmegaContext omegaContext) { this.omegaContext = omegaContext; } @Override public ClientHttpResponse intercept(HttpRequest request, byte[] body, ClientHttpRequestExecution execution) throws IOException { if (omegaContext!= null \u0026amp;\u0026amp; omegaContext.globalTxId() != null) { request.getHeaders().add(GLOBAL_TX_ID_KEY, omegaContext.globalTxId()); request.getHeaders().add(LOCAL_TX_ID_KEY, omegaContext.localTxId()); LOG.debug(\u0026#34;Added {} {} and {} {} to request header\u0026#34;, GLOBAL_TX_ID_KEY, omegaContext.globalTxId(), LOCAL_TX_ID_KEY, omegaContext.localTxId()); } return execution.execute(request, body); } } 通过HandlerInterceptor在调用具体业务方法前将传递来的分布式上下文信息保存进OmegaContext，代码如下：\nincubator-servicecomb-saga/omega/omega-transport/omega-transport-resttemplate/src/main/java/org/apache/servicecomb/saga/omega/transport/resttemplate/TransactionHandlerInterceptor.java\nclass TransactionHandlerInterceptor implements HandlerInterceptor { private static final Logger LOG = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass()); private final OmegaContext omegaContext; TransactionHandlerInterceptor(OmegaContext omegaContext) { this.omegaContext = omegaContext; } @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) { if (omegaContext != null) { String globalTxId = request.getHeader(GLOBAL_TX_ID_KEY); if (globalTxId == null) { LOG.debug(\u0026#34;no such header: {}\u0026#34;, GLOBAL_TX_ID_KEY); } else { omegaContext.setGlobalTxId(globalTxId); omegaContext.setLocalTxId(request.getHeader(LOCAL_TX_ID_KEY)); } } return true; } @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object o, ModelAndView mv) { } @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object o, Exception e) { } } 分布式事务开始与结束时记录saga执行事件 通过对@SagaStart这个annotation的AOP拦截处理，在整个分布式事务开始与结束时记录saga执行事件，代码如下：\nincubator-servicecomb-saga/omega/omega-transaction/src/main/java/org/apache/servicecomb/saga/omega/transaction/SagaStartAspect.java\n@Aspect public class SagaStartAspect { ...... @Around(\u0026#34;execution(@org.apache.servicecomb.saga.omega.context.annotations.SagaStart * *(..)) \u0026amp;\u0026amp; @annotation(sagaStart)\u0026#34;) Object advise(ProceedingJoinPoint joinPoint, SagaStart sagaStart) throws Throwable { ...... sagaStartAnnotationProcessor.preIntercept(context.globalTxId(), method.toString(), sagaStart.timeout(), \u0026#34;\u0026#34;, 0); LOG.debug(\u0026#34;Initialized context {} before execution of method {}\u0026#34;, context, method.toString()); try { Object result = joinPoint.proceed(); sagaStartAnnotationProcessor.postIntercept(context.globalTxId(), method.toString()); LOG.debug(\u0026#34;Transaction with context {} has finished.\u0026#34;, context); return result; } catch (Throwable throwable) { // We don\u0026#39;t need to handle the OmegaException here  if (!(throwable instanceof OmegaException)) { sagaStartAnnotationProcessor.onError(context.globalTxId(), method.toString(), throwable); LOG.error(\u0026#34;Transaction {} failed.\u0026#34;, context.globalTxId()); } throw throwable; } finally { ...... } } ...... } incubator-servicecomb-saga/omega/omega-transaction/src/main/java/org/apache/servicecomb/saga/omega/transaction/SagaStartAnnotationProcessor.java\nclass SagaStartAnnotationProcessor implements EventAwareInterceptor { private final OmegaContext omegaContext; private final MessageSender sender; SagaStartAnnotationProcessor(OmegaContext omegaContext, MessageSender sender) { this.omegaContext = omegaContext; this.sender = sender; } @Override public AlphaResponse preIntercept(String parentTxId, String compensationMethod, int timeout, String retriesMethod, int retries, Object... message) { try { return sender.send(new SagaStartedEvent(omegaContext.globalTxId(), omegaContext.localTxId(), timeout)); } catch (OmegaException e) { throw new TransactionalException(e.getMessage(), e.getCause()); } } @Override public void postIntercept(String parentTxId, String compensationMethod) { AlphaResponse response = sender.send(new SagaEndedEvent(omegaContext.globalTxId(), omegaContext.localTxId())); if (response.aborted()) { throw new OmegaException(\u0026#34;transaction \u0026#34; + parentTxId + \u0026#34; is aborted\u0026#34;); } } @Override public void onError(String parentTxId, String compensationMethod, Throwable throwable) { String globalTxId = omegaContext.globalTxId(); sender.send(new TxAbortedEvent(globalTxId, omegaContext.localTxId(), null, compensationMethod, throwable)); } } 本地事务方法执行前后记录saga执行事件 通过对@Compensable这个annotation的AOP拦截处理，在本地事务开始与结束时记录saga执行事件，代码如下：\nincubator-servicecomb-saga/omega/omega-transaction/src/main/java/org/apache/servicecomb/saga/omega/transaction/DefaultRecovery.java\npublic class DefaultRecovery implements RecoveryPolicy { private static final Logger LOG = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass()); @Override public Object apply(ProceedingJoinPoint joinPoint, Compensable compensable, CompensableInterceptor interceptor, OmegaContext context, String parentTxId, int retries) throws Throwable { ...... AlphaResponse response = interceptor.preIntercept(parentTxId, compensationSignature, compensable.timeout(), retrySignature, retries, joinPoint.getArgs()); ...... try { Object result = joinPoint.proceed(); interceptor.postIntercept(parentTxId, compensationSignature); return result; } catch (Throwable throwable) { interceptor.onError(parentTxId, compensationSignature, throwable); throw throwable; } } ...... } incubator-servicecomb-saga/omega/omega-transaction/src/main/java/org/apache/servicecomb/saga/omega/transaction/CompensableInterceptor.java\nclass CompensableInterceptor implements EventAwareInterceptor { private final OmegaContext context; private final MessageSender sender; CompensableInterceptor(OmegaContext context, MessageSender sender) { this.sender = sender; this.context = context; } @Override public AlphaResponse preIntercept(String parentTxId, String compensationMethod, int timeout, String retriesMethod, int retries, Object... message) { return sender.send(new TxStartedEvent(context.globalTxId(), context.localTxId(), parentTxId, compensationMethod, timeout, retriesMethod, retries, message)); } @Override public void postIntercept(String parentTxId, String compensationMethod) { sender.send(new TxEndedEvent(context.globalTxId(), context.localTxId(), parentTxId, compensationMethod)); } @Override public void onError(String parentTxId, String compensationMethod, Throwable throwable) { sender.send( new TxAbortedEvent(context.globalTxId(), context.localTxId(), parentTxId, compensationMethod, throwable)); } } 收到补偿事件后的处理流程 通过Server streaming的gRPC，当从alpha收到补偿事件后，调用消息处理器，消息处理器则会执行对应的补偿方法，并记录saga补偿执行事件，代码如下：\nincubator-servicecomb-saga/omega/omega-connector/omega-connector-grpc/src/main/java/org/apache/servicecomb/saga/omega/connector/grpc/GrpcCompensateStreamObserver.java\nclass GrpcCompensateStreamObserver implements StreamObserver\u0026lt;GrpcCompensateCommand\u0026gt; { ...... @Override public void onNext(GrpcCompensateCommand command) { LOG.info(\u0026#34;Received compensate command, global tx id: {}, local tx id: {}, compensation method: {}\u0026#34;, command.getGlobalTxId(), command.getLocalTxId(), command.getCompensationMethod()); messageHandler.onReceive( command.getGlobalTxId(), command.getLocalTxId(), command.getParentTxId().isEmpty() ? null : command.getParentTxId(), command.getCompensationMethod(), deserializer.deserialize(command.getPayloads().toByteArray())); } ...... } incubator-servicecomb-saga/omega/omega-transaction/src/main/java/org/apache/servicecomb/saga/omega/transaction/CompensationMessageHandler.java\npublic class CompensationMessageHandler implements MessageHandler { ...... @Override public void onReceive(String globalTxId, String localTxId, String parentTxId, String compensationMethod, Object... payloads) { context.apply(globalTxId, localTxId, compensationMethod, payloads); sender.send(new TxCompensatedEvent(globalTxId, localTxId, parentTxId, compensationMethod)); } } 以上就是omega主体流程的代码解读了，下面说一些框架实现的其它特性。\nsaga消息发送支持多alpha负载均衡及重试 通过LoadBalancedClusterMessageSender、RetryableMessageSender（这个貌似没有实现完）包装原始的GrpcClientMessageSender，以支持saga消息发送的多alpha负载均衡、发送失败重试，代码如一下：\nincubator-servicecomb-saga/omega/omega-connector/omega-connector-grpc/src/main/java/org/apache/servicecomb/saga/omega/connector/grpc/LoadBalancedClusterMessageSender.java\npublic class LoadBalancedClusterMessageSender implements MessageSender { ...... private final Map\u0026lt;MessageSender, Long\u0026gt; senders = new ConcurrentHashMap\u0026lt;\u0026gt;(); private final Collection\u0026lt;ManagedChannel\u0026gt; channels; private final BlockingQueue\u0026lt;Runnable\u0026gt; pendingTasks = new LinkedBlockingQueue\u0026lt;\u0026gt;(); private final BlockingQueue\u0026lt;MessageSender\u0026gt; availableMessageSenders = new LinkedBlockingQueue\u0026lt;\u0026gt;(); private final MessageSender retryableMessageSender = new RetryableMessageSender( availableMessageSenders); private final Supplier\u0026lt;MessageSender\u0026gt; defaultMessageSender = new Supplier\u0026lt;MessageSender\u0026gt;() { @Override public MessageSender get() { return retryableMessageSender; } }; ...... public LoadBalancedClusterMessageSender(AlphaClusterConfig clusterConfig, MessageSerializer serializer, MessageDeserializer deserializer, ServiceConfig serviceConfig, MessageHandler handler, int reconnectDelay) { ...... for (String address : clusterConfig.getAddresses()) { ManagedChannel channel; if (clusterConfig.isEnableSSL()) { if (sslContext == null) { try { sslContext = buildSslContext(clusterConfig); } catch (SSLException e) { throw new IllegalArgumentException(\u0026#34;Unable to build SslContext\u0026#34;, e); } } channel = NettyChannelBuilder.forTarget(address) .negotiationType(NegotiationType.TLS) .sslContext(sslContext) .build(); } else { channel = ManagedChannelBuilder.forTarget(address).usePlaintext() .build(); } channels.add(channel); senders.put( new GrpcClientMessageSender( address, channel, serializer, deserializer, serviceConfig, new ErrorHandlerFactory(), handler), 0L); } ...... } ...... @Override public void onConnected() { for(MessageSender sender :senders.keySet()){ try { sender.onConnected(); } catch (Exception e) { LOG.error(\u0026#34;Failed connecting to alpha at {}\u0026#34;, sender.target(), e); } } } @Override public void onDisconnected() { for (MessageSender sender :senders.keySet()) { try { sender.onDisconnected(); } catch (Exception e) { LOG.error(\u0026#34;Failed disconnecting from alpha at {}\u0026#34;, sender.target(), e); } } } @Override public void close() { scheduler.shutdown(); for(ManagedChannel channel : channels) { channel.shutdownNow(); } } @Override public String target() { return \u0026#34;UNKNOWN\u0026#34;; } @Override public AlphaResponse send(TxEvent event) { return send(event, new FastestSender()); } AlphaResponse send(TxEvent event, MessageSenderPicker messageSenderPicker) { do { MessageSender messageSender = messageSenderPicker.pick(senders, defaultMessageSender); try { long startTime = System.nanoTime(); AlphaResponse response = messageSender.send(event); senders.put(messageSender, System.nanoTime() - startTime); return response; } catch (OmegaException e) { throw e; } catch (Exception e) { LOG.error(\u0026#34;Retry sending event {} due to failure\u0026#34;, event, e); // very large latency on exception  senders.put(messageSender, Long.MAX_VALUE); } } while (!Thread.currentThread().isInterrupted()); throw new OmegaException(\u0026#34;Failed to send event \u0026#34; + event + \u0026#34; due to interruption\u0026#34;); } ...... } /** * The strategy of picking the fastest {@link MessageSender} */ class FastestSender implements MessageSenderPicker { @Override public MessageSender pick(Map\u0026lt;MessageSender, Long\u0026gt; messageSenders, Supplier\u0026lt;MessageSender\u0026gt; defaultSender) { Long min = Long.MAX_VALUE; MessageSender sender = null; for (Map.Entry\u0026lt;MessageSender, Long\u0026gt; entry : messageSenders.entrySet()) { if (entry.getValue() != Long.MAX_VALUE) { if (min \u0026gt; entry.getValue()) { min = entry.getValue(); sender = entry.getKey(); } } } if (sender == null) { return defaultSender.get(); } else { return sender; } } } Server streaming gRPC连接中断尝试重连 当与alpha的Server streaming gRPC连接中断后，会往任务队列里扔进一个重新建立Server streaming gRPC连接的任务，而有一个定时执行的单线程池，其会定时扫描该队列里的任务，如有新的任务则会拿出来执行，代码如下：\nincubator-servicecomb-saga/omega/omega-connector/omega-connector-grpc/src/main/java/org/apache/servicecomb/saga/omega/connector/grpc/GrpcCompensateStreamObserver.java\nclass GrpcCompensateStreamObserver implements StreamObserver\u0026lt;GrpcCompensateCommand\u0026gt; { ...... @Override public void onError(Throwable t) { LOG.error(\u0026#34;failed to process grpc compensate command.\u0026#34;, t); errorHandler.run(); } ...... } incubator-servicecomb-saga/omega/omega-connector/omega-connector-grpc/src/main/java/org/apache/servicecomb/saga/omega/connector/grpc/LoadBalancedClusterMessageSender.java\npublic class LoadBalancedClusterMessageSender implements MessageSender { ...... private final BlockingQueue\u0026lt;Runnable\u0026gt; pendingTasks = new LinkedBlockingQueue\u0026lt;\u0026gt;(); ...... private final ScheduledExecutorService scheduler = Executors.newSingleThreadScheduledExecutor(); public LoadBalancedClusterMessageSender(AlphaClusterConfig clusterConfig, MessageSerializer serializer, MessageDeserializer deserializer, ServiceConfig serviceConfig, MessageHandler handler, int reconnectDelay) { ...... scheduleReconnectTask(reconnectDelay); } ...... private void scheduleReconnectTask(int reconnectDelay) { scheduler.scheduleWithFixedDelay(new Runnable() { @Override public void run() { try { pendingTasks.take().run(); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } } }, 0, reconnectDelay, MILLISECONDS); } class ErrorHandlerFactory { Runnable getHandler(MessageSender messageSender) { final Runnable runnable = new PushBackReconnectRunnable(messageSender, senders, pendingTasks, availableMessageSenders); return new Runnable() { @Override public void run() { pendingTasks.offer(runnable); } }; } } ...... } incubator-servicecomb-saga/omega/omega-connector/omega-connector-grpc/src/main/java/org/apache/servicecomb/saga/omega/connector/grpc/PushBackReconnectRunnable.java\nclass PushBackReconnectRunnable implements Runnable { private static final Logger LOG = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass()); private final MessageSender messageSender; private final Map\u0026lt;MessageSender, Long\u0026gt; senders; private final BlockingQueue\u0026lt;Runnable\u0026gt; pendingTasks; private final BlockingQueue\u0026lt;MessageSender\u0026gt; connectedSenders; PushBackReconnectRunnable( MessageSender messageSender, Map\u0026lt;MessageSender, Long\u0026gt; senders, BlockingQueue\u0026lt;Runnable\u0026gt; pendingTasks, BlockingQueue\u0026lt;MessageSender\u0026gt; connectedSenders) { this.messageSender = messageSender; this.senders = senders; this.pendingTasks = pendingTasks; this.connectedSenders = connectedSenders; } @Override public void run() { try { LOG.info(\u0026#34;Retry connecting to alpha at {}\u0026#34;, messageSender.target()); messageSender.onDisconnected(); messageSender.onConnected(); senders.put(messageSender, 0L); connectedSenders.offer(messageSender); LOG.info(\u0026#34;Retry connecting to alpha at {} is successful\u0026#34;, messageSender.target()); } catch (Exception e) { LOG.error(\u0026#34;Failed to reconnect to alpha at {}\u0026#34;, messageSender.target(), e); pendingTasks.offer(this); } } } 方法执行参数的序列化 在记录saga事件时需要将Compensable方法的执行参数序列化保存下来，用于后面调用补偿方法时使用，这里使用了在java领域比较高效的kryo序列化技术，代码如下：\nincubator-servicecomb-saga/omega/omega-format/src/main/java/org/apache/servicecomb/saga/omega/format/KryoMessageFormat.java\npublic class KryoMessageFormat implements MessageFormat { private static final int DEFAULT_BUFFER_SIZE = 4096; private static final KryoFactory factory = new KryoFactory() { @Override public Kryo create() { return new Kryo(); } }; private static final KryoPool pool = new KryoPool.Builder(factory).softReferences().build(); @Override public byte[] serialize(Object[] objects) { Output output = new Output(DEFAULT_BUFFER_SIZE, -1); Kryo kryo = pool.borrow(); kryo.writeObjectOrNull(output, objects, Object[].class); pool.release(kryo); return output.toBytes(); } @Override public Object[] deserialize(byte[] message) { try { Input input = new Input(new ByteArrayInputStream(message)); Kryo kryo = pool.borrow(); Object[] objects = kryo.readObjectOrNull(input, Object[].class); pool.release(kryo); return objects; } catch (KryoException e) { throw new OmegaException(\u0026#34;Unable to deserialize message\u0026#34;, e); } } } omega主体流程的代码解读就到这里了。\nomega的golang实现 servicecomb-saga整个是java实现的，而对于golang语言实现的业务来说，不太好接入，这里为了加深对框架的理解，顺手写了一个omega的golang实现，github地址：https://github.com/jeremyxu2010/matrix-saga-go\n参考  https://github.com/apache/incubator-servicecomb-saga  ","permalink":"https://jeremyxu2010.github.io/2018/08/servicecomb-saga%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/","tags":["microservice","java"],"title":"servicecomb-saga源码解读"},{"categories":["容器编排"],"contents":"以前外部访问k8s里的服务，都是直接以http方式进行的，缺少TLS安全，今天抽空把这块处理一下。\n生成并信任自签名证书 首先这里生成自签名的服务器证书，官方介绍了easyrsa, openssl 、 cfssl三个工具，这里使用cfssl。\nbrew install -y cfssl # 生成默认配置文件 cfssl print-defaults config \u0026gt; config.json cfssl print-defaults csr \u0026gt; csr.json # 生成自定义的config.json文件 cp config.json ca-config.json # 生成ca和server的证书请求json文件 cp csr.json ca-csr.json cp csr.json server-csr.json 编辑ca-config.json，内容如下：\n{ \u0026#34;signing\u0026#34;: { \u0026#34;default\u0026#34;: { \u0026#34;expiry\u0026#34;: \u0026#34;168h\u0026#34; }, \u0026#34;profiles\u0026#34;: { \u0026#34;k8s-local\u0026#34;: { \u0026#34;expiry\u0026#34;: \u0026#34;8760h\u0026#34;, \u0026#34;usages\u0026#34;: [ \u0026#34;signing\u0026#34;, \u0026#34;key encipherment\u0026#34;, \u0026#34;server auth\u0026#34;, \u0026#34;client auth\u0026#34; ] } } } } 编辑ca-csr.json，内容如下：\n{ \u0026#34;CN\u0026#34;: \u0026#34;k8s-local\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;GuangDong\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;Shenzhen\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;my self signed certificate\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;self signed\u0026#34; } ] } 编辑server-csr.json，内容如下：\n{ \u0026#34;CN\u0026#34;: \u0026#34;k8s.local\u0026#34;, \u0026#34;hosts\u0026#34;: [ \u0026#34;127.0.0.1\u0026#34;, \u0026#34;*.k8s.local\u0026#34; ], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;GuangDong\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;Shenzhen\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;my self signed certificate\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;self signed\u0026#34; } ] } 执行以下命令，生成CA证书及服务器证书\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca cfssl gencert -ca=ca.pem -ca-key=ca-key.pem --config=ca-config.json -profile=k8s-local server-csr.json | cfssljson -bare server 这样就得到ca.pem，server-key.pem，server.pem三个证书文件，其中ca.pem是ca的证书，server-key.pem是服务器证书的密钥，server.pem是服务器证书。\n用Keychain Access打开ca.pem文件，然后修改设置，信任该CA，如下图如示：\n在k8s里使用自签名证书 创建默认的tls secret：\nkubectl -n kube-system create secret tls default-tls-cert --key=server-key.pem --cert=server.pem 这里举例，现在有一个服务k8s-dashboard，它是以下面的方式部署进k8s的：\nhelm install --name=local-k8s-dashboard --namespace kube-system stable/kubernetes-dashboard 而该k8s集群已经部署了nginx-ingress-controller，使用的以下命令：\nhelm install --name local-nginx-ingress stable/nginx-ingress 这里就可以创建k8s-dashboard这个服务的ingress规则了，如下：\ncat \u0026lt;\u0026lt; EOF | kubectl create -f - apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/ssl-redirect: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/rewrite-target: / nginx.ingress.kubernetes.io/secure-backends: \u0026#34;true\u0026#34; name: k8s-dashboard-ingress namespace: kube-system spec: tls: - hosts: - k8s-dashboard.k8s.local secretName: default-tls-cert rules: - host: k8s-dashboard.k8s.local http: paths: - backend: serviceName: local-k8s-dashboard-kubernetes-dashboard servicePort: 443 EOF 注意，这里因为k8s-dashboard这个服务本身是以https提供服务的，所以才加上了一些与ssl相关的annotations，如果只是普通http服务，则不需要这些annotations。\n最后在chrome浏览器中就可以以https://k8s-dashboard.k8s.local访问k8s-dashboard服务了，而且浏览器地址栏是安全的绿色哦。\n为何选nginx-ingress 在上述过程中对比了k8s里两个比较重要的ingress controller：traefik-ingress和nginx-ingress，比较起来，还是nginx-ingress功能更强大，与k8s整合更好一些，看来有k8s官方维护支持果然很强大。\nnginx-ingress的用户指南也写得很详细，以后可以多看看。\n参考  https://kubernetes.io/docs/concepts/cluster-administration/certificates/ https://github.com/helm/charts/blob/master/stable/kubernetes-dashboard https://github.com/helm/charts/tree/master/stable/nginx-ingress https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx/examples/tls https://docs.traefik.io/configuration/backends/kubernetes/  ","permalink":"https://jeremyxu2010.github.io/2018/08/k8s%E5%8A%A0%E5%85%A5tls%E5%AE%89%E5%85%A8%E8%AE%BF%E9%97%AE/","tags":["kubernetes","tls"],"title":"k8s加入TLS安全访问"},{"categories":["微服务"],"contents":"这周的工作是要为已有的系统搭建一套监控系统，主要监控以下指标：\n 宿主机的CPU、内存使用情况 自身系统的各进程占用CPU、内存使用情况 自身系统本身的重要业务指标  以前用过nagios和zabbix，nagios用起来太过原始，配置文件维护得很累，监控的图表也比较难看；zabbix的主要开发语言是C和PHP，要暴露一些自定义的监控指标较困难。网上一些云原生的项目都是用prometheus+grafana方案的，刚好花时间研究一下这个。\n以下的概述摘自 https://github.com/1046102779/prometheus/blob/master/introduction/overview.md\n什么是prometheus？ Prometheus是一个开源监控系统，它前身是SoundCloud的警告工具包。从2012年开始，许多公司和组织开始使用Prometheus。该项目的开发人员和用户社区非常活跃，越来越多的开发人员和用户参与到该项目中。目前它是一个独立的开源项目，且不依赖与任何公司。 为了强调这点和明确该项目治理结构，Prometheus在2016年继Kurberntes之后，加入了Cloud Native Computing Foundation。\n特征 Prometheus的主要特征有：\n 多维度数据模型 灵活的查询语言 不依赖分布式存储，单个服务器节点是自主的 以HTTP方式，通过pull模型拉去时间序列数据 也通过中间网关支持push模型 通过服务发现或者静态配置，来发现目标服务对象 支持多种多样的图表和界面展示，grafana也支持它  组件 Prometheus生态包括了很多组件，它们中的一些是可选的：\n 主服务Prometheus Server负责抓取和存储时间序列数据 客户库负责检测应用程序代码 支持短生命周期的PUSH网关 基于Rails/SQL仪表盘构建器的GUI 多种导出工具，可以支持Prometheus存储数据转化为HAProxy、StatsD、Graphite等工具所需要的数据存储格式 警告管理器 命令行查询工具 其他各种支撑工具  多数Prometheus组件是Go语言写的，这使得这些组件很容易编译和部署。\n架构 下面这张图说明了Prometheus的整体架构，以及生态中的一些组件作用: Prometheus服务，可以直接通过目标拉取数据，或者间接地通过中间网关拉取数据。它在本地存储抓取的所有数据，并通过一定规则进行清理和整理数据，并把得到的结果存储到新的时间序列中，PromQL和其他API可视化地展示收集的数据\n适用场景 Prometheus在记录纯数字时间序列方面表现非常好。它既适用于面向服务器等硬件指标的监控，也适用于高动态的面向服务架构的监控。对于现在流行的微服务，Prometheus的多维度数据收集和数据筛选查询语言也是非常的强大。\nPrometheus是为服务的可靠性而设计的，当服务出现故障时，它可以使你快速定位和诊断问题。它的搭建过程对硬件和服务没有很强的依赖关系。\n不适用场景 Prometheus，它的价值在于可靠性，甚至在很恶劣的环境下，你都可以随时访问它和查看系统服务各种指标的统计信息。 如果你对统计数据需要100%的精确，它并不适用，例如：它不适用于实时计费系统\nprometheus的概念 prometheus里的概念比较少，重要的只有以下几个。\nJobs和Instances(任务和实例) 就Prometheus而言，pull拉取采样点的端点服务称之为instance。多个这样pull拉取采样点的instance, 则构成了一个job。\n例如, 一个被称作api-server的任务有四个相同的实例。\njob: api-server instance 1：1.2.3.4:5670 instance 2：1.2.3.4:5671 instance 3：5.6.7.8:5670 instance 4：5.6.7.8:5671 自动化生成的标签和时间序列 当Prometheus拉取一个目标, 会自动地把两个标签添加到度量名称的标签列表中，分别是：\njob: 目标所属的配置任务名称api-server。 instance: 采样点所在服务: host:port 如果以上两个标签二者之一存在于采样点中，这个取决于honor_labels配置选项。\n对于每个采样点所在服务instance，Prometheus都会存储以下的度量指标采样点：\n up{job=\u0026rdquo;[job-name]\u0026quot;, instance=\u0026quot;instance-id\u0026rdquo;}: up值=1，表示采样点所在服务健康; 否则，网络不通, 或者服务挂掉了 scrape_duration_seconds{job=\u0026rdquo;[job-name]\u0026quot;, instance=\u0026rdquo;[instance-id]\u0026quot;}: 尝试获取目前采样点的时间开销 scrape_samples_scraped{job=\u0026rdquo;[job-name]\u0026quot;, instance=\u0026rdquo;[instance-id]\u0026quot;}: 这个采样点目标暴露的样本点数量  up度量指标对服务健康的监控是非常有用的。\n数据模型 Prometheus从根本上存储的所有数据都是时间序列: 具有时间戳的数据流只属于单个度量指标和该度量指标下的多个标签维度。除了存储时间序列数据外，Prometheus也可以利用查询表达式存储5分钟的返回结果中的时间序列数据\nmetrics和labels(度量指标名称和标签) 每一个时间序列数据由metric度量指标名称和它的标签labels键值对集合唯一确定。\n这个metric度量指标名称指定监控目标系统的测量特征（如：http_requests_total- 接收http请求的总计数）. metric度量指标命名ASCII字母、数字、下划线和冒号，他必须配正则表达式[a-zA-Z_:][a-zA-Z0-9_:]*。\n标签开启了Prometheus的多维数据模型：对于相同的度量名称，通过不同标签列表的结合, 会形成特定的度量维度实例。(例如：所有包含度量名称为/api/tracks的http请求，打上method=POST的标签，则形成了具体的http请求)。这个查询语言在这些度量和标签列表的基础上进行过滤和聚合。改变任何度量上的任何标签值，则会形成新的时间序列图\n标签label名称可以包含ASCII字母、数字和下划线。它们必须匹配正则表达式[a-zA-Z_][a-zA-Z0-9_]*。带有_下划线的标签名称被保留内部使用。\n标签labels值包含任意的Unicode码。\n具体详见metrics和labels命名最佳实践。\n有序的采样值 有序的采样值形成了实际的时间序列数据列表。每个采样值包括：\n 一个64位的浮点值 一个精确到毫秒级的时间戳 一个样本数据集是针对一个指定的时间序列在一定时间范围的数据收集。这个时间序列是由\u0026lt;metric_name\u0026gt;{\u0026lt;label_name\u0026gt;=\u0026lt;label_value\u0026gt;, \u0026hellip;}  \u0026lsquo;\u0026lsquo;小结：指定度量名称和度量指标下的相关标签值，则确定了所关心的目标数据，随着时间推移形成一个个点，在图表上实时绘制动态变化的线条\u0026rsquo;\u0026rsquo;\nNotation(符号) 表示一个度量指标和一组键值对标签，需要使用以下符号：\n [metric name]{[label name]=[label value], \u0026hellip;}\n 例如，度量指标名称是api_http_requests_total， 标签为method=\u0026quot;POST\u0026quot;, handler=\u0026quot;/messages\u0026quot; 的示例如下所示：\n api_http_requests_total{method=\u0026quot;POST\u0026rdquo;, handler=\u0026rdquo;/messages\u0026rdquo;}\n 这些命名和OpenTSDB使用方法是一样的\nmetrics类型  Prometheus客户库提供了四个核心的metrics类型。这四种类型目前仅在客户库和wire协议中区分。Prometheus服务还没有充分利用这些类型。不久的将来就会发生改变。\nCounter(计数器) counter 是一个累计度量指标，它是一个只能递增的数值。计数器主要用于统计服务的请求数、任务完成数和错误出现的次数等等。计数器是一个递增的值。反例：统计goroutines的数量。\nGauge(测量器) gauge是一个度量指标，它表示一个既可以递增, 又可以递减的值。\n测量器主要测量类似于温度、当前内存使用量等，也可以统计当前服务运行随时增加或者减少的Goroutines数量\nHistogram(柱状图) histogram，是柱状图，在Prometheus系统中的查询语言中，有三种作用：\n 对每个采样点进行统计，打到各个分类值中(bucket) 对每个采样点值累计和(sum) 对采样点的次数累计和(count)  度量指标名称: [basename]的柱状图, 上面三类的作用度量指标名称\n [basename]_bucket{le=\u0026quot;上边界\u0026rdquo;}, 这个值为小于等于上边界的所有采样点数量 [basename]_sum [basename]_count  小结：所以如果定义一个度量类型为Histogram，则Prometheus系统会自动生成三个对应的指标\n使用histogram_quantile()函数, 计算直方图或者是直方图聚合计算的分位数阈值。 一个直方图计算Apdex值也是合适的, 当在buckets上操作时，记住直方图是累计的。\n[Summary]总结 类似histogram柱状图，summary是采样点分位图统计，(通常的使用场景：请求持续时间和响应大小)。 它也有三种作用：\n 对于每个采样点进行统计，并形成分位图。（如：正态分布一样，统计低于60分不及格的同学比例，统计低于80分的同学比例，统计低于95分的同学比例） 统计班上所有同学的总成绩(sum) 统计班上同学的考试总人数(count)  带有度量指标的[basename]的summary 在抓取时间序列数据展示。\n 观察时间的φ-quantiles (0 ≤ φ ≤ 1), 显示为[basename]{分位数=\u0026quot;[φ]\u0026quot;} [basename]_sum， 是指所有观察值的总和 [basename]_count, 是指已观察到的事件计数值  *summary的最简单的理解, DEMO\n详见histogram和summaries\n上面的几个概念单纯讲还是比较难讲清楚，下面还是安装部署好prometheus，结合实例具体说一下。\n安装部署prometheus 安装部署prometheus也是极简单的，我这里用docker-compose部署，docker-compose.yml文件内容如下：\nversion: \u0026#39;3\u0026#39; services: prometheus: image: prom/prometheus ports: - 9090:9090/tcp user: root volumes: - ${PWD}/prometheus/data:/prometheus - ${PWD}/prometheus/conf/prometheus.yml:/etc/prometheus/prometheus.yml grafana: environment: - GF_SECURITY_ADMIN_PASSWORD=admin123 image: grafana/grafana volumes: - ${PWD}/grafana/data:/var/lib/grafana ports: - 3000:3000/tcp user: grafana 上述描述文件将启动两个容器，prometheus和grafana，两个服务均使用本机的配置文件，使用本机的目录作为数据目录。\nprometheus本地的配置文件直接用( docker run --rm -ti --entrypoint '' prom/prometheus cat/etc/prometheus/prometheus.yml ) \u0026gt; ./prometheus/conf/prometheus.yml 就得到了。\n然后用docker-compose up -d命令即可启动prometheus和grafana。可直接用浏览器访问，prometheus的访问地址是http://127.0.0.1:9090， grafana的访问地址是http://127.0.0.1:3000。\n解读prometheus的概念 我们打开prometheus的配置文件，看一下内容：\nglobal: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. alerting: alertmanagers: - static_configs: - targets: [] rule_files: [] scrape_configs: - job_name: \u0026#39;prometheus\u0026#39; static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;] 这里可以看到配置了一个名叫prometheus的job：\nscrape_configs: - job_name: \u0026#39;prometheus\u0026#39; static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;] 而这个job的服务端点是静态配置的，目前只有localhost:9090这一个。以上配置说明prometheus将会每15s从prometheus这个job定义的服务端点localhost:9090拉取监控指标数据，并将之存入TSDB。当然prometheus还支持其它的服务端点定义方式，参见配置里一堆以_sd_config结尾的配置。例如可配置基于consul的服务端点发现：\n - job_name: 'consul_srvs' consul_sd_configs: server: '127.0.0.1:8500' services: - 'serviceA' - 'serviceB' 我们用浏览器访问http://127.0.0.1:9090/metrics，即可看到一个instance向外暴露的监控指标。除了注释外，其它每一行都是一个监控指标项，大部分指标形如：\ngo_info{version=\u0026quot;go1.10.3\u0026quot;} 1 这里go_info即为度量指标名称，version为这个度量指标的标签，go1.10.3为这个度量指标version标签的值，1为这个度量指标当前采样的值，一个度量指标的标签可以有0个或多个标签。这就是上面说到的监控指标数据模型。\n可以看到有些度量指标的形式如下：\ngo_memstats_frees_total 135196 按prometheus官方建议的规范，以_total为后缀的度量指标一般类型是counter计数器类型。\n有些度量指标的形式如下：\ngo_memstats_gc_sys_bytes 913408 这种度量指标一般类型是gauge测量器类型。\n有些度量指标的形式如下：\nprometheus_http_response_size_bytes_bucket{handler=\u0026quot;/metrics\u0026quot;,le=\u0026quot;100\u0026quot;} 0 prometheus_http_response_size_bytes_bucket{handler=\u0026quot;/metrics\u0026quot;,le=\u0026quot;1000\u0026quot;} 0 prometheus_http_response_size_bytes_bucket{handler=\u0026quot;/metrics\u0026quot;,le=\u0026quot;10000\u0026quot;} 46 prometheus_http_response_size_bytes_bucket{handler=\u0026quot;/metrics\u0026quot;,le=\u0026quot;100000\u0026quot;} 46 prometheus_http_response_size_bytes_bucket{handler=\u0026quot;/metrics\u0026quot;,le=\u0026quot;1e+06\u0026quot;} 46 prometheus_http_response_size_bytes_bucket{handler=\u0026quot;/metrics\u0026quot;,le=\u0026quot;1e+07\u0026quot;} 46 prometheus_http_response_size_bytes_bucket{handler=\u0026quot;/metrics\u0026quot;,le=\u0026quot;1e+08\u0026quot;} 46 prometheus_http_response_size_bytes_bucket{handler=\u0026quot;/metrics\u0026quot;,le=\u0026quot;1e+09\u0026quot;} 46 prometheus_http_response_size_bytes_bucket{handler=\u0026quot;/metrics\u0026quot;,le=\u0026quot;+Inf\u0026quot;} 46 prometheus_http_response_size_bytes_sum{handler=\u0026quot;/metrics\u0026quot;} 234233 prometheus_http_response_size_bytes_count{handler=\u0026quot;/metrics\u0026quot;} 46 这种就是histogram柱状图类型。\n还有的形式如下：\ngo_gc_duration_seconds{quantile=\u0026quot;0\u0026quot;} 7.3318e-05 go_gc_duration_seconds{quantile=\u0026quot;0.25\u0026quot;} 0.000118693 go_gc_duration_seconds{quantile=\u0026quot;0.5\u0026quot;} 0.000236845 go_gc_duration_seconds{quantile=\u0026quot;0.75\u0026quot;} 0.000337872 go_gc_duration_seconds{quantile=\u0026quot;1\u0026quot;} 0.000707002 go_gc_duration_seconds_sum 0.003731953 go_gc_duration_seconds_count 14 这种就是summary总结类型。\n这就是上面说到的监控指标的数据类型。\n监控指标上报 prometheus已经部署好了，接下来就要埋点上报监控指标数据了。\n各类exporter 在prometheus的世界里70%的场景并不需要专门写埋点逻辑代码，因为已经有现成的各类exporter了，只要找到合适的exporter，启动exporter就直接暴露出一个符合prometheus规范的服务端点了。\nexporter列表参见这里，另外官方git仓库里也有一些exporter。\n举个栗子，在某个宿主机上运行node_exporter后，用浏览器访问http://${host_ip}:9100/metrics即可看到node_exporter暴露出的这个宿主机各类监控指标数据，然后在prometheus的配置文件里加入以下一段：\nscrape_configs: ...... - job_name: \u0026#39;node_monitor_demo\u0026#39; static_configs: - targets: [\u0026#39;${host_ip}:9100\u0026#39;] 然后在prometheus的web管理控制台里就可以查询到相应的监控指标了。在http://127.0.0.1:9090/graph界面里输入go_memstats_alloc_bytes{instance=\u0026quot;${host_ip}:9100\u0026quot;}点击Execute按钮即可。\n编写监控指标上报代码 如果不幸，你的监控指标很特殊，需要自己写埋点上报逻辑代码，也是比较简单的。已经有各个语言的Client Libraries了，照着示例写就可以了。\n下面举一个go语言的示例。\n首先创建一个http服务\nimport ( \u0026#34;flag\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promhttp\u0026#34; ) var addr = flag.String(\u0026#34;listen-address\u0026#34;, \u0026#34;:8080\u0026#34;, \u0026#34;The address to listen on for HTTP requests.\u0026#34;) func main() { flag.Parse() http.Handle(\u0026#34;/metrics\u0026#34;, promhttp.Handler()) log.Fatal(http.ListenAndServe(*addr, nil)) } 然后初始化metric对象（这里采用go-metrics库，方便不少）\nimport ( prometheussink \u0026#34;github.com/armon/go-metrics/prometheus\u0026#34; \u0026#34;github.com/armon/go-metrics\u0026#34; ) sink, _ := prometheussink.NewPrometheusSink() metrics.NewGlobal(metrics.DefaultConfig(\u0026#34;service-name\u0026#34;), sink) 最后在需要埋点的地方调用metrics的相应方法就可以了\n// Run some code metrics.SetGauge([]string{\u0026#34;foo\u0026#34;}, 42) metrics.EmitKey([]string{\u0026#34;bar\u0026#34;}, 30) metrics.IncrCounter([]string{\u0026#34;baz\u0026#34;}, 42) metrics.IncrCounter([]string{\u0026#34;baz\u0026#34;}, 1) metrics.IncrCounter([]string{\u0026#34;baz\u0026#34;}, 80) metrics.AddSample([]string{\u0026#34;method\u0026#34;, \u0026#34;wow\u0026#34;}, 42) metrics.AddSample([]string{\u0026#34;method\u0026#34;, \u0026#34;wow\u0026#34;}, 100) metrics.AddSample([]string{\u0026#34;method\u0026#34;, \u0026#34;wow\u0026#34;}, 22) 查询监控指标数据 Prometheus提供一个函数式的表达式语言，可以使用户实时地查找和聚合时间序列数据。表达式计算结果可以在图表中展示，也可以在Prometheus表达式浏览器中以表格形式展示，或者作为数据源, 以HTTP API的方式提供给外部系统使用。prometheus的查询表达式语言也比较简单，官方文档大概花了两三个网页就讲完了，我的感觉是看看官方文档，再结合官方给出的示例，到prometheus的web管理控制台做做实验就掌握得差不多了。\n图表里查看监控状态 监控数据采集上来了，当然不是只在prometheus的管理控制台里查询，业务上肯定需要在图表中展现监控状态，这里采用grafana完成这个工作，具体整合步骤参考官方文档即可。在grafana里的图表panel可share出去，集成到其它业务系统的web界面里。\n部署优化 远端存储 prometheus默认是将监控数据保存在本地磁盘中的，当然在分布式架构环境下，这样是不太可取的。不过它支持远端存储，可与远端存储系统集成。\nPrometheus integrates with remote storage systems in two ways:\n Prometheus can write samples that it ingests to a remote URL in a standardized format. Prometheus can read (back) sample data from a remote URL in a standardized format.  目前支持的远端存储系统如下：\nThe remote write and remote read features of Prometheus allow transparently sending and receiving samples. This is primarily intended for long term storage. It is recommended that you perform careful evaluation of any solution in this space to confirm it can handle your data volumes.\n AppOptics: write Chronix: write Cortex: read and write CrateDB: read and write Elasticsearch: write Gnocchi: write Graphite: write InfluxDB: read and write IRONdb: read and write M3DB: read and write OpenTSDB: write PostgreSQL/TimescaleDB: read and write SignalFx: write  联邦模式 如果prometheus仅能够中心化地进行数据采集存储、分析，不支持集群模式，带来的性能问题显而易见。Prometheus给出了一种联邦的部署方式，就是Prometheus server可以从其他的Prometheus server采集数据，实施步骤直接参考官方文档。\n监控告警 一个监控系统必须要有告警，否则产生不了太大的价值，prometheus的监控告警也是比较成熟的，不过本次没有深入使用，这里就不细谈了，直接参考官方文档就可以了，写得还算详细。\n最佳实践 官方还给出一些最佳实践，可以简单浏览一下。主要是一些指标命名建议、监控指标类型的选择、告警策略、采用Recording rules提前生成监控指标、何时部署Pushgateway。\nTHE END\n参考  https://prometheus.io/docs https://mp.weixin.qq.com/s/2m6x7MoNdHlzRCSenESvnw https://github.com/1046102779/prometheus https://github.com/armon/go-metrics https://github.com/prometheus/node_exporter  ","permalink":"https://jeremyxu2010.github.io/2018/08/%E7%A0%94%E7%A9%B6%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F%E4%B9%8Bprometheus/","tags":["golang","prometheus"],"title":"研究监控系统之prometheus"},{"categories":["微服务"],"contents":"最近在做微服务构架里有关调用链跟踪（也有叫分布式追踪）的部分，有一些心得，这里总结一些。\n为什么有必要跟踪调用链 当我们进行微服务架构开发时，通常会根据业务来划分微服务，各业务之间通过REST进行调用。一个用户操作，可能需要很多微服务的协同才能完成，如果在业务调用链路上任何一个微服务出现问题或者网络超时，都会导致功能失败。随着业务越来越多，对于微服务之间的调用链的分析会越来越复杂。通过追踪调用链，我们可以很方便的理清各微服务间的调用关系，同时调用链还可以帮助我们：\n 耗时分析: 通过Sleuth可以很方便的了解到每个采样请求的耗时，从而分析出哪些服务调用比较耗时; 可视化错误: 对于程序未捕捉的异常，可以通过集成的界面上看到; 链路优化: 对于调用比较频繁的服务，可以针对这些服务实施一些优化措施。  调用链跟踪系统选型 拿Distributed Tracing这个关键词在google里搜索，基本第一页就列出了最流行的分布式追踪系统：OpenZipkin、Jaeger。那就直接在这两个里选型好了。\n特性对比 两者的特性对比矩阵（摘自https://sematext.com/blog/jaeger-vs-zipkin-opentracing-distributed-tracers/，截至May 23, 2018)：\n    JAEGER ZIPKIN     OpenTracing compatibility Yes Yes   OpenTracing-compatible clients PythonGoNodeJavaC++C#Ruby *PHP*Rust GoJavaRuby*C++Python (work in progress)   Storage support In-memoryCassandraElasticsearchScyllaDB (work in progress) In-memoryMySQLCassandraElasticsearch   Sampling Dynamic sampling rate (supports rate limiting and probabilistic sampling strategies) Fixed sampling rate (supports probabilistic sampling strategy)   Span transport UDPHTTP HTTPKafkaScribeAMQP   Docker ready Yes Yes    * non-official OpenTracing clients\n从上面的特性矩阵来年，在兼容的客户端语言和采样策略上jaeger胜出，在支持的传输层技术上zipkin胜出，其它都是差不多的水平。\n架构对比 两者都是 Google Dapper 这篇pager的实现，因此原理及架构是差不多了，但两者的架构有一点点小差异。\nopenzipkin的架构图\njaeger架构图\n从以上架构图可以看出，jaeger将jaeger-agent从业务应用中抽出，部署在宿主机或容器中，专门负责向collector异步上报调用链跟踪数据，这样做将业务应用与collector解耦了，同时也减少了业务应用的第三方依赖。因此从架构上来看，明显jaeger胜出了。\n另外jaeger整体是用go语言编写的，在并发性能、对系统资源的消耗上也对基于java的openzipkin好不少。\n社区活跃性对比 github上项目的关键指标如下：\n$ curl \u0026#39;https://api.github.com/repos/openzipkin/zipkin\u0026#39; 2\u0026gt;/dev/null | grep -E \u0026#39;created_at|updated_at|stargazers_count|watchers_count|forks_count\u0026#39; \u0026#34;created_at\u0026#34;: \u0026#34;2012-06-06T18:26:16Z\u0026#34;, \u0026#34;updated_at\u0026#34;: \u0026#34;2018-07-22T13:36:01Z\u0026#34;, \u0026#34;stargazers_count\u0026#34;: 9039, \u0026#34;watchers_count\u0026#34;: 9039, \u0026#34;forks_count\u0026#34;: 1500, $ curl \u0026#39;https://api.github.com/repos/jaegertracing/jaeger\u0026#39; 2\u0026gt;/dev/null | grep -E \u0026#39;created_at|updated_at|stargazers_count|watchers_count|forks_count\u0026#39; \u0026#34;created_at\u0026#34;: \u0026#34;2016-04-15T18:49:02Z\u0026#34;, \u0026#34;updated_at\u0026#34;: \u0026#34;2018-07-22T10:46:39Z\u0026#34;, \u0026#34;stargazers_count\u0026#34;: 5184, \u0026#34;watchers_count\u0026#34;: 5184, \u0026#34;forks_count\u0026#34;: 414, 两者最近都属于活跃开发中，值得注意的是jaeger比openzipkin晚诞生4年，但start及watch的数量已有后者的一半了，可谓发展迅猛。另外还有一点值得注意的是jaeger是Cloud Native Computing Foundation的项目，因此云原生的项目都会支持它。\n结论 综上所述，这里就愉快地选择jaeger了。\nGetting Started 为了便于研究，先把官方的Getting started跑起来。不过为了理解其架构，我这里就不用官方的all-in-one启动了，而是将各个组件逐个部署启动起来。另外为了后面能整合ES搜索方案，我这里的storage使用了elasticsearch，这个jaeger也是支持的。下面的部署过程就直接贴docker-compose文件了，比较简单。\n.env\nCOMPOSE_PROJECT_NAME=jaeger_demo MY_HOST_IP=${your_host_ip} docker-compose.yml\nversion: \u0026#39;3\u0026#39; services: elasticsearch: image: elasticsearch command: -Enode.name=jaegerESNode restart: always volumes: - \u0026#34;esdata:/usr/share/elasticsearch/data\u0026#34; ports: - \u0026#34;9200:9200\u0026#34; jaeger-collector: image: jaegertracing/jaeger-collector restart: always environment: SPAN_STORAGE_TYPE: elasticsearch ES_SERVER_URLS: http://${MY_HOST_IP:-127.0.0.1}:9200 ports: - \u0026#34;14267:14267\u0026#34; - \u0026#34;14268:14268\u0026#34; - \u0026#34;9411:9411\u0026#34; jaeger-query: image: jaegertracing/jaeger-query restart: always environment: SPAN_STORAGE_TYPE: elasticsearch ES_SERVER_URLS: http://${MY_HOST_IP:-127.0.0.1}:9200 ports: - \u0026#34;16686:16686\u0026#34; jaeger-agent: image: jaegertracing/jaeger-agent restart: always command: --collector.host-port=${MY_HOST_IP:-127.0.0.1}:14267 ports: - \u0026#34;5775:5775/udp\u0026#34; - \u0026#34;6831:6831/udp\u0026#34; - \u0026#34;6832:6832/udp\u0026#34; - \u0026#34;5778:5778/tcp\u0026#34; jaeger-spark-dependencies: image: jaegertracing/spark-dependencies restart: always environment: STORAGE: elasticsearch ES_NODES: http://${MY_HOST_IP:-127.0.0.1}:9200 ES_NODES_WAN_ONLY: \u0026#39;true\u0026#39; JAVA_OPTS: -Dspark.testing.memory=481859200 example-hotrod: image: jaegertracing/example-hotrod restart: always command: all --jaeger-agent.host-port=${MY_HOST_IP:-127.0.0.1}:6831 ports: - \u0026#34;8080-8083:8080-8083\u0026#34; volumes: esdata: driver: local driver_opts: type: none o: bind device: \u0026#34;${PWD}/esdata\u0026#34; docker-compose.yml文件中配置的各个组件可按照jaeger的架构图部署在多台宿主机上，只要配置好正确的地址引用即可。\n跑起来后访问http://127.0.0.1:8080/，点击不同的按钮，用以模拟不同的客户订购car。\n然后访问http://127.0.0.1:16686/，即可查询调用链信息。\n基本功能大概就是这样了，一些强悍的功能可以查看这篇文章。\nTrace Instrumentation写法 从jaeger的架构图中可以看到，微服务接入分布式调用追踪需要插入一些代码用于进行Trace Instrumentation。因为我们的项目大部分是go语言编写的，因此这里重点说一下go语言的Trace Instrumentation写法，其它语言应该类似。\n初始化Tracer 根据相关的配置选项初始化Tracer，初始化方法可参考https://github.com/jaegertracing/jaeger/blob/master/examples/hotrod/pkg/tracing/init.go\n// Init creates a new instance of Jaeger tracer. func Init(serviceName string, metricsFactory metrics.Factory, logger log.Factory, backendHostPort string) opentracing.Tracer { cfg := config.Configuration{ Sampler: \u0026amp;config.SamplerConfig{ Type: \u0026#34;const\u0026#34;, Param: 1, }, } // TODO(ys) a quick hack to ensure random generators get different seeds, which are based on current time. \ttime.Sleep(100 * time.Millisecond) jaegerLogger := jaegerLoggerAdapter{logger.Bg()} var sender jaeger.Transport if strings.HasPrefix(backendHostPort, \u0026#34;http://\u0026#34;) { sender = transport.NewHTTPTransport( backendHostPort, transport.HTTPBatchSize(1), ) } else { if s, err := jaeger.NewUDPTransport(backendHostPort, 0); err != nil { logger.Bg().Fatal(\u0026#34;cannot initialize UDP sender\u0026#34;, zap.Error(err)) } else { sender = s } } tracer, _, err := cfg.New( serviceName, config.Reporter(jaeger.NewRemoteReporter( sender, jaeger.ReporterOptions.BufferFlushInterval(1*time.Second), jaeger.ReporterOptions.Logger(jaegerLogger), )), config.Logger(jaegerLogger), config.Metrics(metricsFactory), config.Observer(rpcmetrics.NewObserver(metricsFactory, rpcmetrics.DefaultNameNormalizer)), ) if err != nil { logger.Bg().Fatal(\u0026#34;cannot initialize Jaeger Tracer\u0026#34;, zap.Error(err)) } return tracer } 嵌入代码至http.Handler 如果微服务是用HTTP提交的restful接口，则需要嵌入代码至http.Handler，可参考https://github.com/jaegertracing/jaeger/blob/master/examples/hotrod/pkg/tracing/mux.go，这里的主要处理逻辑在https://github.com/opentracing-contrib/go-stdlib/blob/master/nethttp/server.go。\n// Handle implements http.ServeMux#Handle func (tm *TracedServeMux) Handle(pattern string, handler http.Handler) { middleware := nethttp.Middleware( tm.tracer, handler, nethttp.OperationNameFunc(func(r *http.Request) string { return \u0026#34;HTTP \u0026#34; + r.Method + \u0026#34; \u0026#34; + pattern })) tm.mux.Handle(pattern, middleware) } 其实就是用nethttp包里的middleware将http.Handler包裹起来，可以猜测这个middleware的处理逻辑，如没有Trace的上下文信息，则创建一个全新的Trace，并将Trace的上下文信息放入请求处理上下文；如有Trace的上下文信息，则直接使用该Trace的上下文信息，并将Trace的上下文信息放入请求处理上下文。\n嵌入代码至http.Client 如果微服务是用http.Client调用其它微服务的restful接口，则需要嵌入代码至http.Client，可参考https://github.com/jaegertracing/jaeger/blob/master/examples/hotrod/pkg/tracing/http.go，这里的主要处理逻辑在https://github.com/opentracing-contrib/go-stdlib/blob/master/nethttp/client.go。\n// HTTPClient wraps an http.Client with tracing instrumentation. type HTTPClient struct { Tracer opentracing.Tracer Client *http.Client } // GetJSON executes HTTP GET against specified url and tried to parse // the response into out object. func (c *HTTPClient) GetJSON(ctx context.Context, endpoint string, url string, out interface{}) error { req, err := http.NewRequest(\u0026#34;GET\u0026#34;, url, nil) if err != nil { return err } req = req.WithContext(ctx) req, ht := nethttp.TraceRequest(c.Tracer, req, nethttp.OperationName(\u0026#34;HTTP GET: \u0026#34;+endpoint)) defer ht.Finish() res, err := c.Client.Do(req) if err != nil { return err } defer res.Body.Close() if res.StatusCode \u0026gt;= 400 { body, err := ioutil.ReadAll(res.Body) if err != nil { return err } return errors.New(string(body)) } decoder := json.NewDecoder(res.Body) return decoder.Decode(out) } 其实就是用nethttp.TraceRequest方法来跟踪请求，同时将当前Trace的上下文信息传递给下一个微服务。\n给Span添加自定义tag 可以给Span添加自定义tag，可参考https://github.com/jaegertracing/jaeger/blob/master/examples/hotrod/services/customer/database.go\n// simulate opentracing instrumentation of an SQL query \tif span := opentracing.SpanFromContext(ctx); span != nil { span := d.tracer.StartSpan(\u0026#34;SQL SELECT\u0026#34;, opentracing.ChildOf(span.Context())) tags.SpanKindRPCClient.Set(span) tags.PeerService.Set(span, \u0026#34;mysql\u0026#34;) span.SetTag(\u0026#34;sql.query\u0026#34;, \u0026#34;SELECT * FROM customer WHERE customer_id=\u0026#34;+customerID) defer span.Finish() ctx = opentracing.ContextWithSpan(ctx, span) } 这样在jaeger的UI上展开这个Span时，即可看到一些详细的信息。\n给Span添加相关的log 可以给Span添加自定义log，可参考https://github.com/jaegertracing/jaeger/blob/master/examples/hotrod/pkg/log/spanlogger.go\ntype spanLogger struct { logger *zap.Logger span opentracing.Span } func (sl spanLogger) Info(msg string, fields ...zapcore.Field) { sl.logToSpan(\u0026#34;info\u0026#34;, msg, fields...) sl.logger.Info(msg, fields...) } func (sl spanLogger) Error(msg string, fields ...zapcore.Field) { sl.logToSpan(\u0026#34;error\u0026#34;, msg, fields...) sl.logger.Error(msg, fields...) } func (sl spanLogger) Fatal(msg string, fields ...zapcore.Field) { sl.logToSpan(\u0026#34;fatal\u0026#34;, msg, fields...) tag.Error.Set(sl.span, true) sl.logger.Fatal(msg, fields...) } // With creates a child logger, and optionally adds some context fields to that logger. func (sl spanLogger) With(fields ...zapcore.Field) Logger { return spanLogger{logger: sl.logger.With(fields...), span: sl.span} } func (sl spanLogger) logToSpan(level string, msg string, fields ...zapcore.Field) { // TODO rather than always converting the fields, we could wrap them into a lazy logger \tfa := fieldAdapter(make([]log.Field, 0, 2+len(fields))) fa = append(fa, log.String(\u0026#34;event\u0026#34;, msg)) fa = append(fa, log.String(\u0026#34;level\u0026#34;, level)) for _, field := range fields { field.AddTo(\u0026amp;fa) } sl.span.LogFields(fa...) } 在打日志的地址使用下面的语句\nd.logger.For(ctx).Info(\u0026#34;Loading customer\u0026#34;, zap.String(\u0026#34;customer_id\u0026#34;, customerID)) 这样在jaeger的UI里展开Span就可看到该Span相关的日志。\n其它接口调用的Trace Instrumentation 除了常规restful接口，其它类型如何做Trace Instrumentation可参考opentracing-contrib。\njaeger的源码解析 简单浏览下jaeger的源码，整体逻辑还是比较清晰的，通过阅读它的源码还是学到了不少coding技巧的。在网上还找到一位小哥写的jaeger源码解析，写得还挺详细的，这里就不赘述了，直接参考jaeger源码解析就可以了。\nTHE END\n参考  https://medium.com/opentracing/take-opentracing-for-a-hotrod-ride-f6e3141f7941 https://www.jaegertracing.io/ https://zipkin.io/ http://research.google.com/pubs/pub36356.html https://sematext.com/blog/jaeger-vs-zipkin-opentracing-distributed-tracers/ http://opentracing.io/documentation/  ","permalink":"https://jeremyxu2010.github.io/2018/07/%E7%A0%94%E7%A9%B6%E8%B0%83%E7%94%A8%E9%93%BE%E8%B7%9F%E8%B8%AA%E6%8A%80%E6%9C%AF%E4%B9%8Bjaeger/","tags":["golang","jaeger"],"title":"研究调用链跟踪技术之jaeger"},{"categories":["微服务"],"contents":"之前一直是将consul当成一个服务发现、分布式KV服务、服务健康检查服务等，不过前几天consul发布了1.2版本，宣称其实现了Service Mesh方案，最近在做Service Mesh相关的工作，正好有一点时间，就花时间研究一下。\n试用consul的service mesh 升级consul至1.2版本 macOS下升级consul很简单，简单用brew命令就好\nbrew update consul\r为了方便后面修改consul的配置文件，添加一个-config-dir参数\n/usr/local/opt/consul/homebrew.mxcl.consul.plist\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt;\r\u0026lt;!DOCTYPE plist PUBLIC \u0026#34;-//Apple//DTD PLIST 1.0//EN\u0026#34; \u0026#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd\u0026#34;\u0026gt;\r\u0026lt;plist version=\u0026#34;1.0\u0026#34;\u0026gt;\r\u0026lt;dict\u0026gt;\r\u0026lt;key\u0026gt;KeepAlive\u0026lt;/key\u0026gt;\r\u0026lt;dict\u0026gt;\r\u0026lt;key\u0026gt;SuccessfulExit\u0026lt;/key\u0026gt;\r\u0026lt;false/\u0026gt;\r\u0026lt;/dict\u0026gt;\r\u0026lt;key\u0026gt;Label\u0026lt;/key\u0026gt;\r\u0026lt;string\u0026gt;homebrew.mxcl.consul\u0026lt;/string\u0026gt;\r\u0026lt;key\u0026gt;ProgramArguments\u0026lt;/key\u0026gt;\r\u0026lt;array\u0026gt;\r\u0026lt;string\u0026gt;/usr/local/opt/consul/bin/consul\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;agent\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;-dev\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;-advertise\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;127.0.0.1\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;-config-dir\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;/usr/local/etc/consul.d\u0026lt;/string\u0026gt;\r\u0026lt;/array\u0026gt;\r\u0026lt;key\u0026gt;RunAtLoad\u0026lt;/key\u0026gt;\r\u0026lt;true/\u0026gt;\r\u0026lt;key\u0026gt;WorkingDirectory\u0026lt;/key\u0026gt;\r\u0026lt;string\u0026gt;/usr/local/var\u0026lt;/string\u0026gt;\r\u0026lt;key\u0026gt;StandardErrorPath\u0026lt;/key\u0026gt;\r\u0026lt;string\u0026gt;/usr/local/var/log/consul.log\u0026lt;/string\u0026gt;\r\u0026lt;key\u0026gt;StandardOutPath\u0026lt;/key\u0026gt;\r\u0026lt;string\u0026gt;/usr/local/var/log/consul.log\u0026lt;/string\u0026gt;\r\u0026lt;/dict\u0026gt;\r\u0026lt;/plist\u0026gt;\r这个配置文件中，我添加了以下两行：\n\u0026lt;string\u0026gt;-config-dir\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;/usr/local/etc/consul.d\u0026lt;/string\u0026gt;\r写两个模拟的微服务 用golang写两个小程序，用以模拟两个微服务。\nservice1.go\npackage main\rimport (\r\u0026#34;net/http\u0026#34;\r\u0026#34;log\u0026#34;\r\u0026#34;io\u0026#34;\r)\rfunc TestServer(w http.ResponseWriter, req *http.Request) {\rresp, err := http.Get(\u0026#34;http://127.0.0.1:8082/test2\u0026#34;)\rif resp != nil \u0026amp;\u0026amp; resp.Body != nil {\rdefer resp.Body.Close()\r}\rif err != nil {\rw.Write([]byte(\u0026#34;make request failed\\n\u0026#34;))\rreturn\r}\rio.Copy(w, resp.Body)\r}\rfunc main() {\rhttp.HandleFunc(\u0026#34;/test1\u0026#34;, TestServer)\rerr := http.ListenAndServe(\u0026#34;:8081\u0026#34;, nil)\rif err != nil {\rlog.Fatal(\u0026#34;ListenAndServe: \u0026#34;, err)\r}\r}\rservice2.go\npackage main\rimport (\r\u0026#34;io\u0026#34;\r\u0026#34;net/http\u0026#34;\r\u0026#34;log\u0026#34;\r)\rfunc TestServer(w http.ResponseWriter, req *http.Request) {\rio.WriteString(w, \u0026#34;hello, world!\\n\u0026#34;)\r}\rfunc main() {\rhttp.HandleFunc(\u0026#34;/test2\u0026#34;, TestServer)\rerr := http.ListenAndServe(\u0026#34;:8082\u0026#34;, nil)\rif err != nil {\rlog.Fatal(\u0026#34;ListenAndServe: \u0026#34;, err)\r}\r}\r这里模拟微服务service1调用service2。\n在consul里配置两个服务 在consul的配置文件目录下新建两个json文件，用来配置上述两个服务。\n/usr/local/etc/consul.d/01_service1.json:\n{\r\u0026#34;service\u0026#34;: {\r\u0026#34;name\u0026#34;: \u0026#34;service1\u0026#34;,\r\u0026#34;port\u0026#34;: 8081,\r\u0026#34;connect\u0026#34;: {\r\u0026#34;proxy\u0026#34;: {\r\u0026#34;config\u0026#34;: {\r\u0026#34;upstreams\u0026#34;: [{\r\u0026#34;destination_name\u0026#34;: \u0026#34;service2\u0026#34;,\r\u0026#34;local_bind_port\u0026#34;: 38082\r}]\r}\r}\r}\r}\r}\r/usr/local/etc/consul.d/01_service2.json:\n{\r\u0026#34;service\u0026#34;: {\r\u0026#34;name\u0026#34;: \u0026#34;service2\u0026#34;,\r\u0026#34;port\u0026#34;: 8082,\r\u0026#34;connect\u0026#34;: {\r\u0026#34;proxy\u0026#34;: {\r}\r}\r}\r}\r然后执行命令重新加载consul的配置\nconsul reload\r修改service1中引用service2的代码：\n......\rfunc TestServer(w http.ResponseWriter, req *http.Request) {\r//resp, err := http.Get(\u0026#34;http://127.0.0.1:8082/test2\u0026#34;)\r\tresp, err := http.Get(\u0026#34;http://127.0.0.1:38082/test2\u0026#34;)\rif resp != nil \u0026amp;\u0026amp; resp.Body != nil {\rdefer resp.Body.Close()\r}\rif err != nil {\rw.Write([]byte(\u0026#34;make request failed\\n\u0026#34;))\rreturn\r}\rio.Copy(w, resp.Body)\r}\r......\r跑起来 将service1、service2跑起来，然后用curl命令访问service1\n\u0026gt; go run service1.go \u0026amp;\u0026gt; /dev/null\r\u0026gt; go run service2.go \u0026amp;\u0026gt; /dev/null\r\u0026gt; curl http://127.0.0.1:8081/test1\r# 如果出现以下输出，则说明一切正常，Bingo!\rhello, world!\r其它使用方法 除了Service Mesh的玩法，consul 1.2还提供了SDK的用法。简单来说就是Go语言开发的微服务按照它的规范修改服务提供方、服务消费方的代码，服务间的调用将会自动使用底层的connect隧道。这个使用方法不太符合service mesh的初衷，做过微服务框架sdk的我不是太喜欢，这里就不详细讲了，可以参考官方给出的文档。\nConnect Native原理\nConnect Native Go语言项目改造指引\nconsul的service mesh原理分析 其实consul的文档本身说的比较明白，这里结合consul-ui及代码大概分析一下。\n当给consul的服务配置里添加了\u0026quot;connect\u0026quot;: { \u0026quot;proxy\u0026quot;: { } }后，consul将会为每个服务实例创建一个专门的隧道代理，如下图所示：\n隧道代理的作用是当以connect模式连入时，会自动建立一条到原服务实例的tcp隧道，后面tcp层以上的应用协议数据流将在这条tcp隧道上传输，具体代码在https://github.com/hashicorp/consul/blob/master/connect/proxy/listener.go#NewPublicListener。\n而涉及服务间调用时，在consul服务配置里添加服务UpstreamListener声明，服务消费方访问服务时需使用UpstreamListener的地址。UpstreamListener实际上是一个反向代理，当访问它时，它会以connect模式连接对应的服务实例Connect Proxy，具体代码在https://github.com/hashicorp/consul/blob/master/connect/proxy/listener.go#NewUpstreamListener。\n结合上述两条规则，整个数据链路就通了。\n这里有一个问题，为啥一定要connect模式的隧道代理呢？反向代理服务不能直接连接原来的目标服务地址吗？\n看了下https://github.com/hashicorp/consul/blob/master/connect/service.go#Dial，大概知道原因了。因为connect模式的隧道代理是使用TLS加密的，这样物理服务器节点之间的网络流量就走TLS安全连接了，再加上intentions机制，服务间的调用安全性上有了很大保障。还有一个原因，如果采用Connect-Native的方式集成consul的service mesh功能，底层连接是TLS，上层就可以很方便地走HTTP/2.0协议了。\nconsul的service mesh优缺点分析 优点：\n  直接使用tcp隧道，因此直接支持各类基于tcp的协议代理，如HTTP/1.1、HTTP/2.0、GRPC。\n  实现原理简单，https://github.com/hashicorp/consul/blob/master/connect/、https://github.com/hashicorp/consul/tree/master/api/connect*.go、https://github.com/hashicorp/consul/tree/master/agent/connect/下的关键文件不超过20个，逻辑很容易就看清了。\n  直接结合consul做服务注册与服务发现，集成度高。\n  缺点：\n  目前的负载均衡算法还很简单，就是随机，见下面：\n  一些微服务框架的基本功能还不具备，如超时、重试、熔断、流量分配等，可以从https://github.com/hashicorp/consul/blob/master/connect/proxy/listener.go#handleConn这里开始扩展。\n  需要手动修改consul的服务配置；服务消费方要根据consul里的服务配置，修改调用其它服务的地址（这里跟service mesh的初衷有些不符）。\n  总结 目前来看consul的service mesh方案还比较简单，功能很基本，但具备进一步扩展的空间，可以好好研究学习它的代码。\n参考  https://www.hashicorp.com/blog/consul-1-2-service-mesh https://www.consul.io/intro/getting-started/connect.html https://www.consul.io/docs/agent/options.html https://www.consul.io/docs/connect/intentions.html https://www.consul.io/docs/connect/native.html https://www.consul.io/docs/connect/native/go.html https://www.consul.io/docs/connect/configuration.html https://www.consul.io/docs/connect/proxies.html https://www.consul.io/docs/connect/dev.html https://www.consul.io/docs/connect/ca/consul.html  ","permalink":"https://jeremyxu2010.github.io/2018/07/%E7%A0%94%E7%A9%B6consul%E7%9A%84service-mesh%E5%8A%9F%E8%83%BD/","tags":["consul","golang","service mesh"],"title":"研究consul的service mesh功能"},{"categories":["java开发"],"contents":"项目中用到了mongodb（3.x版本），业务上需要操作mongodb的多个collections，希望要么同时操作成功，要么回滚操作保持数据的一致性，这个实际上要求在mongodb上实现事务功能，在网上查了下资料，发现了两阶段提交的方案，不过网上基本上都是翻译，很少有人具体分析原理的，今天花了些时间仔细思考了下这个方案，记录在这里以备忘。\nMongoDB两阶段提交原理 下面的内容摘自官方说明的翻译，完整的英文版说明。\nMongoDB数据库中操作单个文档总是原子性的，然而，涉及多个文档的操作，通常被作为一个“事务”，而不是原子性的。因为文档可以是相当复杂并且包含多个嵌套文档，单文档的原子性对许多实际用例提供了支持。尽管单文档操作是原子性的，在某些情况下，需要多文档事务。在这些情况下，使用两阶段提交，提供这些类型的多文档更新支持。因为文档可以表示为Pending数据和状态，可以使用一个两阶段提交确保数据是一致的，在一个错误的情况下，事务前的状态是可恢复的。\n事务最常见的例子是以可靠的方式从A账户转账到B账户，在关系型数据库中，此操作将从A账户减掉金额和给B账户增加金额的操作封装在单个原子事务中。在MongoDB中，可以使用两阶段提交达到相同的效果。本文中的所有示例使用mongo shell与数据库进行交互,并假设有两个集合：首先，一个名为accounts的集合存储每个账户的文档数据，另一个名为transactions的集合存储事务本身。\n首先创建两个名为A和B的账户，使用下面的命令：\ndb.accounts.save({name:\u0026quot;A\u0026quot;, balance:1000, pendingTransactions: []})\rdb.accounts.save({name:\u0026quot;B\u0026quot;, balance:1000, pendingTransactions: []})\r使用find()方法验证这两个操作已经成功：\ndb.accounts.find()\rmongo会返回两个类似下面的文档：\n{ \u0026quot;_id\u0026quot; :ObjectId(\u0026quot;4d7bc66cb8a04f512696151f\u0026quot;), \u0026quot;name\u0026quot; :\u0026quot;A\u0026quot;, \u0026quot;balance\u0026quot; :1000, \u0026quot;pendingTransactions\u0026quot; :[ ]}\r{ \u0026quot;_id\u0026quot; :ObjectId(\u0026quot;4d7bc67bb8a04f5126961520\u0026quot;), \u0026quot;name\u0026quot; :\u0026quot;B\u0026quot;,\r\u0026quot;balance\u0026quot; :1000, \u0026quot;pendingTransactions\u0026quot; :[ ]}\r事务过程 设置事务初始状态initial 通过插入下面的文档创建transaction集合，transaction文档持有源(source)和目标(destination)，它们引用自accounts集合文档的字段名，以及value字段表示改变balance字段数量的数据。最后，state字段反映事务的当前状态。\ndb.transactions.save({source:\u0026quot;A\u0026quot;, destination:\u0026quot;B\u0026quot;, value:100, state:\u0026quot;initial\u0026quot;})\r验证这个操作已经成功，使用find()：\ndb.transactions.find()\r这个操作会返回一个类似下面的文档：\n{ \u0026quot;_id\u0026quot; :ObjectId(\u0026quot;4d7bc7a8b8a04f5126961522\u0026quot;), \u0026quot;source\u0026quot; :\u0026quot;A\u0026quot;, \u0026quot;destination\u0026quot; :\u0026quot;B\u0026quot;, \u0026quot;value\u0026quot; :100, \u0026quot;state\u0026quot; :\u0026quot;initial\u0026quot;}\r切换事务到Pending状态 在修改accounts集合记录之前，将事务状态从initial设置为pending。使用findOne()方法将transaction文档赋值给shell会话中的局部变量t：\nt =db.transactions.findOne({state:\u0026quot;initial\u0026quot;})\r变量t创建后，shell将返回它的值，将会看到如下的输出：\n{ \u0026quot;_id\u0026quot; :ObjectId(\u0026quot;4d7bc7a8b8a04f5126961522\u0026quot;), \u0026quot;source\u0026quot; :\u0026quot;A\u0026quot;,\r\u0026quot;destination\u0026quot; :\u0026quot;B\u0026quot;, \u0026quot;value\u0026quot; :100, \u0026quot;state\u0026quot; :\u0026quot;initial\u0026quot;}\r使用update()改变state的值为pending db.transactions.update({_id:t._id},{$set:{state:\u0026quot;pending\u0026quot;}})\rdb.transactions.find()\rfind()操作将返回transaction集合的内容，类似下面：\n{ \u0026quot;_id\u0026quot; :ObjectId(\u0026quot;4d7bc7a8b8a04f5126961522\u0026quot;), \u0026quot;source\u0026quot; :\u0026quot;A\u0026quot;, \u0026quot;destination\u0026quot; :\u0026quot;B\u0026quot;, \u0026quot;value\u0026quot; :100, \u0026quot;state\u0026quot; :\u0026quot;pending\u0026quot;}\r将事务应用到两个账户 使用update()方法应用事务到两个账户。在update()查询中，条件pendingTransactions:{$ne:t._id}阻止事务更新账户，如果账户的pendingTransaction字段包含事务t的_id：\ndb.accounts.update({name:t.source, pendingTransactions: { $ne: t._id }},\r{$inc:{ balance: -t.value }, $push:{pendingTransactions:t._id }})\rdb.accounts.update({name:t.destination, pendingTransactions: { $ne: t._id }},\r{$inc:{ balance: t.value }, $push:{pendingTransactions:t._id }})\rdb.accounts.find()\rfind()操作将返回accounts集合的内容，现在应该类似于下面的内容：\n{ \u0026quot;_id\u0026quot; :ObjectId(\u0026quot;4d7bc97fb8a04f5126961523\u0026quot;), \u0026quot;balance\u0026quot; :900, \u0026quot;name\u0026quot; :\u0026quot;A\u0026quot;, \u0026quot;pendingTransactions\u0026quot; :[ ObjectId(\u0026quot;4d7bc7a8b8a04f5126961522\u0026quot;) ] }\r{ \u0026quot;_id\u0026quot; :ObjectId(\u0026quot;4d7bc984b8a04f5126961524\u0026quot;), \u0026quot;balance\u0026quot; :1100, \u0026quot;name\u0026quot; :\u0026quot;B\u0026quot;, \u0026quot;pendingTransactions\u0026quot; :[ ObjectId(\u0026quot;4d7bc7a8b8a04f5126961522\u0026quot;) ] }\r设置事务状态为committed 使用下面的update()操作设置事务的状态为committed：\ndb.transactions.update({_id:t._id},{$set:{state:\u0026quot;committed\u0026quot;}})db.transactions.find()\rfind()操作发回transactions集合的内容，现在应该类似下面的内容：\n{ \u0026quot;_id\u0026quot; :ObjectId(\u0026quot;4d7bc7a8b8a04f5126961522\u0026quot;), \u0026quot;destination\u0026quot; :\u0026quot;B\u0026quot;, \u0026quot;source\u0026quot; :\u0026quot;A\u0026quot;, \u0026quot;state\u0026quot; :\u0026quot;committed\u0026quot;, \u0026quot;value\u0026quot; :100}\r移除pending事务 使用下面的update()操作从accounts集合中移除pending事务：\ndb.accounts.update({name:t.source},{$pull:{pendingTransactions: t._id}})\rdb.accounts.update({name:t.destination},{$pull:{pendingTransactions: t._id}})\rdb.accounts.find()\rfind()操作返回accounts集合内容，现在应该类似下面内容：\n{ \u0026quot;_id\u0026quot; :ObjectId(\u0026quot;4d7bc97fb8a04f5126961523\u0026quot;), \u0026quot;balance\u0026quot; :900, \u0026quot;name\u0026quot; :\u0026quot;A\u0026quot;, \u0026quot;pendingTransactions\u0026quot; :[ ] }\r{ \u0026quot;_id\u0026quot; :ObjectId(\u0026quot;4d7bc984b8a04f5126961524\u0026quot;), \u0026quot;balance\u0026quot; :1100, \u0026quot;name\u0026quot; :\u0026quot;B\u0026quot;, \u0026quot;pendingTransactions\u0026quot; :[ ] }\r设置事务状态为done 通过设置transaction文档的state为done完成事务：\ndb.transactions.update({_id:t._id},{$set:{state:\u0026quot;done\u0026quot;}})\rdb.transactions.find()\rfind()操作返回transaction集合的内容，此时应该类似下面：\n{ \u0026quot;_id\u0026quot; :ObjectId(\u0026quot;4d7bc7a8b8a04f5126961522\u0026quot;), \u0026quot;destination\u0026quot; :\u0026quot;B\u0026quot;, \u0026quot;source\u0026quot; :\u0026quot;A\u0026quot;, \u0026quot;state\u0026quot; :\u0026quot;done\u0026quot;, \u0026quot;value\u0026quot; :100}\r从失败场景中恢复 最重要的部分不是上面的典型例子，而是从各种失败场景中恢复未完成的事务的可能性。这部分将概述可能的失败，并提供方法从这些事件中恢复事务。这里有两种类型的失败：\n 所有发生在第一步（即设置事务的初始状态initial）之后，但在第三步（即应用事务到两个账户）之前的失败。为了还原事务，应用应该获取一个pending状态的transaction列表并且从第二步（即切换事务到pending状态）中恢复。 所有发生在第三步之后（即应用事务到两个账户）但在第五步(即设置事务状态为done)之前的失败。为了还原事务，应用需要获取一个committed状态的事务列表，并且从第四步（即移除pending事务）恢复。  因此应用程序总是能够恢复事务，最终达到一个一致的状态。应用程序开始捕获到每个未完成的事务时运行下面的恢复操作。你可能还希望定期运行恢复操作，以确保数据处于一致状态。达成一致状态所需要的时间取决于应用程序需要多长时间恢复每个事务。\n回滚 在某些情况下可能需要“回滚”或“撤消”事务，当应用程序需要“取消”该事务时，或者是因为它永远需要恢复当其中一个帐户不存在的情况下，或停止现有的事务。这里有两种可能的回滚操作：\n 应用事务（即第三步）之后，你已经完全提交事务，你不应该回滚事务。相反，创建一个新的事务，切换源(源)和目标(destination)的值。 创建事务（即第一步）之后，在应用事务（即第三步）之前，使用下面的处理过程：  设置事务状态为canceling 首先设置事务状态为canceling，使用下面的update()操作：\ndb.transactions.update({_id:t._id},{$set:{state:\u0026quot;canceling\u0026quot;}})\r撤销事务 使用下面的操作顺序从两个账户中撤销事务：\ndb.accounts.update({name:t.source, pendingTransactions: t._id},\r{$inc:{balance: t.value}, $pull:{pendingTransactions:t._id}})\rdb.accounts.update({name:t.destination, pendingTransactions: t._id},\r{$inc:{balance: -t.value}, $pull:{pendingTransactions:t._id}})\rdb.accounts.find()\rfind()操作返回acounts集合的内容，应该类似下面：\n{ \u0026quot;_id\u0026quot; :ObjectId(\u0026quot;4d7bc97fb8a04f5126961523\u0026quot;), \u0026quot;balance\u0026quot; :1000, \u0026quot;name\u0026quot; :\u0026quot;A\u0026quot;, \u0026quot;pendingTransactions\u0026quot; :[ ] }\r{ \u0026quot;_id\u0026quot; :ObjectId(\u0026quot;4d7bc984b8a04f5126961524\u0026quot;), \u0026quot;balance\u0026quot; :1000, \u0026quot;name\u0026quot; :\u0026quot;B\u0026quot;, \u0026quot;pendingTransactions\u0026quot; :[ ] }\r设置事务状态为canceled 最后，使用下面的update()状态将事务状态设置为canceled：\ndb.transactions.update({_id:t._id},{$set:{state:\u0026quot;canceled\u0026quot;}})\r原理理解 这种通过代码模拟两阶段提交可以大概如下理解：\n 首先给要修改的collections添加pendingTransactions字段，用来标记该条记录与哪个事务相关 创建事务记录，初始状态为initial，将变更操作涉及到的属性保存在这条记录里 然后把事务记录修改为pending状态 然后修改目标的collection记录，且将经过修改的记录打上pendingTransactions标记（注意这里用了CAS的方法进行记录的更改） 再将事务记录修改为applied状态 再将目标collection记录中的pendingTransactions标记删除 最后将事务记录修改为done状态  上述基本所有修改操作都是使用了CAS的方法进行记录的更改，这样保证只在前置条件满足的情况下才更新记录。\n接下来考虑一下故障恢复：\n 如果在上述第3步之后第5步之前出现故障了，服务进程重启后，只需要找到pending状态的事务记录（超过某个修改时间阀值），这时可以根据具体情况可以有两种方案继续进行：1）重新从第4步往下继续执行就可以了 2）根据事务里保存的变更相关属性，执行取消流程，目标记录进行进行反向补偿 如果在第5步之后第7步之前出现故障了，服务进程重启后，只需要找到applied状态的事务记录（超过某个修改时间阀值），重新从第6步往下继续执行就可以了  一个更完整的例子 这里找到一个用java语言写的较完整的例子，并增加了一个较完整的测试用例方法：\n@Test\rpublic void testNormalDemo() throws Exception {\r// insert test data\r accounts.insert(\r\u0026#34;[\u0026#34; +\r\u0026#34; { _id: \\\u0026#34;A\\\u0026#34;, balance: 1000, pendingTransactions: [] },\\n\u0026#34; +\r\u0026#34; { _id: \\\u0026#34;B\\\u0026#34;, balance: 1000, pendingTransactions: [] }\\n\u0026#34; +\r\u0026#34;]\u0026#34;\r);\rString txId = ObjectId.get().toString();\rtry {\rtransactions.insert(\r\u0026#34;{ _id:#, source: \\\u0026#34;A\\\u0026#34;, destination: \\\u0026#34;B\\\u0026#34;, value: 100, state: #, lastModified: #}\u0026#34;, txId, TransactionState.INITIAL, System.currentTimeMillis()\r);\rTransaction transaction = transactions.findOne(\u0026#34;{_id:#}\u0026#34;, new Object[]{txId}).as(Transaction.class);\rtransferService.transfer(transaction);\rAccount accountA = accounts.findOne(\u0026#34;{_id: \\\u0026#34;A\\\u0026#34;}\u0026#34;).as(Account.class);\rassertThat(accountA.getBalance(), is(900));\rassertThat(accountA.getPendingTransactions(), is(emptyArray()));\rAccount accountB = accounts.findOne(\u0026#34;{_id: \\\u0026#34;B\\\u0026#34;}\u0026#34;).as(Account.class);\rassertThat(accountB.getBalance(), is(1100));\rassertThat(accountB.getPendingTransactions(), is(emptyArray()));\rTransaction finalTransaction = transactions.findOne().as(Transaction.class);\rassertThat(finalTransaction.getState(), is(TransactionState.DONE));\r} catch (Exception e){\rTransaction transaction = transactions.findOne(\u0026#34;{_id:#}\u0026#34;, txId).as(Transaction.class);\rif (transaction == null) {\rSystem.err.printf(\u0026#34;insert transaction failed, txId=%s\\n\u0026#34;, txId);\r}\rif (transaction.getState() == TransactionState.INITIAL){\rSystem.err.printf(\u0026#34;execute transaction failed, txId=%s, current transaction state is: %s, try to recover the transaction\\n\u0026#34;, txId, TransactionState.INITIAL.toString());\rtransferService.transfer(transaction);\r} if (transaction.getState() == TransactionState.PENDING) {\r// 这里可以选择是取消事务或者恢复事务\r System.err.printf(\u0026#34;execute transaction failed, txId=%s, current transaction state is: %s, try to cancel the transaction\\n\u0026#34;, txId, TransactionState.PENDING.toString());\rtransferService.cancelPending(transaction);\r// System.err.printf(\u0026#34;execute transaction failed, txId=%s, current transaction state is: %s, try to recover the transaction\\n\u0026#34;, txId, TransactionState.PENDING.toString());\r // transferService.recoverPending(transaction);\r } else if (transaction.getState() == TransactionState.APPLIED){\r// 这里事务已经是APPLIED状态了，只差最后设置为DONE状态了，这里可以恢复事务\r System.err.printf(\u0026#34;execute transaction failed, txId=%s, current transaction state is: %s, try to recover the transaction\\n\u0026#34;, txId, TransactionState.APPLIED.toString());\rtransferService.recoverApplied(transaction);\r} else if (transaction.getState() == TransactionState.CANCELING){\rSystem.err.printf(\u0026#34;execute transaction failed, txId=%s, current transaction state is: %s, try to cancel the transaction\\n\u0026#34;, txId, TransactionState.CANCELING.toString());\rtransferService.cancelPending(transaction);\r}\r// 另外可以在后台运行一个定时任务，将超期的上述四种状态事务按上述逻辑处理\r }\r}\r注释里对故障恢复说得比较清楚，就不赘述了。\n这个例子里仅是一个简单转帐的示例，如果业务操作中还涉及插入新记录、删除记录、复杂的记录修改，则在事务记录中还需要将要操作的记录新旧状态都记录下来，便于出现故障时能提供足够的信息进行回滚，这样想一想，要构造一个通用的事务记录模式还是挺复杂的。\n总结 实现mongodb的两阶段提交过程还是比较复杂的，上述的例子只是一个简单的转账，代码就已经很复杂了，因此在mongodb4.0支持事务的情况下，还真不推荐搞mongodb的两阶段提交。\n参考  https://acupple.github.io/2016/08/09/MongoDB%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%E5%AE%9E%E7%8E%B0%E4%BA%8B%E5%8A%A1/ https://docs.mongodb.com/tutorials/perform-two-phase-commits https://jackywu.github.io/articles/MongoDB%E7%9A%84%E4%BA%8B%E5%8A%A1/  ","permalink":"https://jeremyxu2010.github.io/2018/07/mongodb%E7%9A%84%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%E5%AE%9E%E6%88%98/","tags":["mongodb","java","transactions"],"title":"mongodb的两阶段提交实战"},{"categories":["微服务"],"contents":"最近的工作主要是微服务框架的设计与开发，期间要解决多个微服务的分布式事务问题，由于要解决的主要场景是用spring boot写的java项目，最终选择了业界成熟的servicecomb-saga方案，这里稍微记录下以备忘。\n为何会有分布式事务 这里不作深入叙述，网上有相当好的参考资料-聊聊分布式事务，再说说解决方案。\n为何选择saga方案 参考聊聊分布式事务，再说说解决方案，可以看到\n 两阶段提交方案实现较复杂，而且对性能影响太大； TCC方案好像只有阿里内部在大规模使用； 本地消息表方案消息表会耦合到业务系统，不太优雅； MQ事务消息方案依赖于有事务消息的MQ中间件。  最后好像也只好选择saga方案，另外有了servicecomb-saga后，spring-boot应用要使用分布式事务还是挺容易的。\nservicecomb-saga的架构 servicecomb-saga的架构可直接参考其官方文档，写得还是比较详细的。\n概览 Pack中包含两个组件，即 alpha 和 omega。\n alpha充当协调者的角色，主要负责对事务的事件进行持久化存储以及协调子事务的状态，使其得以最终与全局事务的状态保持一致。 omega是微服务中内嵌的一个agent，负责对网络请求进行拦截并向alpha上报事务事件，并在异常情况下根据alpha下发的指令执行相应的补偿操作。  \nOmega内部运行机制 omega是微服务中内嵌的一个agent。当服务收到请求时，omega会将其拦截并从中提取请求信息中的全局事务id作为其自身的全局事务id（即Saga事件id），并提取本地事务id作为其父事务id。在预处理阶段，alpha会记录事务开始的事件；在后处理阶段，alpha会记录事务结束的事件。因此，每个成功的子事务都有一一对应的开始及结束事件。\n\n服务间通信流程 服务间通信的流程与Zipkin的类似。在服务生产方，omega会拦截请求中事务相关的id来提取事务的上下文。在服务消费方，omega会在请求中注入事务相关的id来传递事务的上下文。通过服务提供方和服务消费方的这种协作处理，子事务能连接起来形成一个完整的全局事务。\n\n具体处理流程 成功场景 成功场景下，每个开始的事件都会有对应的结束事件。\n\n异常场景 异常场景下，omega会向alpha上报中断事件，然后alpha会向该全局事务的其它已完成的子事务发送补偿指令，确保最终所有的子事务要么都成功，要么都回滚。\n\n超时场景 超时场景下，已超时的事件会被alpha的定期扫描器检测出来，与此同时，该超时事务对应的全局事务也会被中断。\n\n使用servicecomb-saga 下面的过程也是参考官方文档，但由于我这里使用mysql数据库作为底层数据库，修改了少量操作。\n准备环境   安装JDK 1.8\n  安装Maven 3.x\n  编译 获取源码：\n$ git clone https://github.com/apache/incubator-servicecomb-saga.git $ cd incubator-servicecomb-saga 构建mysql的可执行文件：\n$ mvn clean install -DskipTests -Pmysql 创建数据库 创建数据库并给予用户访问该数据库的权限\n$ mysql mysql\u0026gt; create database saga default character set utf8; mysql\u0026gt; GRANT ALL PRIVILEGES ON saga.* to \u0026#39;saga\u0026#39;@\u0026#39;localhost\u0026#39; identified by \u0026#39;123456\u0026#39;; mysql\u0026gt; flush priveleges; mysql\u0026gt; exit 启动alpha-server 直接使用java命令启动alpha-server\njava -Dspring.profiles.active=mysql -D\u0026#34;spring.datasource.url=jdbc:mysql://localhost:3306/saga?useSSL=false\u0026#34; -D\u0026#34;spring.datasource.username=saga\u0026#34; -D\u0026#34;spring.datasource.password=123456\u0026#34; -jar alpha/alpha-server/target/saga/alpha-server-0.3.0-SNAPSHOT-exec.jar 配置Omega 按照servicecomb-saga的架构，所有支持分布式事务的spring-boot应用须配置Omega。其实也比较简单，大概有以下这些步骤。\n引入Saga的依赖 应用的pom.xml配置文件中引入servicecomb-saga的依赖\n \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.servicecomb.saga\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;omega-spring-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.3.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.servicecomb.saga\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;omega-transport-resttemplate\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.3.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 添加Saga的注解及相应的补偿方法 以一个转账应用为例：\n  在应用入口添加 @EnableOmega 的注解来初始化omega的配置并与alpha建立连接。\n@SpringBootApplication @EnableOmega public class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); } }   在全局事务的起点添加 @SagaStart 的注解。\n@SagaStart(timeout=10) public boolean transferMoney(String from, String to, int amount) { transferOut(from, amount); transferIn(to, amount); } 注意: 默认情况下，超时设置需要显式声明才生效。\n  在子事务处添加 @Compensable 的注解并指明其对应的补偿方法。\n@Compensable(timeout=5, compensationMethod=\u0026quot;cancel\u0026quot;) public boolean transferOut(String from, int amount) { repo.reduceBalanceByUsername(from, amount); } public boolean cancel(String from, int amount) { repo.addBalanceByUsername(from, amount); } 注意: 实现的服务和补偿必须满足幂等的条件。\n注意: 默认情况下，超时设置需要显式声明才生效。\n注意: 若全局事务起点与子事务起点重合，需同时声明 @SagaStart 和 @Compensable 的注解。\n  对转入服务重复第三步即可。\n  配置omega的spring配置项 在application.properties中添加下面的配置项：\nalpha.cluster.address=127.0.0.1:8080 #这个指向alpha server的grpc地址 然后就可以运行相关的微服务了，可通过访问http://127.0.0.1:8090/events 来查询所有的saga事件信息。\n总结 本篇只大概介绍了下servicecomb-saga的使用过程，主要内容都是参考其官方文档，其实也花了些时间走读它的源码，对其实现原理有一定了解了，后面抽时间再写一篇具体分析其源代码。\n参考  https://github.com/apache/incubator-servicecomb-saga/blob/master/docs https://www.cnblogs.com/savorboard/p/distributed-system-transaction-consistency.html  ","permalink":"https://jeremyxu2010.github.io/2018/07/servicecomb-saga%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98/","tags":["microservice","java"],"title":"servicecomb-saga开发实战"},{"categories":["devops"],"contents":"好长一段时间没有写博文了，最近的工作主要涉及docker及golang开发，这里输出两篇博文小结一下。\n其实以前的工作也涉及过docker，只是没有记录总结下来，这次争取记录得完善一点，以备以后查阅。\n安装docker环境 安装docker环境就不用再提了，直接参考官方文档就可以了，需要注意在中国境内玩docker，最好配好镜像加速器，可参考这里。\ndocker常用操作 获取镜像 docker pull centos docker pull centos:6.7 docker pull ${inner_docker_hub_ip}/${hub_user}/${image_name}:${image_tag} 运行镜像 docker run -it --rm ubuntu bash docker run -d ubuntu docker run -d -p 80:80 nginx docker run -d -v /tmp/data:/var/lib/mysql -p 3306:3306 mysql 操作镜像 docker image ls docker image ls ${repository_name} docker rmi ${image_id} docker rmi -f ${image_id} docker image prune docker tag ${image_id} ${image_name}:${image_tag} 编译镜像 docker build --rm -t ${image_name}:${image_tag} ./Dockerfile 操作容器 docker ps docker stop ${container_id} docker start ${container_id} docker logs ${container_id} docker exec -it ${container_id} bash selected_container_ids=$(docker ps | grep ${filter_word} | awk \u0026#39;{print $1}\u0026#39;) docker rm -f ${container_id} 推送镜像 docker tag ${image_name}:${image_tag} ${inner_docker_hub_ip}/${hub_user}/${image_name}:${image_tag} docker login ${inner_docker_hub_ip} docker push ${inner_docker_hub_ip}/${hub_user}/${image_name}:${image_tag} 编写Dockerfile 最近的工作还涉及编写一些镜像的Dockerfile文件，Dockerfile的语法比较简单，常用的大概是以下的指令\nARG指令 ARG CODE_VERSION=latest FROMbase:${CODE_VERSION}可参考这里。\nFROM指令 FROMcentos:6.7可参考这里。\nLABEL指令 LABEL maintainer=\u0026#34;SvenDowideit@home.org.au\u0026#34;可参考这里。\nCOPY指令 COPY package.json /usr/src/app/COPY hom* /mydir/COPY hom?.txt /mydir/可参考这里。\nADD指令 ADD ubuntu-xenial-core-cloudimg-amd64-root.tar.gz /可参考这里。\nRUN指令 RUN /bin/bash -c \u0026#39;source $HOME/.bashrc; echo $HOME\u0026#39;可参考这里。\nEXPOSE指令 EXPOSE3306EXPOSE80/tcpEXPOSE80/udp可参考这里。\nVOLUME指令 VOLUME [\u0026#34;/data\u0026#34;]可参考这里。\nWORKDIR指令 WORKDIR/path/to/workdir可参考这里。\nUSER指令 USERmysql可参考这里。\nCMD指令 CMD [\u0026#34;/usr/bin/wc\u0026#34;,\u0026#34;--help\u0026#34;]可参考这里。\nENTRYPOINT指令 CMD [\u0026#34;/usr/bin/wc\u0026#34;,\u0026#34;--help\u0026#34;]可参考这里。\nENTRYPOINT指令与CMD指令共同作用时，实际效果比较复杂，可参考下面的表格：\n The table below shows what command is executed for different ENTRYPOINT / CMD combinations:\n    No ENTRYPOINT ENTRYPOINT exec_entry p1_entry ENTRYPOINT [“exec_entry”, “p1_entry”]     No CMD error, not allowed /bin/sh -c exec_entry p1_entry exec_entry p1_entry   CMD [“exec_cmd”, “p1_cmd”] exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry exec_cmd p1_cmd   CMD [“p1_cmd”, “p2_cmd”] p1_cmd p2_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry p1_cmd p2_cmd   CMD exec_cmd p1_cmd /bin/sh -c exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry /bin/sh -c exec_cmd p1_cmd     还有一些指令ONBUILD、HEALTHCHECK、ENV不太常用，直接参考官方文档就可以了。\n另外再附一个Dockerfile最佳实践。\n编写docker-compose.yml docker-compose.yml的编写也比较简单，参考下面的例子：\nversion: \u0026#39;3\u0026#39; services: web: build: . depends_on: - db - redis redis: image: redis db: image: postgres 简单扩展就可以了。下面说一下平时常用的一些指令关键字。\nbuild 指定 Dockerfile 所在文件夹的路径（可以是绝对路径，或者相对 docker-compose.yml 文件的路径）。 Compose 将会利用它自动构建这个镜像，然后使用这个镜像。我比较少用到它，习惯于先生成好镜像，再直接使用镜像\ndepends_on 解决容器的依赖、启动先后的问题。以下例子中会先启动 redis db 再启动 web，如下面的例子：\nversion: \u0026#39;3\u0026#39; services: web: build: . depends_on: - db - redis redis: image: redis db: image: postgres env_file 从文件中获取环境变量，可以为单独的文件路径或列表。\n如果通过 docker-compose -f FILE 方式来指定 Compose 模板文件，则 env_file 中变量的路径会基于模板文件路径。\n如果有变量名称与 environment 指令冲突，则按照惯例，以后者为准。\nenv_file: .env env_file: - ./common.env - ./apps/web.env - /opt/secrets.env 环境变量文件中每一行必须符合格式，支持 # 开头的注释行。\n# common.env: Set development environment PROG_ENV=development expose 暴露端口，但不映射到宿主机，只被连接的服务访问。\n仅可以指定内部端口为参数\nexpose: - \u0026#34;3000\u0026#34; - \u0026#34;8000\u0026#34; extra_hosts 类似 Docker 中的 --add-host 参数，指定额外的 host 名称映射信息。\nextra_hosts: - \u0026#34;googledns:8.8.8.8\u0026#34; - \u0026#34;dockerhub:52.1.157.61\u0026#34; 会在启动后的服务容器中 /etc/hosts 文件中添加如下两条条目。\n8.8.8.8 googledns 52.1.157.61 dockerhub image 指定为镜像名称或镜像 ID。如果镜像在本地不存在，Compose 将会尝试拉取这个镜像。\nimage: ubuntu image: orchardup/postgresql image: a4bc65fd networks 配置容器连接的网络。\nversion: \u0026#34;3\u0026#34; services: some-service: networks: - some-network - other-network networks: some-network: other-network: ports 暴露端口信息。\n使用宿主端口：容器端口 (HOST:CONTAINER) 格式，或者仅仅指定容器的端口（宿主将会随机选择端口）都可以。\nports: - \u0026#34;3000\u0026#34; - \u0026#34;8000:8000\u0026#34; - \u0026#34;49100:22\u0026#34; - \u0026#34;127.0.0.1:8001:8001\u0026#34; 注意：当使用 HOST:CONTAINER 格式来映射端口时，如果你使用的容器端口小于 60 并且没放到引号里，可能会得到错误结果，因为 YAML 会自动解析 xx:yy 这种数字格式为 60 进制。为避免出现这种问题，建议数字串都采用引号包括起来的字符串格式。\nvolumes 数据卷所挂载路径设置。可以设置宿主机路径 （HOST:CONTAINER） 或加上访问模式 （HOST:CONTAINER:ro）。\n该指令中路径支持相对路径。\nvolumes: - /var/lib/mysql - cache/:/tmp/cache - ~/configs:/etc/configs/:ro 完整的指令关键字列表见这里。\n运行整个容器项目 使用以下命令运行起整个容器项目：\ndocker-compose up -f ./docker_compose.yml -d # 停止整个容器项目 # docker-compose down -f ./docker_compose.yml 其它发现 整个容器项目做完后，在网上又找到一个官方给出的写可复用docker-compose方案，简单看了下文档，貌似很简单：\n# 生成docker-compose.yml文件 docker-app render # 用生成的docker-compose.yml文件运行整个容器项目 docker-app render | docker-compose -f - up # 生成docker-compose.yml时指定一些选项 docker-app render --set version=0.2.3 --set port=4567 --set text=\u0026#34;hello production\u0026#34; # 生成helm的Chart，这个很方便啊，有木有 docker-app helm 参考  https://yeasy.gitbooks.io/docker_practice https://docs.docker.com/engine/reference/builder https://github.com/docker/app  ","permalink":"https://jeremyxu2010.github.io/2018/06/docker%E5%AE%9E%E6%88%98%E5%B0%8F%E7%BB%93/","tags":["docker","deployment","automation"],"title":"docker实战小结"},{"categories":["容器编排"],"contents":"部署镜像仓库harbor 官方文档中写得很清楚，这里简单概括一下：\n# 克隆git项目 $ git clone https://github.com/vmware/harbor $ cd harbor/contrib/helm/harbor # Download external dependent charts $ helm dependency update # 编辑部分配置项 $ vim values.yml ...... externalDomain: harbor.local ...... # 因为我本机的traefik ingress未开启TLS的endpoint，这里设置不自动跳转至https ingress.kubernetes.io/ssl-redirect: \u0026#34;false\u0026#34; ...... # 使用helm安装 $ helm install . --debug --name harbor-release -f values.yaml 过一会儿使用命令helm status harbor-release查看下部署的状态：\n$ helm status harbor-release ...... ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE harbor-release-postgresql-66f5477d9c-4wncj 1/1 Running 0 8m harbor-release-harbor-clair-55f6d7899d-hm8cx 1/1 Running 1 8m harbor-release-harbor-jobservice-878896998-glbvw 1/1 Running 4 8m harbor-release-harbor-notary-server-6ccddbcd98-knk2n 1/1 Running 0 8m harbor-release-harbor-notary-signer-5f4df97cd5-2vbb8 1/1 Running 0 8m harbor-release-harbor-ui-5bbf974497-skpnr 1/1 Running 3 8m harbor-release-redis-master-0 1/1 Running 0 8m harbor-release-harbor-adminserver-0 1/1 Running 1 8m harbor-release-harbor-mysql-0 1/1 Running 0 8m harbor-release-harbor-notary-db-0 1/1 Running 0 8m harbor-release-harbor-registry-0 1/1 Running 0 8m ...... NOTES: Please wait for several minutes for Harbor deployment to complete. Then follow the steps below to use Harbor. 1. Add the Harbor CA certificate to Docker by executing the following command: sudo mkdir -p /etc/docker/certs.d/harbor.local kubectl get secret \\  --namespace default harbor-release-harbor-ingress \\  -o jsonpath=\u0026#34;{.data.ca\\.crt}\u0026#34; | base64 --decode | \\  sudo tee /etc/docker/certs.d/harbor.local/ca.crt 2. Get Harbor admin password by executing the following command: kubectl get secret --namespace default harbor-release-harbor-adminserver -o jsonpath=\u0026#34;{.data.HARBOR_ADMIN_PASSWORD}\u0026#34; | base64 --decode; echo 3. Add DNS resolution entry for Harbor FQDN harbor.local to K8s Ingress Controller IP on DNS Server or in file /etc/hosts. Add DNS resolution entry for Notary FQDN notary-harbor.local to K8s Ingress Controller IP on DNS Server or in file /etc/hosts. 4. Access Harbor UI via https://harbor.local 5. Login Harbor with Docker CLI: docker login harbor.local 这里有几个提示：\n 从harbor-release-harbor-ingress 中导出data.ca.crt，并导入到docker的证书目录中，这样docker就会信任该镜像仓库 通过kubectl get secret --namespace default harbor-release-harbor-adminserver -o jsonpath=\u0026quot;{.data.HARBOR_ADMIN_PASSWORD}\u0026quot; | base64 --decode; echo 命令可以得到harbor的管理员密码 要添加两个域名映射harbor.local、notary-harbor.local，本机开发的话，把这两个域名指向127.0.0.1就可以了  在/etc/hosts文件中配上两个域名的映射，然后在浏览器中直接访问http://harbor.local，页面很快显示出来了，管理员登录凭证为admin/Harbor12345。\nOVER\n参考  https://github.com/vmware/harbor/tree/master/contrib/helm/harbor  ","permalink":"https://jeremyxu2010.github.io/2018/05/k8s%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%9502/","tags":["k8s","devops"],"title":"k8s学习记录02"},{"categories":["微服务"],"contents":"本文档目标 工作中要保证生产环境部署的consul的集群能够安全稳定地对外提供服务，即使出现系统故障也能快速恢复，这里将讲述部分的备份还原操作及KV的导入导出操作。\n备份与还原 需要备份的主要有两类数据：consul相关的配置文件、consul的服务器状态，采用下面的脚本备份就可以了：\nts=$(date +%Y%m%d%H%M%S) # 备份配置文件 tar -czpf consul_config_$ts.tar.gz /etc/consul/config.json /etc/consul/consul.d # 备份consul的服务器状态，注意由于该consul开启了ACL，执行consul snapshot save时必须带Management Token，关于consul ACL token的说明见上一篇\u0026#34;consul安全加固\u0026#34; consul snapshot save --http-addr=http://10.12.142.216:8500 -token=b3a9bca3-6e8e-9678-ea35-ccb8fb272d42 consul_state_$ts.snap # 查看一下生成的consul服务器状态文件 consul snapshot inspect consul_state_$ts.snap 最后将生成的consul_config_xxx.tar.gz、consul_state_xxx.snap拷贝到其它服务器妥善存储。\n还原也比较简单，采用下面的脚本就可以了：\n# 还原配置文件 tar -xzpf consul_config_20180521145032.tar.gz -C / # 还原consul服务器状态 consul snapshot restore --http-addr=http://10.12.142.216:8500 -token=b3a9bca3-6e8e-9678-ea35-ccb8fb272d42 consul_state_20180521145032.snap KV存储的导入导出 consul直接提供命令对KV里存储的数据进行导入导出，如下：\n$ ts=$(date +%Y%m%d%H%M%S) # 导出所有kv键值对，注意最后一个参数是导出键值对的前缀，为空字符串说明要导出所有 $ consul kv export --http-addr=http://10.12.142.216:8500 -token=b3a9bca3-6e8e-9678-ea35-ccb8fb272d42 \u0026#39;\u0026#39; \u0026gt; consul_kv_$ts.json # 查看下导出的json文件格式 $ cat consul_kv_$ts.json [ { \u0026#34;key\u0026#34;: \u0026#34;xxxxxx\u0026#34;, \u0026#34;flags\u0026#34;: 0, \u0026#34;value\u0026#34;: \u0026#34;yyyyyy\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;xxxxxx2\u0026#34;, \u0026#34;flags\u0026#34;: 0, \u0026#34;value\u0026#34;: \u0026#34;eyJ2ZXJzaW9uX3RpbWVzdGFtcCI6IC0xfQ==\u0026#34; }, ] 发现是每个键值对都是json数值中一项，其中key为键值对Key的名称，value为键值对Value的base64编码，使用base64 -d命令编码就可以看到原始的value值，如：\n$ echo \u0026#39;eyJ2ZXJzaW9uX3RpbWVzdGFtcCI6IC0xfQ==\u0026#39; | base64 -d {\u0026#34;version_timestamp\u0026#34;: -1} 导入就更简单了：\nconsul kv import --http-addr=http://10.12.142.216:8500 -token=b3a9bca3-6e8e-9678-ea35-ccb8fb272d42 @consul_kv_20180521150322.json OVER\n参考  https://www.consul.io/docs/commands/snapshot.html https://www.consul.io/docs/commands/kv/import.html https://www.consul.io/docs/commands/kv/export.html  ","permalink":"https://jeremyxu2010.github.io/2018/05/consul%E5%9F%BA%E7%A1%80%E8%BF%90%E7%BB%B4-%E5%A4%87%E4%BB%BD%E8%BF%98%E5%8E%9F%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA/","tags":["microservice","consul"],"title":"consul基础运维-备份还原导入导出"},{"categories":["容器编排"],"contents":"最近在本机macOS安装了开发用的k8s集群之后，花了些时间研究k8s，在这个过程中有一些零零星星的实操技巧，在这里记录一下，这些实际操作技巧均是在之前搭建的单机环境验证过的，可以作为其它环境的参考。\n利其器 发现两个工具可以极大提高效率，这里首先提一下。\nkube-ps1 为命令行终端增加k8s相关的$PROMPT字段，安装方法如下：\nbrew install kube-ps1 # 然后在~/.zshrc最后添加以下两行 # source \u0026#34;/usr/local/opt/kube-ps1/share/kube-ps1.sh\u0026#34; # PROMPT=\u0026#39;$(kube_ps1)\u0026#39;$PROMPT # 重新加载一下zshrc的配置 source ~/.zshrc 然后在执行kubectl命令里就可以明确地知道上下文及命名空间了。\nkube-shell 这个就更强大了，交互式带命令提示的kubectl终端。不过官方最新版本有问题，可以安装我这里准备好的稳定版本：\ngit clone https://github.com/jeremyxu2010/kube-shell.git cd kube-shell git checkout stable pip install . --user -U 然后就可以敲kube-shell命令使用了，功能很强大，使用文档见这里。\nk8s里的基本概念 k8s里的基本概念比较多，不过设计上还是比较简单的，大概浏览下Jimmy Song写的kubernetes-handbook这些章节3.1.** Kubernetes架构、3.4. Pod状态与生命周期管理、3.5. 集群资源管理、3.6. 控制器、3.7. 服务发现、3.8. 身份与权限控制、3.9. 存储，就差不多了。\nIngress Controller 部署在k8s里的服务总要想办法让外部访问到，不可能每次都是用type:NodePort来解决问题，这里我用traefik-ingress-controller及nginx-ingress-controller，分别解决http和tcp协议服务的外部暴露问题。\ntraefik-ingress-controller 安装起来参考官方文档就好了，这里简要列一下步骤：\n# 创建相关服务帐户及集群角色、集群角色绑定 kubectl apply -f https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/traefik-rbac.yaml # 以DaemonSet方式部署 kubectl apply -f https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/traefik-ds.yaml # 这里再编辑一下traefik-ingress-service，去掉nodePort, 将port修改为80, type修改为LoadBalancer kubectl edit service traefik-ingress-service # 最后重启一下Docker for macOS docker ps -q | xargs -L1 docker stop test -z \u0026#34;$(docker ps -q 2\u0026gt;/dev/null)\u0026#34; \u0026amp;\u0026amp; osascript -e \u0026#39;quit app \u0026#34;Docker\u0026#34;\u0026#39; open --background -a Docker 等docker及k8s都启动完成，这时会发现本机的80端口处于监听状态了，用浏览器直接访问，当然是看不到正常的页面的，因为还要提交Ingress，也比较简单：\n$ cat traefik-web-ui.yml apiVersion: v1 kind: Service metadata: name: traefik-web-ui namespace: kube-system spec: selector: k8s-app: traefik-ingress-lb ports: - port: 80 targetPort: 8080 --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: traefik-web-ui namespace: kube-system annotations: kubernetes.io/ingress.class: traefik spec: rules: - host: traefik-ui.local http: paths: - backend: serviceName: traefik-web-ui servicePort: 80 # 用上述描述文件部署 $ kubectl apply -f traefik-web-ui.yml 再在/etc/hosts文件中把traefik-ui.local这个域名指向本机，然后就可以在浏览器中访问http://traefik-ui.local/了。\n类似的，其它http协议的service以后type都可以只设为ClusterIP，然后配置一个Ingress通过traefik-ingress-controller暴露出去了。比如现在暴露kubernetes-dashboard就很方便了：\n# 将hostPort删除， 将type修改为ClusterIP $ kubectl edit service kubernetes-dashboard $ cat kubernetes-dashboard.yml apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: traefik name: k8s-dashboard namespace: kube-system spec: rules: - host: k8s-dashboard.local http: paths: - backend: serviceName: kubernetes-dashboard servicePort: 443 path: / # 用上述描述文件部署 $ kubectl apply -f kubernetes-dashboard.yml 再在/etc/hosts文件中把k8s-dashboard.local这个域名指向本机，然后就可以在浏览器中访问http://k8s-dashboard.local/了。\n这里有一个小插曲，因为本机安装的k8s-dashboard的证书不合法，为了让traefik-ingress-controller可正常反向代理到它，需要修改traefik-ingress-controller的一个参数：\n# 给容器添加一个--insecureSkipVerify=true的启动参数 kubectl edit daemonset traefik-ingress-controller nginx-ingress-controller 假设k8s集群中有一个mysql服务需要暴露给外部访问，这时就用得上nginx-ingress-controller了，安装方法也很类似：\n# 部署nginx-ingress-controller相关的服务帐户、集群角色、集群角色绑定、Deployment、ConfigMap $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml # 暴露某些端口 $ cat nginx-ingress-service.yml kind: Service apiVersion: v1 metadata: name: ingress-nginx namespace: ingress-nginx labels: app: ingress-nginx spec: externalTrafficPolicy: Local type: LoadBalancer selector: app: ingress-nginx ports: - name: mysql port: 3306 targetPort: 3306 # 用上述描述文件部署 $ kubectl apply -f nginx-ingress-service.yml # 等一会儿后，重启Docker for macOS后，应该有进程监听3306端口了 $ lsof -i :3306 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME com.docke 36484 jeremy 37u IPv4 0xe746861636421a57 0t0 TCP *:mysql (LISTEN) com.docke 36484 jeremy 39u IPv6 0xe7468616205d110f 0t0 TCP localhost:mysql (LISTEN) # 然后创建tcp服务相关的ConfigMap，其中mysql是mysql服务的名称，如要反向代理其它tcp服务，相应地修改data里的定义 $ cat nginx-tcp-configmap.yml apiVersion: v1 kind: ConfigMap metadata: name: nginx-tcp-configmap namespace: kube-system data: \u0026#34;3306\u0026#34;: default/mysql:3306 # 最后修改nginx-ingress-controller运行时的参数，指定tcp服务反向代理的configmap，添加--tcp-services-configmap=kube-system/nginx-tcp-configmap启动参数 kubectl edit deployment nginx-ingress-controller 这时在本机就可以访问mysql服务了：\nmysql -uroot -p -h127.0.0.1 -P3306 至此，无论是http协议还是tcp协议的服务，都可以很方便地暴露给外部使用了。\n部署基础服务 常规的基础服务都已经用别人已经打好的包，可以通过helm来安装，helm的安装方法也比较简单：\n$ brew install kubernetes-helm # helm在k8s里初始化 $ helm init # 查询mq相关的包 $ helm search mq NAME CHART VERSION\tAPP VERSION\tDESCRIPTION stable/prometheus-rabbitmq-exporter\t0.1.1 v0.28.0 Rabbitmq metrics exporter for prometheus stable/rabbitmq 1.1.2 3.7.5 Open source message broker software that implem... stable/rabbitmq-ha 1.5.0 3.7.4 Highly available RabbitMQ cluster, the open sou... # 这样就会将别人打好的rabbitmq包部署起来 $ helm install stable/rabbitmq -n testmq 安装的时候还可以指定定制的参数，参见这里。\n使用命令helm search可以看到目前仓库里别人打好的helm chart，发现redis, mysql, rabbitmq等常用基础组件都有了，真的是很方便。即使有一些组件比较特殊没有，也可以参考kubernets/charts及Developing Templates模仿写一个chart。\n参考  https://forums.docker.com/t/restart-docker-from-command-line/9420 https://jimmysong.io/kubernetes-handbook https://docs.traefik.io/user-guide/kubernetes https://github.com/cloudnativelabs/kube-shell https://kubernetes.github.io/ingress-nginx https://docs.helm.sh  ","permalink":"https://jeremyxu2010.github.io/2018/05/k8s%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%9501/","tags":["k8s","devops"],"title":"k8s学习记录01"},{"categories":["微服务"],"contents":"[TOC]\n本文档目标 最近的工作需要对默认安装的consul集群进行安全加固，这里将安全加固的步骤记录下来。\nconsul 术语 首先介绍下在 consul 中会经常见到的术语：\n node：节点，需要 consul 注册发现或配置管理的服务器。 agent：consul 中的核心程序，它将以守护进程的方式在各个节点运行，有 client 和 server 启动模式。每个 agent 维护一套服务和注册发现以及健康信息。 client：agent 以 client 模式启动的节点。在该模式下，该节点会采集相关信息，通过 RPC 的方式向 server 发送。 server：agent 以 server 模式启动的节点。一个数据中心中至少包含 1 个 server 节点。不过官方建议使用 3 或 5 个 server 节点组建成集群，以保证高可用且不失效率。server 节点参与 Raft、维护会员信息、注册服务、健康检查等功能。 datacenter：数据中心，私有的，低延迟的和高带宽的网络环境。一般的多个数据中心之间的数据是不会被复制的，但可用过 ACL replication 或使用外部工具 onsul-replicate。 Consensus，共识协议，使用它来协商选出 leader。 Gossip：consul 是建立在 Serf，它提供完整的 gossip protocol，维基百科。 LAN Gossip，Lan gossip 池，包含位于同一局域网或数据中心上的节点。 WAN Gossip，只包含 server 的 WAN Gossip 池，这些服务器主要位于不同的数据中心，通常通过互联网或广域网进行通信。 members：成员，对 consul 成员的称呼。提供会员资格，故障检测和事件广播。有兴趣的朋友可以深入研究下。  consul 架构 consul 的架构是什么，官方给出了一个很直观的图片：\n这里存在两个数据中心：DATACENTER1、DATACENTER2。每个数据中心有着 3 到 5 台 server（该数量使得在故障转移和性能之间达到平衡）。\n单个数据中心的所有节点都参与 LAN Gossip 池，也就是说该池包含了这个数据中心的所有节点。这有几个目的：\n 不需要给客户端配置服务器地址，发现自动完成。 检测节点故障的工作不是放在服务器上，而是分布式的。这使得故障检测比心跳方案更具可扩展性。 事件广播，以便在诸如领导选举等重要事件发生时通知。  所有 server 节点也单独加入 WAN Gossip 池，因为它针对互联网的高延迟进行了优化。这个池的目的是允许数据中心以低调的方式发现对方。在线启动新的数据中心与加入现有的 WAN Gossip 一样简单。因为这些服务器都在这个池中运行，所以它也支持跨数据中心的请求。当服务器收到对不同数据中心的请求时，它会将其转发到正确数据中心中的随机服务器。那个服务器可能会转发给当地的领导。\n这导致数据中心之间的耦合非常低，但是由于故障检测，连接缓存和复用，跨数据中心请求相对快速可靠。\n一般来说，数据不会在不同的领事数据中心之间复制。当对另一数据中心的资源进行请求时，本地 consul 服务器将 RPC 请求转发给该资源的远程 consul 服务器并返回结果。如果远程数据中心不可用，那么这些资源也将不可用，但这不会影响本地数据中心。有一些特殊情况可以复制有限的数据子集，例如使用 consul 内置的 ACL replication功能，或外部工具如 consul-replicate。\n 更多协议详情，你可以 Consensus Protocol 和 Gossip Protocol。\n consul 端口说明 consul 内使用了很多端口，理解这些端口的用处对你理解 consul 架构很有帮助：\n   端口 说明     TCP/8300 8300 端口用于服务器节点。客户端通过该端口 RPC 协议调用服务端节点。服务器节点之间相互调用   TCP/UDP/8301 8301 端口用于单个数据中心所有节点之间的互相通信，即对 LAN 池信息的同步。它使得整个数据中心能够自动发现服务器地址，分布式检测节点故障，事件广播（如领导选举事件）。   TCP/UDP/8302 8302 端口用于单个或多个数据中心之间的服务器节点的信息同步，即对 WAN 池信息的同步。它针对互联网的高延迟进行了优化，能够实现跨数据中心请求。   8500 8500 端口基于 HTTP 协议，用于 API 接口或 WEB UI 访问。   8600 8600 端口作为 DNS 服务器，它使得我们可以通过节点名查询节点信息。    consul多数据中心搭建 参见consul多数据中心搭建，可以看到多数据中心的搭建也是比较容易的，关键在于要在每个数据中心选择一个边界节点，并配好-advertise-wan=参数，再执行consul join -wan $other_wlan_ip。\n定制datacenter名称 在我们私有部署的场景里，暂时不需要配置多datacenter，只用一个datacenter即可。默认安装的consul集群datacenter名称都为dc1，不太友好，首先将这个修改下，在每个consul节点（包括server节点及client节点）执行以下命令即可：\n# 备份原来的配置文件 cp -f /etc/consul/config.json /etc/consul/config.json.orig # 将datacenter名称修改为tstack_dc sed -i -e \u0026#39;s/\u0026#34;datacenter\u0026#34;.*/\u0026#34;datacenter\u0026#34;: \u0026#34;tstack_dc\u0026#34;,/\u0026#39; /etc/consul/config.json # 重启consul systemctl restart consul.service 再登录consul的web ui，即可看到datacenter的名称发生了改变。\n启用consul ACL Consul默认没有启用ACL（Access Control List），任何连上consul的node节点可以访问consul的所有功能，下面是consul里按功能分类的策略列表。\n   Policy Scope     agent Utility operations in the Agent API, other than service and check registration   event Listing and firing events in the Event API   key Key/value store operations in the KV Store API   keyring Keyring operations in the Keyring API   node Node-level catalog operations in the Catalog API, Health API, Prepared Query API, Network Coordinate API, and Agent API   operator Cluster-level operations in the Operator API, other than the Keyring API   query Prepared query operations in the Prepared Query API   service Service-level catalog operations in the Catalog API, Health API, Prepared Query API, and Agent API   session Session operations in the Session API    显然让任何连上consul的node节点访问consul的所有功能是不安全的，所以有必要启用ACL，以下是启用的步骤：\n  在consul的配置文件中添加以下3个配置项\n{ ...... \u0026#34;acl_datacenter\u0026#34;: \u0026#34;tstack_dc\u0026#34;, \u0026#34;acl_default_policy\u0026#34;: \u0026#34;allow\u0026#34;, \u0026#34;acl_down_policy\u0026#34;: \u0026#34;extend-cache\u0026#34;, ...... } 注意这里先将acl_default_policy设置为allow，后面得到client token，并在所有客户端中都配置了client token后，再将其修改为deny\n  得到bootstrap的management token\ncurl --request PUT https://10.12.142.217:8500/v1/acl/bootstrap {\u0026#34;ID\u0026#34;:\u0026#34;b3a9bca3-6e8e-9678-ea35-ccb8fb272d42\u0026#34;} # 这里b3a9bca3-6e8e-9678-ea35-ccb8fb272d42就是bootstrap的management token   因为我们只使用到consul的node、service、key、session、agent相关功能，因此只创建拥有这些功能访问权限的client token\ncurl -X PUT --header \u0026#34;X-Consul-Token: b3a9bca3-6e8e-9678-ea35-ccb8fb272d42\u0026#34; --data \\ \u0026#39;{ \u0026#34;Name\u0026#34;: \u0026#34;AgentToken\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;client\u0026#34;, \u0026#34;Rules\u0026#34;: \u0026#34;node \\\u0026#34;\\\u0026#34; { policy = \\\u0026#34;read\\\u0026#34; } node \\\u0026#34;\\\u0026#34; { policy = \\\u0026#34;write\\\u0026#34; } service \\\u0026#34;\\\u0026#34; { policy = \\\u0026#34;read\\\u0026#34; } service \\\u0026#34;\\\u0026#34; { policy = \\\u0026#34;write\\\u0026#34; } key \\\u0026#34;\\\u0026#34; { policy = \\\u0026#34;read\\\u0026#34; } key \\\u0026#34;\\\u0026#34; { policy = \\\u0026#34;write\\\u0026#34; } agent \\\u0026#34;\\\u0026#34; { policy = \\\u0026#34;read\\\u0026#34; } agent \\\u0026#34;\\\u0026#34; { policy = \\\u0026#34;write\\\u0026#34; } session \\\u0026#34;\\\u0026#34; { policy = \\\u0026#34;read\\\u0026#34; } session \\\u0026#34;\\\u0026#34; { policy = \\\u0026#34;write\\\u0026#34; }\u0026#34; }\u0026#39; http://10.12.142.217:8500/v1/acl/create {\u0026#34;ID\u0026#34;:\u0026#34;0b7df19e-6eab-5748-bba3-2f56bf85a6a9\u0026#34;} # 这里0b7df19e-6eab-5748-bba3-2f56bf85a6a9就是client token   为了运维管理方便，consul的web ui管理节点直接配置上management token，在这些节点的consul配置文件中加入下面的配置项：\n{ ...... \u0026#34;acl_master_token\u0026#34;: \u0026#34;b3a9bca3-6e8e-9678-ea35-ccb8fb272d42\u0026#34;, \u0026#34;acl_agent_token\u0026#34;: \u0026#34;b3a9bca3-6e8e-9678-ea35-ccb8fb272d42\u0026#34;, ...... } 并重启consul\n  验证ACL，在consul的web ui中配置访问时所用的token，观察使用该token是否只能使用正确的功能。配置浏览器访问时所用的token方法如下图所示：\n  如token的权限是正常的，则可以将acl_default_policy设置为deny，并将client token分发给客户端，连上consul的node节点必需使用该token才可能使用权限指定的功能。\n  consul的ACL控制文档写得比较难理解，想了解具体细节，可以参考官方文档、consul ACL配置使用。\nconsul web ui的安全 consul本身并没有提供web ui的安全性保证，只要防火墙允许，则在外网的任何人也可以访问其web ui，这一点比较危险，这里采用基本的auth_basic来保证consul web ui的安全性，方案简述如下：\n  以server模式运行consul agent的服务器，其配置网络策略，仅允许在内网范围内其它节点可访问其8500端口。\n  以client模式运行consul agent的节点，其如果打开web ui，则只绑定地址127.0.0.1；其可以以8500端口连接consul server agent，但在使用consul相关功能时，必须使用client token或management token。\n  在内网中采用nginx或apache做反向代理至consul server agent节点的8500端口，并在nginx或apache中配置auth_basic认证。反向代理及auth_basic认证的配置参考下面：\nyum install -y httpd-tools htpasswd -c /etc/nginx/htpsswd consul_access # 执行后会要求你输入密码，完了就完成了账号密码的生成 # 下面以配置nginx示例，apache的配置类似 upstream consul { server 10.12.142.216:8500; server 10.12.142.217:8500; server 10.12.142.218:8500; } server { listen 18500; server_name consul.xxxx.com; location / { proxy_pass http://consul; proxy_read_timeout 300; proxy_connect_timeout 300; proxy_redirect off; auth_basic \u0026#34;Restricted\u0026#34;; auth_basic_user_file /etc/nginx/htpasswd; } }   配置网络策略，在外网仅允许访问nginx的反向代理地址，访问时需输入auth_basic认证信息，并且在使用consul相关功能时，必须使用client token（原则上不允许将management token带出到外网）。\n  链路安全 consul 由于采用了 gossip、RPC、HTTPS、HTTP来提供功能。其中 gossip、RPC、HTTPS分别采用了不同的安全机制。其中 gossip 使用对称密钥提供加密，RPC 则可以使用客户端认证的端到端 TLS，HTTPS 也是使用客户端认证的端到端 TLS。而我们的使用场景实际上是只使用了gossip、HTTP，因此可参考这篇文章酌情进行链路安全方面的设置，目前来看，只能加入gossip 加密。\n参考  http://www.xiaomastack.com/2016/05/20/consul02 https://www.consul.io/docs/guides/acl.html http://www.xiaomastack.com/2016/06/11/cousnl-acl https://deepzz.com/post/the-consul-of-discovery-and-configure-services.html  ","permalink":"https://jeremyxu2010.github.io/2018/05/consul%E5%AE%89%E5%85%A8%E5%8A%A0%E5%9B%BA/","tags":["microservice","consul"],"title":"consul安全加固"},{"categories":["容器编排"],"contents":"最近的工作跟微服务有关，偶然在网上发现一个用k8s写微服务的小例子，觉得这样写微服务真的好简单，都不用在程序框架层面实现服务注册与服务发现了，这个后面可以好好研究一下。在使用这种方式写微服务前，需要在个人开发机上搭建k8s集群。我的开发机是macOS系统，今天研究了一下，找到一种极为简易的方法，终于不用为搭一个开发用的k8s集群而专门启动虚拟机了，这里记录一下。\n安装Docker for macOS 安装 下载最新的Docker for Mac Edge 版本，跟普通mac软件一样安装，然后运行它，会在右上角菜单栏看到多了一个鲸鱼图标，这个图标表明了 Docker 的运行状态。\n配置镜像加速地址 鉴于国内网络问题，国内从 Docker Hub 拉取镜像有时会遇到困难，此时可以配置镜像加速器。Docker 官方和国内很多云服务商都提供了国内加速器服务。\n点击设置菜单\n设置镜像加速地址\n检查docker环境 可执行以下命令检查docker环境\n$ docker --version Docker version 18.05.0-ce-rc1, build 33f00ce $ docker-compose --version docker-compose version 1.21.0, build 5920eb0 $ docker-machine --version docker-machine version 0.14.0, build 89b8332 # 如果 docker version、docker info 都正常的话，可以尝试运行一个 Nginx 服务器： $ docker run -d -p 80:80 --name webserver nginx # 访问一下Nginx服务器 $ curl http://localhost # 停止 Nginx 服务器并删除 $ docker stop webserver $ docker rm webserver 搭建k8s本地开发环境 启用k8s 点击设置菜单\n点击启动k8s的checkbox，这里会拉取比较多的镜像，可能要等好一会儿。\n检查k8s环境 可执行以下命令检查k8s环境\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION docker-for-desktop Ready master 3h v1.9.6 $ kubectl cluster-info Kubernetes master is running at https://localhost:6443 KubeDNS is running at https://localhost:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. 部署kubernetes-dashboard服务 按以下步骤部署k8s-dashboard服务\n$ kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml # 开发环境推荐用NodePort的方式访问dashboard，因此编辑一下该部署 $ kubectl -n kube-system edit service kubernetes-dashboard # 这里将type: ClusterIP修改为type: NodePort # 获取dashboard服务暴露的访问端口 $ kubectl -n kube-system get service kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard NodePort 10.98.82.248 \u0026lt;none\u0026gt; 443:31241/TCP 2h 按上述输出，dashboard服务暴露的访问端口是31241，因此可以用浏览器访问https://localhost:31241/，我们可以看到登录界面\n此时可暂时直接跳过，进入到控制面板中\n使用k8s本地开发环境 这里尝试用Skaffold往本地开发环境部署微服务应用。\n安装Skaffold curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-darwin-amd64 \u0026amp;\u0026amp; chmod +x skaffold \u0026amp;\u0026amp; sudo mv skaffold /usr/local/bin 获取微服务示例代码 git clone https://github.com/GoogleContainerTools/skaffold cd skaffold/examples/microservices 部署到本地k8s环境 skaffold run # 获取leeroy-web服务暴露的访问端口 $ kubectl get service leeroy-web NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE leeroy-web NodePort 10.98.162.88 \u0026lt;none\u0026gt; 8080:30789/TCP 56m 按上述输出，dashboard服务暴露的访问端口是30789，因此可以用浏览器访问http://localhost:30789/\nk8s的dashboard中检查部署 删除无用的docker实例及镜像 用skaffold反复进行部署时会产生一些无用的docker实例及镜像，这里用一个脚本将它们删除\n# 删除停止或一直处于已创建状态的实例 docker ps --filter \u0026#34;status=exited\u0026#34;|sed -n -e \u0026#39;2,$p\u0026#39;|awk \u0026#39;{print $1}\u0026#39;|xargs docker rm docker ps --filter \u0026#34;status=created\u0026#34;|sed -n -e \u0026#39;2,$p\u0026#39;|awk \u0026#39;{print $1}\u0026#39;|xargs docker rm # 删除虚悬镜像 docker image prune --force # 删除REPOSITORY是长长uuid的镜像 docker images | sed -n -e \u0026#39;2,$p\u0026#39;|awk \u0026#39;{if($1 ~ /[0-9a-f]{32}/) print $1\u0026#34;:\u0026#34;$2}\u0026#39;|xargs docker rmi # 删除TAG是长长uuid的镜像 docker images | sed -n -e \u0026#39;2,$p\u0026#39;|awk \u0026#39;{if($2 ~ /[0-9a-f]{64}/) print $1\u0026#34;:\u0026#34;$2}\u0026#39;|xargs docker rmi OVER\n参考  https://juejin.im/post/5a5cbad5518825734216e14f https://yeasy.gitbooks.io/docker_practice/content/install/mac.html https://github.com/kubernetes/dashboard/wiki/Accessing-Dashboard---1.7.X-and-above https://yeasy.gitbooks.io/docker_practice/content/image/list.html https://github.com/GoogleContainerTools/skaffold  ","permalink":"https://jeremyxu2010.github.io/2018/05/%E5%9F%BA%E4%BA%8Edocker-for-macos%E7%9A%84kubernetes%E6%9C%AC%E5%9C%B0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E4%B8%8E%E5%BA%94%E7%94%A8%E9%83%A8%E7%BD%B2/","tags":["k8s","devops"],"title":"基于Docker for macOS的Kubernetes本地环境搭建与应用部署"},{"categories":["devops"],"contents":"工作中要经常要给centos6, centos7打某应用程序的rpm包。原来安装了两个虚拟机专门干这个工作，但经常会因为打包给系统装上许多没用的软件包，占用空间，而且要频繁在两个虚拟机间切换，很是麻烦。经同事介绍，发现mock这个工具，终于较完美地解决了此问题。\n使用mock前的准备工作 yum -y install epel-release # 由于mock是在epel仓库里的，所以还需要先装epel仓库 yum -y install mock 通常情况下使用rpmbuild会新开一个用户，比如builder，这样就不会污染系统环境。我们需要把builder用户加入mock用户组：\nusermod -a -G mock builder mock打rpm包时需要src.rpm文件，还是用老方法生成src.rpm文件：\nrpmbuild -bs test.spec 然后需要初始化mock环境，在/etc/mock文件夹下有各个环境的配置文件，比如centos 6就是centos-6-x86_64，centos 7就是centos-7-x86_64，初始化命令就是：\nmock -r centos-6-x86_64 --init 可以在初始环境前修改配置文件中yum源的地址，这样生成rpm包的过程中下载相关依赖的rpm包会快很多。\n使用mock 生成rpm包\nmock -r centos-6-x86_64 rebuild test-1.1-1.src.rpm 构建完毕，rpm文件会存放在/var/lib/mock/epel-6-x86_64/result目录下。当然我们可以通过–resultdir参数来指定rpm文件的生成目录\nmock -r centos-6-x86_64 rebuild test-1.1-1.src.rpm --resultdir=/home/builder/rpms 最后执行clean命令清理环境\nmock -r centos-6-x86_64 --clean mock的实现原理 简单看了看mock的实现原理，就是在chroot环境中打rpm包，很自然。\n参考  https://leo108.com/pid-2207/  ","permalink":"https://jeremyxu2010.github.io/2018/04/centos%E4%B8%8B%E6%97%A0%E6%B1%A1%E6%9F%93%E5%9C%B0%E6%89%93rpm%E5%8C%85/","tags":["centos","rpmbuild"],"title":"centos下无污染地打rpm包"},{"categories":["devops"],"contents":"Playbooks 简介 Playbooks 与 adhoc 相比,是一种完全不同的运用 ansible 的方式,是非常之强大的.\n简单来说,playbooks 是一种简单的配置管理系统与多机器部署系统的基础.与现有的其他系统有不同之处,且非常适合于复杂应用的部署.\nPlaybooks 可用于声明配置,更强大的地方在于,在 playbooks 中可以编排有序的执行过程,甚至于做到在多组机器间,来回有序的执行特别指定的步骤.并且可以同步或异步的发起任务.\n我们使用 adhoc 时,主要是使用 /usr/bin/ansible 程序执行任务.而使用 playbooks 时,更多是将之放入源码控制之中,用之推送你的配置或是用于确认你的远程系统的配置是否符合配置规范.\nPlaybook示例 首先看一个最简单的示例，基本全是YAML语法：\n$ tree -L 2 . ├── ansible.cfg ├── example1.yml ├── hosts.yml └── templates └── httpd.conf.j2 $ cat hosts.yml [webservers] 192.168.1.1 192.168.1.2 $ cat example1.yml --- - hosts: webservers vars: http_port: 80 max_clients: 200 remote_user: root tasks: - name: ensure apache is at the latest version yum: pkg=httpd state=latest - name: write the apache config file template: src=./templates/httpd.conf.j2 dest=/etc/httpd/conf/httpd.conf notify: - restart apache - name: ensure apache is running service: name=httpd state=started handlers: - name: restart apache service: name=httpd state=restarted $ cat httpd.conf.j2 ... MaxClients {{ max_clients }} ... Listen {{ http_port }} ... 这里主要看example1.yml这个文件，其代表的意义是在webservers这组主机上执行一个任务列表（先确保安装了httpd的软件包，再通过模板写入一个配置文件，再确保httpd服务已启动），很简单吧。\n执行一下：\nansible-playbook example1.yml 创建可重用的Playbook 但为了代码的可维护性与重用，一般会重新组织下代码，如下：\n$ tree -L 4 . ├── ansible.cfg ├── example1.yml ├── hosts.yml └── roles └── httpd ├── handlers │ └── main.yml ├── tasks │ └── main.yml ├── templates │ └── httpd.conf.j2 └── vars └── main.yml $ cat example1.yml --- - hosts: webservers remote_user: root roles: - httpd $ cat roles/httpd/tasks/main.yml --- - name: ensure apache is at the latest version yum: pkg=httpd state=latest - name: write the apache config file template: src=httpd.conf.j2 dest=/etc/httpd/conf/httpd.conf notify: - restart apache - name: ensure apache is running service: name=httpd state=started $ cat roles/httpd/vars/main.yml --- - http_port: 80 - max_clients: 200 $ cat roles/httpd/handlers/main.yml --- - name: restart apache service: name=httpd state=restarted $ cat roles/httpd/templates/httpd.conf.j2 ... MaxClients {{ max_clients }} ... Listen {{ http_port }} ... 比较简单，就是将ansible脚本封装到一个所谓的role里面，每个role里按照tasks、vars、templates、handlers等目录组织代码。tasks、vars、templates、handlers目录默认会加载目录中的main.yml，也可以继续拆分main.yml，并用import或include引入起来。\n这样做的好处：一是可以让每个role的功能更内敛，另一方面可以比较方便地利用role，如下：\n--- - hosts: buzservers remote_user: root roles: - httpd - tomcat 上面的将在buzservers这组主机上安装httpd和tomcat（这两个各是一个已经写好的role）。role除了自己手写外，还可以通过ansible-galaxy安装得到，如：\nansible-galaxy install --roles-path ./roles bennojoy.mysql 在ansible-galaxy上有大量别人写的role，基本覆盖了常用的运维需求，很多直接拿来使用就好。\nTasks 列表 role的tasks目录下可定义任务列表，即在目标主机上执行的指令队列。ansible会按照顺序依次执行该指令队列里的指令。如下所示：\n- name: ensure apache is at the latest version yum: pkg=httpd state=latest - name: write the apache config file template: src=./templates/httpd.conf.j2 dest=/etc/httpd/conf/httpd.conf notify: - restart apache - name: ensure apache is running service: name=httpd state=started 这里每一个指令可以用name给命个名，这样输出时方便观察当前执行的指令。\n每个指令其实是执行ansible里的模块Module，完整的模块列表在这里。每个模块都有很详尽的示例，照着写就可以了。比较常用的有\n Files Modules Net Tools Modules Network Modules Packaging Modules Source Control Modules System Modules Utilities Modules Windows Modules 如果要操作windows的话  Playbook中的变量 变量在Playbook中算是比较复杂的，可以在很多地方定义变量。\n定义变量 Inventory中定义变量 # hosts.yml [atlanta] host1 http_port=80 maxRequestsPerChild=808 host2 http_port=303 maxRequestsPerChild=909 [atlanta:vars] ntp_server=ntp.atlanta.example.com proxy=proxy.atlanta.example.com Playbook中定义变量 # exampl2.yml - hosts: webservers vars: http_port: 80 role的vars目录下定义变量 --- # roles/httpd/vars/main.yml - http_port: 80 - max_clients: 200 role的defaults目录下定义默认变量 --- # roles/httpd/defaults/main.yml - http_port: 80 - max_clients: 200 include指令可以传递变量 # roles/httpd/tasks/main.yml - include: wordpress.yml vars: wp_user: timmy some_list_variable: - alpha - beta - gamma 命令行中传递变量 ansible-playbook release.yml --extra-vars \u0026#34;version=1.23.45 other_variable=foo\u0026#34; 自动发现的变量 $ ansible hostname -m setup ... \u0026#34;ansible_all_ipv4_addresses\u0026#34;: [ \u0026#34;REDACTED IP ADDRESS\u0026#34; ], \u0026#34;ansible_all_ipv6_addresses\u0026#34;: [ \u0026#34;REDACTED IPV6 ADDRESS\u0026#34; ], \u0026#34;ansible_architecture\u0026#34;: \u0026#34;x86_64\u0026#34;, \u0026#34;ansible_bios_date\u0026#34;: \u0026#34;09/20/2012\u0026#34;, \u0026#34;ansible_bios_version\u0026#34;: \u0026#34;6.00\u0026#34;, ... 通过register注册变量 # roles/httpd/tasks/main.yml - shell: /usr/bin/foo register: foo_result ignore_errors: True - shell: /usr/bin/bar when: foo_result.rc == 5 通过vars_files引入外部变量文件 --- - hosts: all remote_user: root vars: favcolor: blue vars_files: - /vars/external_vars.yml 使用变量 模板文件里使用变量 ansible里使用了Jinja2模板，在模板里使用变量还是比较简单的\n# roles/httpd/templates/test.j2 My amp goes to {{ max_amp_value }} 模板里使用变量还可以使用一些内置的过滤器，参见这里，如下：\n{{ \u0026quot;%s - %s\u0026quot;|format(\u0026quot;Hello?\u0026quot;, \u0026quot;Foo!\u0026quot;) }} -\u0026gt; Hello? - Foo! YAML文件里使用变量 yaml文件里使用变量跟Jinja2模板里一样，也是用{{ }}将变量包起来，不过要注意YAML语法要求如果值以{{ foo }}开头的话，需要将整行用双引号包起来，这是为了确认不想声明一个YAML字典。\n- hosts: app_servers vars: app_path: \u0026#34;{{ base_path }}/22\u0026#34; Playbook中的流程控制 Playbook也算一种编程语言了，自然少不了流程控制。\n条件选择 when语句 # roles/httpd/tasks/main.yml - name: \u0026#34;shutdown Debian flavored systems\u0026#34; command: /sbin/shutdown -t now when: ansible_os_family == \u0026#34;Debian\u0026#34; 在roles 和 includes 上面应用’when’语句 根据条件决定是否执行一段任务列表：\n- include: tasks/sometasks.yml when: \u0026#34;\u0026#39;reticulating splines\u0026#39; in output\u0026#34; 根据条件决定是否执行一个role上的所有操作序列：\n- hosts: webservers roles: - { role: debian_stock_config, when: ansible_os_family == \u0026#39;Debian\u0026#39; } 基于变量选择文件和模版 怎样根据不同的系统选择不同的模板：\n- name: template a file template: src={{ item }} dest=/etc/myapp/foo.conf with_first_found: - files: - {{ ansible_distribution }}.conf - default.conf paths: - search_location/ 循环 ansible里循环的用法较多，最常用的是with_items，如下：\n- name: add several users user: name={{ item }} state=present groups=wheel with_items: - testuser1 - testuser2 其它高级循环用法参见这里\n其它技巧 YAML里的函数 ansible里批量删除文件，如果要删除的文件不存在，如果用file模块删除会报错，因此可以写一个工具yaml文件，相当于一个函数，然后使用include指令动态导入它，相当于调用函数。如下：\n# delete_files.yml --- - name: Check file exists stat: path: \u0026#34;{{ file_path }}\u0026#34; register: stat_result - name: Delete file file: path: \u0026#34;{{ file_path }}\u0026#34; state: absent when: stat_result.stat.exists # other.yml - include_tasks: util/delete_files.yml with_items: - \u0026#39;/var/log/sss\u0026#39; - \u0026#39;/tmp/xxx\u0026#39; loop_control: loop_var: file_path 类似的，一些重复的代码可以用这种方式简化。\n后面发现ansible2.0后添加了一个Blocks的功能，可以把多个指令当成一个块执行，这下一些简单的多指令操作可以直接用Blocks搞定了\n查看自动获取的变量 有时需要使用到从目标主机自动获取的变量，但又清楚变量名是什么，这时可以使用setup模块单独获取该主机的所有自动获取变量：\nansible -i hosts.yml 192.168.1.1 -m setup 拆分Playbook文件 如果部署的项目很复杂，这时Playbook文件会很大，这时可以用import_playbook按不同业务维度拆分Playbook文件，如下：\n- import_playbook: playbooks/buz1.yml - import_playbook: playbooks/buz2.yml 复用其它role 如果在一个role的task list里想复用另一个role，可以使用import_role，如下：\n# roles/httpd/tasks/main.yml ... - import_role: name: other_role ... 快速失败 有时执行某个指令，其结果不正确，这时可以使用fail进行快速失败，如下：\n# Example playbook using fail and when together - fail: msg: \u0026#34;The system may not be provisioned according to the CMDB status.\u0026#34; when: cmdb_status != \u0026#34;to-be-staged\u0026#34; 最佳实践 官方文档中的最佳实践\n参考  http://www.ansible.com.cn/docs/ https://www.the5fire.com/ansible-guide-cn.html https://github.com/ansible/ansible-examples https://galaxy.ansible.com/intro  ","permalink":"https://jeremyxu2010.github.io/2018/03/ansible%E7%AE%80%E6%98%93%E5%85%A5%E9%97%A8%E4%B9%8Bplaybook/","tags":["centos","ansible","deployment","automation"],"title":"ansible简易入门之playbook"},{"categories":["devops"],"contents":"工作中要使用ansible进行自动化部署，这两天花了点时间看了下ansible的文档，也稍稍体验了下，后面会用于项目实战，这里将实验过程中的一些经验记录下来方便后续查阅。\n什么是ansible ansible是个什么东西呢？官方的title是“Ansible is Simple IT Automation”——简单的自动化IT工具。这个工具的目标有这么几项：让我们自动化部署APP；自动化管理配置项；自动化的持续交付；自动化的（AWS）云服务管理。\n所有的这几个目标本质上来说都是在一个台或者几台服务器上，执行一系列的命令而已。——批量的在远程服务器上执行命令 。\nAnsible提供了一套简单的流程，你要按照它的流程来做，就能轻松完成任务。这就像是库和框架的关系一样。\nAnsible是基于 paramiko 开发的。这个paramiko是什么呢？它是一个纯Python实现的ssh协议库。因此fabric和ansible还有一个共同点就是不需要在远程主机上安装client/agents，因为它们是基于ssh来和远程主机通讯的。\n快速安装 我实验过程中管理主机的操作系统是macOS 10.13.3，托管主机的操作系统是CentOS 6.7，IP是10.211.55.10。\n管理主机上安装ansible\nbrew install ansible 托管主机上安装ansible\n# 启用epel源，并修改地址至sng源镜像地址 yum install -y http://mirror-sng.oa.com/epel/epel-release-latest-6.noarch.rpm sed -i 's/^mirrorlist/#mirrorlist/g' /etc/yum.repos.d/epel.repo sed -i 's/^#baseurl=http:\\/\\/download.fedoraproject.org\\/pub\\/epel/baseurl=http://mirror-sng.oa.com/epel/g' /etc/yum.repos.d/epel.repo yum install -y ansible 到管理主机执行命令简单测试一下\nmkdir ansible_test cd ansible_test # 创建hosts文件 echo ' [centos6.7] 10.211.55.10 ' \u0026gt; hosts # 创建ansible配置文件，指定hosts文件使用当前目录下的hosts文件 echo ' [defaults] inventory=./hosts ' \u0026gt; ansible.cfg # 使用ansible执行一条ad-hoc命令，按照指示输入托管主机的root密码即可 ansible all -m ping -u root -k 使用ansible 主机与组 Ansible 可同时操作属于一个组的多台主机,组和主机之间的关系通过 inventory 文件配置. 默认的文件路径为 /etc/ansible/hosts，也可在ansible.cfg里指定inventory。\n除默认文件外,你还可以同时使用多个 inventory 文件，也可以从动态源,或云上拉取 inventory 配置信息.详见 动态 Inventory.\n详见inventoryformat\nPatterns 在Ansible中,Patterns 是指我们怎样确定由哪一台主机来管理. 意思就是与哪台主机进行交互. 但是在:doc:playbooks 中它指的是对应主机应用特定的配置或执行特定进程.\nad-hoc命令里使用patterns:\nansible \u0026lt;pattern_goes_here\u0026gt; -m \u0026lt;module_name\u0026gt; -a \u0026lt;arguments\u0026gt; playbook里使用patterns:\n# example.yml里会指定不同role对应的patterns，参见https://github.com/ansible/ansible-examples/blob/master/tomcat-memcached-failover/site.yml ansible-playbook example.yml 设置公钥认证登录托管主机 每次执行命令时都要输入密码显然很难进行自动化部署，因此在实际使用一般会设置公钥认证。\n在管理主机上输入以下命令\nssh-keygen -f $HOME/.ssh/id_rsa -t rsa -N '' # 生成本地的ssh登录密钥 ssh-add $HOME/.ssh/id_rsa ssh-copy-id -i $HOME/.ssh/id_rsa root@10.211.55.10 # 输入托管主机的root密码 echo ' host_key_checking = False' \u0026gt;\u0026gt; ansible.cfg # 禁用对key信息的确认提示 后面就不用再输入托管主机的密码了，可直接远程执行命令了。\nansible all -m ping -u root ad-hoc 命令 ad hoc——临时的，在ansible中是指需要快速执行，并且不需要保存的命令。说白了就是执行简单的命令——一条命令。对于复杂的命令后面会说playbook。\nansible有许多模块,默认是 ‘command’,也就是命令模块,我们可以通过 -m 选项来指定不同的模块.\n执行命令 ansible all -m command -a '/bin/echo hello' -u root ansible all -m shell -a 'free -m' -u root command 模块不支持 shell 变量,也不支持管道等 shell 相关的东西.如果你想使用 shell相关的这些东西, 请使用’shell’ 模块.两个模块之前的差别请参考 模块相关 .\n文件传输 # 拷贝文件 ansible all -m copy -a \u0026quot;src=/etc/hosts dest=/tmp/hosts\u0026quot; # 修改文件的属主和权限 ansible all -m file -a \u0026quot;dest=/srv/foo/a.txt mode=600\u0026quot; ansible all -m file -a \u0026quot;dest=/srv/foo/b.txt mode=600 owner=mdehaan group=mdehaan\u0026quot; # 创建目录 ansible all -m file -a \u0026quot;dest=/path/to/c mode=755 owner=mdehaan group=mdehaan state=directory\u0026quot; # 删除目录(递归的删除)和删除文件 ansible all -m file -a \u0026quot;dest=/path/to/c state=absent\u0026quot; 管理软件包 # 确认一个软件包已经安装,但不去升级它 ansible webservers -m yum -a \u0026quot;name=acme state=present\u0026quot; # 确认一个软件包的安装版本 ansible webservers -m yum -a \u0026quot;name=acme-1.5 state=present\u0026quot; # 确认一个软件包还没有安装 ansible webservers -m yum -a \u0026quot;name=acme state=absent\u0026quot; 用户与用户组 使用 ‘user’ 模块可以方便的创建账户,删除账户,或是管理现有的账户\n# 创建账户 ansible all -m user -a \u0026quot;name=foo password=\u0026lt;crypted password here\u0026gt;\u0026quot; # 删除账户 ansible all -m user -a \u0026quot;name=foo state=absent\u0026quot; 管理服务 # 确认某个服务已经启动 ansible webservers -m service -a \u0026quot;name=httpd state=started\u0026quot; # 重启某个服务 ansible webservers -m service -a \u0026quot;name=httpd state=restarted\u0026quot; # 确认某个服务已经停止 ansible webservers -m service -a \u0026quot;name=httpd state=stopped\u0026quot; 限时后台任务 # 后台执行长时间任务，其中 -B 1800 表示最多运行30分钟, -P 60 表示每隔60秒获取一次状态信息. ansible all -B 1800 -P 60 -a \u0026quot;/usr/bin/long_running_operation --do-stuff\u0026quot; # 前面执行后台命令后会返回一个 job id, 将这个 id 传给 async_status 模块，可查询任务的执行状态 ansible web1.example.com -m async_status -a \u0026quot;jid=488359678239.2844\u0026quot; Playbooks Playbooks 简介 Playbooks 与 adhoc 相比,是一种完全不同的运用 ansible 的方式,是非常之强大的.\n简单来说,playbooks 是一种简单的配置管理系统与多机器部署系统的基础.与现有的其他系统有不同之处,且非常适合于复杂应用的部署.\nPlaybooks 可用于声明配置,更强大的地方在于,在 playbooks 中可以编排有序的执行过程,甚至于做到在多组机器间,来回有序的执行特别指定的步骤.并且可以同步或异步的发起任务.\n我们使用 adhoc 时,主要是使用 /usr/bin/ansible 程序执行任务.而使用 playbooks 时,更多是将之放入源码控制之中,用之推送你的配置或是用于确认你的远程系统的配置是否符合配置规范.\nplaybooks就是按“约定大于配置“的方式组织需要远程执行的命令，先放在下一篇详细说明了，详见Playbooks 介绍和Playbook 角色(Roles) 和 Include 语句。\n参考  http://www.ansible.com.cn/docs/ https://www.the5fire.com/ansible-guide-cn.html https://github.com/ansible/ansible-examples  ","permalink":"https://jeremyxu2010.github.io/2018/02/ansible%E7%AE%80%E6%98%93%E5%85%A5%E9%97%A8/","tags":["centos","ansible","deployment","automation"],"title":"ansible简易入门"},{"categories":["英文"],"contents":"发现欧路词典这款软件每天推送的一句英文挺好的，既文艺又鸡汤，但好像没找到导出的办法，这里将每天的每日一句记录下。\n You may be disappointed if you fail, but you are doomed if you don't try. 如果你失败了，你可能会失望，但如果你不去尝试，你就注定要失败。 Weep no more, no sigh, nor groan. Sorrow calls no time that's gone. 别哭泣，别叹息，别呻吟；悲伤唤不回流逝的时光。 Some birds are not meant to be caged; their feathers are just too bright. 有些鸟儿是注定不会被关在牢笼里，它们的每一片羽毛都闪耀着光辉。 I am a slow walker,but I never walk back. 既然选择了远方，便只顾风雨兼程。 Face the past with the least regrets, face the present with the least waste and face the future with the most dreams. 不悔恨过去；不荒废现在；充满梦想，面对未来。 Success is the sum of small efforts, repeated day in and day out. 成功就是日复一日那一点点小小努力的积累。 Smile and silence are two powerful tools. Smile is the way to solve many problems and silence is the way to avoid many problems. 微笑和沉默是两把利器:微笑解决很多问题，沉默避免许多问题。 The life's battle is never won by the fastest or the strongest, but by the man who thinks he can. 人生之战的胜利并不取决于最快或最强，而是你认为自己能不能。 When work is a pleasure, life is joy! When work is duty, life is slavery. 工作是一种乐趣时，生活是一种享受，工作是一种义务时，生活则是一种苦役。 A man is not old as long as he is seeking something. A man is not old until regrets take the place of dreams. 只要一个人还有追求，他就没有老。直到后悔取代了梦想，一个人才算老。  ","permalink":"https://jeremyxu2010.github.io/2018/02/%E8%8B%B1%E6%96%87%E6%AF%8F%E5%A4%A9%E9%87%91%E5%8F%A5/","tags":["english","study"],"title":"英文每天金句"},{"categories":["devops"],"contents":"项目中用到了keepalived及haproxy来实现服务的高可用，防止单点故障。以前其实也用过keepalived及nginx实现类似的功能，当时没有作记录，这里作一下记录以备忘。\nKeepalived keepalived是什么 keepalived是集群管理中保证集群高可用的一个服务软件，其功能类似于heartbeat，用来防止单点故障。\nkeepalived工作原理 keepalived是以VRRP协议为实现基础的，VRRP全称Virtual Router Redundancy Protocol，即虚拟路由冗余协议。\n虚拟路由冗余协议，可以认为是实现路由器高可用的协议，即将N台提供相同功能的路由器组成一个路由器组，这个组里面有一个master和多个backup，master上面有一个对外提供服务的vip（该路由器所在局域网内其他机器的默认路由为该vip），master会发组播，当backup收不到vrrp包时就认为master宕掉了，这时就需要根据VRRP的优先级来选举一个backup当master。这样的话就可以保证路由器的高可用了。\nkeepalived主要有三个模块，分别是core、check和vrrp。core模块为keepalived的核心，负责主进程的启动、维护以及全局配置文件的加载和解析。check负责健康检查，包括常见的各种检查方式。vrrp模块是来实现VRRP协议的。\nkeepalived的配置文件 keepalived只有一个配置文件keepalived.conf，里面主要包括以下几个配置区域，分别是global_defs、static_ipaddress、static_routes、vrrp_script、vrrp_instance和virtual_server。\nglobal_defs区域 主要是配置故障发生时的通知对象以及机器标识\nglobal_defs { notification_email { a@abc.com b@abc.com ... } notification_email_from alert@abc.com smtp_server smtp.abc.com smtp_connect_timeout 30 enable_traps router_id host163 }  notification_email 故障发生时给谁发邮件通知。 notification_email_from 通知邮件从哪个地址发出。 smpt_server 通知邮件的smtp地址。 smtp_connect_timeout 连接smtp服务器的超时时间。 enable_traps 开启SNMP陷阱（Simple Network Management Protocol）。 router_id 标识本节点的字条串，通常为hostname，但不一定非得是hostname。故障发生时，邮件通知会用到。  static_ipaddress和static_routes区域 static_ipaddress和static_routes区域配置的是是本节点的IP和路由信息。如果你的机器上已经配置了IP和路由，那么这两个区域可以不用配置。其实，一般情况下你的机器都会有IP地址和路由信息的，因此没必要再在这两个区域配置。\nstatic_ipaddress { 10.210.214.163/24 brd 10.210.214.255 dev eth0 ... } static_routes { 10.0.0.0/8 via 10.210.214.1 dev eth0 ... } 以上分别表示启动/关闭keepalived时在本机执行的如下命令：\n# /sbin/ip addr add 10.210.214.163/24 brd 10.210.214.255 dev eth0 # /sbin/ip route add 10.0.0.0/8 via 10.210.214.1 dev eth0 # /sbin/ip addr del 10.210.214.163/24 brd 10.210.214.255 dev eth0 # /sbin/ip route del 10.0.0.0/8 via 10.210.214.1 dev eth0 注意： 请忽略这两个区域，因为我坚信你的机器肯定已经配置了IP和路由。\nvrrp_script区域 用来做健康检查的，当时检查失败时会将vrrp_instance的priority减少相应的值。\nvrrp_script chk_http_port { script \u0026quot;\u0026lt;/dev/tcp/127.0.0.1/80\u0026quot; interval 1 weight -10 } 以上意思是如果script中的指令执行失败，那么相应的vrrp_instance的优先级会减少10个点。\nvrrp_instance和vrrp_sync_group区域 vrrp_instance用来定义对外提供服务的VIP区域及其相关属性。\nvrrp_rsync_group用来定义vrrp_intance组，使得这个组内成员动作一致。举个例子来说明一下其功能：\n两个vrrp_instance同属于一个vrrp_rsync_group，那么其中一个vrrp_instance发生故障切换时，另一个vrrp_instance也会跟着切换（即使这个instance没有发生故障）。\nvrrp_sync_group VG_1 { group { inside_network # name of vrrp_instance (below) outside_network # One for each moveable IP. ... } notify_master /path/to_master.sh notify_backup /path/to_backup.sh notify_fault \u0026quot;/path/fault.sh VG_1\u0026quot; notify /path/notify.sh smtp_alert } vrrp_instance VI_1 { state MASTER interface eth0 use_vmac \u0026lt;VMAC_INTERFACE\u0026gt; dont_track_primary track_interface { eth0 eth1 } mcast_src_ip \u0026lt;IPADDR\u0026gt; lvs_sync_daemon_interface eth1 garp_master_delay 10 virtual_router_id 1 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 12345678 } virtual_ipaddress { 10.210.214.253/24 brd 10.210.214.255 dev eth0 192.168.1.11/24 brd 192.168.1.255 dev eth1 } virtual_routes { 172.16.0.0/12 via 10.210.214.1 192.168.1.0/24 via 192.168.1.1 dev eth1 default via 202.102.152.1 } track_script { chk_http_port } nopreempt preempt_delay 300 debug notify_master \u0026lt;STRING\u0026gt;|\u0026lt;QUOTED-STRING\u0026gt; notify_backup \u0026lt;STRING\u0026gt;|\u0026lt;QUOTED-STRING\u0026gt; notify_fault \u0026lt;STRING\u0026gt;|\u0026lt;QUOTED-STRING\u0026gt; notify \u0026lt;STRING\u0026gt;|\u0026lt;QUOTED-STRING\u0026gt; smtp_alert }  notify_master/backup/fault 分别表示切换为主/备/出错时所执行的脚本。 notify 表示任何一状态切换时都会调用该脚本，并且该脚本在以上三个脚本执行完成之后进行调用，keepalived会自动传递三个参数（$1 = \u0026ldquo;GROUP\u0026quot;|\u0026quot;INSTANCE\u0026rdquo;，$2 = name of group or instance，$3 = target state of transition(MASTER/BACKUP/FAULT)）。 smtp_alert 表示是否开启邮件通知（用全局区域的邮件设置来发通知）。 state 可以是MASTER或BACKUP，不过当其他节点keepalived启动时会将priority比较大的节点选举为MASTER，因此该项其实没有实质用途。 interface 节点固有IP（非VIP）的网卡，用来发VRRP包。 use_vmac 是否使用VRRP的虚拟MAC地址。 dont_track_primary 忽略VRRP网卡错误。（默认未设置） track_interface 监控以下网卡，如果任何一个不通就会切换到FALT状态。（可选项） mcast_src_ip 修改vrrp组播包的源地址，默认源地址为master的IP。（由于是组播，因此即使修改了源地址，该master还是能收到回应的） lvs_sync_daemon_interface 绑定lvs syncd的网卡。 garp_master_delay 当切为主状态后多久更新ARP缓存，默认5秒。 virtual_router_id 取值在0-255之间，用来区分多个instance的VRRP组播。  注意： 同一网段中virtual_router_id的值不能重复，否则会出错，相关错误信息如下。\nKeepalived_vrrp[27120]: ip address associated with VRID not present in received packet : one or more VIP associated with VRID mismatch actual MASTER advert bogus VRRP packet received on eth1 !!! receive an invalid ip number count associated with VRID! VRRP_Instance(xxx) ignoring received advertisment... 可以用这条命令来查看该网络中所存在的vrid：tcpdump -nn -i any net 224.0.0.0/8\n priority 用来选举master的，要成为master，那么这个选项的值最好高于其他机器50个点，该项取值范围是1-255（在此范围之外会被识别成默认值100）。 advert_int 发VRRP包的时间间隔，即多久进行一次master选举（可以认为是健康查检时间间隔）。 authentication 认证区域，认证类型有PASS和HA（IPSEC），推荐使用PASS（密码只识别前8位）。 virtual_ipaddress vip，不解释了。 virtual_routes 虚拟路由，当IP漂过来之后需要添加的路由信息。 virtual_ipaddress_excluded 发送的VRRP包里不包含的IP地址，为减少回应VRRP包的个数。在网卡上绑定的IP地址比较多的时候用。 nopreempt 允许一个priority比较低的节点作为master，即使有priority更高的节点启动。  首先nopreemt必须在state为BACKUP的节点上才生效（因为是BACKUP节点决定是否来成为MASTER的），其次要实现类似于关闭auto failback的功能需要将所有节点的state都设置为BACKUP，或者将master节点的priority设置的比BACKUP低。我个人推荐使用将所有节点的state都设置成BACKUP并且都加上nopreempt选项，这样就完成了关于autofailback功能，当想手动将某节点切换为MASTER时只需去掉该节点的nopreempt选项并且将priority改的比其他节点大，然后重新加载配置文件即可（等MASTER切过来之后再将配置文件改回去再reload一下）。\n当使用track_script时可以不用加nopreempt，只需要加上preempt_delay 5，这里的间隔时间要大于vrrp_script中定义的时长。\n preempt_delay master启动多久之后进行接管资源（VIP/Route信息等），并提是没有nopreempt选项。  上述的文字来自于这里，一般我们使用keepalived就是用上述的功能来监测服务，主从切换时自动地将vip进行漂移。\n其实keepalived也可以用来作负载均衡，如下。\nvirtual_server_group和virtual_server区域 virtual_server_group一般在超大型的LVS中用到，一般LVS用不过这东西，因此不多说。\nvirtual_server IP Port { delay_loop \u0026lt;INT\u0026gt; lb_algo rr|wrr|lc|wlc|lblc|sh|dh lb_kind NAT|DR|TUN persistence_timeout \u0026lt;INT\u0026gt; persistence_granularity \u0026lt;NETMASK\u0026gt; protocol TCP ha_suspend virtualhost \u0026lt;STRING\u0026gt; alpha omega quorum \u0026lt;INT\u0026gt; hysteresis \u0026lt;INT\u0026gt; quorum_up \u0026lt;STRING\u0026gt;|\u0026lt;QUOTED-STRING\u0026gt; quorum_down \u0026lt;STRING\u0026gt;|\u0026lt;QUOTED-STRING\u0026gt; sorry_server \u0026lt;IPADDR\u0026gt; \u0026lt;PORT\u0026gt; real_server \u0026lt;IPADDR\u0026gt; \u0026lt;PORT\u0026gt; { weight \u0026lt;INT\u0026gt; inhibit_on_failure notify_up \u0026lt;STRING\u0026gt;|\u0026lt;QUOTED-STRING\u0026gt; notify_down \u0026lt;STRING\u0026gt;|\u0026lt;QUOTED-STRING\u0026gt; # HTTP_GET|SSL_GET|TCP_CHECK|SMTP_CHECK|MISC_CHECK HTTP_GET|SSL_GET { url { path \u0026lt;STRING\u0026gt; # Digest computed with genhash digest \u0026lt;STRING\u0026gt; status_code \u0026lt;INT\u0026gt; } connect_port \u0026lt;PORT\u0026gt; connect_timeout \u0026lt;INT\u0026gt; nb_get_retry \u0026lt;INT\u0026gt; delay_before_retry \u0026lt;INT\u0026gt; } } }  delay_loop 延迟轮询时间（单位秒）。 lb_algo 后端调试算法（load balancing algorithm）。 lb_kind LVS调度类型NAT/DR/TUN。 virtualhost 用来给HTTP_GET和SSL_GET配置请求header的。 sorry_server 当所有real server宕掉时，sorry server顶替。 real_server 真正提供服务的服务器。 weight 权重。 notify_up/down 当real server宕掉或启动时执行的脚本。 健康检查的方式，N多种方式。 path 请求real serserver上的路径。 digest/status_code 分别表示用genhash算出的结果和http状态码。 connect_port 健康检查，如果端口通则认为服务器正常。 connect_timeout,nb_get_retry,delay_before_retry分别表示超时时长、重试次数，下次重试的时间延迟。  其他选项暂时不作说明。\nkeepalived主从切换 主从切换比较让人蛋疼，需要将backup配置文件的priority选项的值调整的比master高50个点，然后reload配置文件就可以切换了。当时你也可以将master的keepalived停止，这样也可以进行主从切换。\nHAProxy HAProxy是什么 HAProxy是一个免费的负载均衡软件，可以运行于大部分主流的Linux操作系统上。\nHAProxy提供了L4(TCP)和L7(HTTP)两种负载均衡能力，具备丰富的功能。HAProxy的社区非常活跃，版本更新快速（最新稳定版1.7.2于2017/01/13推出）。最关键的是，HAProxy具备媲美商用负载均衡器的性能和稳定性。\n因为HAProxy的上述优点，它当前不仅仅是免费负载均衡软件的首选，更几乎成为了唯一选择。\nHAProxy的核心能力和关键特性 HAProxy的核心功能  负载均衡：L4和L7两种模式，支持RR/静态RR/LC/IP Hash/URI Hash/URL_PARAM Hash/HTTP_HEADER Hash等丰富的负载均衡算法 健康检查：支持TCP和HTTP两种健康检查模式 会话保持：对于未实现会话共享的应用集群，可通过Insert Cookie/Rewrite Cookie/Prefix Cookie，以及上述的多种Hash方式实现会话保持 SSL：HAProxy可以解析HTTPS协议，并能够将请求解密为HTTP后向后端传输 HTTP请求重写与重定向 监控与统计：HAProxy提供了基于Web的统计信息页面，展现健康状态和流量数据。基于此功能，使用者可以开发监控程序来监控HAProxy的状态  从这个核心功能来看，haproxy实现的功能类似于nginx的L4、L7反向代理。\nHAProxy的关键特性 性能  采用单线程、事件驱动、非阻塞模型，减少上下文切换的消耗，能在1ms内处理数百个请求。并且每个会话只占用数KB的内存。 大量精细的性能优化，如O(1)复杂度的事件检查器、延迟更新技术、Single-buffereing、Zero-copy forwarding等等，这些技术使得HAProxy在中等负载下只占用极低的CPU资源。 HAProxy大量利用操作系统本身的功能特性，使得其在处理请求时能发挥极高的性能，通常情况下，HAProxy自身只占用15%的处理时间，剩余的85%都是在系统内核层完成的。 HAProxy作者在8年前（2009）年使用1.4版本进行了一次测试，单个HAProxy进程的处理能力突破了10万请求/秒，并轻松占满了10Gbps的网络带宽。  稳定性 作为建议以单进程模式运行的程序，HAProxy对稳定性的要求是十分严苛的。按照作者的说法，HAProxy在13年间从未出现过一个会导致其崩溃的BUG，HAProxy一旦成功启动，除非操作系统或硬件故障，否则就不会崩溃（我觉得可能多少还是有夸大的成分）。\n在上文中提到过，HAProxy的大部分工作都是在操作系统内核完成的，所以HAProxy的稳定性主要依赖于操作系统，作者建议使用2.6或3.x的Linux内核，对sysctls参数进行精细的优化，并且确保主机有足够的内存。这样HAProxy就能够持续满负载稳定运行数年之久。\nHAProxy关键配置详解 总览 HAProxy的配置文件共有5个域\n global：用于配置全局参数 default：用于配置所有frontend和backend的默认属性 frontend：用于配置前端服务（即HAProxy自身提供的服务）实例 backend：用于配置后端服务（即HAProxy后面接的服务）实例组 listen：frontend+backend的组合配置，可以理解成更简洁的配置方法  global域的关键配置  daemon：指定HAProxy以后台模式运行，通常情况下都应该使用这一配置 user [username] ：指定HAProxy进程所属的用户 group [groupname] ：指定HAProxy进程所属的用户组 log [address][device][maxlevel][minlevel]：日志输出配置，如log 127.0.0.1 local0 info warning，即向本机rsyslog或syslog的local0输出info到warning级别的日志。其中[minlevel]可以省略。HAProxy的日志共有8个级别，从高到低为emerg/alert/crit/err/warning/notice/info/debug pidfile ：指定记录HAProxy进程号的文件绝对路径。主要用于HAProxy进程的停止和重启动作。 maxconn ：HAProxy进程同时处理的连接数，当连接数达到这一数值时，HAProxy将停止接收连接请求  frontend域的关键配置  acl [name][criterion] [flags][operator] [value]：定义一条ACL，ACL是根据数据包的指定属性以指定表达式计算出的true/false值。如\u0026quot;acl url_ms1 path_beg -i /ms1/\u0026ldquo;定义了名为url_ms1的ACL，该ACL在请求uri以/ms1/开头（忽略大小写）时为true bind [ip]:[port]：frontend服务监听的端口 default_backend [name]：frontend对应的默认backend disabled：禁用此frontend http-request [operation][condition]：对所有到达此frontend的HTTP请求应用的策略，例如可以拒绝、要求认证、添加header、替换header、定义ACL等等。 http-response [operation][condition]：对所有从此frontend返回的HTTP响应应用的策略，大体同上 log：同global域的log配置，仅应用于此frontend。如果要沿用global域的log配置，则此处配置为log global maxconn：同global域的maxconn，仅应用于此frontend mode：此frontend的工作模式，主要有http和tcp两种，对应L7和L4两种负载均衡模式 option forwardfor：在请求中添加X-Forwarded-For Header，记录客户端ip option http-keep-alive：以KeepAlive模式提供服务 option httpclose：与http-keep-alive对应，关闭KeepAlive模式，如果HAProxy主要提供的是接口类型的服务，可以考虑采用httpclose模式，以节省连接数资源。但如果这样做了，接口的调用端将不能使用HTTP连接池 option httplog：开启httplog，HAProxy将会以类似Apache HTTP或Nginx的格式来记录请求日志 option tcplog：开启tcplog，HAProxy将会在日志中记录数据包在传输层的更多属性 stats uri [uri]：在此frontend上开启监控页面，通过[uri]访问 stats refresh [time]：监控数据刷新周期 stats auth [user]:[password]：监控页面的认证用户名密码 timeout client [time]：指连接创建后，客户端持续不发送数据的超时时间 timeout http-request [time]：指连接创建后，客户端没能发送完整HTTP请求的超时时间，主要用于防止DoS类攻击，即创建连接后，以非常缓慢的速度发送请求包，导致HAProxy连接被长时间占用 use_backend [backend] if|unless [acl]：与ACL搭配使用，在满足/不满足ACL时转发至指定的backend  backend域的关键配置  acl：同frontend域 balance [algorithm]：在此backend下所有server间的负载均衡算法，常用的有roundrobin和source，完整的算法说明见官方文档configuration.html#4.2-balance cookie：在backend server间启用基于cookie的会话保持策略，最常用的是insert方式，如cookie HA_STICKY_ms1 insert indirect nocache，指HAProxy将在响应中插入名为HA_STICKY_ms1的cookie，其值为对应的server定义中指定的值，并根据请求中此cookie的值决定转发至哪个server。indirect代表如果请求中已经带有合法的HA_STICK_ms1 cookie，则HAProxy不会在响应中再次插入此cookie，nocache则代表禁止链路上的所有网关和缓存服务器缓存带有Set-Cookie头的响应。 default-server：用于指定此backend下所有server的默认设置。具体见下面的server配置。 disabled：禁用此backend http-request/http-response：同frontend域 log：同frontend域 mode：同frontend域 option forwardfor：同frontend域 option http-keep-alive：同frontend域 option httpclose：同frontend域 option httpchk [METHOD][URL] [VERSION]：定义以http方式进行的健康检查策略。如option httpchk GET /healthCheck.html HTTP/1.1 option httplog：同frontend域 option tcplog：同frontend域 server [name][ip]:[port][params]：定义backend中的一个后端server，[params]用于指定这个server的参数，常用的包括有：   check：指定此参数时，HAProxy将会对此server执行健康检查，检查方法在option httpchk中配置。同时还可以在check后指定inter, rise, fall三个参数，分别代表健康检查的周期、连续几次成功认为server UP，连续几次失败认为server DOWN，默认值是inter 2000ms rise 2 fall 3 cookie [value]：用于配合基于cookie的会话保持，如cookie ms1.srv1代表交由此server处理的请求会在响应中写入值为ms1.srv1的cookie（具体的cookie名则在backend域中的cookie设置中指定） maxconn：指HAProxy最多同时向此server发起的连接数，当连接数到达maxconn后，向此server发起的新连接会进入等待队列。默认为0，即无限 maxqueue：等待队列的长度，当队列已满后，后续请求将会发至此backend下的其他server，默认为0，即无限 weight：server的权重，0-256，权重越大，分给这个server的请求就越多。weight为0的server将不会被分配任何新的连接。所有server默认weight为1\n  timeout connect [time]：指HAProxy尝试与backend server创建连接的超时时间 timeout check [time]：默认情况下，健康检查的连接+响应超时时间为server命令中指定的inter值，如果配置了timeout check，HAProxy会以inter作为健康检查请求的连接超时时间，并以timeout check的值作为健康检查请求的响应超时时间 timeout server [time]：指backend server响应HAProxy请求的超时时间  default域 上文所属的frontend和backend域关键配置中，除acl、bind、http-request、http-response、use_backend外，其余的均可以配置在default域中。default域中配置了的项目，如果在frontend或backend域中没有配置，将会使用default域中的配置。\nlisten域 listen域是frontend域和backend域的组合，frontend域和backend域中所有的配置都可以配置在listen域下\n官方配置文档 HAProxy的配置项非常多，支持非常丰富的功能，上文只列出了作为L7负载均衡器使用HAProxy时的一些关键参数。完整的参数说明请参见官方文档 configuration.html\n使用实例 使用HAProxy搭建L7负载均衡器 总体方案 本节中，我们将使用HAProxy搭建一个L7负载均衡器，应用如下功能\n  负载均衡\n  会话保持\n  健康检查\n  根据URI前缀向不同的后端集群转发\n  监控页面\n  HAProxy配置文件 global daemon maxconn 30000 #ulimit -n至少为60018 user ha pidfile /home/ha/haproxy/conf/haproxy.pid log 127.0.0.1 local0 info log 127.0.0.1 local1 warning defaults mode http log global option http-keep-alive #使用keepAlive连接 option forwardfor #记录客户端IP在X-Forwarded-For头域中 option httplog #开启httplog，HAProxy会记录更丰富的请求信息 timeout connect 5000ms timeout client 10000ms timeout server 50000ms timeout http-request 20000ms #从连接创建开始到从客户端读取完整HTTP请求的超时时间，用于避免类DoS攻击 option httpchk GET /healthCheck.html #定义默认的健康检查策略 frontend http-in bind *:9001 maxconn 30000 #定义此端口上的maxconn acl url_ms1 path_beg -i /ms1/ #定义ACL，当uri以/ms1/开头时，ACL[url_ms1]为true acl url_ms2 path_beg -i /ms2/ #同上，url_ms2 use_backend ms1 if url_ms1 #当[url_ms1]为true时，定向到后端服务群ms1中 use_backend ms2 if url_ms2 #当[url_ms2]为true时，定向到后端服务群ms2中 default_backend default_servers #其他情况时，定向到后端服务群default_servers中 backend ms1 #定义后端服务群ms1 balance roundrobin #使用RR负载均衡算法 cookie HA_STICKY_ms1 insert indirect nocache #会话保持策略，insert名为\u0026quot;HA_STICKY_ms1\u0026quot;的cookie #定义后端server[ms1.srv1]，请求定向到该server时会在响应中写入cookie值[ms1.srv1] #针对此server的maxconn设置为300 #应用默认健康检查策略，健康检查间隔和超时时间为2000ms，两次成功视为节点UP，三次失败视为节点DOWN server ms1.srv1 192.168.8.111:8080 cookie ms1.srv1 maxconn 300 check inter 2000ms rise 2 fall 3 #同上，inter 2000ms rise 2 fall 3是默认值，可以省略 server ms1.srv2 192.168.8.112:8080 cookie ms1.srv2 maxconn 300 check backend ms2 #定义后端服务群ms2 balance roundrobin cookie HA_STICKY_ms2 insert indirect nocache server ms2.srv1 192.168.8.111:8081 cookie ms2.srv1 maxconn 300 check server ms2.srv2 192.168.8.112:8081 cookie ms2.srv2 maxconn 300 check backend default_servers #定义后端服务群default_servers balance roundrobin cookie HA_STICKY_def insert indirect nocache server def.srv1 192.168.8.111:8082 cookie def.srv1 maxconn 300 check server def.srv2 192.168.8.112:8082 cookie def.srv2 maxconn 300 check listen stats #定义监控页面 bind *:1080 #绑定端口1080 stats refresh 30s #每30秒更新监控数据 stats uri /stats #访问监控页面的uri stats realm HAProxy\\ Stats #监控页面的认证提示 stats auth admin:admin #监控页面的用户名和密码 使用HAProxy搭建L4负载均衡器 HAProxy作为L4负载均衡器工作时，不会去解析任何与HTTP协议相关的内容，只在传输层对数据包进行处理。也就是说，以L4模式运行的HAProxy，无法实现根据URL向不同后端转发、通过cookie实现会话保持等功能。\n同时，在L4模式下工作的HAProxy也无法提供监控页面。\n但作为L4负载均衡器的HAProxy能够提供更高的性能，适合于基于套接字的服务（如数据库、消息队列、RPC、邮件服务、Redis等），或不需要逻辑规则判断，并已实现了会话共享的HTTP服务。\nHAProxy配置文件 global daemon maxconn 30000 #ulimit -n至少为60018 user ha pidfile /home/ha/haproxy/conf/haproxy.pid log 127.0.0.1 local0 info log 127.0.0.1 local1 warning defaults mode tcp log global option tcplog #开启tcplog timeout connect 5000ms timeout client 10000ms timeout server 10000ms #TCP模式下，应将timeout client和timeout server设置为一样的值，以防止出现问题 option httpchk GET /healthCheck.html #定义默认的健康检查策略 frontend http-in bind *:9002 maxconn 30000 #定义此端口上的maxconn default_backend default_servers #请求定向至后端服务群default_servers backend default_servers #定义后端服务群default_servers balance source #基于客户端IP的会话保持 server def.srv1 192.168.8.111:8082 maxconn 300 check server def.srv2 192.168.8.112:8082 maxconn 300 check 使用Keepalived实现HAProxy高可用 尽管HAProxy非常稳定，但仍然无法规避操作系统故障、主机硬件故障、网络故障甚至断电带来的风险。所以必须对HAProxy实施高可用方案。\n下文将介绍利用Keepalived实现的HAProxy热备方案。即两台主机上的两个HAProxy实例同时在线，其中权重较高的实例为MASTER，MASTER出现问题时，另一台实例自动接管所有流量。\n原理 在两台HAProxy的主机上分别运行着一个Keepalived实例，这两个Keepalived争抢同一个虚IP地址，两个HAProxy也尝试去绑定这同一个虚IP地址上的端口。 显然，同时只能有一个Keepalived抢到这个虚IP，抢到了这个虚IP的Keepalived主机上的HAProxy便是当前的MASTER。 Keepalived内部维护一个权重值，权重值最高的Keepalived实例能够抢到虚IP。同时Keepalived会定期check本主机上的HAProxy状态，状态OK时权重值增加。\nkeepalived配置文件 global_defs { router_id LVS_DEVEL #虚拟路由名称 } #HAProxy健康检查配置 vrrp_script chk_haproxy { script \u0026quot;killall -0 haproxy\u0026quot; #使用killall -0检查haproxy实例是否存在，性能高于ps命令 interval 2 #脚本运行周期 weight 2 #每次检查的加权权重值 } #虚拟路由配置 vrrp_instance VI_1 { state MASTER #本机实例状态，MASTER/BACKUP，备机配置文件中请写BACKUP interface enp0s25 #本机网卡名称，使用ifconfig命令查看 virtual_router_id 51 #虚拟路由编号，主备机保持一致 priority 101 #本机初始权重，备机请填写小于主机的值（例如100） advert_int 1 #争抢虚地址的周期，秒 virtual_ipaddress { 192.168.8.201 #虚地址IP，主备机保持一致 } track_script { chk_haproxy #对应的健康检查配置 } } 参考  https://github.com/chenzhiwei/linux/tree/master/keepalived https://www.jianshu.com/p/c9f6d55288c0 http://seanlook.com/2015/05/18/nginx-keepalived-ha/  ","permalink":"https://jeremyxu2010.github.io/2018/02/%E9%AB%98%E5%8F%AF%E7%94%A8%E4%B9%8Bkeepalivedhaproxy/","tags":["高可用","keepalived","haproxy","nginx","lvs"],"title":"高可用之keepalived\u0026haproxy"},{"categories":["devops"],"contents":"背景 项目中使用的mariadb+gelera集群模式部署，之前一直用的是mysql的master/slave方式部署数据库的，这种集群模式以前没怎么搞过，这里研究并记录一下。\nMariaDB Galera Cluster 介绍 MariaDB 集群是 MariaDB 同步多主机集群。它仅支持 XtraDB/ InnoDB 存储引擎（虽然有对 MyISAM 实验支持 - 看 wsrep_replicate_myisam 系统变量）。\n主要功能:\n 同步复制 真正的 multi-master，即所有节点可以同时读写数据库 自动的节点成员控制，失效节点自动被清除 新节点加入数据自动复制 真正的并行复制，行级 用户可以直接连接集群，使用感受上与MySQL完全一致  优势:\n 因为是多主，所以不存在Slavelag(延迟) 不存在丢失事务的情况 同时具有读和写的扩展能力 更小的客户端延迟 节点间数据是同步的,而 Master/Slave 模式是异步的,不同 slave 上的 binlog 可能是不同的  技术:\nGalera 集群的复制功能基于 Galeralibrary 实现,为了让 MySQL 与 Galera library 通讯，特别针对 MySQL 开发了 wsrep API。\nGalera 插件保证集群同步数据，保持数据的一致性，靠的就是可认证的复制，工作原理如下图：\n当客户端发出一个 commit 的指令，在事务被提交之前，所有对数据库的更改都会被write-set收集起来,并且将 write-set 纪录的内容发送给其他节点。\nwrite-set 将在每个节点进行认证测试，测试结果决定着节点是否应用write-set更改数据。\n如果认证测试失败，节点将丢弃 write-set ；如果认证测试成功，则事务提交。\nMariaDB Galera Cluster搭建 我这里实验时使用的操作系统是CentOS7，使用了3台虚拟机，IP分别为10.211.55.6、10.211.55.7、10.211.55.8\n关闭防火墙及selinux 为了先把MariaDB Galera Cluster部署起来，不受防火墙、selinux的干扰，先把3台虚拟机上这俩关闭了。如果防火墙一定要打开，可参考这里设置防火墙规则。\nsystemctl disable firewalld.service systemctl stop firewalld.service setenforce 0 sed -i \u0026#39;s/^SELINUX=.*$/SELINUX=disabled/\u0026#39; /etc/selinux/config 添加mariadb的yum源 在3台虚拟机上执行以下命令\n# 已使用国内yum镜像，原镜像地址是http://yum.mariadb.org echo \u0026#39; [mariadb] name = MariaDB baseurl = http://mirrors.ustc.edu.cn/mariadb/yum/10.1/centos7-amd64 gpgkey=http://mirrors.ustc.edu.cn/mariadb/yum/RPM-GPG-KEY-MariaDB gpgcheck=1\u0026#39; \u0026gt; /etc/yum.repos.d/MariaDB.repo 安装软件包 在3台虚拟机上执行以下命令\nyum install -y mariadb mariadb-server mariadb-common galera rsync 数据库初始化 在10.211.55.6上执行以下命令\nsystemctl start mariadb mysql_secure_installation # 注意这一步是有交互的，需要回答一些问题，做一些设置 systemctl stop mariadb 修改galera相关配置 在3台虚拟机上均打开/etc/my.cnf.d/server.cnf进行编辑，修改片断如下：\n... [galera] wsrep_on=ON wsrep_provider=/usr/lib64/galera/libgalera_smm.so wsrep_cluster_name=galera_cluster wsrep_cluster_address=\u0026quot;gcomm://10.211.55.6,10.211.55.7,10.211.55.8\u0026quot; wsrep_node_name=10.211.55.6 # 注意这里改成本机IP wsrep_node_address=10.211.55.6 # 注意这里改成本机IP binlog_format=row default_storage_engine=InnoDB innodb_autoinc_lock_mode=2 ... 启动MariaDB Galera Cluster服务 先在第1台虚拟机执行以下命令：\nsudo -u mysql /usr/sbin/mysqld --wsrep-new-cluster \u0026amp;\u0026gt; /tmp/wsrep_new_cluster.log \u0026amp; disown $! tail -f /tmp/wsrep_new_cluster.log 出现 ready for connections ,证明启动成功，继续在另外两个虚拟机里执行命令：\nsystemctl start mariadb 等后面两个虚拟机里mariadb服务启动后，再到第1台虚拟机里执行以下命令：\n(ps -ef|grep mysqld|grep -v grep|awk '{print $2}'|xargs kill -9) \u0026amp;\u0026gt;/dev/null systemctl start mariadb 验证MariaDB Galera Cluster服务 在任意虚拟机里执行以下命令：\nmysql -e \u0026#34;show status like \u0026#39;wsrep_cluster_size\u0026#39;\u0026#34; # 这里应该显示集群里有3个节点 mysql -e \u0026#34;show status like \u0026#39;wsrep_connected\u0026#39;\u0026#34; # 这里应该显示ON mysql -e \u0026#34;show status like \u0026#39;wsrep_incoming_addresses\u0026#39;\u0026#34; # 这里应该显示10.211.55.7:3306,10.211.55.8:3306,10.211.55.6:3306 mysql -e \u0026#34;show status like \u0026#39;wsrep_local_state_comment\u0026#39;\u0026#34; # 这里节点的同步状态 查看集群全部相关状态参数可执行以下命令：\nmysql -e \u0026#34;show status like \u0026#39;wsrep_%\u0026#39;\u0026#34; 至此，MariaDB Galera Cluster已经成功部署。\nMariaDB Galera Cluster的自启动 在实际使用中发现一个问题，Galera集群启动时必须按照一个特定的规则启动，研究了下，发现规则如下：\n  如果集群从来没有启动过（3个节点上都没有/var/lib/mysql/grastate.dat文件），则必要由其中一个节点以--wsrep-new-cluster参数启动，另外两个节点正常启动即可\n  如果集群以前启动过，则参考/var/lib/mysql/grastate.dat，找到safe_to_bootstrap为1的节点，在该节点上以--wsrep-new-cluster参数启动，另外两个节点正常启动即可\n  如果集群以前启动过，但参考/var/lib/mysql/grastate.dat，找不到safe_to_bootstrap为1的节点（一般是因为mariadb服务非正常停止造成），则在3个节点中随便找1个节点，将/var/lib/mysql/grastate.dat中的safe_to_bootstrap修改为1，再在该节点上以--wsrep-new-cluster参数启动，另外两个节点正常启动即可\n  从以上3种场景可知，正常情况下很难保证mariadb galera cluster可以无人值守地完成开机自启动。国外论坛上也有人反映了这个问题，但好像官方的人员好像说设计上就是这样，怎么可以这样。。。\n最后写了个脚本，放在3个虚拟机上面，解决了这个问题。脚本如下：\ncat /usr/local/bin/mariadb_cluster_helper.sh #!/bin/bash GRASTATE_FILE=/var/lib/mysql/grastate.dat WSREP_NEW_CLUSTER_LOG_FILE=/tmp/wsrep_new_cluster.log # 如果启动mariadb超过10秒还没返回0，则认为失败了 START_MARIADB_TIMEOUT=10 # 以--wsrep-new-cluster参数启动，超过5次检查，发现仍没有其它节点加入集群，则认为此路不通 SPECIAL_START_WAIT_MAX_COUNT=5 # 得到本机IP MY_IP=$(grep \u0026#39;wsrep_node_address\u0026#39; /etc/my.cnf.d/server.cnf | awk -F \u0026#39;=\u0026#39; \u0026#39;{print $2}\u0026#39;) # 杀掉mysqld进程 function kill_mysqld_process() { (ps -ef|grep mysqld|grep -v grep|awk \u0026#39;{print $2}\u0026#39;|xargs kill -9) \u0026amp;\u0026gt;/dev/null } # 正常启动mariadb function start_mariadb_normal(){ # 首先确保safe_to_bootstrap标记为0 sed -i \u0026#39;s/^safe_to_bootstrap.*$/safe_to_bootstrap: 0/\u0026#39; $GRASTATE_FILE timeout $START_MARIADB_TIMEOUT systemctl start mariadb \u0026amp;\u0026gt; /dev/null return $? } # 以--wsrep-new-cluster参数启动mariadb function start_mariadb_special(){ # 首先确保safe_to_bootstrap标记为1 sed -i \u0026#39;s/^safe_to_bootstrap.*$/safe_to_bootstrap: 1/\u0026#39; $GRASTATE_FILE # 以--wsrep-new-cluster参数启动mariadb /usr/sbin/mysqld --user=mysql --wsrep-new-cluster \u0026amp;\u0026gt; $WSREP_NEW_CLUSTER_LOG_FILE \u0026amp; disown $! try_count=0 # 循环检查 while [ 1 ]; do # 如果超过SPECIAL_START_WAIT_MAX_COUNT次检查，仍没有其它节点加入集群，则认为此路不通，尝试正常启动，跳出循环 if [ $try_count -gt $SPECIAL_START_WAIT_MAX_COUNT ] ; then kill_mysqld_process start_mariadb_normal return $? fi new_joined_count=$(grep \u0026#39;synced with group\u0026#39; /tmp/wsrep_new_cluster.log | grep -v $MY_IP|wc -l) exception_count=$(grep \u0026#39;exception from gcomm, backend must be restarted\u0026#39; $WSREP_NEW_CLUSTER_LOG_FILE | wc -l) # 如果新加入的节点数大于0，则认为集群就绪了，可正常启动了，跳出循环 # 如果运行日志中发现了异常(两个节点都以--wsrep-new-cluster参数启动，其中一个会报错)，则认为此路不通，尝试正常启动，跳出循环 if [ $new_joined_count -gt 0 ] || [ $exception_count -gt 0 ] ; then kill_mysqld_process start_mariadb_normal return $? else try_count=$(( $try_count + 1 )) fi sleep 5 done } # 首先杀掉mysqld进程 kill_mysqld_process ret=-1 # 如果safe_to_bootstrap标记为1，则立即以--wsrep-new-cluster参数启动 if [ -f $GRASTATE_FILE ]; then safe_bootstrap_flag=$(grep \u0026#39;safe_to_bootstrap\u0026#39; $GRASTATE_FILE | awk -F \u0026#39;: \u0026#39; \u0026#39;{print $2}\u0026#39;) if [ $safe_bootstrap_flag -eq 1 ] ; then start_mariadb_special ret=$? else start_mariadb_normal ret=$? fi else start_mariadb_normal ret=$? fi # 随机地按某种方式启动，直到以某种方式正常启动以止；否则杀掉mysqld进程，随机休息一会儿，重试 while [ $ret -ne 0 ]; do kill_mysqld_process sleep_time=$(( $RANDOM % 10 )) sleep $sleep_time choice=$(( $RANDOM % 2 )) ret=-1 if [ $choice -eq 0 ] ; then start_mariadb_special ret=$? else start_mariadb_normal ret=$? fi done # 使上述脚本开机自启动 chmod +x /usr/local/bin/mariadb_cluster_helper.sh chmod +x /etc/rc.d/rc.local echo \u0026#39; /usr/local/bin/mariadb_cluster_helper.sh \u0026amp;\u0026gt; /var/log/mariadb_cluster_helper.log \u0026amp;\u0026#39; \u0026gt;\u0026gt; /etc/rc.d/rc.local 然后3个节点终于可以开机自启动自动组成集群了。\n搭配keepalived+haproxy+clustercheck 为了保证mariadb galera集群的高可用，可以使用haproxy进行请求负载均衡，同时为了实现haproxy的高可用，可使用keepalived实现haproxy的热备方案。keepalived实现haproxy的热备方案可参见之前的博文。这里重点说一下haproxy对mariadb galera集群的请求负载均衡。\n这里使用了 https://github.com/olafz/percona-clustercheck 所述方案，使用外部脚本在应用层检查galera节点的状态。\n首先在mariadb里进行授权：\nGRANT PROCESS ON *.* TO \u0026#39;clustercheckuser\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;clustercheckpassword!\u0026#39; 下载检测脚本：\nwget -O /usr/bin/clustercheck https://raw.githubusercontent.com/olafz/percona-clustercheck/master/clustercheck chmod +x /usr/bin/clustercheck 准备检测脚本用到的配置文件：\nMYSQL_USERNAME=\u0026quot;clustercheckuser\u0026quot; MYSQL_PASSWORD=\u0026quot;clustercheckpassword!\u0026quot; MYSQL_HOST=\u0026quot;$db_ip\u0026quot; MYSQL_PORT=\u0026quot;3306\u0026quot; AVAILABLE_WHEN_DONOR=0 测试一下监控脚本：\n# /usr/bin/clustercheck \u0026gt; /dev/null # echo $? 0 # synced 1 # un-synced 使用xinetd暴露http接口，用于检测galera节点同步状态：\ncat \u0026gt; /etc/xinetd.d/mysqlchk \u0026lt;\u0026lt; EOF # default: on # description: mysqlchk service mysqlchk { disable = no flags = REUSE socket_type = stream port = 9200 wait = no user = nobody server = /usr/bin/clustercheck log_on_failure += USERID only_from = 0.0.0.0/0 per_source = UNLIMITED } EOF service xinetd restart 测试一下暴露出的http接口：\ncurl http://127.0.0.1:9200 Galera cluster node is synced. # synced Galera cluster node is not synced # un-synced 最后在/etc/haproxy/haproxy.cfg里配置负载均衡：\n... frontend vip-mysql bind $vip:3306 timeout client 900m log global option tcplog mode tcp default_backend vms-mysql backend vms-mysql option httpchk stick-table type ip size 1000 stick on dst balance leastconn timeout server 900m server mysql1 $db1_ip:3306 check inter 1s port 9200 backup on-marked-down shutdown-sessions maxconn 60000 server mysql2 $db2_ip:3306 check inter 1s port 9200 backup on-marked-down shutdown-sessions maxconn 60000 server mysql2 $db3_ip:3306 check inter 1s port 9200 backup on-marked-down shutdown-sessions maxconn 60000 ... 搭配galera仲裁服务 官方也提到gelera集群最少要三节点部署，但每增加一个节点，要付出相应的资源，因此也可以最少两节点部署，再加上一个galera仲裁服务。\n The recommended deployment of Galera Cluster is that you use a minimum of three instances. Three nodes, three datacenters and so on.\nIn the event that the expense of adding resources, such as a third datacenter, is too costly, you can use Galera Arbitrator. Galera Arbitrator is a member of the cluster that participates in voting, but not in the actual replication\n 这种部署模式有两个好处：\n 使集群刚好是奇数节点，不易产生脑裂。 可能通过它得到一个一致的数据库状态快照，可以用来备份。  这种部署模式的架构图如下：\n部署方法也比较简单：\n# 假设已经构建了一个两节点的galera集群，在第3个节点部署garbd服务 echo \u0026#39; GALERA_NODES=\u0026#34;10.211.55.6:4567 10.211.55.7:4567\u0026#34; # 这里是两节点的地址 GALERA_GROUP=\u0026#34;galera_cluster\u0026#34; # 这里的group名称保持与两节点的wsrep_cluster_name属性一致 LOG_FILE=\u0026#34;/var/log/garb.log\u0026#34; \u0026#39; \u0026gt; /etc/sysconfig/garb systemctl start garb # 启动garbd服务 测试一下效果。\n首先看一下两节点部署产生脑裂的场景。\n# 首先在第3个节点停止garb服务 systemctl stop garb # 然后在第2个节点drop掉去住第1个节点和仲裁节点的数据包 iptables -A OUTPUT -d 10.211.55.6 -j DROP iptables -A OUTPUT -d 10.211.55.9 -j DROP # 这时检查前两个节点的同步状态，发生脑裂了，都不是同步状态了 mysql -e \u0026#34;show status like \u0026#39;wsrep_local_state_comment\u0026#39;\u0026#34; +---------------------------+-------------+ | Variable_name | Value | +---------------------------+-------------+ | wsrep_local_state_comment | Initialized | +---------------------------+-------------+ 再试验下有仲裁节点参与的场景。\n# 首先在第3个节点启动garb服务 systemctl start garb # 在前两个节点查看集群节点数，发现是3个，说明包括了仲裁节点 mysql -e \u0026#34;show status like \u0026#39;wsrep_cluster_size\u0026#39;\u0026#34; +---------------------------+-------------+ | Variable_name | Value | +---------------------------+-------------+ | wsrep_cluster_size | 3 | +---------------------------+-------------+ # 然后在第2个节点drop掉去住第1个节点和仲裁节点的数据包 iptables -A OUTPUT -d 10.211.55.6 -j DROP iptables -A OUTPUT -d 10.211.55.9 -j DROP # 这时检查第1个节点的同步状态，仍然是同步状态 mysql -e \u0026#34;show status like \u0026#39;wsrep_local_state_comment\u0026#39;\u0026#34; +---------------------------+-------------+ | Variable_name | Value | +---------------------------+-------------+ | wsrep_local_state_comment | Synced | +---------------------------+-------------+ # 再在第1个节点查看集群节点数，发现是2个 mysql -e \u0026#34;show status like \u0026#39;wsrep_cluster_size\u0026#39;\u0026#34; +---------------------------+-------------+ | Variable_name | Value | +---------------------------+-------------+ | wsrep_cluster_size | 2 | +---------------------------+-------------+ # 这时检查第2个节点的同步状态，发现是未同步的 mysql -e \u0026#34;show status like \u0026#39;wsrep_local_state_comment\u0026#39;\u0026#34; +---------------------------+-------------+ | Variable_name | Value | +---------------------------+-------------+ | wsrep_local_state_comment | Initialized | +---------------------------+-------------+ 以前试验说明采用了仲裁节点后，因为集群节点数变为了奇数，有效地避免了脑裂，同时将真正有故障的节点隔离出去了。\n参考  https://segmentfault.com/a/1190000002955693 https://github.com/olafz/percona-clustercheck http://galeracluster.com/documentation-webpages/arbitrator.html  ","permalink":"https://jeremyxu2010.github.io/2018/02/mariadb-galera-cluster%E9%83%A8%E7%BD%B2%E5%AE%9E%E6%88%98/","tags":["mariadb","database","galera"],"title":"MariaDB Galera Cluster部署实战"},{"categories":["devops"],"contents":"最近的工作需要将以前编译安装的软件包打包成rpm包，这里将打包过程记录一下以备忘。\n准备rpm打包环境 我这里用的操作系统是CentOS6.7，redhat系的其它发行版应该也类似。\n安装rpm-build sudo yum install -y gcc make rpm-build redhat-rpm-config vim lrzsz\r创建必须的文件夹和文件 mkdir -p ~/rpmbuild/{BUILD,RPMS,SOURCES,SPECS,SRPMS}\recho \u0026#39;%_topdir %(echo $HOME)/rpmbuild\u0026#39; \u0026gt; ~/.rpmmacros\r制作spec文件 找spec模板文件 一般找一个类似的rpm源码包，将其安装，然后参照它写自己软件包的spec文件。\nmkdir ~/rpms\rwget -O ~/rpms/python-2.6.6-64.el6.src.rpm http://vault.centos.org/6.7/os/Source/SPackages/python-2.6.6-64.el6.src.rpm\rrpm -ivh ~/rpms/python-2.6.6-64.el6.src.rpm\rvim ~/rpmbuild/SPECS/python.spec # 参照这个文件来写自己软件包的spec文件\r写自己软件包的spec文件 spec文件中各个选项的意义参照这里\ncd ~/rpmbuild\rcat ./SPECS/python27-tstack.spec\r%debug_package %{nil}\r%define install_dir /usr/local/python27\rName:\tpython27-tstack\rVersion:\t2.7.10\rRelease:\t1%{?dist}\rSummary:\tpython27 modified by tstack\rURL: http://www.python.org/\rGroup:\tDevelopment/Languages\rLicense:\tPython\rProvides: python-abi = 2.7\rProvides: python(abi) = 2.7\rSource0:\tPython-2.7.10.tgz\rBuildRequires: readline-devel, openssl-devel, gmp-devel, pcre-devel, mysql-devel, libffi-devel\rRequires:\treadline, openssl, gmp, pcre, mysql, libffi\rAutoreq: 0\r%description\rPython is an interpreted, interactive, object-oriented programming\rlanguage often compared to Tcl, Perl, Scheme or Java. Python includes\rmodules, classes, exceptions, very high level dynamic data types and\rdynamic typing. Python supports interfaces to many system calls and\rlibraries, as well as to various windowing systems (X11, Motif, Tk,\rMac and MFC).\rProgrammers can write new built-in modules for Python in C or C++.\rPython can be used as an extension language for applications that need\ra programmable interface.\rNote that documentation for Python is provided in the python-docs\rpackage.\r%prep\r%setup -q -n Python-%{version}\r%build\r./configure --prefix=%{install_dir} --with-cxx-main=/usr/bin/g++\rmake %{?_smp_mflags}\r%install\rrm -rf %{buildroot}\rmake install DESTDIR=%{buildroot}\r%clean rm -rf %{buildroot}\r%files\r%defattr (-,root,root)\r%{install_dir}/bin/\r%{install_dir}/include/\r%{install_dir}/lib/\r%{install_dir}/share/\r%doc\r%changelog\r制作rpm包 上传必要的source文件 cp ${some_where}/Python-2.7.10.tgz ~/rpmbuild/SOURCES/\r开始制作 cd ~/rpmbuild\rrpmbuild -bb --target x86_64 SPECS/python27-tstack.spec \u0026amp;\u0026gt; rpmbuild.log # 这时可以打开另一个终端观察下rpmbuild.log\r一切顺序的话，最终会在~/rpmbuild/RPMS/x86_64/目录下找到编译好的rpm包。\n技巧总结   不打debug的rpm包\n在spec文件中加入%debug_package %{nil}即可\n  禁止自动分析源码添加不应该加入的依赖 在spec文件中加入Autoreq: 0即可\n  sepc文件中一些宏的用法 在spec文件中经常出现一些宏，比如%setup、%patch，这两个宏的选项较多，使用时要特别注意，参见这里\n  安装卸载rpm包前后的动作 可以通过%pre, %post, %preun, %postun指定rpm包在安装卸载前后的动作，比如在安装前用脚本做一些准备、在安装后用脚本做一些初始化动作、在卸载前用脚本做一些准备、在卸载后用脚本做一些清理动作\n  rpmbuild命令的选项 rpmbuild命令有不少选项，参见这里，个人用得比较多的有：\n -bp 只解压源码及应用补丁 -bc 只进行编译 -bi 只进行安装到%{buildroot} -bb 只生成二进制rpm包 -bs 只生成源码rpm包 -ba 生成二进制rpm包和源码rpm包 --target 指定生成rpm包的平台，默认会生成i686和x86_64的rpm包，但一般我只需要x86_64的rpm包    参考  http://vault.centos.org/6.7/os/Source/SPackages/ http://tkdchen.github.io/blog/2013/05/19/rpm-spec-for-python-gist.html http://www.dahouduan.com/2015/06/15/linux-centos-make-rpm/ http://www.centoscn.com/CentOS/Intermediate/2014/0419/2826.html http://wiki.centos.org/HowTos/SetupRpmBuildEnvironment http://ftp.rpm.org/max-rpm/rpmbuild.8.html http://ftp.rpm.org/max-rpm/s1-rpm-inside-macros.html  ","permalink":"https://jeremyxu2010.github.io/2018/02/centos6%E4%B8%8Brpm%E6%89%93%E5%8C%85%E5%AE%9E%E6%88%98/","tags":["centos","rpm","rpmbuild"],"title":"CentOS6下rpm打包实战"},{"categories":["devops"],"contents":"背景 早就听说微软的powershell非常强大，凭借它可以全命令行操控windows服务器了。最近终于要在工作中用到它了，于是花了几个小时将powershell的基础教程看了下，这里将学习过程中的一些要点记录一下。\n环境准备 欲善其事，先利其器，先准备一个开发环境。\n个人的开发电脑是macOS 11.13.3，为了开发powershell脚本，在本机安装了一个windows 7 sp1的虚拟机。\n升级powershell版本 win7自带的powershell版本较低，这里将windows 7 sp1里自带的powershell升级到5.1版本。 先安装.NET 4.5，然后再安装Win7AndW2K8R2-KB3191566-x64.zip。\n设置允许运行本机powershell脚本 以管理员的身份运行PowerShell，在powershell窗口里输出以下命令：\nSet-ExecutionPolicy RemoteSigned -Force 设置macOS系统远程连到windows系统的powershell 本地还是更喜欢iTerm2的终端，windows里带的powershell终端实在是用不惯，于是设置了下通过ssh连接到windows系统的powershell。\n从https://github.com/PowerShell/Win32-OpenSSH/releases/latest/下载OpenSSH for windows的64位二进制包，安装到windows的C:\\Program Files\\OpenSSH目录。\n以管理员的身份运行PowerShell，在powershell窗口里输出以下命令：\ncd C:\\Program Files\\OpenSSH powershell.exe -ExecutionPolicy Bypass -File install-sshd.ps1 # 安装sshd服务 netsh advfirewall firewall add rule name=sshd dir=in action=allow protocol=TCP localport=22 # 允许外部访问ssh端口 net start sshd # 启动sshd服务 Set-Service sshd -StartupType Automatic # 设置sshd服务开机自启动 Set-Service ssh-agent -StartupType Automatic # 设置ssh-agent服务开机自启动 New-Item -type Directory HKLM:\\SOFTWARE\\OpenSSH New-Item -itemType String HKLM:\\SOFTWARE\\OpenSSH\\DefaultShell -value \u0026quot;C:\\Windows\\system32\\WindowsPowerShell\\v1.0\\powershell.exe\u0026quot; # 设置ssh登录的默认shell为powershell 给windows安装一个命令行的编辑器vim 运程操控windows服务器免不了要修改某些配置文件，个人还是比较适应vim，这里在windows里安装好vim。\n从https://vim.sourceforge.io/download.php#pc下载vim的windows安装包gvim80.exe，在windows里以默认选项安装一下，正常情况下会安装到C:/Program Files (x86)/Vim目录下。\n在windows里以普通身份运行PowerShell，在powershell窗口里输出以下命令：\nnew-item -path $profile -itemtype file -force @' set-alias vim \u0026quot;C:/Program Files (x86)/Vim/vim80/vim.exe\u0026quot; Function Edit-Profile { vim $profile } # To edit Vim settings Function Edit-Vimrc { vim $HOME\\_vimrc } '@ \u0026gt; @profile # 创建并初始化powershell的配置文件 @' syntax on colorscheme desert set backspace=indent,eol,start '@ \u0026gt; $HOME\\_vimrc # 初始化vim的配置文件 在iTerm2里创建该连接的profile 现在已经可以在iTerm2里通过命令/usr/local/bin/sshpass -p 123456 ssh jeremy@10.211.55.5连接到windows的powershell，其中123456是windows用户jeremy的密码，10.211.55.5是windows的密码。为了连接方便，在iTerm2里创建一个新的profile，登录的命令设置为/usr/local/bin/sshpass -p 123456 ssh jeremy@10.211.55.5，以后以这个profile创建会话就会直接连接到windows的powershell。\npowershell学习要点 Powershell基础 基本数学计算 基本数学计算比较简单，不单独说了，参见这里\n执行外部命令 可直接执行windows命令行命令，甚至可以直接执行cmd命令。如果一个外部命令必须用引号括起来，为了让powershell执行字符串里的命令，可在字符串前加\u0026amp;，这样即可让powershell执行该命令，参见这里\n命令集cmdlets cmdlets是Powershell的内部命令。\nGet-Command -Name Get-Content | Get-Member # 察看一个cmdlet的所有属性、方法、ScriptProperty Get-Command -CommandType Cmdlet 列出所有cmdlets Get-Command -CommandType Cmdlet *Service* # 列出名称里包含Service的cmdlets Get-Help Get-Content #获得某个cmdlet的使用帮助 详细参见这里\n别名 cmdlet 的名称由一个动词和一个名词组成，其功能对用户来讲一目了然。但是对于一个经常使用powershell命令的人每天敲那么多命令也很麻烦，于是别名就应运而生了。\nGet-Alias -name ls # 查看某一个别名的定义 Get-Alias # 查看所有别名 dir alias: | where {$_.Definition.Startswith(\u0026quot;Remove\u0026quot;)} # 查看所有以Remove打头的cmdlet的命令的别名呢 Set-Alias -Name Edit -Value notepad # 创建别名 del alias:Edit # 删除别名 Export-Alias alias.ps1 # 导出别名 Import-Alias -Force alias.ps1 # 导入别名 详细参见这里\n管道和重定向 powershell里支持管道，但要注意不像linux的管道，powershell里管道里输出、输入都是对象，如下：\nls | sort -Descending Name | Format-Table Name,Mode powershell支持重定向，\u0026gt;为覆盖，\u0026gt;\u0026gt;追加，注意可直接将字符串重定向到文件，如下：\n\u0026quot;Powershell Routing\u0026quot; \u0026gt; test.txt \u0026quot;Powershell Routing\u0026quot; \u0026gt;\u0026gt; test.txt 变量 变量可以临时保存数据，因此可以把数据保存在变量中，以便进一步操作，powershell 不需要显示地去声明，可以自动创建变量，只须记住变量的前缀为$。\n#定义变量 $a=10 $b=4 #计算变量 $result=$a*$b $msg=\u0026quot;保存文本\u0026quot; #输出变量 $result $msg #交换变量的值 $value1=10 $value2=20 $value1,$value2=$value2,$value1 #查看所有变量 ls variable: #查询以value打头的变量名 ls variable:value* #验证变量是否存在 Test-Path variable:value1 #删除变量 del variable:value1 #变量写保护 New-Variable num -Value 100 -Force -Option readonly #给变量添加描述 new-variable name -Value \u0026quot;me\u0026quot; -Description \u0026quot;This is my name\u0026quot; ls Variable:name | fl * 详细参见这里\n内置变量 Powershell 内置变量是指那些一旦打开Powershell就会自动加载的变量。 这些变量一般存放的内容包括\n 用户信息：例如用户的根目录$HOME 配置信息：例如powershell控制台的大小，颜色，背景等。 运行时信息：例如一个函数由谁调用，一个脚本运行的目录等。  较常用的内置变量如下：\n$? 包含最后一个操作的执行状态。如果最后一个操作成功，则包含 TRUE，失败则包含 FALSE。 $_ 包含管道对象中的当前对象。在对管道中的每个对象或所选对象执行操作的命令中，可以使用此变量。 $Args 包含由未声明参数和/或传递给函数、脚本或脚本块的参数值组成的数组。 在创建函数时可以声明参数，方法是使用 param 关键字或在函数名称后添加以圆括号括起、逗号 分隔的参数列表。 $Error 包含错误对象的数组，这些对象表示最近的一些错误。最近的错误是该数组中的第一个错误对象 ($Error[0])。 $False 包含 FALSE。可以使用此变量在命令和脚本中表示 FALSE，而不是使用字符串”false”。如果 该字符串转换为非空字符串或非零整数，则可将该字符串解释为 TRUE。 $ForEach 包含 ForEach-Object 循环的枚举数。可以对 $ForEach 变量的值使用枚举数的属性和方法。 此变量仅在运行 For 循环时存在，循环完成即会删除。 $Home 包含用户的主目录的完整路径。此变量等效于 %homedrive%%homepath% 环境变量。 $Host 包含一个对象，该对象表示 Windows PowerShell 的当前主机应用程序。可以使用此变量在命 令中表示当前主机，或者显示或更改主机的属性，如 $Host.version、$Host.CurrentCulture 或 $host.ui.rawui.setbackgroundcolor(“Red”)。 $Input 一个枚举数，它包含传递给函数的输入。$Input 变量区分大小写，只能用于函数和脚本块。（脚 本块本质上是未命名的函数。）在函数的 Process 块中，$Input 变量包含当前位于管道中的对 象。在 Process 块完成后，$Input 的值为 NULL。如果函数没有 Process 块，则 $Input 的值可用于 End 块，它包含函数的所有输入。 $LastExitCode 包含运行的最后一个基于 Windows 的程序的退出代码。 $Matches $Matches 变量与 -match 和 -not match 运算符一起使用。 将标量输入提交给 -match 或 -notmatch 运算符时，如果检测到匹配，则会返回一个布尔值， 并使用由所有匹配字符串值组成的哈希表填充 $Matches 自动变量。有关 -match 运算符的详细 信息，请参阅 about_comparison_operators。 $MyInvocation 包含一个对象，该对象具有有关当前命令（如脚本、函数或脚本块）的信息。可以使用该对象中的 信息（如脚本的路径和文件名 ($myinvocation.mycommand.path) 或函数的名称 ($myinvocation.mycommand.name)）来标识当前命令。对于查找正在运行的脚本的名称，这非常有用。 $NULL 包含 NULL 或空值。可以在命令和脚本中使用此变量表示 NULL，而不是使用字符串”NULL”。 如果该字符串转换为非空字符串或非零整数，则可将该字符串解释为 TRUE。 $PID 包含承载当前 Windows PowerShell 会话的进程的进程标识符 (PID)。 $Profile 包含当前用户和当前主机应用程序的 Windows PowerShell 配置文件的完整路径。可以在命令 中使用此变量表示配置文件。例如，可以在命令中使用此变量确定是否已创建某个配置文件： test-path $profile 也可以在命令中使用此变量创建配置文件： new-item -type file -path $pshome -force $PsHome 包含 Windows PowerShell 的安装目录的完整路径（通常为 %windir%System32WindowsPowerShellv1.0）。可以在 Windows PowerShell 文件 的路径中使用此变量。例如，下面的命令在概念性帮助主题中搜索”variable”一词： select-string -pattern variable -path $pshome*.txt $PSScriptRoot 包含要从中执行脚本模块的目录。 通过此变量，脚本可以使用模块路径来访问其他资源。 $PsVersionTable 包含一个只读哈希表，该哈希表显示有关在当前会话中运行的 Windows PowerShell 版本的详 细信息。 该表包括下列项： CLRVersion： 公共语言运行时 (CLR) 的版本 BuildVersion： 当前版本的内部版本号 PSVersion： Windows PowerShell 版本号 WSManStackVersion： WS-Management 堆栈的版本号 PSCompatibleVersions： 与当前版本兼容的 Windows PowerShell 版本 SerializationVersion ：序列化方法的版本 PSRemotingProtocolVersion：Windows PowerShell 远程管理协议的版本 $Pwd 包含一个路径对象，该对象表示当前目录的完整路径。 $ShellID 包含当前 shell 的标识符。 $True 包含 TRUE。可以在命令和脚本中使用此变量表示 TRUE。 详细参见这里\n环境变量 传统的控制台一般没有象Powershell这么高级的变量系统。它们都是依赖于机器本身的环境变量，进行操作 。环境变量对于powershell显得很重要，因为它涵盖了许多操作系统的细节信息。\n$env:windir # windows目录 $env:ProgramFiles # 默认程序安装目录 $env:COMPUTERNAME # 主机名 $env:OS # 操作系统名称 ls env: # 查找环境变量 $env:TestVar1=\u0026quot;This is my environment variable\u0026quot; # 创建新的环境变量 ls env:Test* # 模糊查找环境变量 $env:TestVar1=\u0026quot;This is my new environment variable\u0026quot; # 更新环境变量 del env:TestVar1 # 删除环境变量 $env:Path+=\u0026quot;;C:\\PowerShell\\myscript\u0026quot; # 更改Path环境变量 [environment]::SetEnvironmentvariable(\u0026quot;Path\u0026quot;, \u0026quot;;c:\\powershell\\myscript\u0026quot;, \u0026quot;User\u0026quot;) # 修改系统的环境变量 [environment]::GetEnvironmentvariable(\u0026quot;Path\u0026quot;, \u0026quot;User\u0026quot;) # 从系统读取环境变量 详细参见这里\n变量的作用域 Powershell所有的变量都有一个决定变量是否可用的作用域。Powershell支持四个作用域：全局、当前、私有和脚本。有了这些作用域就可以限制变量的可见性了，尤其是在函数和脚本中。\n设置单个变量的作用域 $global 全局变量，在所有的作用域中有效，如果你在脚本或者函数中设置了全局变量，即使脚本和函数都运行结束，这个变量也任然有效。\n$script 脚本变量，只会在脚本内部有效，包括脚本中的函数，一旦脚本运行结束，这个变量就会被回收。\n$private 私有变量，只会在当前作用域有效，不能贯穿到其他作用域。\n$local 默认变量，可以省略修饰符，在当前作用域有效，其它作用域只对它有只读权限。\n详细参见这里\n指定类型定义变量 # 解析日期 [DateTime]$date=\u0026quot;2012-12-20 12:45:00\u0026quot; $date # 解析XML [ XML ]$xml=(Get-Content .LogoTestConfig.xml) $xml.LogoTest # 解析IP地址 [Net.IPAddress]$ip='10.3.129.71' 详细参见这里\n命令返回数组 当我们把一个外部命令的执行结果保存到一个变量中时，Powershell会把文本按每一行作为元素存为数组。\n#ipconfig的输出结果是一个数组 $ip=ipconfig $ip -is [array] 真正的Powershell命令返回的数组元素可不止一个字符串，它是一个内容丰富的对象。\n#ls的输出结果仍然是一个数组 $result=ls $result -is [array] #数组里的元素是一个对象 $result[0].gettype().fullname $result[0] | fl * 数组 #使用逗号创建数组 $nums=2,0,1,2 #创建连续数字的数组 $nums=1..5 #创建空数组 $a=@() #判断是否是一个数组 $a -is [array] #得到数组里元素的个数 $a.Count #访问数组 $books=\u0026quot;元素1\u0026quot;,\u0026quot;元素2\u0026quot;,\u0026quot;元素3\u0026quot; $books[0] $books[($books.Count-1)] #从数组中选择多个元素 $books[0,2] #将数组逆序输出 $books[($books.Count)..0] #给数组添加元素 $books+=\u0026quot;元素4\u0026quot; #删除第3个元素 $books=$books[0..1]+$books[3] #复制数组 $booksNew=$books.Clone() #创建强类型数组 [int[]] $nums=@() 哈希表 #创建哈希表 $stu=@{ Name = \u0026quot;小明\u0026quot;;Age=\u0026quot;12\u0026quot;;sex=\u0026quot;男\u0026quot; } #访问哈希键值 $stu[\u0026quot;Name\u0026quot;] #得到哈希表里元素的个数 $stu.Count #得到所有哈希键 $stu.Keys #得到所有哈希值 $stu.Values #插入新的键值 $stu.Name=\u0026quot;令狐冲\u0026quot; #更新哈希表值 $stu.Name=\u0026quot;赵强\u0026quot; #删除哈希表值 $stu.Remove(\u0026quot;Name\u0026quot;) #在哈希表中存储数组 $stu=@{ Name = \u0026quot;小明\u0026quot;;Age=\u0026quot;12\u0026quot;;sex=\u0026quot;男\u0026quot;;Books=\u0026quot;三国演义\u0026quot;,\u0026quot;围城\u0026quot;,\u0026quot;哈姆雷特\u0026quot; } 使用哈希表格式化输出 #控制输出哪些列 Dir | Format-Table FullName,Mode #自定义输出列的格式 $column1 = @{expression=\u0026quot;Name\u0026quot;; width=30;label=\u0026quot;filename\u0026quot;; alignment=\u0026quot;left\u0026quot;} $column2 = @{expression=\u0026quot;LastWriteTime\u0026quot;; width=40;label=\u0026quot;last modification\u0026quot;; alignment=\u0026quot;right\u0026quot;} ls | Format-Table $column1, $column2 管道处理 常用的对管道结果进一步处理的命令有：\nCompare-Object: 比较两组对象。 ConvertTo-Html: 将 Microsoft .NET Framework 对象转换为可在 Web 浏览器中显示的 HTML。 Export-Clixml: 创建对象的基于 XML 的表示形式并将其存储在文件中。 Export-Csv: 将 Microsoft .NET Framework 对象转换为一系列以逗号分隔的、长度可变的 (CSV) 字符串，并将这些字符串保存到一个 CSV 文件中。 ForEach-Object: 针对每一组输入对象执行操作。 Format-List: 将输出的格式设置为属性列表，其中每个属性均各占一行显示。 Format-Table: 将输出的格式设置为表。 Format-Wide: 将对象的格式设置为只能显示每个对象的一个属性的宽表。 Get-Unique: 从排序列表返回唯一项目。 Group-Object: 指定的属性包含相同值的组对象。 Import-Clixml: 导入 CLIXML 文件，并在 Windows PowerShell 中创建相应的对象。 Measure-Object: 计算对象的数字属性以及字符串对象（如文本文件）中的字符数、单词数和行数。 more: 对结果分屏显示。 Out-File: 将输出发送到文件。 Out-Null: 删除输出，不将其发送到控制台。 Out-Printer: 将输出发送到打印机。 Out-String: 将对象作为一列字符串发送到主机。 Select-Object: 选择一个对象或一组对象的指定属性。它还可以从对象的数组中选择唯一对象，也可以从对象数组的开头或末尾选择指定个数的对象。 Sort-Object: 按属性值对象进行排序。 Tee-Object: 将命令输出保存在文件或变量中，并将其显示在控制台中。 Where-Object: 创建控制哪些对象沿着命令管道传递的筛选器。 其中:\nFormat的管道处理用法参见这里\n排序和分组的管道处理用法参见这里\nSelect-Object、Where-Object、ForEach-Object用法参见这里\n导出的管道处理用法参见这里\n对象、控制流、函数 对象=属性+方法 Powershell中的对象和现实生活很相似。例如要在现实生活中描述一把小刀。我们可能会分两方面描述它。 属性：一把小刀拥有一些特殊的属性，比如它的颜色、制造商、大小、刀片数。这个对象是红色的，重55克，有3个刀片，ABC公司生产的。因此属性描述了一个对象是什么。 方法：可以使用这个对象做什么，比如切东西、当螺丝钉用、开啤酒盖。一个对象能干什么就属于这个对象的方法。\n#创建对象 $pocketknife=New-Object object #增加属性 Add-Member -InputObject $pocketknife -Name Color -Value \u0026quot;Red\u0026quot; -MemberType NoteProperty $pocketknife | Add-Member NoteProperty Blades 3 #增加方法 Add-Member -memberType ScriptMethod -In $pocketknife -name cut -Value { \u0026quot;I'm whittling now\u0026quot; } $pocketknife | Add-Member ScriptMethod corkscrew { \u0026quot;Pop! Cheers!\u0026quot; } 对象的属性 #直接使用点访问对象的属性 $Host.Version #查看Version的具体类型 $Host.Version.GetType().FullName #查看对象所有属性 $Host | Get-Member -memberType property 对象的方法 #查看对象所有的方法 $Host | Get-Member -MemberType Method #调用方法 $Host.GetType() $Host.UI.WriteDebugLine(\u0026quot;Hello 2012 !\u0026quot;) #列出重载方法 $method=$Host.UI | Get-Member WriteLine $method.Definition.Replace(\u0026quot;),\u0026quot;,\u0026quot;)`n\u0026quot;) 静态方法 #查看某类型的静态方法 [System.DateTime] | Get-Member -static -memberType Method #调用静态方法 [System.DateTime]::Parse(\u0026quot;2012-10-13 23:42:55\u0026quot;) #根据IP反查域名 [system.Net.Dns]::GetHostByAddress('8.8.8.8').HostName #查询某类型下的所有枚举 [System.Enum]::GetNames([System.ConsoleColor]) 条件操作符 比较运算符 -eq ：等于 -ne ：不等于 -gt ：大于 -ge ：大于等于 -lt ：小于 -le ：小于等于 -contains ：包含 -notcontains :不包含\n求反 求反运算符为-not 但是像高级语言一样!也支持求反\n布尔运算 -and ：和 -or ：或 -xor ：异或 -not ：逆\nPS C:Powershell\u0026gt; (3,4,5 ) -contains 2 False PS C:Powershell\u0026gt; (3,4,5 ) -contains 5 True PS C:Powershell\u0026gt; (3,4,5 ) -notcontains 6 True PS C:Powershell\u0026gt; 2 -eq 10 False PS C:Powershell\u0026gt; \u0026quot;A\u0026quot; -eq \u0026quot;a\u0026quot; True PS C:Powershell\u0026gt; \u0026quot;A\u0026quot; -ieq \u0026quot;a\u0026quot; True PS C:Powershell\u0026gt; \u0026quot;A\u0026quot; -ceq \u0026quot;a\u0026quot; False PS C:Powershell\u0026gt; 1gb -lt 1gb+1 True PS C:Powershell\u0026gt; 1gb -lt 1gb-1 False PS C:Powershell\u0026gt; $a= 2 -eq 3 PS C:Powershell\u0026gt; $a False PS C:Powershell\u0026gt; -not $a True PS C:Powershell\u0026gt; !($a) True PS C:Powershell\u0026gt; $true -and $true True PS C:Powershell\u0026gt; $true -and $false False PS C:Powershell\u0026gt; $true -or $true True PS C:Powershell\u0026gt; $true -or $false True PS C:Powershell\u0026gt; $true -xor $false True PS C:Powershell\u0026gt; $true -xor $true False PS C:Powershell\u0026gt; -not $true False #过滤数组中的元素 PS C:Powershell\u0026gt; 1,2,3,4,3,2,1 -eq 3 3 3 PS C:Powershell\u0026gt; 1,2,3,4,3,2,1 -ne 3 1 2 4 2 1 #验证一个数组是否存在特定元素 PS C:Powershell\u0026gt; 1,9,4,5 -contains 9 True PS C:Powershell\u0026gt; 1,9,4,5 -contains 10 False PS C:Powershell\u0026gt; 1,9,4,5 -notcontains 10 True IF-ELSEIF-ELSE 条件 Where-Object 进行条件判断很方便，如果在判断后执行很多代码可以使用IF-ELSEIF-ELSE语句。语句模板：\nIf（条件满足）{ 如果条件满足就执行代码 } Else { 如果条件不满足 } ForEach-Object 循环 #杀掉名字里包含rar的进程 Get-Process | Where-Object {$_.ProcessName -like '*rar*'} | ForEach-Object {$_.Kill()} Foreach 循环 $array=7..10 foreach ($n in $array) { $n*$n } Do While 循环 do { $n=Read-Host } while( $n -ne 0) #单独使用While $n=5 while($n -gt 0) { $n $n=$n-1 } #终止当前循环 $n=1 while($n -lt 6) { if($n -eq 4) { $n=$n+1 continue } else { $n } $n=$n+1 } #跳出循环 $n=1 while($n -lt 6) { if($n -eq 4) { break } $n $n++ } For 循环 $sum=0 for($i=1;$i -le 100;$i++) { $sum+=$i } $sum 函数 #定义函数模板 Function FuncName （args[]） { code; } #删除函数 del Function:myPing #万能参数 function sayHello { if($args.Count -eq 0) { \u0026quot;No argument!\u0026quot; } else { $args | foreach {\u0026quot;Hello,$($_)\u0026quot;} } } #设置参数名称 function StringContact($str1,$str2) { return $str1+$str2 } StringContact -str1 word -str2 press #给参数定义默认值 function stringContact($str1=\u0026quot;moss\u0026quot;,$str2=\u0026quot;fly\u0026quot;) { return $str1+$str2 } stringContact # 列出所有函数 dir function: | ft -AutoSize #函数过滤器管道 Filter MarkEXE { # 记录当前控制台的背景色 $oldcolor = $host.ui.rawui.ForegroundColor # 当前的管道元素保存在 $_ 变量中 # 如果后缀名为 \u0026quot;.exe\u0026quot;, # 改变背景色为红色: If ($_.name.toLower().endsWith(\u0026quot;.exe\u0026quot;)) { $host.ui.Rawui.ForegroundColor = \u0026quot;red\u0026quot; } Else { # 否则使用之前的背景色 $host.ui.Rawui.ForegroundColor = $oldcolor } # 输出当前元素 $_ # 最后恢复控制台颜色: $host.ui.Rawui.ForegroundColor = $oldcolor } Function MarkEXE { begin { # 记录控制台的背景色 $oldcolor = $host.ui.rawui.ForegroundColor } process { # 当前管道的元素 $_ # 如果后缀名为 \u0026quot;.exe\u0026quot;, # 改变背景色为红色: If ($_.name.toLower().endsWith(\u0026quot;.exe\u0026quot;)) { $host.ui.Rawui.ForegroundColor = \u0026quot;red\u0026quot; } Else { # 否则, 使用正常的背景色: $host.ui.Rawui.ForegroundColor = $oldcolor } # 输出当前的背景色 $_ } end { # 最后,恢复控制台的背景色: $host.ui.Rawui.ForegroundColor = $oldcolor } } 其它技巧 阻止变量解析 PS E:\u0026gt; @' \u0026gt;\u0026gt; Get-Date \u0026gt;\u0026gt; $Env:CommonProgramFiles \u0026gt;\u0026gt; #Script End \u0026gt;\u0026gt; \u0026quot;files count\u0026quot; \u0026gt;\u0026gt; (ls).Count \u0026gt;\u0026gt; #Script Really End \u0026gt;\u0026gt; \u0026gt;\u0026gt; '@ \u0026gt; myscript.ps1 \u0026gt;\u0026gt; 给脚本传递参数 @' For($i=0;$i -lt $args.Count; $i++) { Write-Host \u0026quot;parameter $i : $($args[$i])\u0026quot; } '@ \u0026gt; MyScript.ps1 PS E:\u0026gt; .\\MyScript.ps1 www moss fly com parameter 0 : www parameter 1 : moss parameter 2 : fly parameter 3 : com 在脚本中使用参数名 @' param($Directory,$FileName) \u0026quot;Directory= $Directory\u0026quot; \u0026quot;FileName=$FileName\u0026quot; '@ \u0026gt; MyScript.ps1 PS E:\u0026gt; .\\MyScript.ps1 -Directory $env:windir -FileName config.xml Directory= C:\\windows FileName=config.xml 管道脚本 @' begin { Write-Host \u0026quot;管道脚本环境初始化\u0026quot; } process { $ele=$_ if($_.Extension -ne \u0026quot;\u0026quot;) { switch($_.Extension.tolower()) { \u0026quot;.ps1\u0026quot; {\u0026quot;脚本文件：\u0026quot;+ $ele.name} \u0026quot;.txt\u0026quot; {\u0026quot;文本文件：\u0026quot;+ $ele.Name} \u0026quot;.gz\u0026quot; {\u0026quot;压缩文件：\u0026quot;+ $ele.Name} } } } end { Write-Host \u0026quot;管道脚本环境恢复\u0026quot; } '@ \u0026gt; pipeline.ps1 ls | .\\pipeline.ps1 识别和处理异常 # 错误不抛出，脚本也会继续执行 $ErrorActionPreference='SilentlyContinue' Remove-Item \u0026quot;文件不存在\u0026quot; | Out-Null If (!$?) { \u0026quot;发生异常，异常信息为$($error[0])\u0026quot;; break } \u0026quot;删除文件成功!\u0026quot; 操作字符串 详细参见这里\n操作正则表达式 详细参见这里\n操作文件 详细参见这里\n操作注册表 详细参见这里\n操作ini文件 详细参见这里\n导入模块 详细参见这里\n操作IIS Win2008 *,角色\u0026ndash;\u0026gt;添加角色\u0026mdash;\u0026gt;功能工具下面的'IIS管理脚本和工具\u0026rsquo; Win7 在卸载程序中，点击'打开或关闭Windows功能\u0026rsquo;\u0026mdash;\u0026gt;'Internet信息服务\u0026rsquo;\u0026mdash;\u0026gt;'Web管理工具\u0026rsquo;\u0026mdash;\u0026gt;'IIS管理脚本和工具\u0026rsquo;\n主要用到的方法有\n#创建站点 $site = New-Item IIS:\\Sites\\$siteName -bindings $bindings -physicalPath $physicalPath -ErrorAction Stop ... #创建应用程序池 $apool = New-Item IIS:\\AppPools\\$appPool Set-ItemProperty IIS:\\AppPools\\$appPool managedRuntimeVersion $runtimeVersion #1:Classic or 0:Integrated Set-ItemProperty IIS:\\AppPools\\$appPool managedPipelineMode $pipelineMode ... #关联程序池 Set-ItemProperty IIS:\\Sites\\$siteName -name applicationPool -value $apool ... #创建应用程序 $app = New-Item IIS:\\Sites\\$siteName\\$appName -physicalPath $appPhysPath -type Application $site = Get-Item \u0026quot;IIS:\\Sites\\$siteName\u0026quot; Set-ItemProperty IIS:\\Sites\\$siteName\\$appName -name applicationPool -value $site.applicationPool ... #获取站点 $site = Get-Item \u0026quot;IIS:\\Sites\\$siteName\u0026quot; -ErrorAction Stop ... #获取应用程序 $app = Get-Item \u0026quot;IIS:\\Sites\\$siteName\\$appName\u0026quot; -ErrorAction Stop ... 详细参见这里\n参考  https://social.technet.microsoft.com/wiki/contents/articles/21016.how-to-install-windows-powershell-4-0.aspx https://github.com/PowerShell/Win32-OpenSSH/wiki/Install-Win32-OpenSSH http://www.wowotech.net/soft/vim_in_powershell.html http://www.pstips.net/powershell-online-tutorials/  ","permalink":"https://jeremyxu2010.github.io/2018/02/powershell%E5%AD%A6%E4%B9%A0%E5%A4%87%E5%BF%98/","tags":["powershell","windows"],"title":"powershell学习备忘"},{"categories":["容器编排"],"contents":"环境介绍及准备 操作系统采用Centos7.3 64位，细节如下：\n[root@k8s-master ~]# uname -a Linux k8s-master 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux [root@k8s-master ~]# cat /etc/redhat-release CentOS Linux release 7.2.1511 (Core) 主机信息 本文准备了三台机器用于部署k8s的运行环境，细节如下：\n   节点及功能 主机名 IP     master、etcd、registry k8s-master 10.211.55.6   node1 k8s-node-1 10.211.55.7   node2 k8s-node-2 10.211.55.8    设置三台机器的主机名： master上执行：\n[root@localhost ~]# hostnamectl --static set-hostname k8s-master node1上执行：\n[root@localhost ~]# hostnamectl --static set-hostname k8s-node-1 node2上执行：\n[root@localhost ~]# hostnamectl --static set-hostname k8s-node-2 在三台机器上设置hosts，均执行如下命令：\necho '10.211.55.6 k8s-master 10.211.55.6 etcd 10.211.55.6 registry 10.211.55.7 k8s-node-1 10.211.55.8 k8s-node-2' \u0026gt;\u0026gt; /etc/hosts 关闭三台机器上的防火墙 systemctl disable firewalld.service systemctl stop firewalld.service 部署etcd k8s运行依赖etcd，需要先部署etcd，本文采用yum方式安装：\n# yum install -y etcd yum安装的etcd默认配置文件在/etc/etcd/etcd.conf。编辑配置文件，更改以下信息：\nETCD_NAME=master ETCD_LISTEN_CLIENT_URLS=\u0026quot;http://0.0.0.0:2379,http://0.0.0.0:4001\u0026quot; ETCD_ADVERTISE_CLIENT_URLS=\u0026quot;http://etcd:2379,http://etcd:4001\u0026quot; 启动并验证状态\n# systemctl start etcd # systemctl enable etcd # etcdctl set testdir/testkey0 0 0 # etcdctl get testdir/testkey0 0 # etcdctl -C http://etcd:4001 cluster-health member 8e9e05c52164694d is healthy: got healthy result from http://0.0.0.0:2379 cluster is healthy # etcdctl -C http://etcd:2379 cluster-health member 8e9e05c52164694d is healthy: got healthy result from http://0.0.0.0:2379 cluster is healthy 扩展：Etcd集群部署参见——http://www.cnblogs.com/zhenyuyaodidiao/p/6237019.html\n部署master 安装Docker [root@k8s-master ~]# yum install -y docker 配置Docker配置文件 使其允许从registry中拉取镜像。增加如下一行： OPTIONS=\u0026rsquo;–insecure-registry registry:5000\u0026rsquo;\n[root@k8s-master ~]# vim /etc/sysconfig/docker # /etc/sysconfig/docker # Modify these options if you want to change the way the docker daemon runs OPTIONS='--selinux-enabled --log-driver=journald --signature-verification=false' if [ -z \u0026quot;${DOCKER_CERT_PATH}\u0026quot; ]; then DOCKER_CERT_PATH=/etc/docker fi OPTIONS='--insecure-registry registry:5000' 设置使用阿里云的docker加速器 cp -n /lib/systemd/system/docker.service /etc/systemd/system/docker.service sed -i \u0026quot;s|ExecStart=/usr/bin/dockerd-current|ExecStart=/usr/bin/dockerd-current --registry-mirror=\u0026lt;your accelerate address\u0026gt;|g\u0026quot; /etc/systemd/system/docker.service systemctl daemon-reload systemctl restart docker.service 设置开机自启动并开启服务 # systemctl enable docker.service # systemctl restart docker.service 安装kubernets [root@k8s-master ~]# yum install -y kubernetes 搭建及运行registry docker pull registry:2 // 将registry的数据卷与本地关联，便于管理和备份registry数据 docker run -d -p 5000:5000 --name registry -v /data/registry:/var/lib/registry registry:2 配置并启动kubernetes 在kubernetes master上需要运行以下组件：\n Kubernets API Server Kubernets Controller Manager Kubernets Scheduler  相应的要更改以下几个配置中带颜色部分信息：\n修改/etc/kubernetes/apiserver KUBE_API_ADDRESS=”–insecure-bind-address=0.0.0.0” KUBE_API_PORT=”–port=8080” KUBE_ETCD_SERVERS=”–etcd-servers=http://etcd:2379“ KUBE_ADMISSION_CONTROL=”–admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ResourceQuota” 修改/etc/kubernetes/config KUBE_MASTER=\u0026quot;–master=http://k8s-master:8080\u0026quot; 启动服务并设置开机自启动 # systemctl enable kube-apiserver.service # systemctl start kube-apiserver.service # systemctl enable kube-controller-manager.service # systemctl start kube-controller-manager.service # systemctl enable kube-scheduler.service # systemctl start kube-scheduler.service 部署node 安装docker 参见master的docker安装步骤\n安装kubernets 参见master的kubernets安装步骤\n配置并启动kubernetes 在kubernetes node上需要运行以下组件：\n Kubelet Kubernets Proxy  相应的要更改以下几个配置文中信息：\n修改 /etc/kubernetes/config KUBE_MASTER=\u0026quot;–master=http://k8s-master:8080\u0026quot; 修改/etc/kubernetes/kubelet KUBELET_ADDRESS=\u0026quot;–address=0.0.0.0\u0026quot; KUBELET_HOSTNAME=\u0026quot;–hostname-override=k8s-node-1\u0026quot; (注意第二台要写 k8s-node-2) KUBELET_API_SERVER=\u0026quot;–api-servers=http://k8s-master:8080\u0026quot; 启动服务并设置开机自启动 # systemctl enable kubelet.service # systemctl start kubelet.service # systemctl enable kube-proxy.service # systemctl start kube-proxy.service 查看状态 在master上查看集群中节点及节点状态\n# kubectl -s http://k8s-master:8080 get node NAME STATUS AGE k8s-node-1 Ready 3m k8s-node-2 Ready 16s # kubectl get nodes NAME STATUS AGE k8s-node-1 Ready 3m k8s-node-2 Ready 43s 至此，已经搭建了一个kubernetes集群。\n创建Overlay网络——Flannel 安装Flannel 在master、node上均执行如下命令，进行安装\n# yum install -y flannel 配置Flannel master、node上均编辑/etc/sysconfig/flanneld，修改以下配置：\n# etcd url location. Point this to the server where etcd runs FLANNEL_ETCD_ENDPOINTS=\u0026quot;http://etcd:2379\u0026quot; 配置etcd中关于flannel的key Flannel使用Etcd进行配置，来保证多个Flannel实例之间的配置一致性，所以需要在etcd上进行如下配置：\n# etcdctl mk /atomic.io/network/config '{ \u0026quot;Network\u0026quot;: \u0026quot;10.0.0.0/16\u0026quot; }' { \u0026quot;Network\u0026quot;: \u0026quot;10.0.0.0/16\u0026quot; } 启动 启动Flannel之后，需要依次重启docker、kubernete。 在master执行：\nsystemctl enable flanneld.service systemctl start flanneld.service service docker restart systemctl restart kube-apiserver.service systemctl restart kube-controller-manager.service systemctl restart kube-scheduler.service 在node上执行：\nsystemctl enable flanneld.service systemctl start flanneld.service service docker restart systemctl restart kubelet.service systemctl restart kube-proxy.service Flannel网络 Flannel算是k8s里最简单的网络了，这里找到一篇文章可以帮忙理解Flannel网络。\n测试 # docker pull nginx # 从外网registry拉一个nginx镜像过来 # docker tag nginx registry:5000/nginx # 为本地镜像打tag # docker push registry:5000/nginx # 推送至本地registry # docker rmi registry:5000/nginx # 删除本地镜像 cat \u0026lt;\u0026lt; EOF \u0026gt;nginx.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx spec: replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: registry:5000/nginx ports: - containerPort: 80 resources: requests: cpu: 400m EOF # kubectl create -f nginx.yaml #创建nginx-dpmt部署 cat \u0026lt;\u0026lt; EOF \u0026gt;nginx-svc.yaml apiVersion: v1 kind: Service metadata: name: nginx-svc labels: app: nginx-svc spec: type: NodePort selector: app: nginx ports: - port: 80 targetPort: 80 nodePort: 30088 EOF # kubectl create -f nginx-svc.yaml # 创建nginx-svc服务 # kubectl describe service nginx-svc Name:\tnginx-svc Namespace:\tdefault Labels:\tapp=nginx-svc Selector:\tapp=nginx Type:\tNodePort IP:\t10.254.53.185 Port:\t\u0026lt;unset\u0026gt;\t80/TCP NodePort:\t\u0026lt;unset\u0026gt;\t30088/TCP Endpoints:\t10.0.19.2:80,10.0.4.2:80 Session Affinity:\tNone No events. # curl http://k8s-node-1:30088/ # 通过nodePort测试nginx服务 测试过程中遇到两个问题：\n  pod服务一直处于 ContainerCreating状态，后来参考这里，安装了rhsm相关的包解决了。\n  nginx-svc.yaml文件中spec.selector.app的名称与nginx.yaml中的spec.template.metadata.labels.app不一致，这个导致一直无法通过NodePort访问服务。\n  参考  http://qinghua.github.io/kubernetes-deployment/ http://wdxtub.com/2017/06/05/k8s-note/ https://jimmysong.io/kubernetes-handbook/guide/accessing-kubernetes-pods-from-outside-of-the-cluster.html http://tonybai.com/2017/01/17/understanding-flannel-network-for-kubernetes/ http://www.cnblogs.com/puroc/p/6297851.html https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/  ","permalink":"https://jeremyxu2010.github.io/2018/01/centos7%E9%83%A8%E7%BD%B2k8s%E9%9B%86%E7%BE%A4/","tags":["docker","k8s"],"title":"CentOS7部署k8s集群"},{"categories":["python开发"],"contents":"写py2、py3兼容的代码 用到一段时间python，之前也重点复习了一次python3。但工作中运行环境是python2.7，于是要求写出py2、py3都兼容的代码。下面将涉及到的几点技巧列举出来以备忘。\nprint函数 py3中print语句没有了，取而代之的是print()函数。 Python 2.6与Python 2.7部分地支持这种形式的print语法。因此保险起见，新写的代码都使用print函数。\nfrom __future__ import print_function print(\u0026#34;fish\u0026#34;, \u0026#34;panda\u0026#34;, sep=\u0026#39;, \u0026#39;) Unicode Python 2 有 ASCII str() 类型，unicode() 是单独的，不是 byte 类型。\n现在， 在 Python 3，我们最终有了 Unicode (utf-8) 字符串，以及一个字节类：byte 和 bytearrays。\n由于 Python3.X 源码文件默认使用utf-8编码，这就使得以下代码是合法的：\n\u0026gt;\u0026gt;\u0026gt; 中国 = 'china' \u0026gt;\u0026gt;\u0026gt;print(中国) china Python 2.x\n\u0026gt;\u0026gt;\u0026gt; str = \u0026quot;我爱北京天安门\u0026quot; \u0026gt;\u0026gt;\u0026gt; str '\\xe6\\x88\\x91\\xe7\\x88\\xb1\\xe5\\x8c\\x97\\xe4\\xba\\xac\\xe5\\xa4\\xa9\\xe5\\xae\\x89\\xe9\\x97\\xa8' \u0026gt;\u0026gt;\u0026gt; str = u\u0026quot;我爱北京天安门\u0026quot; \u0026gt;\u0026gt;\u0026gt; str u'\\u6211\\u7231\\u5317\\u4eac\\u5929\\u5b89\\u95e8' Python 3.x\n\u0026gt;\u0026gt;\u0026gt; str = \u0026quot;我爱北京天安门\u0026quot; \u0026gt;\u0026gt;\u0026gt; str '我爱北京天安门' 个人还是喜欢py3的这种明确两种不同类型的方案，因此新写的代码都使用以下方案。\nfrom __future__ import unicode_literals txt=\u0026#39;中国\u0026#39; \u0026gt;\u0026gt;\u0026gt; txt u\u0026#39;\\u4e2d\\u56fd\u0026#39; \u0026gt;\u0026gt;\u0026gt; type(txt) \u0026lt;type \u0026#39;unicode\u0026#39;\u0026gt; \u0026gt;\u0026gt;\u0026gt; print(txt) 中国 arr=b\u0026#39;abcd\u0026#39; \u0026gt;\u0026gt;\u0026gt; arr \u0026#39;abcd\u0026#39; \u0026gt;\u0026gt;\u0026gt; type(arr) \u0026lt;type \u0026#39;str\u0026#39;\u0026gt; \u0026gt;\u0026gt;\u0026gt; print(arr) abcd 除法运算 在python 2.x中/除法就跟我们熟悉的大多数语言，比如Java啊C啊差不多，整数相除的结果是一个整数，把小数部分完全忽略掉，浮点数除法会保留小数点的部分得到一个浮点数的结果。\n在python 3.x中/除法不再这么做了，对于整数之间的相除，结果也会是浮点数。\n而对于//除法，这种除法叫做floor除法，会对除法的结果自动进行一个floor操作，在python 2.x和python 3.x中是一致的。注意的是floor除法并不是舍弃小数部分，而是执行floor操作，如果要截取小数部分，那么需要使用math模块的trunc函数。\n个人还是喜欢py3这种方案，毕竟是从java转过来的，因此新定的代码都使用以下方案。\nfrom __future__ import division \u0026gt;\u0026gt;\u0026gt; 1/2 0.5 \u0026gt;\u0026gt;\u0026gt; 1//2 0 \u0026gt;\u0026gt;\u0026gt; trunc(1/2) 0 \u0026gt;\u0026gt;\u0026gt; -1//2 -1 \u0026gt;\u0026gt;\u0026gt; trunc(-1/2) 0 异常 在 Python 3 中处理异常也轻微的改变了，在 Python 3 中我们现在使用 as 作为关键词。\n捕获异常的语法由 except exc, var 改为 except exc as var。\n使用语法except (exc1, exc2) as var可以同时捕获多种类别的异常。 Python 2.6已经支持这两种语法。\n 在2.x时代，所有类型的对象都是可以被直接抛出的，在3.x时代，只有继承自BaseException的对象才可以被抛出。 2.x raise语句使用逗号将抛出对象类型和参数分开，3.x取消了这种奇葩的写法，直接调用构造函数抛出对象即可。  这里倒没有异议了，本来就常见原来py2那种奇葩写法很奇怪，只使用py3的写法就可以了。\ntry: raise BaseException(\u0026#39;fdf\u0026#39;) except BaseException as err: print(err) 八进制字面量表示 八进制数必须写成0o777，原来的形式0777不能用了；二进制必须写成0b111。\n新增了一个bin()函数用于将一个整数转换成二进制字串。 Python 2.6已经支持这两种语法。\n在Python 3.x中，表示八进制字面量的方式只有一种，就是0o1000。\n很简单，只使用py3支持的写法。\n不等运算符 Python 2.x中不等于有两种写法 != 和 \u0026lt;\u0026gt;。\nPython 3.x中去掉了\u0026lt;\u0026gt;, 只有!=一种写法，还好，我从来没有使用\u0026lt;\u0026gt;的习惯。\n数据类型  Py3.X去除了long类型，现在只有一种整型——int，但它的行为就像2.X版本的long 新增了bytes类型，对应于2.X版本的八位串  这里如果要进行类型判断，优先使用six模块提供的兼容功能。\n   six.class_types\nPossible class types. In Python 2, this encompasses old-style and new-style classes. In Python 3, this is just new-styles.\n  six.integer_types\nPossible integer types. In Python 2, this is long and int, and in Python 3, just int.\n  six.string_types\nPossible types for text data. This is basestring() in Python 2 and str in Python 3.\n  six.text_type\nType for representing (Unicode) textual data. This is unicode() in Python 2 and str in Python 3.\n  six.binary_type\nType for representing binary data. This is str in Python 2 and bytes in Python 3.\n   import six def dispatch_types(value): if isinstance(value, six.integer_types): handle_integer(value) elif isinstance(value, six.class_types): handle_class(value) elif isinstance(value, six.string_types): handle_string(value) dict的相关方法调整 dict的.keys()、.items 和.values()方法返回迭代器，而之前的iterkeys()等函数都被废弃。同时去掉的还有 dict.has_key()，用 in替代它吧。\n这里还是使用six模块提供的兼容功能。\n   six.iterkeys(dictionary, **kwargs)\nReturns an iterator over dictionary‘s keys. This replaces dictionary.iterkeys() on Python 2 and dictionary.keys() on Python 3. kwargs are passed through to the underlying method.\n  six.itervalues(dictionary, **kwargs)\nReturns an iterator over dictionary‘s values. This replaces dictionary.itervalues() on Python 2 and dictionary.values() on Python 3. kwargs are passed through to the underlying method.\n  six.iteritems(dictionary, **kwargs)\nReturns an iterator over dictionary‘s items. This replaces dictionary.iteritems() on Python 2 and dictionary.items() on Python 3. kwargs are passed through to the underlying method.\n  six.iterlists(dictionary, **kwargs)\nCalls dictionary.iterlists() on Python 2 and dictionary.lists() on Python 3. No builtin Python mapping type has such a method; this method is intended for use with multi-valued dictionaries like Werkzeug’s.kwargs are passed through to the underlying method.\n  six.viewkeys(dictionary)\nReturn a view over dictionary‘s keys. This replaces dict.viewkeys() on Python 2.7 and dict.keys() on Python 3.\n  six.viewvalues(dictionary)\nReturn a view over dictionary‘s values. This replaces dict.viewvalues() on Python 2.7 and dict.values() on Python 3.\n  six.viewitems(dictionary)\nReturn a view over dictionary‘s items. This replaces dict.viewitems() on Python 2.7 and dict.items() on Python 3.\n   标准库及函数名称变更 py3重新组织了一些标准库及一些函数，为了保证在py2、py3下代码都工作正常，这里使用six模块提供的兼容功能。\nfrom six.moves.cPickle import loads Supported renames:\n   Name Python 2 name Python 3 name     builtins __builtin__ builtins   configparser ConfigParser configparser   copyreg copy_reg copyreg   cPickle cPickle pickle   cStringIO cStringIO.StringIO() io.StringIO   dbm_gnu gdbm dbm.gnu   _dummy_thread dummy_thread _dummy_thread   email_mime_multipart email.MIMEMultipart email.mime.multipart   email_mime_nonmultipart email.MIMENonMultipart email.mime.nonmultipart   email_mime_text email.MIMEText email.mime.text   email_mime_base email.MIMEBase email.mime.base   filter itertools.ifilter() filter()   filterfalse itertools.ifilterfalse() itertools.filterfalse()   getcwd os.getcwdu() os.getcwd()   getcwdb os.getcwd() os.getcwdb()   http_cookiejar cookielib http.cookiejar   http_cookies Cookie http.cookies   html_entities htmlentitydefs html.entities   html_parser HTMLParser html.parser   http_client httplib http.client   BaseHTTPServer BaseHTTPServer http.server   CGIHTTPServer CGIHTTPServer http.server   SimpleHTTPServer SimpleHTTPServer http.server   input raw_input() input()   intern intern() sys.intern()   map itertools.imap() map()   queue Queue queue   range xrange() range   reduce reduce() functools.reduce()   reload_module reload() imp.reload(), importlib.reload() on Python 3.4+   reprlib repr reprlib   shlex_quote pipes.quote shlex.quote   socketserver SocketServer socketserver   _thread thread _thread   tkinter Tkinter tkinter   tkinter_dialog Dialog tkinter.dialog   tkinter_filedialog FileDialog tkinter.FileDialog   tkinter_scrolledtext ScrolledText tkinter.scrolledtext   tkinter_simpledialog SimpleDialog tkinter.simpledialog   tkinter_ttk ttk tkinter.ttk   tkinter_tix Tix tkinter.tix   tkinter_constants Tkconstants tkinter.constants   tkinter_dnd Tkdnd tkinter.dnd   tkinter_colorchooser tkColorChooser tkinter.colorchooser   tkinter_commondialog tkCommonDialog tkinter.commondialog   tkinter_tkfiledialog tkFileDialog tkinter.filedialog   tkinter_font tkFont tkinter.font   tkinter_messagebox tkMessageBox tkinter.messagebox   tkinter_tksimpledialog tkSimpleDialog tkinter.simpledialog   urllib.parse See six.moves.urllib.parse urllib.parse   urllib.error See six.moves.urllib.error urllib.error   urllib.request See six.moves.urllib.request urllib.request   urllib.response See six.moves.urllib.response urllib.response   urllib.robotparser robotparser urllib.robotparser   urllib_robotparser robotparser urllib.robotparser   UserDict UserDict.UserDict collections.UserDict   UserList UserList.UserList collections.UserList   UserString UserString.UserString collections.UserString   winreg _winreg winreg   xmlrpc_client xmlrpclib xmlrpc.client   xmlrpc_server SimpleXMLRPCServer xmlrpc.server   xrange xrange() range   zip itertools.izip() zip()   zip_longest itertools.izip_longest() itertools.zip_longest()    这里用得比较多的是：\nimport six.moves.configparser import six.moves.cPickle import six.moves.cStringIO import six.moves.filter import six.moves.filterfalse import six.moves.getcwd import six.moves.http_cookies import six.moves.html_entities import six.moves.html_parser import six.moves.http_client import six.moves.BaseHTTPServer import six.moves.CGIHTTPServer import six.moves.SimpleHTTPServer import six.moves.input import six.moves.map import six.moves.queue import six.moves.range import six.moves.reduce import six.moves.socketserver import six.moves.zip import six.moves.zip_longest import six.moves.urllib.parse import six.moves.urllib.error import six.moves.urllib.request import six.moves.urllib.response 只有按这个方案导入其它模块，即可保证在py2、py3下都可正确导入模块，详细可参看six模块的文档。\n版本指示变量 最后如果在py2、py3下逻辑不一致，可使用版本指示变量。\n   six.PY2\nA boolean indicating if the code is running on Python 2.\n  six.PY3\nA boolean indicating if the code is running on Python 3.\n   import six if six.PY2: # do some thing pass elif six.PY3: # do other thing pass ","permalink":"https://jeremyxu2010.github.io/2017/11/%E5%86%99py2py3%E5%85%BC%E5%AE%B9%E7%9A%84%E4%BB%A3%E7%A0%81/","tags":["python"],"title":"写py2、py3兼容的代码"},{"categories":["java开发"],"contents":"最近阿里发布了一个插件p3c，用于进行Java开发规约的检查扫描。由于插件的代码是开源，于是第一时间也翻查了代码，发现目前实现的检查规则主要在/p3c-pmd/src/main/resources/rulesets、/idea-plugin/p3c-common/src/main/kotlin/com/alibaba/p3c/idea/inspection/standalone目录下。将规则大致看了下，这里将自己平时开发不太注意但它提到的几点记录一下。\n注释掉代码时留下注释原因 平时由于某些原因，不会删除代码，而只是临时将代码注释起来，应按照规范留下注释原因。For codes which are temporarily removed and likely to be reused, use /// to add a reasonable note.代码示例：\npublic static void hello() { /// Business is stopped temporarily by the owner.  // Business business = new Business();  // business.active();  System.out.println(\u0026#34;it\u0026#39;s finished\u0026#34;); } 线程池必要使用ThreadPoolExecutor的方式 线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样 的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。 说明：Executors 返回的线程池对象的弊端如下：\n FixedThreadPool 和 SingleThreadPool:  允许的请求队列长度为 Integer.MAX_VALUE，可能会堆积大量的请求，从而导致 OOM。\n CachedThreadPool 和 ScheduledThreadPool:  允许的创建线程数量为 Integer.MAX_VALUE，可能会创建大量的线程，从而导致 OOM。\n平时在开发中，不要手动地创建线程，最好使用线程池，并且线程要命名。\n有两种办法创建线程池：编码法、配置法。\n编码法：\n// use org.apache.commons.lang3.concurrent.BasicThreadFactory ScheduledExecutorService executorService = new ScheduledThreadPoolExecutor( 1, new BasicThreadFactory.Builder().namingPattern(\u0026#34;example-schedule-pool-%d\u0026#34;).daemon(true).build()); 或\n// use com.google.common.util.concurrent.ThreadFactoryBuilder ThreadFactory namedThreadFactory = new ThreadFactoryBuilder() .setNameFormat(\u0026#34;demo-pool-%d\u0026#34;).build(); //Common Thread Pool ExecutorService pool = new ThreadPoolExecutor( 5, 200, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy()); 配置法：\n\u0026lt;bean id=\u0026#34;threadFactory\u0026#34; class=\u0026#34;org.springframework.scheduling.concurrent.CustomizableThreadFactory\u0026#34;\u0026gt; \u0026lt;constructor-arg value=\u0026#34;Custom-prefix-\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;daemon\u0026#34; value=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;rejectedExecutionHandler\u0026#34; class=\u0026#34;java.util.concurrent.ThreadPoolExecutor.AbortPolicy\u0026#34;\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;userThreadPool\u0026#34; class=\u0026#34;org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;corePoolSize\u0026#34; value=\u0026#34;10\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;maxPoolSize\u0026#34; value=\u0026#34;100\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;queueCapacity\u0026#34; value=\u0026#34;2000\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;threadFactory\u0026#34; ref=\u0026#34;threadFactory\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;rejectedExecutionHandler\u0026#34; ref=\u0026#34;rejectedExecutionHandler\u0026#34;/\u0026gt; \u0026lt;/bean\u0026gt; 在代码里使用：\nuserThreadPool.execute(new Runnable() { @Override public void run() { System.out.println(Thread.currentThread().getName()); } }); 不要调用静态的SimpleDateFormat属性的format方法 由于SimpleDateFormat是非线程安全的，不要定义一个静态的SimpleDateFormat属性，然后调用其format方法。有3种方法避免这种并发冲突：\nprivate static final String FORMAT = \u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;; public String getFormat(Date date){ SimpleDateFormat dateFormat = new SimpleDateFormat(FORMAT); return sdf.format(date); } private static final SimpleDateFormat SIMPLE_DATE_FORMAT = new SimpleDateFormat(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;); public void getFormat(){ synchronized (sdf){ sdf.format(new Date()); } …; } private static final ThreadLocal\u0026lt;DateFormat\u0026gt; DATE_FORMATTER = new ThreadLocal\u0026lt;DateFormat\u0026gt;() { @Override protected DateFormat initialValue() { return new SimpleDateFormat(\u0026#34;yyyy-MM-dd\u0026#34;); } }; 如果是 JDK8 的应用，可以使用 Instant 代替 Date，LocalDateTime 代替 Calendar，DateTimeFormatter 代替 SimpleDateFormat，官方给出的解释：simple beautiful strong immutable thread-safe。\nThreadLocal里设置的属性要记得删除 public class UserHolder { private static final ThreadLocal\u0026lt;User\u0026gt; userThreadLocal = new ThreadLocal\u0026lt;User\u0026gt;(); public static void set(User user){ userThreadLocal.set(user); } public static User get(){ return userThreadLocal.get(); } public static void remove(){ userThreadLocal.remove(); } } public class UserInterceptor extends HandlerInterceptorAdapter { @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { UserHolder.set(new User()); return true; } @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { UserHolder.remove(); } } 避免 Random 实例被多线程使用 避免 Random 实例被多线程使用，虽然共享该实例是线程安全的，但会因竞争同一 seed 导致的性能下降。 说明：Random 实例包括 java.util.Random 的实例或者 Math.random()的方式。 正例：在 JDK7 之后，可以直接使用 API ThreadLocalRandom，而在 JDK7 之前，需要编码保 证每个线程持有一个实例。\n事务必须标注回滚策略 @Service @Transactional(rollbackFor = Exception.class) public class UserServiceImpl implements UserService { @Override public void save(User user) { //some code  //db operation  } } @Service public class UserServiceImpl implements UserService { @Override @Transactional(rollbackFor = Exception.class) public void save(User user) { //some code  //db operation  } } @Service public class UserServiceImpl implements UserService { @Autowired private DataSourceTransactionManager transactionManager; @Override @Transactional public void save(User user) throws UserException { DefaultTransactionDefinition def = new DefaultTransactionDefinition(); // explicitly setting the transaction name is something that can only be done programmatically  def.setName(\u0026#34;SomeTxName\u0026#34;); def.setPropagationBehavior(TransactionDefinition.PROPAGATION_REQUIRED); TransactionStatus status = transactionManager.getTransaction(def); try { // execute your business logic here  //db operation  transactionManager.commit(status); } catch (Exception ex) { transactionManager.rollback(status); throw new UserException(ex); } } } 避免使用Apache的BeanUtils Apache BeanUtils性能较差，可以使用其他方案比如Spring BeanUtils, Cglib BeanCopier。\n使用System.currentTimeMillis()获取当前毫秒数 获取当前毫秒数：System.currentTimeMillis(); 而不是new Date().getTime();\n集合类的一些注意事项  List转化为数组  Integer[] b = (Integer [])c.toArray(new Integer[c.size()]);   使用工具类 Arrays.asList()把数组转换成集合时，不能使用其修改集合相关的方 法，它的 add/remove/clear 方法会抛出 UnsupportedOperationException 异常。\n  ArrayList的subList结果不可强转成ArrayList，否则会抛出ClassCastException 异常，即 java.util.RandomAccessSubList cannot be cast to java.util.ArrayList.\n说明：subList 返回的是 ArrayList 的内部类 SubList，并不是 ArrayList ，而是 ArrayList 的一个视图，对于 SubList 子列表的所有操作最终会反映到原列表上。\n  集合初始化时，指定集合初始值大小。 说明：HashMap 使用 HashMap(int initialCapacity) 初始化， 正例：initialCapacity = (需要存储的元素个数 / 负载因子) + 1。注意负载因子（即 loader factor）默认为 0.75，如果暂时无法确定初始值大小，请设置为 16（即默认值）。 反例：HashMap 需要放置 1024 个元素，由于没有设置容量初始大小，随着元素不断增加，容 量 7 次被迫扩大，resize 需要重建 hash 表，严重影响性能。\n  高度注意 Map 类集合 K/V 能不能存储 null 值的情况，如下表格:\n  总结 上面只列了一些个人平时没在注意的地方，完整的编码规约见阿里巴巴Java开发手册。另外网上还有一个白话版。\n","permalink":"https://jeremyxu2010.github.io/2017/10/java%E5%BC%80%E5%8F%91%E5%B0%8F%E6%8A%80%E5%B7%A7_02/","tags":["java","p3c","concurrent","log"],"title":"Java开发小技巧_02"},{"categories":["python开发"],"contents":"并发编程 启动与停止线程 # Code to execute in an independent thread import time def countdown(n): while n \u0026gt; 0: print(\u0026#39;T-minus\u0026#39;, n) n -= 1 time.sleep(5) # Create and launch a thread from threading import Thread t = Thread(target=countdown, args=(10,)) t.start() if t.is_alive(): print(\u0026#39;Still running\u0026#39;) else: print(\u0026#39;Completed\u0026#39;) t.join() t.daemon=True 如果线程执行一些像I/O这样的阻塞操作，那么通过轮询来终止线程将使得线程之间的协调变得非常棘手。比如，如果一个线程一直阻塞在一个I/O操作上，它就永远无法返回，也就无法检查自己是否已经被结束了。要正确处理这些问题，你需要利用超时循环来小心操作线程。 例子如下：\nclass IOTask: def terminate(self): self._running = False def run(self, sock): # sock is a socket sock.settimeout(5) # Set timeout period while self._running: # Perform a blocking I/O operation w/ timeout try: data = sock.recv(8192) break except socket.timeout: continue # Continued processing ... # Terminated return 判断线程是否已经启动 from threading import Thread, Event import time # Code to execute in an independent thread def countdown(n, started_evt): print(\u0026#39;countdown starting\u0026#39;) started_evt.set() while n \u0026gt; 0: print(\u0026#39;T-minus\u0026#39;, n) n -= 1 time.sleep(5) # Create the event object that will be used to signal startup started_evt = Event() # Launch the thread and pass the startup event print(\u0026#39;Launching countdown\u0026#39;) t = Thread(target=countdown, args=(10,started_evt)) t.start() # Wait for the thread to start started_evt.wait() print(\u0026#39;countdown is running\u0026#39;) wait与notify机制：\nfrom threading import Condition, Thread import time import random num = 0 cv = Condition() def decr_num(): global num while True: while num \u0026lt;=0: with cv: cv.wait(3) num = num - 1 time.sleep(random.randint(0, 3)) def incr_num(): global num while True: time.sleep(random.randint(0, 3)) num = num + 1 with cv: cv.notify_all() def print_num(): while True: print(num) time.sleep(1) if __name__ == \u0026#39;__main__\u0026#39;: decr_thread = Thread(target=decr_num) decr_thread.start() incr_thread = Thread(target=incr_num) incr_thread.start() print_thread = Thread(target=print_num) print_thread.start() 信号量：\nfrom threading import Semaphore, Thread # Worker thread def worker(n, sema): # Wait to be signaled sema.acquire() try: # Do some work print(\u0026#39;Working\u0026#39;, n) finally: sema.release() # Create some threads sema = Semaphore(3) nworkers = 10 for n in range(nworkers): t = Thread(target=worker, args=(n, sema,)) t.start() 线程间通信 从一个线程向另一个线程发送数据最安全的方式可能就是使用 queue 库中的队列了。创建一个被多个线程共享的 Queue 对象，这些线程通过使用 put() 和 get() 操作来向队列中添加或者删除元素。 例如：\nfrom queue import Queue from threading import Thread # A thread that produces data def producer(out_q): while True: # Produce some data ... out_q.put(data) # A thread that consumes data def consumer(in_q): while True: # Get some data data = in_q.get() # Process the data ... # Create the shared queue and launch both threads q = Queue() t1 = Thread(target=consumer, args=(q,)) t2 = Thread(target=producer, args=(q,)) t1.start() t2.start() 当使用队列时，协调生产者和消费者的关闭问题可能会有一些麻烦。一个通用的解决方法是在队列中放置一个特殊的值，当消费者读到这个值的时候，终止执行。例如：\nfrom queue import Queue from threading import Thread # Object that signals shutdown _sentinel = object() # A thread that produces data def producer(out_q): while running: # Produce some data ... out_q.put(data) # Put the sentinel on the queue to indicate completion out_q.put(_sentinel) # A thread that consumes data def consumer(in_q): while True: # Get some data data = in_q.get() # Check for termination if data is _sentinel: in_q.put(_sentinel) break # Process the data ... 使用队列来进行线程间通信是一个单向、不确定的过程。通常情况下，你没有办法知道接收数据的线程是什么时候接收到的数据并开始工作的。不过队列对象提供一些基本完成的特性，比如下边这个例子中的 task_done() 和 join() ：\nfrom queue import Queue from threading import Thread # A thread that produces data def producer(out_q): while running: # Produce some data ... out_q.put(data) # A thread that consumes data def consumer(in_q): while True: # Get some data data = in_q.get() # Process the data ... # Indicate completion in_q.task_done() # Create the shared queue and launch both threads q = Queue() t1 = Thread(target=consumer, args=(q,)) t2 = Thread(target=producer, args=(q,)) t1.start() t2.start() # Wait for all produced items to be consumed q.join() 如果一个线程需要在一个“消费者”线程处理完特定的数据项时立即得到通知，你可以把要发送的数据和一个 Event 放到一起使用，这样“生产者”就可以通过这个Event对象来监测处理的过程了。示例如下：\nfrom queue import Queue from threading import Thread, Event # A thread that produces data def producer(out_q): while running: # Produce some data ... # Make an (data, event) pair and hand it to the consumer evt = Event() out_q.put((data, evt)) ... # Wait for the consumer to process the item evt.wait() # A thread that consumes data def consumer(in_q): while True: # Get some data data, evt = in_q.get() # Process the data ... # Indicate completion evt.set() 给关键部分加锁 import threading class SharedCounter: \u0026#39;\u0026#39;\u0026#39;A counter object that can be shared by multiple threads.\u0026#39;\u0026#39;\u0026#39; def __init__(self, initial_value = 0): self._value = initial_value self._value_lock = threading.Lock() def incr(self,delta=1): \u0026#39;\u0026#39;\u0026#39;Increment the counter with locking\u0026#39;\u0026#39;\u0026#39; with self._value_lock: self._value += delta def decr(self,delta=1): \u0026#39;\u0026#39;\u0026#39;Decrement the counter with locking\u0026#39;\u0026#39;\u0026#39; with self._value_lock: self._value -= delta 防止死锁的加锁机制 import threading from contextlib import contextmanager # Thread-local state to stored information on locks already acquired _local = threading.local() @contextmanager def acquire(*locks): # Sort locks by object identifier locks = sorted(locks, key=lambda x: id(x)) # Make sure lock order of previously acquired locks is not violated acquired = getattr(_local,\u0026#39;acquired\u0026#39;,[]) if acquired and max(id(lock) for lock in acquired) \u0026gt;= id(locks[0]): raise RuntimeError(\u0026#39;Lock Order Violation\u0026#39;) # Acquire all of the locks acquired.extend(locks) _local.acquired = acquired try: for lock in locks: lock.acquire() yield finally: # Release locks in reverse order of acquisition for lock in reversed(locks): lock.release() del acquired[-len(locks):] import threading x_lock = threading.Lock() y_lock = threading.Lock() def thread_1(): while True: with acquire(x_lock, y_lock): print(\u0026#39;Thread-1\u0026#39;) def thread_2(): while True: with acquire(y_lock, x_lock): print(\u0026#39;Thread-2\u0026#39;) t1 = threading.Thread(target=thread_1) t1.daemon = True t1.start() t2 = threading.Thread(target=thread_2) t2.daemon = True t2.start() 避免死锁的主要思想是，单纯地按照对象id递增的顺序加锁不会产生循环依赖，而循环依赖是 死锁的一个必要条件，从而避免程序进入死锁状态。\n保存线程的状态信息 from socket import socket, AF_INET, SOCK_STREAM import threading class LazyConnection: def __init__(self, address, family=AF_INET, type=SOCK_STREAM): self.address = address self.family = AF_INET self.type = SOCK_STREAM self.local = threading.local() def __enter__(self): if hasattr(self.local, \u0026#39;sock\u0026#39;): raise RuntimeError(\u0026#39;Already connected\u0026#39;) self.local.sock = socket(self.family, self.type) self.local.sock.connect(self.address) return self.local.sock def __exit__(self, exc_ty, exc_val, tb): self.local.sock.close() del self.local.sock from functools import partial def test(conn): with conn as s: s.send(b\u0026#39;GET /index.html HTTP/1.0\\r\\n\u0026#39;) s.send(b\u0026#39;Host: www.python.org\\r\\n\u0026#39;) s.send(b\u0026#39;\\r\\n\u0026#39;) resp = b\u0026#39;\u0026#39;.join(iter(partial(s.recv, 8192), b\u0026#39;\u0026#39;)) print(\u0026#39;Got {} bytes\u0026#39;.format(len(resp))) if __name__ == \u0026#39;__main__\u0026#39;: conn = LazyConnection((\u0026#39;www.python.org\u0026#39;, 80)) t1 = threading.Thread(target=test, args=(conn,)) t2 = threading.Thread(target=test, args=(conn,)) t1.start() t2.start() t1.join() t2.join() 创建一个线程池 python自带的concurrent.futures.ThreadPoolExecutor，连提交的任务队列都没有限制的，还好可以比较方便的扩展功能：\nfrom concurrent.futures.thread import ThreadPoolExecutor from threading import Lock, Condition import time class MaxPendingThreadPoolExecutor(ThreadPoolExecutor): def __init__(self, *args, max_pending=0, **kwargs): super().__init__(*args, **kwargs) self._pending_count = 0 self._max_pending = max_pending self._count_lock = Lock() self._cv = Condition() def _decr_pending_count(self): with self._count_lock: self._pending_count -= 1 with self._cv: self._cv.notify_all() def submit(self, fn, *args, **kwargs): while self._max_pending \u0026gt; 0 and self._pending_count \u0026gt;= self._max_pending: with self._cv: self._cv.wait(2) f = super().submit(fn, *args, **kwargs) with self._count_lock: self._pending_count += 1 f.add_done_callback(lambda _ : self._decr_pending_count()) return f def do_work(n): time.sleep(5) print(n) if __name__ == \u0026#39;__main__\u0026#39;: with MaxPendingThreadPoolExecutor(max_workers=2, max_pending=10) as executor: for n in range(50): executor.submit(do_work, n) executor.shutdown() 简单的并行编程 有个程序要执行CPU密集型工作，想利用多核CPU的优势来运行的快一点。concurrent.futures 库提供了一个 ProcessPoolExecutor 类， 可被用来在一个单独的Python解释器中执行计算密集型函数。\nProcessPoolExecutor 的典型用法如下：\nfrom concurrent.futures import ProcessPoolExecutor with ProcessPoolExecutor() as pool: ... do work in parallel using pool ... 其原理是，一个 ProcessPoolExecutor 创建N个独立的Python解释器， N是系统上面可用CPU的个数。你可以通过提供可选参数给 ProcessPoolExecutor(N) 来修改 处理器数量。这个处理池会一直运行到with块中最后一个语句执行完成， 然后处理池被关闭。不过，程序会一直等待直到所有提交的工作被处理完成。\nPython的全局锁问题 # Processing pool (see below for initiazation) pool = None # Performs a large calculation (CPU bound) def some_work(args): ... return result # A thread that calls the above function def some_thread(): while True: ... r = pool.apply(some_work, (args)) ... # Initiaze the pool if __name__ == \u0026#39;__main__\u0026#39;: import multiprocessing pool = multiprocessing.Pool() 这个通过使用一个技巧利用进程池解决了GIL的问题。 当一个线程想要执行CPU密集型工作时，会将任务发给进程池。 然后进程池会在另外一个进程中启动一个单独的Python解释器来工作。 当线程等待结果的时候会释放GIL。 并且，由于计算任务在单独解释器中执行，那么就不会受限于GIL了。 在一个多核系统上面，你会发现这个技术可以让你很好的利用多CPU的优势。\n定义一个Actor任务 from queue import Queue from threading import Thread, Event # Sentinel used for shutdown class ActorExit(Exception): pass class Actor: def __init__(self): self._mailbox = Queue() def send(self, msg): \u0026#39;\u0026#39;\u0026#39;Send a message to the actor\u0026#39;\u0026#39;\u0026#39; self._mailbox.put(msg) def recv(self): \u0026#39;\u0026#39;\u0026#39;Receive an incoming message\u0026#39;\u0026#39;\u0026#39; msg = self._mailbox.get() if msg is ActorExit: raise ActorExit() return msg def close(self): \u0026#39;\u0026#39;\u0026#39;Close the actor, thus shutting it down\u0026#39;\u0026#39;\u0026#39; self.send(ActorExit) def start(self): \u0026#39;\u0026#39;\u0026#39;Start concurrent execution\u0026#39;\u0026#39;\u0026#39; self._terminated = Event() t = Thread(target=self._bootstrap) t.daemon = True t.start() def _bootstrap(self): try: self.run() except ActorExit: pass finally: self._terminated.set() def join(self): self._terminated.wait() def run(self): \u0026#39;\u0026#39;\u0026#39;Run method to be implemented by the user\u0026#39;\u0026#39;\u0026#39; while True: msg = self.recv() # Sample ActorTask class PrintActor(Actor): def run(self): while True: msg = self.recv() print(\u0026#39;Got:\u0026#39;, msg) # Sample use p = PrintActor() p.start() p.send(\u0026#39;Hello\u0026#39;) p.send(\u0026#39;World\u0026#39;) p.close() p.join() 可以以元组形式传递标签消息，让actor执行不同的操作，如下：\nclass TaggedActor(Actor): def run(self): while True: tag, *payload = self.recv() getattr(self,\u0026#39;do_\u0026#39;+tag)(*payload) # Methods correponding to different message tags def do_A(self, x): print(\u0026#39;Running A\u0026#39;, x) def do_B(self, x, y): print(\u0026#39;Running B\u0026#39;, x, y) # Example a = TaggedActor() a.start() a.send((\u0026#39;A\u0026#39;, 1)) # Invokes do_A(1) a.send((\u0026#39;B\u0026#39;, 2, 3)) # Invokes do_B(2,3) 许在一个工作者中运行任意的函数， 并且通过一个特殊的Result对象返回结果：\nfrom threading import Event class Result: def __init__(self): self._evt = Event() self._result = None def set_result(self, value): self._result = value self._evt.set() def result(self): self._evt.wait() return self._result class Worker(Actor): def submit(self, func, *args, **kwargs): r = Result() self.send((func, args, kwargs, r)) return r def run(self): while True: func, args, kwargs, r = self.recv() r.set_result(func(*args, **kwargs)) # Example use worker = Worker() worker.start() r = worker.submit(pow, 2, 3) print(r.result()) 如果放宽对于同步和异步消息发送的要求（标准的Actor模型里消息是异步发送的），类actor对象还可以通过生成器来简化定义。例如：\ndef print_actor(): while True: try: msg = yield # Get a message print(\u0026#39;Got:\u0026#39;, msg) except GeneratorExit: print(\u0026#39;Actor terminating\u0026#39;) # Sample use p = print_actor() next(p) # Advance to the yield (ready to receive) p.send(\u0026#39;Hello\u0026#39;) p.send(\u0026#39;World\u0026#39;) p.close() Actor模型的理解可参考10 分钟了解 Actor 模型\n实现消息发布/订阅模型 from contextlib import contextmanager from collections import defaultdict class Exchange: def __init__(self): self._subscribers = set() def attach(self, task): self._subscribers.add(task) def detach(self, task): self._subscribers.remove(task) @contextmanager def subscribe(self, *tasks): for task in tasks: self.attach(task) try: yield finally: for task in tasks: self.detach(task) def send(self, msg): for subscriber in self._subscribers: subscriber.send(msg) # Dictionary of all created exchanges _exchanges = defaultdict(Exchange) # Return the Exchange instance associated with a given name def get_exchange(name): return _exchanges[name] # Example of using the subscribe() method exc = get_exchange(\u0026#39;name\u0026#39;) with exc.subscribe(task_a, task_b): ... exc.send(\u0026#39;msg1\u0026#39;) exc.send(\u0026#39;msg2\u0026#39;) ... # task_a and task_b detached here 使用生成器代替线程 # Two simple generator functions def countdown(n): while n \u0026gt; 0: print(\u0026#39;T-minus\u0026#39;, n) yield n -= 1 print(\u0026#39;Blastoff!\u0026#39;) def countup(n): x = 0 while x \u0026lt; n: print(\u0026#39;Counting up\u0026#39;, x) yield x += 1 from collections import deque class TaskScheduler: def __init__(self): self._task_queue = deque() def new_task(self, task): \u0026#39;\u0026#39;\u0026#39;Admit a newly started task to the scheduler\u0026#39;\u0026#39;\u0026#39; self._task_queue.append(task) def run(self): \u0026#39;\u0026#39;\u0026#39;Run until there are no more tasks\u0026#39;\u0026#39;\u0026#39; while self._task_queue: task = self._task_queue.popleft() try: # Run until the next yield statement next(task) self._task_queue.append(task) except StopIteration: # Generator is no longer executing pass # Example use sched = TaskScheduler() sched.new_task(countdown(10)) sched.new_task(countdown(5)) sched.new_task(countup(15)) sched.run() 多个线程队列轮询 import queue import socket import os class PollableQueue(queue.Queue): def __init__(self): super().__init__() # Create a pair of connected sockets if os.name == \u0026#39;posix\u0026#39;: self._putsocket, self._getsocket = socket.socketpair() else: # Compatibility on non-POSIX systems server = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server.bind((\u0026#39;127.0.0.1\u0026#39;, 0)) server.listen(1) self._putsocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self._putsocket.connect(server.getsockname()) self._getsocket, _ = server.accept() server.close() def fileno(self): return self._getsocket.fileno() def put(self, item): super().put(item) self._putsocket.send(b\u0026#39;x\u0026#39;) def get(self): self._getsocket.recv(1) return super().get() import select import threading def consumer(queues): \u0026#39;\u0026#39;\u0026#39;Consumer that reads data on multiple queues simultaneously\u0026#39;\u0026#39;\u0026#39; while True: can_read, _, _ = select.select(queues,[],[]) for r in can_read: item = r.get() print(\u0026#39;Got:\u0026#39;, item) q1 = PollableQueue() q2 = PollableQueue() q3 = PollableQueue() t = threading.Thread(target=consumer, args=([q1,q2,q3],)) t.daemon = True t.start() # Feed data to the queues q1.put(1) q2.put(10) q3.put(\u0026#39;hello\u0026#39;) q2.put(15) ... 在Unix系统上面启动守护进程 #!/usr/bin/env python3 # daemon.py import os import sys import atexit import signal def daemonize(pidfile, *, stdin=\u0026#39;/dev/null\u0026#39;, stdout=\u0026#39;/dev/null\u0026#39;, stderr=\u0026#39;/dev/null\u0026#39;): if os.path.exists(pidfile): raise RuntimeError(\u0026#39;Already running\u0026#39;) # First fork (detaches from parent) try: if os.fork() \u0026gt; 0: raise SystemExit(0) # Parent exit except OSError as e: raise RuntimeError(\u0026#39;fork #1 failed.\u0026#39;) os.chdir(\u0026#39;/\u0026#39;) os.umask(0) os.setsid() # Second fork (relinquish session leadership) try: if os.fork() \u0026gt; 0: raise SystemExit(0) except OSError as e: raise RuntimeError(\u0026#39;fork #2 failed.\u0026#39;) # Flush I/O buffers sys.stdout.flush() sys.stderr.flush() # Replace file descriptors for stdin, stdout, and stderr with open(stdin, \u0026#39;rb\u0026#39;, 0) as f: os.dup2(f.fileno(), sys.stdin.fileno()) with open(stdout, \u0026#39;ab\u0026#39;, 0) as f: os.dup2(f.fileno(), sys.stdout.fileno()) with open(stderr, \u0026#39;ab\u0026#39;, 0) as f: os.dup2(f.fileno(), sys.stderr.fileno()) # Write the PID file with open(pidfile,\u0026#39;w\u0026#39;) as f: print(os.getpid(),file=f) # Arrange to have the PID file removed on exit/signal atexit.register(lambda: os.remove(pidfile)) # Signal handler for termination (required) def sigterm_handler(signo, frame): raise SystemExit(1) signal.signal(signal.SIGTERM, sigterm_handler) def main(): import time sys.stdout.write(\u0026#39;Daemon started with pid {}\\n\u0026#39;.format(os.getpid())) while True: sys.stdout.write(\u0026#39;Daemon Alive! {}\\n\u0026#39;.format(time.ctime())) time.sleep(10) if __name__ == \u0026#39;__main__\u0026#39;: PIDFILE = \u0026#39;/tmp/daemon.pid\u0026#39; if len(sys.argv) != 2: print(\u0026#39;Usage: {} [start|stop]\u0026#39;.format(sys.argv[0]), file=sys.stderr) raise SystemExit(1) if sys.argv[1] == \u0026#39;start\u0026#39;: try: daemonize(PIDFILE, stdout=\u0026#39;/tmp/daemon.log\u0026#39;, stderr=\u0026#39;/tmp/dameon.log\u0026#39;) except RuntimeError as e: print(e, file=sys.stderr) raise SystemExit(1) main() elif sys.argv[1] == \u0026#39;stop\u0026#39;: if os.path.exists(PIDFILE): with open(PIDFILE) as f: os.kill(int(f.read()), signal.SIGTERM) else: print(\u0026#39;Not running\u0026#39;, file=sys.stderr) raise SystemExit(1) else: print(\u0026#39;Unknown command {!r}\u0026#39;.format(sys.argv[1]), file=sys.stderr) raise SystemExit(1) 脚本编程与系统管理 通过重定向/管道/文件接受输入 #!/usr/bin/env python3 import fileinput with fileinput.input() as f_input: for line in f_input: print(line, end=\u0026#39;\u0026#39;) 那么你就能以前面提到的所有方式来为此脚本提供输入。假设你将此脚本保存为 filein.py 并将其变为可执行文件， 那么你可以像下面这样调用它，得到期望的输出：\n$ ls | ./filein.py # Prints a directory listing to stdout. $ ./filein.py /etc/passwd # Reads /etc/passwd to stdout. $ ./filein.py \u0026lt; /etc/passwd # Reads /etc/passwd to stdout. 终止程序并给出错误信息 raise SystemExit(\u0026#39;It failed!\u0026#39;) # 也可以使用复杂一些的办法 import sys sys.stderr.write(\u0026#39;It failed!\\n\u0026#39;) raise SystemExit(1) 解析命令行选项 # search.py \u0026#39;\u0026#39;\u0026#39;Hypothetical command-line tool for searching a collection offiles for one or more text patterns.\u0026#39;\u0026#39;\u0026#39; import argparse parser = argparse.ArgumentParser(description=\u0026#39;Search some files\u0026#39;) parser.add_argument(dest=\u0026#39;filenames\u0026#39;,metavar=\u0026#39;filename\u0026#39;, nargs=\u0026#39;*\u0026#39;) parser.add_argument(\u0026#39;-p\u0026#39;, \u0026#39;--pat\u0026#39;,metavar=\u0026#39;pattern\u0026#39;, required=True, dest=\u0026#39;patterns\u0026#39;, action=\u0026#39;append\u0026#39;, help=\u0026#39;text pattern to search for\u0026#39;) parser.add_argument(\u0026#39;-v\u0026#39;, dest=\u0026#39;verbose\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;verbose mode\u0026#39;) parser.add_argument(\u0026#39;-o\u0026#39;, dest=\u0026#39;outfile\u0026#39;, action=\u0026#39;store\u0026#39;, help=\u0026#39;output file\u0026#39;) parser.add_argument(\u0026#39;--speed\u0026#39;, dest=\u0026#39;speed\u0026#39;, action=\u0026#39;store\u0026#39;, choices={\u0026#39;slow\u0026#39;,\u0026#39;fast\u0026#39;}, default=\u0026#39;slow\u0026#39;, help=\u0026#39;search speed\u0026#39;) args = parser.parse_args() # Output the collected arguments print(args.filenames) print(args.patterns) print(args.verbose) print(args.outfile) print(args.speed) 运行时弹出密码输入提示 import getpass user = getpass.getuser() passwd = getpass.getpass() if svc_login(user, passwd): # You must write svc_login() print(\u0026#39;Yay!\u0026#39;) else: print(\u0026#39;Boo!\u0026#39;) 获取终端的大小 \u0026gt;\u0026gt;\u0026gt; import os \u0026gt;\u0026gt;\u0026gt; sz = os.get_terminal_size() \u0026gt;\u0026gt;\u0026gt; sz os.terminal_size(columns=80, lines=24) \u0026gt;\u0026gt;\u0026gt; sz.columns 80 \u0026gt;\u0026gt;\u0026gt; sz.lines 24 \u0026gt;\u0026gt;\u0026gt; 执行外部命令并获取它的输出 import subprocess try: out_bytes = subprocess.check_output([\u0026#39;netstat\u0026#39;,\u0026#39;-a\u0026#39;], timeout=5, stderr=subprocess.STDOUT) out_text = out_bytes.decode(\u0026#39;utf-8\u0026#39;) out_bytes = subprocess.check_output(\u0026#39;grep python | wc \u0026gt; out\u0026#39;, shell=True) except subprocess.CalledProcessError as e: out_bytes = e.output # Output generated before error code = e.returncode # Return code 使用 check_output() 函数是执行外部命令并获取其返回值的最简单方式。 但是，如果你需要对子进程做更复杂的交互，比如给它发送输入，你得采用另外一种方法。 这时候可直接使用 subprocess.Popen 类。例如：\nimport subprocess # Some text to send text = b\u0026#39;\u0026#39;\u0026#39;hello worldthis is a testgoodbye\u0026#39;\u0026#39;\u0026#39; # Launch a command with pipes p = subprocess.Popen([\u0026#39;wc\u0026#39;], stdout = subprocess.PIPE, stdin = subprocess.PIPE) # Send the data and get the output stdout, stderr = p.communicate(text) # To interpret as text, decode out = stdout.decode(\u0026#39;utf-8\u0026#39;) err = stderr.decode(\u0026#39;utf-8\u0026#39;) 复制或者移动文件和目录 import shutil # Copy src to dst. (cp src dst) shutil.copy(src, dst) # Copy files, but preserve metadata (cp -p src dst) shutil.copy2(src, dst) # Copy directory tree (cp -R src dst) shutil.copytree(src, dst) # Move src to dst (mv src dst) shutil.move(src, dst) def ignore_pyc_files(dirname, filenames): return [name in filenames if name.endswith(\u0026#39;.pyc\u0026#39;)] shutil.copytree(src, dst, ignore=ignore_pyc_files) shutil.copytree(src, dst, ignore=shutil.ignore_patterns(\u0026#39;*~\u0026#39;, \u0026#39;*.pyc\u0026#39;)) 创建和解压归档文件 \u0026gt;\u0026gt;\u0026gt; import shutil \u0026gt;\u0026gt;\u0026gt; shutil.unpack_archive(\u0026#39;Python-3.3.0.tgz\u0026#39;) \u0026gt;\u0026gt;\u0026gt; shutil.make_archive(\u0026#39;py33\u0026#39;,\u0026#39;zip\u0026#39;,\u0026#39;Python-3.3.0\u0026#39;) \u0026#39;/Users/beazley/Downloads/py33.zip\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; shutil.get_archive_formats() [(\u0026#39;bztar\u0026#39;, \u0026#34;bzip2\u0026#39;ed tar-file\u0026#34;), (\u0026#39;gztar\u0026#39;, \u0026#34;gzip\u0026#39;ed tar-file\u0026#34;), (\u0026#39;tar\u0026#39;, \u0026#39;uncompressed tar file\u0026#39;), (\u0026#39;zip\u0026#39;, \u0026#39;ZIP file\u0026#39;)] \u0026gt;\u0026gt;\u0026gt; 通过文件名查找文件 #!/usr/bin/env python3.3 import os def findfile(start, name): for relpath, dirs, files in os.walk(start): if name in files: full_path = os.path.join(start, relpath, name) print(os.path.normpath(os.path.abspath(full_path))) if __name__ == \u0026#39;__main__\u0026#39;: findfile(sys.argv[1], sys.argv[2]) 读取配置文件 \u0026gt;\u0026gt;\u0026gt; from configparser import ConfigParser \u0026gt;\u0026gt;\u0026gt; cfg = ConfigParser() \u0026gt;\u0026gt;\u0026gt; cfg.read(\u0026#39;config.ini\u0026#39;) [\u0026#39;config.ini\u0026#39;] \u0026gt;\u0026gt;\u0026gt; cfg.sections() [\u0026#39;installation\u0026#39;, \u0026#39;debug\u0026#39;, \u0026#39;server\u0026#39;] \u0026gt;\u0026gt;\u0026gt; cfg.get(\u0026#39;installation\u0026#39;,\u0026#39;library\u0026#39;) \u0026#39;/usr/local/lib\u0026#39; \u0026gt;\u0026gt;\u0026gt; cfg.getboolean(\u0026#39;debug\u0026#39;,\u0026#39;log_errors\u0026#39;) True \u0026gt;\u0026gt;\u0026gt; cfg.getint(\u0026#39;server\u0026#39;,\u0026#39;port\u0026#39;) 8080 \u0026gt;\u0026gt;\u0026gt; cfg.getint(\u0026#39;server\u0026#39;,\u0026#39;nworkers\u0026#39;) 32 \u0026gt;\u0026gt;\u0026gt; print(cfg.get(\u0026#39;server\u0026#39;,\u0026#39;signature\u0026#39;)) \\================================= Brought to you by the Python Cookbook \\================================= \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; cfg.set(\u0026#39;server\u0026#39;,\u0026#39;port\u0026#39;,\u0026#39;9000\u0026#39;) \u0026gt;\u0026gt;\u0026gt; cfg.set(\u0026#39;debug\u0026#39;,\u0026#39;log_errors\u0026#39;,\u0026#39;False\u0026#39;) \u0026gt;\u0026gt;\u0026gt; import sys \u0026gt;\u0026gt;\u0026gt; cfg.write(sys.stdout) 给简单脚本增加日志功能 import logging def main(): # Configure the logging system logging.basicConfig( filename=\u0026#39;app.log\u0026#39;, level=logging.ERROR, format=\u0026#39;%(levelname)s:%(asctime)s:%(message)s\u0026#39; ) # Variables (to make the calls that follow work) hostname = \u0026#39;www.python.org\u0026#39; item = \u0026#39;spam\u0026#39; filename = \u0026#39;data.csv\u0026#39; mode = \u0026#39;r\u0026#39; # Example logging calls (insert into your program) logging.critical(\u0026#39;Host %sunknown\u0026#39;, hostname) logging.error(\u0026#34;Couldn\u0026#39;t find %r\u0026#34;, item) logging.warning(\u0026#39;Feature is deprecated\u0026#39;) logging.info(\u0026#39;Opening file %r, mode=%r\u0026#39;, filename, mode) logging.debug(\u0026#39;Got here\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: main() 上面的日志配置都是硬编码到程序中的。如果你想使用配置文件， 可以像下面这样修改 basicConfig() 调用：\nimport logging.config def main(): # Configure the logging system logging.config.fileConfig(\u0026#39;logconfig.ini\u0026#39;) ... 创建一个下面这样的文件，名字叫 logconfig.ini ：\n[loggers] keys=root [handlers] keys=defaultHandler [formatters] keys=defaultFormatter [logger_root] level=INFO handlers=defaultHandler qualname=root [handler_defaultHandler] class=FileHandler formatter=defaultFormatter args=('app.log', 'a') [formatter_defaultFormatter] format=%(levelname)s:%(name)s:%(message)s 给函数库增加日志功能 # somelib.py import logging log = logging.getLogger(__name__) log.addHandler(logging.NullHandler()) # Example function (for testing) def func(): log.critical(\u0026#39;A Critical Error!\u0026#39;) log.debug(\u0026#39;A debug message\u0026#39;) 使用这个配置，默认情况下不会打印日志。例如：\n\u0026gt;\u0026gt;\u0026gt; import somelib \u0026gt;\u0026gt;\u0026gt; somelib.func() \u0026gt;\u0026gt;\u0026gt; 不过，如果配置过日志系统，那么日志消息打印就开始生效，例如：\n\u0026gt;\u0026gt;\u0026gt; import logging \u0026gt;\u0026gt;\u0026gt; logging.basicConfig() \u0026gt;\u0026gt;\u0026gt; somelib.func() CRITICAL:somelib:A Critical Error! \u0026gt;\u0026gt;\u0026gt; 实现一个计时器 import time class Timer: def __init__(self, func=time.perf_counter): self.elapsed = 0.0 self._func = func self._start = None def start(self): if self._start is not None: raise RuntimeError(\u0026#39;Already started\u0026#39;) self._start = self._func() def stop(self): if self._start is None: raise RuntimeError(\u0026#39;Not started\u0026#39;) end = self._func() self.elapsed += end - self._start self._start = None def reset(self): self.elapsed = 0.0 @property def running(self): return self._start is not None def __enter__(self): self.start() return self def __exit__(self, *args): self.stop() def countdown(n): while n \u0026gt; 0: n -= 1 with Timer() as t2: countdown(1000000) print(t2.elapsed) time.perf_counter() 和 time.process_time() 都会返回小数形式的秒数时间。 实际的时间值没有任何意义，为了得到有意义的结果，你得执行两次函数然后计算它们的差值。\n限制内存和CPU的使用量 import signal import resource import os def time_exceeded(signo, frame): print(\u0026#34;Time\u0026#39;s up!\u0026#34;) raise SystemExit(1) def set_max_runtime(seconds): # Install the signal handler and set a resource limit soft, hard = resource.getrlimit(resource.RLIMIT_CPU) resource.setrlimit(resource.RLIMIT_CPU, (seconds, hard)) signal.signal(signal.SIGXCPU, time_exceeded) # 程序运行时，SIGXCPU 信号在时间过期时被生成，然后执行清理并退出。 def limit_memory(maxsize): soft, hard = resource.getrlimit(resource.RLIMIT_AS) resource.setrlimit(resource.RLIMIT_AS, (maxsize, hard)) if __name__ == \u0026#39;__main__\u0026#39;: set_max_runtime(15) limit_memory(1024) # 像这样设置了内存限制后，程序运行到没有多余内存时会抛出 MemoryError 异常。 while True: pass 启动一个WEB浏览器 \u0026gt;\u0026gt;\u0026gt; import webbrowser \u0026gt;\u0026gt;\u0026gt; webbrowser.open(\u0026#39;http://www.python.org\u0026#39;) True \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; # Open the page in a new browser window \u0026gt;\u0026gt;\u0026gt; webbrowser.open_new(\u0026#39;http://www.python.org\u0026#39;) True \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; # Open the page in a new browser tab \u0026gt;\u0026gt;\u0026gt; webbrowser.open_new_tab(\u0026#39;http://www.python.org\u0026#39;) True \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; c = webbrowser.get(\u0026#39;firefox\u0026#39;) \u0026gt;\u0026gt;\u0026gt; c.open(\u0026#39;http://www.python.org\u0026#39;) True \u0026gt;\u0026gt;\u0026gt; c.open_new_tab(\u0026#39;http://docs.python.org\u0026#39;) True \u0026gt;\u0026gt;\u0026gt; 测试、调试、异常 测试stdout输出 # mymodule.py def urlprint(protocol, host, domain): url = \u0026#39;{}://{}.{}\u0026#39;.format(protocol, host, domain) print(url) from io import StringIO from unittest import TestCase from unittest.mock import patch import mymodule class TestURLPrint(TestCase): def test_url_gets_to_stdout(self): protocol = \u0026#39;http\u0026#39; host = \u0026#39;www\u0026#39; domain = \u0026#39;example.com\u0026#39; expected_url = \u0026#39;{}://{}.{}\\n\u0026#39;.format(protocol, host, domain) with patch(\u0026#39;sys.stdout\u0026#39;, new=StringIO()) as fake_out: mymodule.urlprint(protocol, host, domain) self.assertEqual(fake_out.getvalue(), expected_url) 在单元测试中给对象打补丁 from unittest.mock import patch import example @patch(\u0026#39;example.func\u0026#39;) def test1(x, mock_func): example.func(x) # Uses patched example.func mock_func.assert_called_with(x) 在单元测试中测试异常情况 import unittest # A simple function to illustrate def parse_int(s): return int(s) class TestConversion(unittest.TestCase): def test_bad_int(self): with self.assertRaisesRegex(ValueError, \u0026#39;invalid literal .*\u0026#39;): r = parse_int(\u0026#39;N/A\u0026#39;) 将测试输出用日志记录到文件中 import unittest class MyTest(unittest.TestCase): pass import sys def main(out=sys.stderr, verbosity=2): loader = unittest.TestLoader() suite = loader.loadTestsFromModule(sys.modules[__name__]) unittest.TextTestRunner(out,verbosity=verbosity).run(suite) if __name__ == \u0026#39;__main__\u0026#39;: with open(\u0026#39;testing.out\u0026#39;, \u0026#39;w\u0026#39;) as f: main(f) unittest模块中一些值得关注的内部工作原理。\nunittest 模块首先会组装一个测试套件。 这个测试套件包含了你定义的各种方法。一旦套件组装完成，它所包含的测试就可以被执行了。\n这两步是分开的，unittest.TestLoader 实例被用来组装测试套件。 loadTestsFromModule() 是它定义的方法之一，用来收集测试用例。 它会为 TestCase 类扫描某个模块并将其中的测试方法提取出来。 如果你想进行细粒度的控制， 可以使用 loadTestsFromTestCase() 方法来从某个继承TestCase的类中提取测试方法。 TextTestRunner 类是一个测试运行类的例子， 这个类的主要用途是执行某个测试套件中包含的测试方法。 这个类跟执行 unittest.main() 函数所使用的测试运行器是一样的。 不过，我们在这里对它进行了一些列底层配置，包括输出文件和提升级别。\n忽略或期望测试失败 unittest 模块有装饰器可用来控制对指定测试方法的处理，例如：\nimport unittest import os import platform class Tests(unittest.TestCase): def test_0(self): self.assertTrue(True) @unittest.skip(\u0026#39;skipped test\u0026#39;) def test_1(self): self.fail(\u0026#39;should have failed!\u0026#39;) @unittest.skipIf(os.name==\u0026#39;posix\u0026#39;, \u0026#39;Not supported on Unix\u0026#39;) def test_2(self): import winreg @unittest.skipUnless(platform.system() == \u0026#39;Darwin\u0026#39;, \u0026#39;Mac specific test\u0026#39;) def test_3(self): self.assertTrue(True) @unittest.expectedFailure def test_4(self): self.assertEqual(2+2, 5) if __name__ == \u0026#39;__main__\u0026#39;: unittest.main() 输出警告信息 import warnings def func(x, y, logfile=None, debug=False): if logfile is not None: warnings.warn(\u0026#39;logfile argument deprecated\u0026#39;, DeprecationWarning) ... 调试基本的程序崩溃错误 如果你的程序因为某个异常而崩溃，运行 python3 -i someprogram.py 可执行简单的调试。 -i 选项可让程序结束后打开一个交互式shell。 然后你就能查看环境，例如，假设你有下面的代码：\n# sample.py def func(n): return n + 10 func(\u0026#39;Hello\u0026#39;) 运行 python3 -i sample.py 会有类似如下的输出：\nbash % python3 -i sample.py Traceback (most recent call last): File \u0026#34;sample.py\u0026#34;, line 6, in \u0026lt;module\u0026gt; func(\u0026#39;Hello\u0026#39;) File \u0026#34;sample.py\u0026#34;, line 4, in func return n + 10 TypeError: Can\u0026#39;t convert \u0026#39;int\u0026#39; object to str implicitly \u0026gt;\u0026gt;\u0026gt; func(10) 20 \u0026gt;\u0026gt;\u0026gt; 给你的程序做性能测试 bash % python3 -m cProfile someprogram.py 859647 function calls in 16.016 CPU seconds Ordered by: standard name ncalls tottime percall cumtime percall filename:lineno(function) 263169 0.080 0.000 0.080 0.000 someprogram.py:16(frange) 513 0.001 0.000 0.002 0.000 someprogram.py:30(generate_mandel) 262656 0.194 0.000 15.295 0.000 someprogram.py:32(\u0026lt;genexpr\u0026gt;) 1 0.036 0.036 16.077 16.077 someprogram.py:4(\u0026lt;module\u0026gt;) 262144 15.021 0.000 15.021 0.000 someprogram.py:4(in_mandelbrot) 1 0.000 0.000 0.000 0.000 os.py:746(urandom) 1 0.000 0.000 0.000 0.000 png.py:1056(_readable) 1 0.000 0.000 0.000 0.000 png.py:1073(Reader) 1 0.227 0.227 0.438 0.438 png.py:163(\u0026lt;module\u0026gt;) 512 0.010 0.000 0.010 0.000 png.py:200(group) ... bash % 或者用装饰器：\nimport time from functools import wraps def timethis(func): @wraps(func) def wrapper(*args, **kwargs): start = time.perf_counter() r = func(*args, **kwargs) end = time.perf_counter() print(\u0026#39;{}.{} : {}\u0026#39;.format(func.__module__, func.__name__, end - start)) return r return wrapper \u0026gt;\u0026gt;\u0026gt; @timethis ... def countdown(n): ... while n \u0026gt; 0: ... n -= 1 ... 或者用上下文：\nfrom contextlib import contextmanager @contextmanager def timeblock(label): start = time.perf_counter() try: yield finally: end = time.perf_counter() print(\u0026#39;{} : {}\u0026#39;.format(label, end - start)) \u0026gt;\u0026gt;\u0026gt; with timeblock(\u0026#39;counting\u0026#39;): ... n = 10000000 ... while n \u0026gt; 0: ... n -= 1 ... 对于测试很小的代码片段运行性能，使用 timeit 模块会很方便，例如：\n\u0026gt;\u0026gt;\u0026gt; timeit(\u0026#39;sqrt(2)\u0026#39;, \u0026#39;from math import sqrt\u0026#39;, number=10000000) 1.0270336690009572 加速程序运行   由于局部变量和全局变量的实现方式（使用局部变量要更快些），因此脚本语句放入函数中会运行得更快一些\n  通常你可以使用 from module import name 这样的导入形式，尽可能去掉属性访问\n  对于频繁访问的名称，通过将这些名称变成局部变量可以加速程序运行\n  任何时候当你使用额外的处理层（比如装饰器、属性访问、描述器）去包装你的代码时，都会让程序运行变慢，避免不必要的抽象\n  使用内置的容器\n  避免创建不必要的数据结构或复制\n  总结 终于大体看完了Python Cookbook 3rd Edition这本书，这次看书还是比较细致的，除了与C语言扩展相关的部分没看外，其它部分都看过来了，将一些容易忘记的代码也摘录到博客里以备忘。另外在网上发现别人写的一篇《流畅的python》阅读笔记，也挺不错的，抽空也去看看这篇笔记。\n","permalink":"https://jeremyxu2010.github.io/2017/10/py3_cookbook_notes_03/","tags":["python"],"title":"py3_cookbook_notes_03"},{"categories":["java开发"],"contents":"最近的工作中需要反编译第三方的apk，以也了解对方的签名逻辑，这里将用到的反编译技巧记录一下。\napk文件转成jar文件 首先需要使用工具将apk文件转成jar文件，这里使用dex2jar，具体使用下面的命令：\nsh d2j-dex2jar.sh -f ~/path/to/apk_to_decompile.apk 这样会在当前目录下生成文件apk_to_decompile-dex2jar.jar。\n反编译jar文件 试用过jad、jd-gui、fernflower，结果发现还是IDEA自带的fernflower效果最好了，命令下执行也非常方便：\njava -cp \u0026#34;/Applications/IntelliJ IDEA.app/Contents/plugins/java-decompiler/lib/java-decompiler.jar\u0026#34; org.jetbrains.java.decompiler.main.decompiler.ConsoleDecompiler -hes=0 -hdc=0 c:\\Temp\\binary\\ -e=c:\\Java\\rt.jar c:\\Temp\\source\\  java -cp \u0026#34;/Applications/IntelliJ IDEA.app/Contents/plugins/java-decompiler/lib/java-decompiler.jar\u0026#34; org.jetbrains.java.decompiler.main.decompiler.ConsoleDecompiler -dgs=1 c:\\Temp\\binary\\library.jar c:\\Temp\\source\\ fernflower支持很多的命令行参数，详见其文档。\n","permalink":"https://jeremyxu2010.github.io/2017/10/apk%E5%8F%8D%E7%BC%96%E8%AF%91%E6%AD%A5%E9%AA%A4/","tags":["android","java","decompiler"],"title":"apk反编译步骤"},{"categories":["python开发"],"contents":"最近在看Python Cookbook第三版，将看书过程中一些平时不太容易注意的知识点记录下来。\n函数 可接受任意数量参数的函数 def avg(first, *rest): return (first + sum(rest)) / (1 + len(rest)) # Sample use avg(1, 2) # 1.5 avg(1, 2, 3, 4) # 2.5 import html def make_element(name, value, **attrs): keyvals = [\u0026#39;%s=\u0026#34;%s\u0026#34;\u0026#39; % item for item in attrs.items()] attr_str = \u0026#39;\u0026#39;.join(keyvals) element = \u0026#39;\u0026lt;{name}{attrs}\u0026gt;{value}\u0026lt;/{name}\u0026gt;\u0026#39;.format( name=name, attrs=attr_str, value=html.escape(value)) return element # Example # Creates \u0026#39;\u0026lt;item size=\u0026#34;large\u0026#34; quantity=\u0026#34;6\u0026#34;\u0026gt;Albatross\u0026lt;/item\u0026gt;\u0026#39; make_element(\u0026#39;item\u0026#39;, \u0026#39;Albatross\u0026#39;, size=\u0026#39;large\u0026#39;, quantity=6) # Creates \u0026#39;\u0026lt;p\u0026gt;\u0026amp;lt;spam\u0026amp;gt;\u0026lt;/p\u0026gt;\u0026#39; make_element(\u0026#39;p\u0026#39;, \u0026#39;\u0026lt;spam\u0026gt;\u0026#39;) 只接受关键字参数的函数 def recv(maxsize, *, block): \u0026#39;Receives a message\u0026#39; pass recv(1024, True) # TypeError recv(1024, block=True) # Ok def mininum(*values, clip=None): m = min(values) if clip is not None: m = clip if clip \u0026gt; m else m return m minimum(1, 5, 2, -5, 10) # Returns -5 minimum(1, 5, 2, -5, 10, clip=0) # Returns 0 给函数参数增加元信息 def add(x:int, y:int) -\u0026gt; int: return x + y 定义匿名或内联函数 \u0026gt;\u0026gt;\u0026gt; add = lambda x, y: x + y \u0026gt;\u0026gt;\u0026gt; add(2,3) 5 \u0026gt;\u0026gt;\u0026gt; add(\u0026#39;hello\u0026#39;, \u0026#39;world\u0026#39;) \u0026#39;helloworld\u0026#39; \u0026gt;\u0026gt;\u0026gt; names = [\u0026#39;David Beazley\u0026#39;, \u0026#39;Brian Jones\u0026#39;, ... \u0026#39;Raymond Hettinger\u0026#39;, \u0026#39;Ned Batchelder\u0026#39;] \u0026gt;\u0026gt;\u0026gt; sorted(names, key=lambda name: name.split()[-1].lower()) [\u0026#39;Ned Batchelder\u0026#39;, \u0026#39;David Beazley\u0026#39;, \u0026#39;Raymond Hettinger\u0026#39;, \u0026#39;Brian Jones\u0026#39;] 匿名函数捕获变量值 \u0026gt;\u0026gt;\u0026gt; x = 10 \u0026gt;\u0026gt;\u0026gt; a = lambda y, x=x: x + y \u0026gt;\u0026gt;\u0026gt; x = 20 \u0026gt;\u0026gt;\u0026gt; b = lambda y, x=x: x + y \u0026gt;\u0026gt;\u0026gt; a(10) 20 \u0026gt;\u0026gt;\u0026gt; b(10) 30 减少可调用对象的参数个数 def spam(a, b, c, d): print(a, b, c, d) \u0026gt;\u0026gt;\u0026gt; from functools import partial \u0026gt;\u0026gt;\u0026gt; s1 = partial(spam, 1) # a = 1 \u0026gt;\u0026gt;\u0026gt; s1(2, 3, 4) 1 2 3 4 \u0026gt;\u0026gt;\u0026gt; s2 = partial(spam, d=42) # d = 42 \u0026gt;\u0026gt;\u0026gt; s2(1, 2, 3) 1 2 3 42 \u0026gt;\u0026gt;\u0026gt; s2(4, 5, 5) 4 5 5 42 \u0026gt;\u0026gt;\u0026gt; s3 = partial(spam, 1, 2, d=42) # a = 1, b = 2, d = 42 \u0026gt;\u0026gt;\u0026gt; s3(3) 将单方法的类转换为函数 from urllib.request import urlopen class UrlTemplate: def __init__(self, template): self.template = template def open(self, **kwargs): return urlopen(self.template.format_map(kwargs)) # Example use. Download stock data from yahoo yahoo = UrlTemplate(\u0026#39;http://finance.yahoo.com/d/quotes.csv?s={names}\u0026amp;f={fields}\u0026#39;) for line in yahoo.open(names=\u0026#39;IBM,AAPL,FB\u0026#39;, fields=\u0026#39;sl1c1v\u0026#39;): print(line.decode(\u0026#39;utf-8\u0026#39;)) 转换写法：\nfrom urllib.request import urlopen def urltemplate(template): def opener(**kwargs): return urlopen(template.format_map(kwargs)) return opener # Example use yahoo = urltemplate(\u0026#39;http://finance.yahoo.com/d/quotes.csv?s={names}\u0026amp;f={fields}\u0026#39;) for line in yahoo(names=\u0026#39;IBM,AAPL,FB\u0026#39;, fields=\u0026#39;sl1c1v\u0026#39;): print(line.decode(\u0026#39;utf-8\u0026#39;)) 带额外状态信息的回调函数 3种解决方法：\n 创建一个类的实例  \u0026gt;\u0026gt;\u0026gt; def add(x, y): ... return x + y ... class ResultHandler: def __init__(self): self.sequence = 0 def handler(self, result): self.sequence += 1 print(\u0026#39;[{}] Got: {}\u0026#39;.format(self.sequence, result)) \u0026gt;\u0026gt;\u0026gt; r = ResultHandler() \u0026gt;\u0026gt;\u0026gt; apply_async(add, (2, 3), callback=r.handler) [1] Got: 5  用一个闭包捕获状态值  \u0026gt;\u0026gt;\u0026gt; def add(x, y): ... return x + y ... def make_handler(): sequence = 0 def handler(result): nonlocal sequence sequence += 1 print(\u0026#39;[{}] Got: {}\u0026#39;.format(sequence, result)) return handler \u0026gt;\u0026gt;\u0026gt; handler = make_handler() \u0026gt;\u0026gt;\u0026gt; apply_async(add, (2, 3), callback=handler) 内联回调函数 def apply_async(func, args, *, callback): # Compute the result result = func(*args) # Invoke the callback with the result callback(result) class Async: def __init__(self, func, args): self.func = func self.args = args def inlined_async(func): @wraps(func) def wrapper(*args): f = func(*args) # func is test function, f is a generator result_queue = Queue() result_queue.put(None) while True: result = result_queue.get() try: a = f.send(result) # a is Async object apply_async(a.func, a.args, callback=result_queue.put) except StopIteration: break return wrapper def add(x, y): return x + y @inlined_async def test(): r = yield Async(add, (2, 3)) print(r) r = yield Async(add, (\u0026#39;hello\u0026#39;, \u0026#39;world\u0026#39;)) print(r) for n in range(10): r = yield Async(add, (n, n)) print(r) print(\u0026#39;Goodbye\u0026#39;) test() 这里特别解释一下操作generator的两个方法send和next：\nhttps://stackoverflow.com/questions/12637768/python-3-send-method-of-generators\n When you use send and expression yield in a generator, you're treating it as a coroutine; a separate thread of execution that can run sequentially interleaved but not in parallel with its caller.\nWhen the caller executes R = m.send(a), it puts the object a into the generator's input slot, transfers control to the generator, and waits for a response. The generator receives object a as the result of X = yield i, and runs until it hits another yield expression e.g. Y = yield j. Then it puts j into its output slot, transfers control back to the caller, and waits until it gets resumed again. The caller receives j as the result of R = m.send(a), and runs until it hits another S = m.send(b) statement, and so on.\nR = next(m) is just the same as R = m.send(None); it's putting None into the generator's input slot, so if the generator checks the result of X = yield i then X will be None.\n 访问闭包中定义的变量 def sample(): n = 0 # Closure function def func(): print(\u0026#39;n=\u0026#39;, n) # Accessor methods for n def get_n(): return n def set_n(value): nonlocal n n = value # Attach as function attributes func.get_n = get_n func.set_n = set_n return func 类与对象 改变对象的字符串显示 class Pair: def __init__(self, x, y): self.x = x self.y = y def __repr__(self): return \u0026#39;Pair({0.x!r}, {0.y!r})\u0026#39;.format(self) def __str__(self): return \u0026#39;({0.x!s}, {0.y!s})\u0026#39;.format(self) 自定义字符串的格式化 _formats = { \u0026#39;ymd\u0026#39; : \u0026#39;{d.year}-{d.month}-{d.day}\u0026#39;, \u0026#39;mdy\u0026#39; : \u0026#39;{d.month}/{d.day}/{d.year}\u0026#39;, \u0026#39;dmy\u0026#39; : \u0026#39;{d.day}/{d.month}/{d.year}\u0026#39; } class Date: def __init__(self, year, month, day): self.year = year self.month = month self.day = day def __format__(self, code): if code == \u0026#39;\u0026#39;: code = \u0026#39;ymd\u0026#39; fmt = _formats[code] return fmt.format(d=self) \u0026gt;\u0026gt;\u0026gt; d = Date(2012, 12, 21) \u0026gt;\u0026gt;\u0026gt; format(d) \u0026#39;2012-12-21\u0026#39; \u0026gt;\u0026gt;\u0026gt; format(d, \u0026#39;mdy\u0026#39;) \u0026#39;12/21/2012\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026#39;The date is {:ymd}\u0026#39;.format(d) \u0026#39;The date is 2012-12-21\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026#39;The date is {:mdy}\u0026#39;.format(d) \u0026#39;The date is 12/21/2012\u0026#39; 让对象支持上下文管理协议 from socket import socket, AF_INET, SOCK_STREAM class LazyConnection: def __init__(self, address, family=AF_INET, type=SOCK_STREAM): self.address = address self.family = family self.type = type self.sock = None def __enter__(self): if self.sock is not None: raise RuntimeError(\u0026#39;Already connected\u0026#39;) self.sock = socket(self.family, self.type) self.sock.connect(self.address) return self.sock def __exit__(self, exc_ty, exc_val, tb): self.sock.close() self.sock = None from functools import partial conn = LazyConnection((\u0026#39;www.python.org\u0026#39;, 80)) # Connection closed with conn as s: # conn.__enter__() executes: connection open s.send(b\u0026#39;GET /index.html HTTP/1.0\\r\\n\u0026#39;) s.send(b\u0026#39;Host: www.python.org\\r\\n\u0026#39;) s.send(b\u0026#39;\\r\\n\u0026#39;) resp = b\u0026#39;\u0026#39;.join(iter(partial(s.recv, 8192), b\u0026#39;\u0026#39;)) # conn.__exit__() executes: connection closed 在类中封装属性名 class A: def __init__(self): self._internal = 0 # An internal attribute self.public = 1 # A public attribute self.__private_field = 0 # 不能被子类覆盖 def public_method(self): \u0026#39;\u0026#39;\u0026#39;A public method\u0026#39;\u0026#39;\u0026#39; pass def _internal_method(self): pass def __private_method(self): pass 创建可管理的属性 class Person: def __init__(self, first_name): self.first_name = first_name # Getter function @property def first_name(self): return self._first_name # Setter function @first_name.setter def first_name(self, value): if not isinstance(value, str): raise TypeError(\u0026#39;Expected a string\u0026#39;) self._first_name = value # Deleter function (optional) @first_name.deleter def first_name(self): raise AttributeError(\u0026#34;Can\u0026#39;t delete attribute\u0026#34;) 调用父类方法 class A: def spam(self): print(\u0026#39;A.spam\u0026#39;) class B(A): def spam(self): print(\u0026#39;B.spam\u0026#39;) super().spam() # Call parent spam() 简化数据结构的初始化 class Structure: # Class variable that specifies expected fields _fields = [] def __init__(self, *args, **kwargs): if len(args) != len(self._fields): raise TypeError(\u0026#39;Expected {} arguments\u0026#39;.format(len(self._fields))) # Set the arguments for name, value in zip(self._fields, args): setattr(self, name, value) # Set the additional arguments (if any) extra_args = kwargs.keys() - self._fields for name in extra_args: setattr(self, name, kwargs.pop(name)) if kwargs: raise TypeError(\u0026#39;Duplicate values for {}\u0026#39;.format(\u0026#39;,\u0026#39;.join(kwargs))) class Stock(Structure): _fields = [\u0026#39;name\u0026#39;, \u0026#39;shares\u0026#39;, \u0026#39;price\u0026#39;] s1 = Stock(\u0026#39;ACME\u0026#39;, 50, 91.1) s2 = Stock(\u0026#39;ACME\u0026#39;, 50, 91.1, date=\u0026#39;8/2/2012\u0026#39;) 定义接口或者抽象基类 from abc import ABCMeta, abstractmethod class IStream(metaclass=ABCMeta): @abstractmethod def read(self, maxbytes=-1): pass @abstractmethod def write(self, data): pass class SocketStream(IStream): def read(self, maxbytes=-1): pass def write(self, data): pass # Register the built-in I/O classes as supporting our interface import io IStream.register(io.IOBase) import collections # Check if x is a sequence if isinstance(x, collections.Sequence): ... # Check if x is iterable if isinstance(x, collections.Iterable): ... # Check if x has a size if isinstance(x, collections.Sized): ... # Check if x is a mapping if isinstance(x, collections.Mapping): 实现数据模型的类型约束 # Base class. Uses a descriptor to set a value class Descriptor: def __init__(self, name=None, **opts): self.name = name for key, value in opts.items(): setattr(self, key, value) def __set__(self, instance, value): instance.__dict__[self.name] = value # Descriptor for enforcing types class Typed(Descriptor): expected_type = type(None) def __set__(self, instance, value): if not isinstance(value, self.expected_type): raise TypeError(\u0026#39;expected \u0026#39; + str(self.expected_type)) super().__set__(instance, value) # Descriptor for enforcing values class Unsigned(Descriptor): def __set__(self, instance, value): if value \u0026lt; 0: raise ValueError(\u0026#39;Expected \u0026gt;= 0\u0026#39;) super().__set__(instance, value) class MaxSized(Descriptor): def __init__(self, name=None, **opts): if \u0026#39;size\u0026#39; not in opts: raise TypeError(\u0026#39;missing size option\u0026#39;) super().__init__(name, **opts) def __set__(self, instance, value): if len(value) \u0026gt;= self.size: raise ValueError(\u0026#39;size must be \u0026lt; \u0026#39; + str(self.size)) super().__set__(instance, value) class Integer(Typed): expected_type = int class UnsignedInteger(Integer, Unsigned): pass class Float(Typed): expected_type = float class UnsignedFloat(Float, Unsigned): pass class String(Typed): expected_type = str class SizedString(String, MaxSized): pass class Stock: # Specify constraints name = SizedString(\u0026#39;name\u0026#39;, size=8) shares = UnsignedInteger(\u0026#39;shares\u0026#39;) price = UnsignedFloat(\u0026#39;price\u0026#39;) def __init__(self, name, shares, price): self.name = name self.shares = shares self.price = price 实现自定义容器 import collections class A(collections.Iterable): def __iter__(): pass 属性的代理访问 class A: def spam(self, x): pass def foo(self): pass class B2: \u0026#34;\u0026#34;\u0026#34;使用__getattr__的代理，代理方法比较多时候\u0026#34;\u0026#34;\u0026#34; def __init__(self): self._a = A() def bar(self): pass # Expose all of the methods defined on class A def __getattr__(self, name): \u0026#34;\u0026#34;\u0026#34;这个方法在访问的attribute不存在的时候被调用the __getattr__() method is actually a fallback methodthat only gets called when an attribute is not found\u0026#34;\u0026#34;\u0026#34; return getattr(self._a, name) 在类中定义多个构造器 import time class Date: \u0026#34;\u0026#34;\u0026#34;方法一：使用类方法\u0026#34;\u0026#34;\u0026#34; # Primary constructor def __init__(self, year, month, day): self.year = year self.month = month self.day = day # Alternate constructor @classmethod def today(cls): t = time.localtime() return cls(t.tm_year, t.tm_mon, t.tm_mday) 创建不调用init方法的实例 class Date: def __init__(self, year, month, day): self.year = year self.month = month self.day = day d = Date.__new__(Date) \u0026gt;\u0026gt;\u0026gt; data = {\u0026#39;year\u0026#39;:2012, \u0026#39;month\u0026#39;:8, \u0026#39;day\u0026#39;:29} \u0026gt;\u0026gt;\u0026gt; for key, value in data.items(): ... setattr(d, key, value) ... 有用的方法扩展其他类的功能 def LoggedMapping(cls): \u0026#34;\u0026#34;\u0026#34;第二种方式：使用类装饰器\u0026#34;\u0026#34;\u0026#34; cls_getitem = cls.__getitem__ cls_setitem = cls.__setitem__ cls_delitem = cls.__delitem__ def __getitem__(self, key): print(\u0026#39;Getting \u0026#39; + str(key)) return cls_getitem(self, key) def __setitem__(self, key, value): print(\u0026#39;Setting {} = {!r}\u0026#39;.format(key, value)) return cls_setitem(self, key, value) def __delitem__(self, key): print(\u0026#39;Deleting \u0026#39; + str(key)) return cls_delitem(self, key) cls.__getitem__ = __getitem__ cls.__setitem__ = __setitem__ cls.__delitem__ = __delitem__ return cls @LoggedMapping class LoggedDict(dict): pass 实现状态对象或者状态机 class Connection: \u0026#34;\u0026#34;\u0026#34;新方案——对每个状态定义一个类\u0026#34;\u0026#34;\u0026#34; def __init__(self): self.new_state(ClosedConnectionState) def new_state(self, newstate): self._state = newstate # Delegate to the state class def read(self): return self._state.read(self) def write(self, data): return self._state.write(self, data) def open(self): return self._state.open(self) def close(self): return self._state.close(self) # Connection state base class class ConnectionState: @staticmethod def read(conn): raise NotImplementedError() @staticmethod def write(conn, data): raise NotImplementedError() @staticmethod def open(conn): raise NotImplementedError() @staticmethod def close(conn): raise NotImplementedError() # Implementation of different states class ClosedConnectionState(ConnectionState): @staticmethod def read(conn): raise RuntimeError(\u0026#39;Not open\u0026#39;) @staticmethod def write(conn, data): raise RuntimeError(\u0026#39;Not open\u0026#39;) @staticmethod def open(conn): conn.new_state(OpenConnectionState) @staticmethod def close(conn): raise RuntimeError(\u0026#39;Already closed\u0026#39;) class OpenConnectionState(ConnectionState): @staticmethod def read(conn): print(\u0026#39;reading\u0026#39;) @staticmethod def write(conn, data): print(\u0026#39;writing\u0026#39;) @staticmethod def open(conn): raise RuntimeError(\u0026#39;Already open\u0026#39;) @staticmethod def close(conn): conn.new_state(ClosedConnectionState) c = Connection() \u0026gt;\u0026gt;\u0026gt; c._state \u0026lt;class \u0026#39;__main__.ClosedConnectionState\u0026#39;\u0026gt; \u0026gt;\u0026gt;\u0026gt; c.open() \u0026gt;\u0026gt;\u0026gt; c._state \u0026lt;class \u0026#39;__main__.OpenConnectionState\u0026#39;\u0026gt; \u0026gt;\u0026gt;\u0026gt; c.read() reading \u0026gt;\u0026gt;\u0026gt; c.write(\u0026#39;hello\u0026#39;) writing \u0026gt;\u0026gt;\u0026gt; c.close() \u0026gt;\u0026gt;\u0026gt; c._state \u0026lt;class \u0026#39;__main__.ClosedConnectionState\u0026#39;\u0026gt; \u0026gt;\u0026gt;\u0026gt; 通过字符串调用对象方法 import math class Point: def __init__(self, x, y): self.x = x self.y = y def __repr__(self): return \u0026#39;Point({!r:},{!r:})\u0026#39;.format(self.x, self.y) def distance(self, x, y): return math.hypot(self.x - x, self.y - y) p = Point(2, 3) d = getattr(p, \u0026#39;distance\u0026#39;)(0, 0) # Calls p.distance(0, 0) import operator operator.methodcaller(\u0026#39;distance\u0026#39;, 0, 0)(p) 实现访问者模式(递归实现法) class Node: pass class UnaryOperator(Node): def __init__(self, operand): self.operand = operand class BinaryOperator(Node): def __init__(self, left, right): self.left = left self.right = right class Add(BinaryOperator): pass class Sub(BinaryOperator): pass class Mul(BinaryOperator): pass class Div(BinaryOperator): pass class Negate(UnaryOperator): pass class Number(Node): def __init__(self, value): self.value = value class NodeVisitor: def visit(self, node): methname = \u0026#39;visit_\u0026#39; + type(node).__name__ meth = getattr(self, methname, None) if meth is None: meth = self.generic_visit return meth(node) def generic_visit(self, node): raise RuntimeError(\u0026#39;No {} method\u0026#39;.format(\u0026#39;visit_\u0026#39; + type(node).__name__)) class Evaluator(NodeVisitor): def visit_Number(self, node): return node.value def visit_Add(self, node): return self.visit(node.left) + self.visit(node.right) def visit_Sub(self, node): return self.visit(node.left) - self.visit(node.right) def visit_Mul(self, node): return self.visit(node.left) * self.visit(node.right) def visit_Div(self, node): return self.visit(node.left) / self.visit(node.right) def visit_Negate(self, node): return -node.operand \u0026gt;\u0026gt;\u0026gt; e = Evaluator() \u0026gt;\u0026gt;\u0026gt; e.visit(t4) class HTTPHandler: def handle(self, request): methname = \u0026#39;do_\u0026#39; + request.request_method getattr(self, methname)(request) def do_GET(self, request): pass def do_POST(self, request): pass def do_HEAD(self, request): pass 实现访问者模式(非递归实现法) import types class Node: pass class NodeVisitor: def visit(self, node): stack = [node] last_result = None while stack: try: last = stack[-1] if isinstance(last, types.GeneratorType): stack.append(last.send(last_result)) last_result = None elif isinstance(last, Node): stack.append(self._visit(stack.pop())) else: last_result = stack.pop() except StopIteration: stack.pop() return last_result def _visit(self, node): methname = \u0026#39;visit_\u0026#39; + type(node).__name__ meth = getattr(self, methname, None) if meth is None: meth = self.generic_visit return meth(node) def generic_visit(self, node): raise RuntimeError(\u0026#39;No {} method\u0026#39;.format(\u0026#39;visit_\u0026#39; + type(node).__name__)) class UnaryOperator(Node): def __init__(self, operand): self.operand = operand class BinaryOperator(Node): def __init__(self, left, right): self.left = left self.right = right class Add(BinaryOperator): pass class Sub(BinaryOperator): pass class Mul(BinaryOperator): pass class Div(BinaryOperator): pass class Negate(UnaryOperator): pass class Number(Node): def __init__(self, value): self.value = value # A sample visitor class that evaluates expressions class Evaluator(NodeVisitor): def visit_Number(self, node): return node.value def visit_Add(self, node): yield (yield node.left) + (yield node.right) def visit_Sub(self, node): yield (yield node.left) - (yield node.right) def visit_Mul(self, node): yield (yield node.left) * (yield node.right) def visit_Div(self, node): yield (yield node.left) / (yield node.right) def visit_Negate(self, node): yield - (yield node.operand) \u0026gt;\u0026gt;\u0026gt; a = Number(0) \u0026gt;\u0026gt;\u0026gt; for n in range(1,100000): ... a = Add(a, Number(n)) ... \u0026gt;\u0026gt;\u0026gt; e = Evaluator() \u0026gt;\u0026gt;\u0026gt; e.visit(a) 4999950000 循环引用数据结构的内存管理 import weakref class Node: def __init__(self, value): self.value = value self._parent = None self.children = [] def __repr__(self): return \u0026#39;Node({!r:})\u0026#39;.format(self.value) # property that manages the parent as a weak-reference @property def parent(self): return None if self._parent is None else self._parent() @parent.setter def parent(self, node): self._parent = weakref.ref(node) def add_child(self, child): self.children.append(child) child.parent = self \u0026gt;\u0026gt;\u0026gt; root = Node(\u0026#39;parent\u0026#39;) \u0026gt;\u0026gt;\u0026gt; c1 = Node(\u0026#39;child\u0026#39;) \u0026gt;\u0026gt;\u0026gt; root.add_child(c1) \u0026gt;\u0026gt;\u0026gt; print(c1.parent) Node(\u0026#39;parent\u0026#39;) \u0026gt;\u0026gt;\u0026gt; del root \u0026gt;\u0026gt;\u0026gt; print(c1.parent) None \u0026gt;\u0026gt;\u0026gt; 让类支持比较操作 from functools import total_ordering class Room: def __init__(self, name, length, width): self.name = name self.length = length self.width = width self.square_feet = self.length * self.width @total_ordering class House: def __init__(self, name, style): self.name = name self.style = style self.rooms = list() @property def living_space_footage(self): return sum(r.square_feet for r in self.rooms) def add_room(self, room): self.rooms.append(room) def __str__(self): return \u0026#39;{}: {} square foot {}\u0026#39;.format(self.name, self.living_space_footage, self.style) def __eq__(self, other): return self.living_space_footage == other.living_space_footage def __lt__(self, other): return self.living_space_footage \u0026lt; other.living_space_footage 创建缓存实例 class CachedSpamManager: def __init__(self): self._cache = weakref.WeakValueDictionary() def get_spam(self, name): if name not in self._cache: temp = Spam._new(name) # Modified creation self._cache[name] = temp else: temp = self._cache[name] return temp def clear(self): self._cache.clear() class Spam: def __init__(self, *args, **kwargs): raise RuntimeError(\u0026#34;Can\u0026#39;t instantiate directly\u0026#34;) # Alternate constructor @classmethod def _new(cls, name): self = cls.__new__(cls) self.name = name return self 元编程 在函数上添加包装器 import time from functools import wraps def timethis(func): \u0026#39;\u0026#39;\u0026#39;Decorator that reports the execution time.\u0026#39;\u0026#39;\u0026#39; @wraps(func) def wrapper(*args, **kwargs): start = time.time() result = func(*args, **kwargs) end = time.time() print(func.__name__, end-start) return result return wrapper \u0026gt;\u0026gt;\u0026gt; @timethis ... def countdown(n): ... \u0026#39;\u0026#39;\u0026#39;... Counts down... \u0026#39;\u0026#39;\u0026#39; ... while n \u0026gt; 0: ... n -= 1 ... ... countdown(100000) ... countdown.__wrapped__(100000) 带可选参数的装饰器 from functools import wraps, partial import logging def logged(func=None, *, level=logging.DEBUG, name=None, message=None): if func is None: return partial(logged, level=level, name=name, message=message) logname = name if name else func.__module__ log = logging.getLogger(logname) logmsg = message if message else func.__name__ @wraps(func) def wrapper(*args, **kwargs): log.log(level, logmsg) return func(*args, **kwargs) return wrapper # Example use @logged def add(x, y): return x + y @logged(level=logging.CRITICAL, name=\u0026#39;example\u0026#39;) def spam(): print(\u0026#39;Spam!\u0026#39;) 利用装饰器强制函数上的类型检查 from inspect import signature from functools import wraps def typeassert(*ty_args, **ty_kwargs): def decorate(func): # If in optimized mode, disable type checking if not __debug__: return func # Map function argument names to supplied types sig = signature(func) bound_types = sig.bind_partial(*ty_args, **ty_kwargs).arguments @wraps(func) def wrapper(*args, **kwargs): bound_values = sig.bind(*args, **kwargs) # Enforce type assertions across supplied arguments for name, value in bound_values.arguments.items(): if name in bound_types: if not isinstance(value, bound_types[name]): raise TypeError( \u0026#39;Argument {} must be {}\u0026#39;.format(name, bound_types[name]) ) return func(*args, **kwargs) return wrapper return decorate \u0026gt;\u0026gt;\u0026gt; @typeassert(int, int) ... def add(x, y): ... return x + y 将装饰器定义为类 import types from functools import wraps class Profiled: def __init__(self, func): wraps(func)(self) self.ncalls = 0 def __call__(self, *args, **kwargs): self.ncalls += 1 return self.__wrapped__(*args, **kwargs) def __get__(self, instance, cls): if instance is None: return self else: return types.MethodType(self, instance) @Profiled def add(x, y): return x + y class Spam: @Profiled def bar(self, x): print(self, x) \u0026gt;\u0026gt;\u0026gt; add(2, 3) 5 \u0026gt;\u0026gt;\u0026gt; add(4, 5) 9 \u0026gt;\u0026gt;\u0026gt; add.ncalls 2 \u0026gt;\u0026gt;\u0026gt; s = Spam() \u0026gt;\u0026gt;\u0026gt; s.bar(1) \u0026lt;__main__.Spam object at 0x10069e9d0\u0026gt; 1 \u0026gt;\u0026gt;\u0026gt; s.bar(2) \u0026lt;__main__.Spam object at 0x10069e9d0\u0026gt; 2 \u0026gt;\u0026gt;\u0026gt; s.bar(3) \u0026lt;__main__.Spam object at 0x10069e9d0\u0026gt; 3 \u0026gt;\u0026gt;\u0026gt; Spam.bar.ncalls 3 装饰器为被包装函数增加参数 from functools import wraps def optional_debug(func): @wraps(func) def wrapper(*args, debug=False, **kwargs): if debug: print(\u0026#39;Calling\u0026#39;, func.__name__) return func(*args, **kwargs) return wrapper \u0026gt;\u0026gt;\u0026gt; @optional_debug ... def spam(a,b,c): ... print(a,b,c) ... \u0026gt;\u0026gt;\u0026gt; spam(1,2,3) 1 2 3 \u0026gt;\u0026gt;\u0026gt; spam(1,2,3, debug=True) Calling spam 1 2 3 \u0026gt;\u0026gt;\u0026gt; 使用装饰器扩充类的功能 def log_getattribute(cls): # Get the original implementation orig_getattribute = cls.__getattribute__ # Make a new definition def new_getattribute(self, name): print(\u0026#39;getting:\u0026#39;, name) return orig_getattribute(self, name) # Attach to the class and return cls.__getattribute__ = new_getattribute return cls # Example use @log_getattribute class A: def __init__(self,x): self.x = x def spam(self): pass \u0026gt;\u0026gt;\u0026gt; a = A(42) \u0026gt;\u0026gt;\u0026gt; a.x getting: x 42 \u0026gt;\u0026gt;\u0026gt; a.spam() getting: spam \u0026gt;\u0026gt;\u0026gt; 使用元类控制实例的创建 只允许调用类的静态方法\nclass NoInstances(type): def __call__(self, *args, **kwargs): raise TypeError(\u0026#34;Can\u0026#39;t instantiate directly\u0026#34;) # Example class Spam(metaclass=NoInstances): @staticmethod def grok(x): print(\u0026#39;Spam.grok\u0026#39;) \u0026gt;\u0026gt;\u0026gt; Spam.grok(42) Spam.grok \u0026gt;\u0026gt;\u0026gt; s = Spam() Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; File \u0026#34;example1.py\u0026#34;, line 7, in __call__ raise TypeError(\u0026#34;Can\u0026#39;t instantiate directly\u0026#34;) TypeError: Can\u0026#39;t instantiate directly \u0026gt;\u0026gt;\u0026gt; 单例模式\nclass Singleton(type): def __init__(self, *args, **kwargs): self.__instance = None super().__init__(*args, **kwargs) def __call__(self, *args, **kwargs): if self.__instance is None: self.__instance = super().__call__(*args, **kwargs) return self.__instance else: return self.__instance # Example class Spam(metaclass=Singleton): def __init__(self): print(\u0026#39;Creating Spam\u0026#39;) 缓存模式\nimport weakref class Cached(type): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) self.__cache = weakref.WeakValueDictionary() def __call__(self, *args): if args in self.__cache: return self.__cache[args] else: obj = super().__call__(*args) self.__cache[args] = obj return obj # Example class Spam(metaclass=Cached): def __init__(self, name): print(\u0026#39;Creating Spam({!r})\u0026#39;.format(name)) self.name = name 不过想了下，上面的一些实现还是有并发冲突问题，搜索到openstack中有一处单例模式的写法，感觉这个写法更正确。\n捕获类的属性定义顺序 from collections import OrderedDict # A set of descriptors for various types class Typed: _expected_type = type(None) def __init__(self, name=None): self._name = name def __set__(self, instance, value): if not isinstance(value, self._expected_type): raise TypeError(\u0026#39;Expected \u0026#39; + str(self._expected_type)) instance.__dict__[self._name] = value class Integer(Typed): _expected_type = int class Float(Typed): _expected_type = float class String(Typed): _expected_type = str # Metaclass that uses an OrderedDict for class body class OrderedMeta(type): def __new__(cls, clsname, bases, clsdict): d = dict(clsdict) order = [] for name, value in clsdict.items(): if isinstance(value, Typed): value._name = name order.append(name) d[\u0026#39;_order\u0026#39;] = order return type.__new__(cls, clsname, bases, d) @classmethod def __prepare__(cls, clsname, bases): return OrderedDict() class Structure(metaclass=OrderedMeta): def as_csv(self): return \u0026#39;,\u0026#39;.join(str(getattr(self,name)) for name in self._order) # Example use class Stock(Structure): name = String() shares = Integer() price = Float() def __init__(self, name, shares, price): self.name = name self.shares = shares self.price = price \u0026gt;\u0026gt;\u0026gt; s = Stock(\u0026#39;GOOG\u0026#39;,100,490.1) \u0026gt;\u0026gt;\u0026gt; s.name \u0026#39;GOOG\u0026#39; \u0026gt;\u0026gt;\u0026gt; s.as_csv() \u0026#39;GOOG,100,490.1\u0026#39; \u0026gt;\u0026gt;\u0026gt; t = Stock(\u0026#39;AAPL\u0026#39;,\u0026#39;a lot\u0026#39;, 610.23) Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; File \u0026#34;dupmethod.py\u0026#34;, line 34, in __init__ TypeError: shares expects \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; \u0026gt;\u0026gt;\u0026gt; 定义有可选参数的元类 class MyMeta(type): # Optional @classmethod def __prepare__(cls, name, bases, *, debug=False, synchronize=False): # Custom processing pass return super().__prepare__(name, bases) # Required def __new__(cls, name, bases, ns, *, debug=False, synchronize=False): # Custom processing pass return super().__new__(cls, name, bases, ns) # Required def __init__(self, name, bases, ns, *, debug=False, synchronize=False): # Custom processing pass super().__init__(name, bases, ns) class Spam(metaclass=MyMeta, debug=True, synchronize=True): pass args和kwargs的强制参数签名 from inspect import Signature, Parameter def make_sig(*names): parms = [Parameter(name, Parameter.POSITIONAL_OR_KEYWORD) for name in names] return Signature(parms) class StructureMeta(type): def __new__(cls, clsname, bases, clsdict): clsdict[\u0026#39;__signature__\u0026#39;] = make_sig(*clsdict.get(\u0026#39;_fields\u0026#39;,[])) return super().__new__(cls, clsname, bases, clsdict) class Structure(metaclass=StructureMeta): _fields = [] def __init__(self, *args, **kwargs): bound_values = self.__signature__.bind(*args, **kwargs) for name, value in bound_values.arguments.items(): setattr(self, name, value) # Example class Stock(Structure): _fields = [\u0026#39;name\u0026#39;, \u0026#39;shares\u0026#39;, \u0026#39;price\u0026#39;] class Point(Structure): _fields = [\u0026#39;x\u0026#39;, \u0026#39;y\u0026#39;] 在类上强制使用编程规约 from inspect import signature import logging class MatchSignaturesMeta(type): def __init__(self, clsname, bases, clsdict): super().__init__(clsname, bases, clsdict) sup = super(self, self) for name, value in clsdict.items(): if name.startswith(\u0026#39;_\u0026#39;) or not callable(value): continue # Get the previous definition (if any) and compare the signatures prev_dfn = getattr(sup,name,None) if prev_dfn: prev_sig = signature(prev_dfn) val_sig = signature(value) if prev_sig != val_sig: logging.warning(\u0026#39;Signature mismatch in %s. %s!= %s\u0026#39;, value.__qualname__, prev_sig, val_sig) # Example class Root(metaclass=MatchSignaturesMeta): pass class A(Root): def foo(self, x, y): pass def spam(self, x, *, z): pass # Class with redefined methods, but slightly different signatures class B(A): def foo(self, a, b): pass def spam(self,x,z): pass 以编程方式定义类 import operator import types import sys def named_tuple(classname, fieldnames): # Populate a dictionary of field property accessors cls_dict = { name: property(operator.itemgetter(n)) for n, name in enumerate(fieldnames) } # Make a __new__ function and add to the class dict def __new__(cls, *args): if len(args) != len(fieldnames): raise TypeError(\u0026#39;Expected {} arguments\u0026#39;.format(len(fieldnames))) return tuple.__new__(cls, args) cls_dict[\u0026#39;__new__\u0026#39;] = __new__ # Make the class cls = types.new_class(classname, (tuple,), {}, lambda ns: ns.update(cls_dict)) # Set the module to that of the caller cls.__module__ = sys._getframe(1).f_globals[\u0026#39;__name__\u0026#39;] return cls types.new_class的详细用法参考https://docs.python.org/3/library/types.html\n在定义的时候初始化类的成员 在类定义时就执行初始化或设置操作是元类的一个典型应用场景。本质上讲，一个元类会在定义时被触发， 这时候你可以执行一些额外的操作。\nimport operator class StructTupleMeta(type): def __init__(cls, *args, **kwargs): super().__init__(*args, **kwargs) for n, name in enumerate(cls._fields): setattr(cls, name, property(operator.itemgetter(n))) class StructTuple(tuple, metaclass=StructTupleMeta): _fields = [] def __new__(cls, *args): if len(args) != len(cls._fields): raise ValueError(\u0026#39;{} arguments required\u0026#39;.format(len(cls._fields))) return super().__new__(cls,args) class Stock(StructTuple): _fields = [\u0026#39;name\u0026#39;, \u0026#39;shares\u0026#39;, \u0026#39;price\u0026#39;] class Point(StructTuple): _fields = [\u0026#39;x\u0026#39;, \u0026#39;y\u0026#39;] \u0026gt;\u0026gt;\u0026gt; s = Stock(\u0026#39;ACME\u0026#39;, 50, 91.1) \u0026gt;\u0026gt;\u0026gt; s (\u0026#39;ACME\u0026#39;, 50, 91.1) \u0026gt;\u0026gt;\u0026gt; s[0] \u0026#39;ACME\u0026#39; \u0026gt;\u0026gt;\u0026gt; s.name \u0026#39;ACME\u0026#39; \u0026gt;\u0026gt;\u0026gt; s.shares * s.price 4555.0 避免重复的属性方法 def typed_property(name, expected_type): storage_name = \u0026#39;_\u0026#39; + name @property def prop(self): return getattr(self, storage_name) @prop.setter def prop(self, value): if not isinstance(value, expected_type): raise TypeError(\u0026#39;{} must be a {}\u0026#39;.format(name, expected_type)) setattr(self, storage_name, value) return prop from functools import partial String = partial(typed_property, expected_type=str) Integer = partial(typed_property, expected_type=int) # Example: class Person: name = String(\u0026#39;name\u0026#39;) age = Integer(\u0026#39;age\u0026#39;) def __init__(self, name, age): self.name = name self.age = age 定义上下文管理器的简单方法 import time from contextlib import contextmanager @contextmanager def timethis(label): start = time.time() try: yield finally: end = time.time() print(\u0026#39;{}: {}\u0026#39;.format(label, end - start)) # Example use with timethis(\u0026#39;counting\u0026#39;): n = 10000000 while n \u0026gt; 0: n -= 1 # 这段代码的作用是任何对列表的修改只有当所有代码运行完成并且不出现异常的情况下才会生效。  @contextmanager def list_transaction(orig_list): working = list(orig_list) yield working orig_list[:] = working \u0026gt;\u0026gt;\u0026gt; items = [1, 2, 3] \u0026gt;\u0026gt;\u0026gt; with list_transaction(items) as working: ... working.append(4) ... working.append(5) ... \u0026gt;\u0026gt;\u0026gt; items [1, 2, 3, 4, 5] \u0026gt;\u0026gt;\u0026gt; with list_transaction(items) as working: ... working.append(6) ... working.append(7) ... raise RuntimeError(\u0026#39;oops\u0026#39;) ... Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 4, in \u0026lt;module\u0026gt; RuntimeError: oops \u0026gt;\u0026gt;\u0026gt; items [1, 2, 3, 4, 5] \u0026gt;\u0026gt;\u0026gt; 在局部变量域中执行代码 \u0026gt;\u0026gt;\u0026gt; def test4(): ... a = 13 ... loc = { \u0026#39;a\u0026#39; : a } ... glb = { } ... exec(\u0026#39;b = a + 1\u0026#39;, glb, loc) ... b = loc[\u0026#39;b\u0026#39;] ... print(b) ... \u0026gt;\u0026gt;\u0026gt; test4() 14 \u0026gt;\u0026gt;\u0026gt; 模块与包 控制模块被全部导入的内容 # somemodule.py def spam(): pass def grok(): pass blah = 42 # Only export \u0026#39;spam\u0026#39; and \u0026#39;grok\u0026#39; __all__ = [\u0026#39;spam\u0026#39;, \u0026#39;grok\u0026#39;] 使用相对路径名导入包中子模块 mypackage/ __init__.py A/ __init__.py spam.py grok.py B/ __init__.py bar.py 如果模块mypackage.A.spam要导入同目录下的模块grok，它应该包括的import语句如下：\n# mypackage/A/spam.py from . import grok 如果模块mypackage.A.spam要导入不同目录下的模块B.bar，它应该使用的import语句如下：\n# mypackage/A/spam.py from ..B import bar 将模块分割成多个文件 mymodule/ __init__.py a.py b.py 在a.py文件中插入以下代码：\n# a.py class A: def spam(self): print(\u0026#39;A.spam\u0026#39;) 在b.py文件中插入以下代码：\n# b.py from .a import A class B(A): def bar(self): print(\u0026#39;B.bar\u0026#39;) 最后，在 __init__.py 中，将2个文件粘合在一起：\n# __init__.py from .a import A from .b import B 如果按照这些步骤，所产生的包MyModule将作为一个单一的逻辑模块：\n\u0026gt;\u0026gt;\u0026gt; import mymodule \u0026gt;\u0026gt;\u0026gt; a = mymodule.A() \u0026gt;\u0026gt;\u0026gt; a.spam() A.spam \u0026gt;\u0026gt;\u0026gt; b = mymodule.B() \u0026gt;\u0026gt;\u0026gt; b.bar() B.bar \u0026gt;\u0026gt;\u0026gt; 利用命名空间导入目录分散的代码 foo-package/ spam/ blah.py bar-package/ spam/ grok.py 在这2个目录里，都有着共同的命名空间spam。在任何一个目录里都没有__init__.py文件。\n如果将foo-package和bar-package都加到python模块路径:\n\u0026gt;\u0026gt;\u0026gt; import sys \u0026gt;\u0026gt;\u0026gt; sys.path.extend([\u0026#39;foo-package\u0026#39;, \u0026#39;bar-package\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; import spam.blah \u0026gt;\u0026gt;\u0026gt; import spam.grok \u0026gt;\u0026gt;\u0026gt; 两个不同的包目录被合并到一起，你可以导入spam.blah和spam.grok，并且它们能够工作。\n在这里工作的机制被称为“包命名空间”的一个特征。从本质上讲，包命名空间是一种特殊的封装设计，为合并不同的目录的代码到一个共同的命名空间。对于大的框架，这可能是有用的，因为它允许一个框架的部分被单独地安装下载。它也使人们能够轻松地为这样的框架编写第三方附加组件和其他扩展。\n包命名空间的关键是确保顶级目录中没有__init__.py文件来作为共同的命名空间。缺失__init__.py文件使得在导入包的时候会发生有趣的事情：这并没有产生错误，解释器创建了一个由所有包含匹配包名的目录组成的列表。特殊的包命名空间模块被创建，只读的目录列表副本被存储在其__path__变量中。\n\u0026gt;\u0026gt;\u0026gt; import spam \u0026gt;\u0026gt;\u0026gt; spam.__path__ _NamespacePath([\u0026#39;foo-package/spam\u0026#39;, \u0026#39;bar-package/spam\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; spam.__file__ Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; AttributeError: \u0026#39;module\u0026#39; object has no attribute \u0026#39;__file__\u0026#39; \u0026gt;\u0026gt;\u0026gt; spam \u0026lt;module \u0026#39;spam\u0026#39; (namespace)\u0026gt; \u0026gt;\u0026gt;\u0026gt; 运行目录或压缩文件 myapplication/ spam.py bar.py grok.py __main__.py 如果__main__.py存在，你可以简单地在顶级目录运行Python解释器：\nbash % python3 myapplication 如果你将你的代码打包成zip文件，这种技术同样也适用，举个例子：\nbash % ls spam.py bar.py grok.py __main__.py bash % zip -r myapp.zip *.py bash % python3 myapp.zip ... output from __main__.py ... 读取位于包中的数据文件 mypackage/ __init__.py somedata.dat spam.py 现在假设spam.py文件需要读取somedata.dat文件中的内容。你可以用以下代码来完成：\n# spam.py import pkgutil data = pkgutil.get_data(__package__, \u0026#39;somedata.dat\u0026#39;) 由此产生的变量是包含该文件的原始内容的字节字符串。\n将文件夹加入到sys.path import sys from os.path import abspath, join, dirname sys.path.insert(0, join(abspath(dirname(__file__)), \u0026#39;libs\u0026#39;)) 通过字符串名导入模块 \u0026gt;\u0026gt;\u0026gt; import importlib \u0026gt;\u0026gt;\u0026gt; math = importlib.import_module(\u0026#39;math\u0026#39;) \u0026gt;\u0026gt;\u0026gt; math.sin(2) 0.9092974268256817 \u0026gt;\u0026gt;\u0026gt; mod = importlib.import_module(\u0026#39;urllib.request\u0026#39;) \u0026gt;\u0026gt;\u0026gt; u = mod.urlopen(\u0026#39;http://www.python.org\u0026#39;) \u0026gt;\u0026gt;\u0026gt; import importlib # Same as \u0026#39;from . import b\u0026#39; b = importlib.import_module(\u0026#39;.b\u0026#39;, __package__) 安装私有的包 python3 setup.py install --user 或者\npip install --user packagename 创建新的Python环境 python3.6 -m venv ./Spam bash ./Span/bin/activate #进入新的python环境 deactivate #退出新的python环境 分发包 projectname/ README.txt Doc/ documentation.txt projectname/ __init__.py foo.py bar.py utils/ __init__.py spam.py grok.py examples/ helloworld.py ... 要编写一个 setup.py ，类似下面这样：\n# setup.py from distutils.core import setup setup(name=\u0026#39;projectname\u0026#39;, version=\u0026#39;1.0\u0026#39;, author=\u0026#39;Your Name\u0026#39;, author_email=\u0026#39;you@youraddress.com\u0026#39;, url=\u0026#39;http://www.you.com/projectname\u0026#39;, packages=[\u0026#39;projectname\u0026#39;, \u0026#39;projectname.utils\u0026#39;], ) 下一步，就是创建一个 MANIFEST.in 文件，列出所有在你的包中需要包含进来的非源码文件：\n# MANIFEST.in include *.txt recursive-include examples * recursive-include Doc * 确保 setup.py 和 MANIFEST.in 文件放在你的包的最顶级目录中。 一旦你已经做了这些，你就可以像下面这样执行命令来创建一个源码分发包了：\n% bash python3 setup.py sdist 它会创建一个文件比如”projectname-1.0.zip” 或 “projectname-1.0.tar.gz”, 具体依赖于你的系统平台。\n网络与Web编程 作为客户端与HTTP服务交互 from urllib import request, parse # Extra headers headers = { \u0026#39;User-agent\u0026#39; : \u0026#39;none/ofyourbusiness\u0026#39;, \u0026#39;Spam\u0026#39; : \u0026#39;Eggs\u0026#39; } # Base URL being accessed url = \u0026#39;http://httpbin.org/get\u0026#39; # Dictionary of query parameters (if any) parms = { \u0026#39;name1\u0026#39; : \u0026#39;value1\u0026#39;, \u0026#39;name2\u0026#39; : \u0026#39;value2\u0026#39; } # Encode the query string querystring = parse.urlencode(parms) # Make a GET request and read the response u = request.urlopen(url+\u0026#39;?\u0026#39; + querystring, headers=headers) resp = u.read() from urllib import request, parse # Base URL being accessed url = \u0026#39;http://httpbin.org/post\u0026#39; # Dictionary of query parameters (if any) parms = { \u0026#39;name1\u0026#39; : \u0026#39;value1\u0026#39;, \u0026#39;name2\u0026#39; : \u0026#39;value2\u0026#39; } # Encode the query string querystring = parse.urlencode(parms) # Make a POST request and read the response u = request.urlopen(url, querystring.encode(\u0026#39;ascii\u0026#39;), headers=headers) resp = u.read() 需要交互的服务比上面的例子都要复杂，也许应该去看看 requests 库（https://pypi.python.org/pypi/requests）。\nimport requests # Base URL being accessed url = \u0026#39;http://httpbin.org/post\u0026#39; # Dictionary of query parameters (if any) parms = { \u0026#39;name1\u0026#39; : \u0026#39;value1\u0026#39;, \u0026#39;name2\u0026#39; : \u0026#39;value2\u0026#39; } # Extra headers headers = { \u0026#39;User-agent\u0026#39; : \u0026#39;none/ofyourbusiness\u0026#39;, \u0026#39;Spam\u0026#39; : \u0026#39;Eggs\u0026#39; } resp = requests.post(url, data=parms, headers=headers) # Decoded text returned by the request text = resp.text 创建TCP服务器 from socketserver import BaseRequestHandler, TCPServer import traceback class EchoHandler(BaseRequestHandler): # Optional settings (defaults shown) timeout = 5 # Timeout on all socket operations rbufsize = -1 # Read buffer size wbufsize = 0 # Write buffer size disable_nagle_algorithm = False # Sets TCP_NODELAY socket option def handle(self): print(\u0026#39;Got connection from\u0026#39;, self.client_address) while True: msg = self.request.recv(8192) if not msg: break self.request.send(msg) class EchoTCPServer(TCPServer): allow_reuse_address = True def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) def _serve_forever(self): try: self.serve_forever() except KeyboardInterrupt: pass except Exception: traceback.print_exc() finally: self.shutdown() def serve(self, nworkers=15): from concurrent.futures.thread import ThreadPoolExecutor with ThreadPoolExecutor(max_workers=nworkers, thread_name_prefix=\u0026#39;echo_tcp_server\u0026#39;) as executor: for _ in range(nworkers): executor.submit(lambda : self._serve_forever()) self._serve_forever() if __name__ == \u0026#39;__main__\u0026#39;: serv = EchoTCPServer((\u0026#39;\u0026#39;, 20000), EchoHandler) serv.serve() 创建UDP服务器 from socketserver import BaseRequestHandler, UDPServer import time import traceback class TimeHandler(BaseRequestHandler): def handle(self): print(\u0026#39;Got connection from\u0026#39;, self.client_address) # Get message and client socket msg, sock = self.request resp = time.ctime() sock.sendto(resp.encode(\u0026#39;ascii\u0026#39;), self.client_address) class TimeUDPServer(UDPServer): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) def _serve_forever(self): try: self.serve_forever() except KeyboardInterrupt: pass except Exception: traceback.print_exc() finally: self.shutdown() def serve(self, nworkers=15): from concurrent.futures.thread import ThreadPoolExecutor with ThreadPoolExecutor(max_workers=nworkers, thread_name_prefix=\u0026#39;time_udp_server\u0026#39;) as executor: for _ in range(nworkers): executor.submit(lambda : self._serve_forever()) self._serve_forever() if __name__ == \u0026#39;__main__\u0026#39;: serv = TimeUDPServer((\u0026#39;\u0026#39;, 20000), TimeHandler) serv.serve() 通过CIDR地址生成对应的IP地址集 \u0026gt;\u0026gt;\u0026gt; import ipaddress \u0026gt;\u0026gt;\u0026gt; net = ipaddress.ip_network(\u0026#39;123.45.67.64/27\u0026#39;) \u0026gt;\u0026gt;\u0026gt; net IPv4Network(\u0026#39;123.45.67.64/27\u0026#39;) \u0026gt;\u0026gt;\u0026gt; for a in net: ... print(a) ... \u0026gt;\u0026gt;\u0026gt; net6 = ipaddress.ip_network(\u0026#39;12:3456:78:90ab:cd:ef01:23:30/125\u0026#39;) \u0026gt;\u0026gt;\u0026gt; net6 IPv6Network(\u0026#39;12:3456:78:90ab:cd:ef01:23:30/125\u0026#39;) \u0026gt;\u0026gt;\u0026gt; for a in net6: ... print(a) ... \u0026gt;\u0026gt;\u0026gt; net.num_addresses 32 \u0026gt;\u0026gt;\u0026gt; net[0] IPv4Address(\u0026#39;123.45.67.64\u0026#39;) \u0026gt;\u0026gt;\u0026gt; net[1] IPv4Address(\u0026#39;123.45.67.65\u0026#39;) \u0026gt;\u0026gt;\u0026gt; net[-1] IPv4Address(\u0026#39;123.45.67.95\u0026#39;) \u0026gt;\u0026gt;\u0026gt; inet = ipaddress.ip_interface(\u0026#39;123.45.67.73/27\u0026#39;) \u0026gt;\u0026gt;\u0026gt; inet.network IPv4Network(\u0026#39;123.45.67.64/27\u0026#39;) \u0026gt;\u0026gt;\u0026gt; inet.ip IPv4Address(\u0026#39;123.45.67.73\u0026#39;) \u0026gt;\u0026gt;\u0026gt; 通过XML-RPC实现简单的远程调用 from xmlrpc.server import SimpleXMLRPCServer import traceback class KeyValueServer: _rpc_methods_ = [\u0026#39;get\u0026#39;, \u0026#39;set\u0026#39;, \u0026#39;delete\u0026#39;, \u0026#39;exists\u0026#39;, \u0026#39;keys\u0026#39;] def __init__(self, address): self._data = {} self._serv = SimpleXMLRPCServer(address, allow_none=True) for name in self._rpc_methods_: self._serv.register_function(getattr(self, name)) def get(self, name): return self._data[name] def set(self, name, value): self._data[name] = value def delete(self, name): del self._data[name] def exists(self, name): return name in self._data def keys(self): return list(self._data) def serve_forever(self): try: self._serv.serve_forever() except KeyboardInterrupt: pass except Exception: traceback.print_exc() finally: self._serv.shutdown() # Example if __name__ == \u0026#39;__main__\u0026#39;: serv = KeyValueServer((\u0026#39;\u0026#39;, 15000)) nworkers = 15 from concurrent.futures.thread import ThreadPoolExecutor with ThreadPoolExecutor(max_workers=nworkers, thread_name_prefix=\u0026#39;keyvalue_rcp_server\u0026#39;) as executor: for _ in range(nworkers): executor.submit(lambda : serv.serve_forever()) serv.serve_forever() \u0026gt;\u0026gt;\u0026gt; from xmlrpc.client import ServerProxy \u0026gt;\u0026gt;\u0026gt; s = ServerProxy(\u0026#39;http://localhost:15000\u0026#39;, allow_none=True) \u0026gt;\u0026gt;\u0026gt; s.set(\u0026#39;foo\u0026#39;, \u0026#39;bar\u0026#39;) \u0026gt;\u0026gt;\u0026gt; s.set(\u0026#39;spam\u0026#39;, [1, 2, 3]) 在不同的Python解释器之间交互 from multiprocessing.connection import Listener import traceback def echo_client(conn): try: while True: msg = conn.recv() conn.send(msg) except EOFError: print(\u0026#39;Connection closed\u0026#39;) def serve_forever(serv): try: while True: client = serv.accept() echo_client(client) except (KeyboardInterrupt, ConnectionAbortedError): pass except Exception: traceback.print_exc() finally: serv.close() if __name__ == \u0026#39;__main__\u0026#39;: serv = Listener((\u0026#39;\u0026#39;, 25000), authkey=b\u0026#39;peekaboo\u0026#39;) nworkers = 15 from concurrent.futures.thread import ThreadPoolExecutor with ThreadPoolExecutor(max_workers=nworkers, thread_name_prefix=\u0026#39;multiprocessing_listener\u0026#39;) as executor: for _ in range(nworkers): executor.submit(serve_forever, serv) serve_forever(serv) \u0026gt;\u0026gt;\u0026gt; from multiprocessing.connection import Client \u0026gt;\u0026gt;\u0026gt; c = Client((\u0026#39;localhost\u0026#39;, 25000), authkey=b\u0026#39;peekaboo\u0026#39;) \u0026gt;\u0026gt;\u0026gt; c.send(\u0026#39;hello\u0026#39;) \u0026gt;\u0026gt;\u0026gt; c.recv() \u0026#39;hello\u0026#39; \u0026gt;\u0026gt;\u0026gt; c.send(42) \u0026gt;\u0026gt;\u0026gt; c.recv() 42 \u0026gt;\u0026gt;\u0026gt; c.send([1, 2, 3, 4, 5]) \u0026gt;\u0026gt;\u0026gt; c.recv() [1, 2, 3, 4, 5] 简单的客户端认证 import hmac import os def client_authenticate(connection, secret_key): \u0026#39;\u0026#39;\u0026#39;Authenticate client to a remote service.connection represents a network connection.secret_key is a key known only to both client/server.\u0026#39;\u0026#39;\u0026#39; message = connection.recv(32) hash = hmac.new(secret_key, message) digest = hash.digest() connection.send(digest) def server_authenticate(connection, secret_key): \u0026#39;\u0026#39;\u0026#39;Request client authentication.\u0026#39;\u0026#39;\u0026#39; message = os.urandom(32) connection.send(message) hash = hmac.new(secret_key, message) digest = hash.digest() response = connection.recv(len(digest)) return hmac.compare_digest(digest,response) from socket import socket, AF_INET, SOCK_STREAM secret_key = b\u0026#39;peekaboo\u0026#39; def echo_handler(client_sock): if not server_authenticate(client_sock, secret_key): client_sock.close() return while True: msg = client_sock.recv(8192) if not msg: break client_sock.sendall(msg) from socket import socket, AF_INET, SOCK_STREAM secret_key = b\u0026#39;peekaboo\u0026#39; s = socket(AF_INET, SOCK_STREAM) s.connect((\u0026#39;localhost\u0026#39;, 18000)) client_authenticate(s, secret_key) s.send(b\u0026#39;Hello World\u0026#39;) resp = s.recv(1024) 在网络服务中加入SSL import ssl class SSLMixin: \u0026#39;\u0026#39;\u0026#39;Mixin class that adds support for SSL to existing servers basedon the socketserver module.\u0026#39;\u0026#39;\u0026#39; def __init__(self, *args, keyfile=None, certfile=None, ca_certs=None, cert_reqs=ssl.CERT_NONE, **kwargs): self._keyfile = keyfile self._certfile = certfile self._ca_certs = ca_certs self._cert_reqs = cert_reqs super().__init__(*args, **kwargs) def get_request(self): client, addr = super().get_request() client_ssl = ssl.wrap_socket(client, keyfile = self._keyfile, certfile = self._certfile, ca_certs = self._ca_certs, cert_reqs = self._cert_reqs, server_side = True) return client_ssl, addr class SSLSimpleXMLRPCServer(SSLMixin, SimpleXMLRPCServer): pass if __name__ == \u0026#39;__main__\u0026#39;: KEYFILE=\u0026#39;server_key.pem\u0026#39; # Private key of the server CERTFILE=\u0026#39;server_cert.pem\u0026#39; # Server certificate kvserv = KeyValueServer((\u0026#39;\u0026#39;, 15000), keyfile=KEYFILE, certfile=CERTFILE) kvserv.serve_forever() 理解事件驱动的IO class EventHandler: def fileno(self): \u0026#39;Return the associated file descriptor\u0026#39; raise NotImplemented(\u0026#39;must implement\u0026#39;) def wants_to_receive(self): \u0026#39;Return True if receiving is allowed\u0026#39; return False def handle_receive(self): \u0026#39;Perform the receive operation\u0026#39; pass def wants_to_send(self): \u0026#39;Return True if sending is requested\u0026#39; return False def handle_send(self): \u0026#39;Send outgoing data\u0026#39; pass import select def event_loop(handlers): while True: wants_recv = [h for h in handlers if h.wants_to_receive()] wants_send = [h for h in handlers if h.wants_to_send()] can_recv, can_send, _ = select.select(wants_recv, wants_send, []) for h in can_recv: h.handle_receive() for h in can_send: h.handle_send() import socket import time class UDPServer(EventHandler): def __init__(self, address): self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) self.sock.bind(address) def fileno(self): return self.sock.fileno() def wants_to_receive(self): return True class UDPTimeServer(UDPServer): def handle_receive(self): msg, addr = self.sock.recvfrom(1) self.sock.sendto(time.ctime().encode(\u0026#39;ascii\u0026#39;), addr) class UDPEchoServer(UDPServer): def handle_receive(self): msg, addr = self.sock.recvfrom(8192) self.sock.sendto(msg, addr) if __name__ == \u0026#39;__main__\u0026#39;: handlers = [ UDPTimeServer((\u0026#39;\u0026#39;,14000)), UDPEchoServer((\u0026#39;\u0026#39;,15000)) ] event_loop(handlers) class TCPServer(EventHandler): def __init__(self, address, client_handler, handler_list): self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True) self.sock.bind(address) self.sock.listen(1) self.client_handler = client_handler self.handler_list = handler_list def fileno(self): return self.sock.fileno() def wants_to_receive(self): return True def handle_receive(self): client, addr = self.sock.accept() # Add the client to the event loop\u0026#39;s handler list self.handler_list.append(self.client_handler(client, self.handler_list)) class TCPClient(EventHandler): def __init__(self, sock, handler_list): self.sock = sock self.handler_list = handler_list self.outgoing = bytearray() def fileno(self): return self.sock.fileno() def close(self): self.sock.close() # Remove myself from the event loop\u0026#39;s handler list self.handler_list.remove(self) def wants_to_send(self): return True if self.outgoing else False def handle_send(self): nsent = self.sock.send(self.outgoing) self.outgoing = self.outgoing[nsent:] class TCPEchoClient(TCPClient): def wants_to_receive(self): return True def handle_receive(self): data = self.sock.recv(8192) if not data: self.close() else: self.outgoing.extend(data) if __name__ == \u0026#39;__main__\u0026#39;: handlers = [] handlers.append(TCPServer((\u0026#39;\u0026#39;,16000), TCPEchoClient, handlers)) event_loop(handlers) 对于阻塞或耗时计算的问题可以通过将事件发送个其他单独的线程池来处理。\nfrom concurrent.futures import ThreadPoolExecutor import os class ThreadPoolHandler(EventHandler): def __init__(self, nworkers): if os.name == \u0026#39;posix\u0026#39;: self.signal_done_sock, self.done_sock = socket.socketpair() else: server = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server.bind((\u0026#39;127.0.0.1\u0026#39;, 0)) server.listen(1) self.signal_done_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.signal_done_sock.connect(server.getsockname()) self.done_sock, _ = server.accept() server.close() self.pending = [] self.pool = ThreadPoolExecutor(nworkers) def fileno(self): return self.done_sock.fileno() # Callback that executes when the thread is done def _complete(self, callback, r): self.pending.append((callback, r.result())) self.signal_done_sock.send(b\u0026#39;x\u0026#39;) # Run a function in a thread pool def run(self, func, args=(), kwargs={},*,callback): r = self.pool.submit(func, *args, **kwargs) r.add_done_callback(lambda r: self._complete(callback, r)) def wants_to_receive(self): return True # Run callback functions of completed work def handle_receive(self): # Invoke all pending callback functions for callback, result in self.pending: callback(result) self.done_sock.recv(1) self.pending = [] # A really bad Fibonacci implementation def fib(n): if n \u0026lt; 2: return 1 else: return fib(n - 1) + fib(n - 2) class UDPFibServer(UDPServer): def handle_receive(self): msg, addr = self.sock.recvfrom(128) n = int(msg) pool.run(fib, (n,), callback=lambda r: self.respond(r, addr)) def respond(self, result, addr): self.sock.sendto(str(result).encode(\u0026#39;ascii\u0026#39;), addr) if __name__ == \u0026#39;__main__\u0026#39;: pool = ThreadPoolHandler(16) handlers = [ pool, UDPFibServer((\u0026#39;\u0026#39;,16000))] event_loop(handlers) ","permalink":"https://jeremyxu2010.github.io/2017/10/py3_cookbook_notes_02/","tags":["python"],"title":"py3_cookbook_notes_02"},{"categories":["python开发"],"contents":"最近在看Python Cookbook第三版，将看书过程中一些平时不太容易注意的知识点记录下来。\n数据结构和算法 解压可迭代对象赋值给多个变量 record = (\u0026#39;Dave\u0026#39;, \u0026#39;dave@example.com\u0026#39;, \u0026#39;773-555-1212\u0026#39;, \u0026#39;847-555-1212\u0026#39;) name, email, *phone_numbers = record 保留最后 N 个元素 from collections import deque def search(lines, pattern, history=5): previous_lines = deque(maxlen=history) for line in lines: if pattern in line: yield line, previous_lines previous_lines.append(line) # Example use on a file if __name__ == \u0026#39;__main__\u0026#39;: with open(r\u0026#39;../../cookbook/somefile.txt\u0026#39;) as f: for line, prevlines in search(f, \u0026#39;python\u0026#39;, 5): for pline in prevlines: print(pline, end=\u0026#39;\u0026#39;) print(line, end=\u0026#39;\u0026#39;) print(\u0026#39;-\u0026#39; * 20) 查找最大或最小的 N 个元素 import heapq nums = [1, 8, 2, 23, 7, -4, 18, 23, 42, 37, 2] print(heapq.nlargest(3, nums)) # Prints [42, 37, 23] print(heapq.nsmallest(3, nums)) # Prints [-4, 1, 2] portfolio = [ {\u0026#39;name\u0026#39;: \u0026#39;IBM\u0026#39;, \u0026#39;shares\u0026#39;: 100, \u0026#39;price\u0026#39;: 91.1}, {\u0026#39;name\u0026#39;: \u0026#39;AAPL\u0026#39;, \u0026#39;shares\u0026#39;: 50, \u0026#39;price\u0026#39;: 543.22}, {\u0026#39;name\u0026#39;: \u0026#39;FB\u0026#39;, \u0026#39;shares\u0026#39;: 200, \u0026#39;price\u0026#39;: 21.09}, {\u0026#39;name\u0026#39;: \u0026#39;HPQ\u0026#39;, \u0026#39;shares\u0026#39;: 35, \u0026#39;price\u0026#39;: 31.75}, {\u0026#39;name\u0026#39;: \u0026#39;YHOO\u0026#39;, \u0026#39;shares\u0026#39;: 45, \u0026#39;price\u0026#39;: 16.35}, {\u0026#39;name\u0026#39;: \u0026#39;ACME\u0026#39;, \u0026#39;shares\u0026#39;: 75, \u0026#39;price\u0026#39;: 115.65} ] cheap = heapq.nsmallest(3, portfolio, key=lambda s: s[\u0026#39;price\u0026#39;]) expensive = heapq.nlargest(3, portfolio, key=lambda s: s[\u0026#39;price\u0026#39;]) nums = [1, 8, 2, 23, 7, -4, 18, 23, 42, 37, 2] heapq.heapify(nums) it = iter(lambda:heapq.heappop(nums) if len(nums)\u0026gt;0 else None, None) for i in it: print(i) 实现一个优先级队列 import heapq class PriorityQueue: def __init__(self): self._queue = [] self._index = 0 def push(self, item, priority): heapq.heappush(self._queue, (-priority, self._index, item)) self._index += 1 def pop(self): return heapq.heappop(self._queue)[-1] 字典中的键映射多个值 from collections import defaultdict d = defaultdict(list) d[\u0026#39;a\u0026#39;].append(1) d[\u0026#39;a\u0026#39;].append(2) d[\u0026#39;b\u0026#39;].append(4) d = defaultdict(set) d[\u0026#39;a\u0026#39;].add(1) d[\u0026#39;a\u0026#39;].add(2) d[\u0026#39;b\u0026#39;].add(4) 字典排序 from collections import OrderedDict d = OrderedDict() d[\u0026#39;foo\u0026#39;] = 1 d[\u0026#39;bar\u0026#39;] = 2 d[\u0026#39;spam\u0026#39;] = 3 d[\u0026#39;grok\u0026#39;] = 4 # Outputs \u0026#34;foo 1\u0026#34;, \u0026#34;bar 2\u0026#34;, \u0026#34;spam 3\u0026#34;, \u0026#34;grok 4\u0026#34; for key in d: print(key, d[key]) 字典的运算 prices = { \u0026#39;ACME\u0026#39;: 45.23, \u0026#39;AAPL\u0026#39;: 612.78, \u0026#39;IBM\u0026#39;: 205.55, \u0026#39;HPQ\u0026#39;: 37.20, \u0026#39;FB\u0026#39;: 10.75 } min_price = min(zip(prices.values(), prices.keys())) # min_price is (10.75, \u0026#39;FB\u0026#39;) max_price = max(zip(prices.values(), prices.keys())) # max_price is (612.78, \u0026#39;AAPL\u0026#39;) 查找两字典的相同点 a = { \u0026#39;x\u0026#39; : 1, \u0026#39;y\u0026#39; : 2, \u0026#39;z\u0026#39; : 3 } b = { \u0026#39;w\u0026#39; : 10, \u0026#39;x\u0026#39; : 11, \u0026#39;y\u0026#39; : 2 } # Find keys in common a.keys() \u0026amp; b.keys() # { \u0026#39;x\u0026#39;, \u0026#39;y\u0026#39; } # Find keys in a that are not in b a.keys() - b.keys() # { \u0026#39;z\u0026#39; } # Find (key,value) pairs in common a.items() \u0026amp; b.items() # { (\u0026#39;y\u0026#39;, 2) } 删除序列相同元素并保持顺序 def dedupe(items, key=None): seen = set() for item in items: val = item if key is None else key(item) if val not in seen: yield item seen.add(val) a = [1, 5, 2, 1, 9, 1, 5, 10] list(dedupe(a)) a = [ {\u0026#39;x\u0026#39;:1, \u0026#39;y\u0026#39;:2}, {\u0026#39;x\u0026#39;:1, \u0026#39;y\u0026#39;:3}, {\u0026#39;x\u0026#39;:1, \u0026#39;y\u0026#39;:2}, {\u0026#39;x\u0026#39;:2, \u0026#39;y\u0026#39;:4}] list(dedupe(a, key=lambda d: (d[\u0026#39;x\u0026#39;],d[\u0026#39;y\u0026#39;]))) 命名切片 ###### 0123456789012345678901234567890123456789012345678901234567890\u0026#39; record = \u0026#39;....................100 .......513.25 ..........\u0026#39; SHARES = slice(20, 23) PRICE = slice(31, 37) cost = int(record[SHARES]) * float(record[PRICE]) a = slice(5, 50, 2) s = \u0026#39;HelloWorld\u0026#39; for i in range(*a.indices(len(s))): print(s[i]) 序列中出现次数最多的元素 words = [ \u0026#39;look\u0026#39;, \u0026#39;into\u0026#39;, \u0026#39;my\u0026#39;, \u0026#39;eyes\u0026#39;, \u0026#39;look\u0026#39;, \u0026#39;into\u0026#39;, \u0026#39;my\u0026#39;, \u0026#39;eyes\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;eyes\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;eyes\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;eyes\u0026#39;, \u0026#39;not\u0026#39;, \u0026#39;around\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;eyes\u0026#39;, \u0026#34;don\u0026#39;t\u0026#34;, \u0026#39;look\u0026#39;, \u0026#39;around\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;eyes\u0026#39;, \u0026#39;look\u0026#39;, \u0026#39;into\u0026#39;, \u0026#39;my\u0026#39;, \u0026#39;eyes\u0026#39;, \u0026#34;you\u0026#39;re\u0026#34;, \u0026#39;under\u0026#39; ] from collections import Counter word_counts = Counter(words) # 出现频率最高的3个单词 top_three = word_counts.most_common(3) print(top_three) # Outputs [(\u0026#39;eyes\u0026#39;, 8), (\u0026#39;the\u0026#39;, 5), (\u0026#39;look\u0026#39;, 4)] 通过某个关键字排序一个字典列表 rows = [ {\u0026#39;fname\u0026#39;: \u0026#39;Brian\u0026#39;, \u0026#39;lname\u0026#39;: \u0026#39;Jones\u0026#39;, \u0026#39;uid\u0026#39;: 1003}, {\u0026#39;fname\u0026#39;: \u0026#39;David\u0026#39;, \u0026#39;lname\u0026#39;: \u0026#39;Beazley\u0026#39;, \u0026#39;uid\u0026#39;: 1002}, {\u0026#39;fname\u0026#39;: \u0026#39;John\u0026#39;, \u0026#39;lname\u0026#39;: \u0026#39;Cleese\u0026#39;, \u0026#39;uid\u0026#39;: 1001}, {\u0026#39;fname\u0026#39;: \u0026#39;Big\u0026#39;, \u0026#39;lname\u0026#39;: \u0026#39;Jones\u0026#39;, \u0026#39;uid\u0026#39;: 1004} ] from operator import itemgetter rows_by_fname = sorted(rows, key=itemgetter(\u0026#39;fname\u0026#39;)) rows_by_uid = sorted(rows, key=itemgetter(\u0026#39;uid\u0026#39;)) print(rows_by_fname) print(rows_by_uid) 排序不支持原生比较的对象 class User: def __init__(self, user_id): self.user_id = user_id def __repr__(self): return \u0026#39;User({})\u0026#39;.format(self.user_id) def sort_notcompare(): users = [User(23), User(3), User(99)] print(users) print(sorted(users, key=lambda u: u.user_id)) 通过某个字段将记录分组 rows = [ {\u0026#39;address\u0026#39;: \u0026#39;5412 N CLARK\u0026#39;, \u0026#39;date\u0026#39;: \u0026#39;07/01/2012\u0026#39;}, {\u0026#39;address\u0026#39;: \u0026#39;5148 N CLARK\u0026#39;, \u0026#39;date\u0026#39;: \u0026#39;07/04/2012\u0026#39;}, {\u0026#39;address\u0026#39;: \u0026#39;5800 E 58TH\u0026#39;, \u0026#39;date\u0026#39;: \u0026#39;07/02/2012\u0026#39;}, {\u0026#39;address\u0026#39;: \u0026#39;2122 N CLARK\u0026#39;, \u0026#39;date\u0026#39;: \u0026#39;07/03/2012\u0026#39;}, {\u0026#39;address\u0026#39;: \u0026#39;5645 N RAVENSWOOD\u0026#39;, \u0026#39;date\u0026#39;: \u0026#39;07/02/2012\u0026#39;}, {\u0026#39;address\u0026#39;: \u0026#39;1060 W ADDISON\u0026#39;, \u0026#39;date\u0026#39;: \u0026#39;07/02/2012\u0026#39;}, {\u0026#39;address\u0026#39;: \u0026#39;4801 N BROADWAY\u0026#39;, \u0026#39;date\u0026#39;: \u0026#39;07/01/2012\u0026#39;}, {\u0026#39;address\u0026#39;: \u0026#39;1039 W GRANVILLE\u0026#39;, \u0026#39;date\u0026#39;: \u0026#39;07/04/2012\u0026#39;}, ] from operator import itemgetter from itertools import groupby # Sort by the desired field first rows.sort(key=itemgetter(\u0026#39;date\u0026#39;)) # Iterate in groups for date, items in groupby(rows, key=itemgetter(\u0026#39;date\u0026#39;)): print(date) for i in items: print(\u0026#39;\u0026#39;, i) 映射名称到序列元素 from collections import namedtuple Subscriber = namedtuple(\u0026#39;Subscriber\u0026#39;, [\u0026#39;addr\u0026#39;, \u0026#39;joined\u0026#39;]) sub = Subscriber(\u0026#39;jonesy@example.com\u0026#39;, \u0026#39;2012-10-19\u0026#39;) sub sub.addr sub.joined Stock = namedtuple(\u0026#39;Stock\u0026#39;, [\u0026#39;name\u0026#39;, \u0026#39;shares\u0026#39;, \u0026#39;price\u0026#39;]) def compute_cost(records): total = 0.0 for rec in records: s = Stock(*rec) total += s.shares * s.price return total 合并多个字典或映射 a = {\u0026#39;x\u0026#39;: 1, \u0026#39;z\u0026#39;: 3 } b = {\u0026#39;y\u0026#39;: 2, \u0026#39;z\u0026#39;: 4 } from collections import ChainMap c = ChainMap(a,b) print(c[\u0026#39;x\u0026#39;]) # Outputs 1 (from a) print(c[\u0026#39;y\u0026#39;]) # Outputs 2 (from b) print(c[\u0026#39;z\u0026#39;]) # Outputs 3 (from a) 字符串和文本 使用多个界定符分割字符串 line = \u0026#39;asdf fjdk; afed, fjek,asdf, foo\u0026#39; import re re.split(r\u0026#39;(?:,|;|\\s)\\s*\u0026#39;, line) 字符串开头或结尾匹配 import os filenames = os.listdir(\u0026#39;.\u0026#39;) (name for name in filenames if name.endswith((\u0026#39;.c\u0026#39;, \u0026#39;.h\u0026#39;))) 字符串匹配和搜索 datepat = re.compile(r\u0026#39;(\\d+)/(\\d+)/(\\d+)\u0026#39;) for m in datepat.finditer(text): print(m.groups()) 删除字符串中不需要的字符 \u0026gt;\u0026gt;\u0026gt; s = \u0026#39;hello world \\n\u0026#39; \u0026gt;\u0026gt;\u0026gt; s.strip() \u0026#39;hello world\u0026#39; \u0026gt;\u0026gt;\u0026gt; s.lstrip() \u0026#39;hello world \\n\u0026#39; \u0026gt;\u0026gt;\u0026gt; s.rstrip() \u0026#39;hello world\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; # Character stripping \u0026gt;\u0026gt;\u0026gt; t = \u0026#39;-----hello=====\u0026#39; \u0026gt;\u0026gt;\u0026gt; t.lstrip(\u0026#39;-\u0026#39;) \u0026#39;hello=====\u0026#39; \u0026gt;\u0026gt;\u0026gt; t.strip(\u0026#39;-=\u0026#39;) \u0026#39;hello\u0026#39; 字符串对齐 \u0026gt;\u0026gt;\u0026gt; text = \u0026#39;Hello World\u0026#39; \u0026gt;\u0026gt;\u0026gt; text.ljust(20) \u0026#39;Hello World \u0026#39; \u0026gt;\u0026gt;\u0026gt; text.rjust(20) \u0026#39;Hello World\u0026#39; \u0026gt;\u0026gt;\u0026gt; text.center(20) \u0026#39;Hello World \u0026#39; \u0026gt;\u0026gt;\u0026gt; text.rjust(20,\u0026#39;=\u0026#39;) \u0026#39;=========Hello World\u0026#39; \u0026gt;\u0026gt;\u0026gt; text.center(20,\u0026#39;*\u0026#39;) \u0026#39;****Hello World*****\u0026#39; \u0026gt;\u0026gt;\u0026gt; format(text, \u0026#39;\u0026gt;20\u0026#39;) \u0026#39;Hello World\u0026#39; \u0026gt;\u0026gt;\u0026gt; format(text, \u0026#39;\u0026lt;20\u0026#39;) \u0026#39;Hello World \u0026#39; \u0026gt;\u0026gt;\u0026gt; format(text, \u0026#39;^20\u0026#39;) \u0026#39;Hello World \u0026#39; \u0026gt;\u0026gt;\u0026gt; format(text, \u0026#39;=\u0026gt;20s\u0026#39;) \u0026#39;=========Hello World\u0026#39; \u0026gt;\u0026gt;\u0026gt; format(text, \u0026#39;*^20s\u0026#39;) \u0026#39;****Hello World*****\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026#39;{:\u0026gt;10s} {:\u0026gt;10s}\u0026#39;.format(\u0026#39;Hello\u0026#39;, \u0026#39;World\u0026#39;) \u0026#39;Hello World\u0026#39; 字符串中插入变量 \u0026gt;\u0026gt;\u0026gt; s = \u0026#39;{name} has {n} messages.\u0026#39; \u0026gt;\u0026gt;\u0026gt; s.format(name=\u0026#39;Guido\u0026#39;, n=37) \u0026#39;Guido has 37 messages.\u0026#39; class safesub(dict): \u0026#34;\u0026#34;\u0026#34;防止key找不到\u0026#34;\u0026#34;\u0026#34; def __missing__(self, key): return \u0026#39;{\u0026#39; + key + \u0026#39;}\u0026#39; \u0026gt;\u0026gt;\u0026gt; name = \u0026#39;Guido\u0026#39; \u0026gt;\u0026gt;\u0026gt; n = 37 \u0026gt;\u0026gt;\u0026gt; s.format_map(safesub(vars())) \u0026#39;Guido has 37 messages.\u0026#39; 以指定列宽格式化字符串 s = \u0026#34;Look into my eyes, look into my eyes, the eyes, the eyes, \\ the eyes, not around the eyes, don\u0026#39;t look around the eyes, \\ look into my eyes, you\u0026#39;re under.\u0026#34; import textwrap print(textwrap.fill(s, 70)) \u0026gt;\u0026gt;\u0026gt; import os \u0026gt;\u0026gt;\u0026gt; os.get_terminal_size().columns 80 在字符串中处理html和xml s = \u0026#39;Elements are written as \u0026#34;\u0026lt;tag\u0026gt;text\u0026lt;/tag\u0026gt;\u0026#34;.\u0026#39; print(html.escape(s, quote=False)) \u0026gt;\u0026gt;\u0026gt; s = \u0026#39;Spicy \u0026amp;quot;Jalape\u0026amp;#241;o\u0026amp;quot.\u0026#39; \u0026gt;\u0026gt;\u0026gt; from html.parser import HTMLParser \u0026gt;\u0026gt;\u0026gt; p = HTMLParser() \u0026gt;\u0026gt;\u0026gt; p.unescape(s) \u0026#39;Spicy \u0026#34;Jalapeño\u0026#34;.\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; t = \u0026#39;The prompt is \u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026#39; \u0026gt;\u0026gt;\u0026gt; from xml.sax.saxutils import unescape \u0026gt;\u0026gt;\u0026gt; unescape(t) \u0026#39;The prompt is \u0026gt;\u0026gt;\u0026gt;\u0026#39; 数字日期和时间 执行精确的浮点数运算 \u0026gt;\u0026gt;\u0026gt; from decimal import Decimal \u0026gt;\u0026gt;\u0026gt; a = Decimal(\u0026#39;4.2\u0026#39;) \u0026gt;\u0026gt;\u0026gt; b = Decimal(\u0026#39;2.1\u0026#39;) \u0026gt;\u0026gt;\u0026gt; a + b Decimal(\u0026#39;6.3\u0026#39;) \u0026gt;\u0026gt;\u0026gt; print(a + b) 6.3 \u0026gt;\u0026gt;\u0026gt; (a + b) == Decimal(\u0026#39;6.3\u0026#39;) True \u0026gt;\u0026gt;\u0026gt; from decimal import localcontext \u0026gt;\u0026gt;\u0026gt; a = Decimal(\u0026#39;1.3\u0026#39;) \u0026gt;\u0026gt;\u0026gt; b = Decimal(\u0026#39;1.7\u0026#39;) \u0026gt;\u0026gt;\u0026gt; print(a / b) 0.7647058823529411764705882353 \u0026gt;\u0026gt;\u0026gt; with localcontext() as ctx: ... ctx.prec = 3 ... print(a / b) ... 0.765 数字的格式化输出 \u0026gt;\u0026gt;\u0026gt; x = 1234.56789 \u0026gt;\u0026gt;\u0026gt; # Two decimal places of accuracy \u0026gt;\u0026gt;\u0026gt; format(x, \u0026#39;0.2f\u0026#39;) \u0026#39;1234.57\u0026#39; \u0026gt;\u0026gt;\u0026gt; # Right justified in 10 chars, one-digit accuracy \u0026gt;\u0026gt;\u0026gt; format(x, \u0026#39;\u0026gt;10.1f\u0026#39;) \u0026#39;1234.6\u0026#39; \u0026gt;\u0026gt;\u0026gt; # Left justified \u0026gt;\u0026gt;\u0026gt; format(x, \u0026#39;\u0026lt;10.1f\u0026#39;) \u0026#39;1234.6 \u0026#39; \u0026gt;\u0026gt;\u0026gt; # Centered \u0026gt;\u0026gt;\u0026gt; format(x, \u0026#39;^10.1f\u0026#39;) \u0026#39;1234.6 \u0026#39; \u0026gt;\u0026gt;\u0026gt; # Inclusion of thousands separator \u0026gt;\u0026gt;\u0026gt; format(x, \u0026#39;,\u0026#39;) \u0026#39;1,234.56789\u0026#39; \u0026gt;\u0026gt;\u0026gt; format(x, \u0026#39;0,.1f\u0026#39;) \u0026#39;1,234.6\u0026#39; \u0026gt;\u0026gt;\u0026gt; format(x, \u0026#39;e\u0026#39;) \u0026#39;1.234568e+03\u0026#39; \u0026gt;\u0026gt;\u0026gt; format(x, \u0026#39;0.2E\u0026#39;) \u0026#39;1.23E+03\u0026#39; 同时指定宽度和精度的一般形式是 '[\u0026lt;\u0026gt;^]?width[,]?(.digits)?' ， 其中 width 和 digits 为整数，？代表可选部分。 同样的格式也被用在字符串的 format() 方法中。\n基本的日期与时间转换 \u0026gt;\u0026gt;\u0026gt; from datetime import timedelta \u0026gt;\u0026gt;\u0026gt; a = timedelta(days=2, hours=6) \u0026gt;\u0026gt;\u0026gt; b = timedelta(hours=4.5) \u0026gt;\u0026gt;\u0026gt; c = a + b \u0026gt;\u0026gt;\u0026gt; c.days 2 \u0026gt;\u0026gt;\u0026gt; c.seconds 37800 \u0026gt;\u0026gt;\u0026gt; c.seconds / 3600 10.5 \u0026gt;\u0026gt;\u0026gt; c.total_seconds() / 3600 58.5 \u0026gt;\u0026gt;\u0026gt; from dateutil.relativedelta import relativedelta \u0026gt;\u0026gt;\u0026gt; a + relativedelta(months=+1) datetime.datetime(2012, 10, 23, 0, 0) \u0026gt;\u0026gt;\u0026gt; a + relativedelta(months=+4) datetime.datetime(2013, 1, 23, 0, 0) 计算最后一个周五的日期 \u0026gt;\u0026gt;\u0026gt; from datetime import datetime \u0026gt;\u0026gt;\u0026gt; from dateutil.relativedelta import relativedelta \u0026gt;\u0026gt;\u0026gt; from dateutil.rrule import * \u0026gt;\u0026gt;\u0026gt; d = datetime.now() \u0026gt;\u0026gt;\u0026gt; print(d) 2012-12-23 16:31:52.718111 \u0026gt;\u0026gt;\u0026gt; # Next Friday \u0026gt;\u0026gt;\u0026gt; print(d + relativedelta(weekday=FR)) 2012-12-28 16:31:52.718111 \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; # Last Friday \u0026gt;\u0026gt;\u0026gt; print(d + relativedelta(weekday=FR(-1))) 2012-12-21 16:31:52.718111 \u0026gt;\u0026gt;\u0026gt; 计算当前月份的日期范围 from datetime import datetime, date, timedelta import calendar def get_month_range(start_date=None): if start_date is None: start_date = date.today().replace(day=1) _, days_in_month = calendar.monthrange(start_date.year, start_date.month) end_date = start_date + timedelta(days=days_in_month) return (start_date, end_date) 字符串转换为日期 \u0026gt;\u0026gt;\u0026gt; from datetime import datetime \u0026gt;\u0026gt;\u0026gt; text = \u0026#39;2012-09-20\u0026#39; \u0026gt;\u0026gt;\u0026gt; y = datetime.strptime(text, \u0026#39;%Y-%m-%d\u0026#39;) 结合时区的日期操作 \u0026gt;\u0026gt;\u0026gt; from datetime import datetime \u0026gt;\u0026gt;\u0026gt; from pytz import timezone \u0026gt;\u0026gt;\u0026gt; d = datetime(2012, 12, 21, 9, 30, 0) \u0026gt;\u0026gt;\u0026gt; # Localize the date for Chicago \u0026gt;\u0026gt;\u0026gt; central = timezone(\u0026#39;US/Central\u0026#39;) \u0026gt;\u0026gt;\u0026gt; loc_d = central.localize(d) \u0026gt;\u0026gt;\u0026gt; print(loc_d) \u0026gt;\u0026gt;\u0026gt; # Convert to Bangalore time \u0026gt;\u0026gt;\u0026gt; bang_d = loc_d.astimezone(timezone(\u0026#39;Asia/Kolkata\u0026#39;)) \u0026gt;\u0026gt;\u0026gt; print(bang_d) \u0026gt;\u0026gt;\u0026gt; utc_d = loc_d.astimezone(pytz.utc) \u0026gt;\u0026gt;\u0026gt; print(utc_d) 迭代器和生成器 手动遍历迭代器 def manual_iter(): with open(\u0026#39;/etc/passwd\u0026#39;) as f: try: while True: line = next(f) print(line, end=\u0026#39;\u0026#39;) except StopIteration: pass 代理迭代和生成器函数 \u0026#34;\u0026#34;\u0026#34;文件说明 ：演示实现深度优先遍历及广度优先遍历\u0026#34;\u0026#34;\u0026#34; class Node: def __init__(self, value): self._value = value self._children = [] def __repr__(self): return \u0026#39;Node{!r}\u0026#39;.format(self._value) def add_child(self, node): self._children.append(node) def __iter__(self): return iter(self._children) def depth_first(self): yield self for ch in self: yield from ch.depth_first() def breadth_first(self): stack = [self] while stack: current = stack.pop(0) yield current stack.extend(current._children) if __name__ == \u0026#39;__main__\u0026#39;: root = Node(0) child1 = Node(1) child2 = Node(2) child3 = Node(3) child4 = Node(4) child5 = Node(5) child6 = Node(6) child7 = Node(7) child8 = Node(8) child9 = Node(9) child10 = Node(10) child11 = Node(11) child12 = Node(12) child13 = Node(13) child14 = Node(14) child3.add_child(child7) child3.add_child(child8) child5.add_child(child9) child5.add_child(child10) child4.add_child(child11) child4.add_child(child12) child6.add_child(child13) child6.add_child(child14) child1.add_child(child3) child1.add_child(child5) child2.add_child(child4) child2.add_child(child6) root.add_child(child1) root.add_child(child2) for ch in root.depth_first(): print(ch) for ch in root.breadth_first(): print(ch) 反向迭代 class Countdown: def __init__(self, start): self.start = start # Forward iterator def __iter__(self): n = self.start while n \u0026gt; 0: yield n n -= 1 # Reverse iterator def __reversed__(self): n = 1 while n \u0026lt;= self.start: yield n n += 1 for rr in reversed(Countdown(30)): print(rr) 带有外部状态的生成器函数 from collections import deque class linehistory: def __init__(self, lines, histlen=3): self.lines = lines self.history = deque(maxlen=histlen) def __iter__(self): for lineno, line in enumerate(self.lines, 1): self.history.append((lineno, line)) yield line def clear(self): self.history.clear() with open(\u0026#39;somefile.txt\u0026#39;) as f: lines = linehistory(f) for line in lines: if \u0026#39;python\u0026#39; in line: for lineno, hline in lines.history: print(\u0026#39;{}:{}\u0026#39;.format(lineno, hline), end=\u0026#39;\u0026#39;) 迭代器切片 \u0026gt;\u0026gt;\u0026gt; def count(n): ... while True: ... yield n ... n += 1 ... \u0026gt;\u0026gt;\u0026gt; c = count(0) \u0026gt;\u0026gt;\u0026gt; import itertools \u0026gt;\u0026gt;\u0026gt; for x in itertools.islice(c, 10, 20): ... print(x) ... 跳过可迭代对象的开始部分 \u0026gt;\u0026gt;\u0026gt; from itertools import dropwhile \u0026gt;\u0026gt;\u0026gt; with open(\u0026#39;/etc/passwd\u0026#39;) as f: ... for line in dropwhile(lambda line: line.startswith(\u0026#39;#\u0026#39;), f): ... print(line, end=\u0026#39;\u0026#39;) ... 序列上索引值迭代 \u0026gt;\u0026gt;\u0026gt; my_list = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;] \u0026gt;\u0026gt;\u0026gt; for idx, val in enumerate(my_list, 1): ... print(idx, val) ... 1 a 2 b 3 c 同时迭代多个序列 \u0026gt;\u0026gt;\u0026gt; xpts = [1, 5, 4, 2, 10, 7] \u0026gt;\u0026gt;\u0026gt; ypts = [101, 78, 37, 15, 62, 99] \u0026gt;\u0026gt;\u0026gt; for x, y in zip(xpts, ypts): ... print(x,y) ... \u0026gt;\u0026gt;\u0026gt; from itertools import zip_longest \u0026gt;\u0026gt;\u0026gt; for i in zip_longest(a,b): ... print(i) ... 不同集合上元素的迭代 from itertools import chain import os def gen_opener(filenames): for filename in filenames: with open(filename, \u0026#39;rt\u0026#39;) as f: yield f def gen_concatenate(files): yield from chain.from_iterable(files) # for file in files: # yield from file files = gen_opener(iter([os.path.expanduser(\u0026#39;~/.zshrc\u0026#39;), os.path.expanduser(\u0026#39;~/.bash_history\u0026#39;)])) print(files) lines = gen_concatenate(files) for line in lines: print(line) 顺序迭代合并后的排序迭代对象 import heapq with open(\u0026#39;sorted_file_1\u0026#39;, \u0026#39;rt\u0026#39;) as file1, \\ open(\u0026#39;sorted_file_2\u0026#39;, \u0026#39;rt\u0026#39;) as file2, \\ open(\u0026#39;merged_file\u0026#39;, \u0026#39;wt\u0026#39;) as outf: for line in heapq.merge(file1, file2): outf.write(line) 迭代器代替while无限循环 \u0026gt;\u0026gt;\u0026gt; import sys \u0026gt;\u0026gt;\u0026gt; f = open(\u0026#39;/etc/passwd\u0026#39;) \u0026gt;\u0026gt;\u0026gt; CHUNKSIZE = 8192 \u0026gt;\u0026gt;\u0026gt; for chunk in iter(lambda: f.read(CHUNKSIZE), b\u0026#39;\u0026#39;): ... n = sys.stdout.write(chunk) ... 文件和IO 打印输出至文件中 with open(\u0026#39;d:/work/test.txt\u0026#39;, \u0026#39;wt\u0026#39;) as f: print(\u0026#39;Hello World!\u0026#39;, file=f) 使用其他分隔符或行终止符打印 \u0026gt;\u0026gt;\u0026gt; print(\u0026#39;ACME\u0026#39;, 50, 91.5) ACME 50 91.5 \u0026gt;\u0026gt;\u0026gt; print(\u0026#39;ACME\u0026#39;, 50, 91.5, sep=\u0026#39;,\u0026#39;) ACME,50,91.5 \u0026gt;\u0026gt;\u0026gt; print(\u0026#39;ACME\u0026#39;, 50, 91.5, sep=\u0026#39;,\u0026#39;, end=\u0026#39;!!\\n\u0026#39;) ACME,50,91.5!! \u0026gt;\u0026gt;\u0026gt; row = (\u0026#39;ACME\u0026#39;, 50, 91.5) \u0026gt;\u0026gt;\u0026gt; print(*row, sep=\u0026#39;,\u0026#39;) 字符串的I/O操作 \u0026gt;\u0026gt;\u0026gt; s = io.StringIO() \u0026gt;\u0026gt;\u0026gt; s.write(\u0026#39;Hello World\\n\u0026#39;) 12 \u0026gt;\u0026gt;\u0026gt; print(\u0026#39;This is a test\u0026#39;, file=s) 15 \u0026gt;\u0026gt;\u0026gt; # Get all of the data written so far \u0026gt;\u0026gt;\u0026gt; s.getvalue() \u0026#39;Hello World\\nThis is a test\\n\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; # Wrap a file interface around an existing string \u0026gt;\u0026gt;\u0026gt; s = io.StringIO(\u0026#39;Hello\\nWorld\\n\u0026#39;) \u0026gt;\u0026gt;\u0026gt; s.read(4) \u0026#39;Hell\u0026#39; \u0026gt;\u0026gt;\u0026gt; s.read() \u0026#39;o\\nWorld\\n\u0026#39; \u0026gt;\u0026gt;\u0026gt; 固定大小记录的文件迭代 from functools import partial RECORD_SIZE = 32 with open(\u0026#39;somefile.data\u0026#39;, \u0026#39;rb\u0026#39;) as f: records = iter(partial(f.read, RECORD_SIZE), b\u0026#39;\u0026#39;) for r in records: ... 读取二进制数据到可变缓冲区中 import os.path def read_into_buffer(filename): buf = bytearray(os.path.getsize(filename)) with open(filename, \u0026#39;rb\u0026#39;) as f: f.readinto(buf) return buf 文件路径名的操作 \u0026gt;\u0026gt;\u0026gt; import os \u0026gt;\u0026gt;\u0026gt; path = \u0026#39;/Users/beazley/Data/data.csv\u0026#39; \u0026gt;\u0026gt;\u0026gt; # Get the last component of the path \u0026gt;\u0026gt;\u0026gt; os.path.basename(path) \u0026#39;data.csv\u0026#39; \u0026gt;\u0026gt;\u0026gt; # Get the directory name \u0026gt;\u0026gt;\u0026gt; os.path.dirname(path) \u0026#39;/Users/beazley/Data\u0026#39; \u0026gt;\u0026gt;\u0026gt; # Join path components together \u0026gt;\u0026gt;\u0026gt; os.path.join(\u0026#39;tmp\u0026#39;, \u0026#39;data\u0026#39;, os.path.basename(path)) \u0026#39;tmp/data/data.csv\u0026#39; \u0026gt;\u0026gt;\u0026gt; # Expand the user\u0026#39;s home directory \u0026gt;\u0026gt;\u0026gt; path = \u0026#39;~/Data/data.csv\u0026#39; \u0026gt;\u0026gt;\u0026gt; os.path.expanduser(path) \u0026#39;/Users/beazley/Data/data.csv\u0026#39; \u0026gt;\u0026gt;\u0026gt; # Split the file extension \u0026gt;\u0026gt;\u0026gt; os.path.splitext(path) (\u0026#39;~/Data/data\u0026#39;, \u0026#39;.csv\u0026#39;) \u0026gt;\u0026gt;\u0026gt; 获取文件夹中的文件列表 import os.path # Get all regular files names = [name for name in os.listdir(\u0026#39;somedir\u0026#39;) if os.path.isfile(os.path.join(\u0026#39;somedir\u0026#39;, name))] # Get all dirs dirnames = [name for name in os.listdir(\u0026#39;somedir\u0026#39;) if os.path.isdir(os.path.join(\u0026#39;somedir\u0026#39;, name))] import glob pyfiles = glob.glob(\u0026#39;somedir/*.py\u0026#39;) from fnmatch import fnmatch pyfiles = [name for name in os.listdir(\u0026#39;somedir\u0026#39;) if fnmatch(name, \u0026#39;*.py\u0026#39;)] 打印不合法的文件名 def bad_filename(filename): temp = filename.encode(sys.getfilesystemencoding(), errors=\u0026#39;surrogateescape\u0026#39;) return temp.decode(\u0026#39;latin-1\u0026#39;) \u0026gt;\u0026gt;\u0026gt; for name in files: ... try: ... print(name) ... except UnicodeEncodeError: ... print(bad_filename(name)) ... 增加或改变已打开文件的编码 \u0026gt;\u0026gt;\u0026gt; import sys \u0026gt;\u0026gt;\u0026gt; sys.stdout.encoding \u0026#39;UTF-8\u0026#39; \u0026gt;\u0026gt;\u0026gt; sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding=\u0026#39;latin-1\u0026#39;) \u0026gt;\u0026gt;\u0026gt; sys.stdout.encoding \u0026#39;latin-1\u0026#39; \u0026gt;\u0026gt;\u0026gt; 将文件描述符包装成文件对象 # Open a low-level file descriptor import os fd = os.open(\u0026#39;somefile.txt\u0026#39;, os.O_WRONLY | os.O_CREAT) # Turn into a proper file f = open(fd, \u0026#39;wt\u0026#39;) f.write(\u0026#39;hello world\\n\u0026#39;) f.close() 创建临时文件和文件夹 from tempfile import NamedTemporaryFile, TemporaryDirectory with NamedTemporaryFile(\u0026#39;wt\u0026#39;, prefix=\u0026#39;tmp_\u0026#39;, suffix=\u0026#39;.txt\u0026#39;) as f: print(f.name) with TemporaryDirectory(prefix=\u0026#39;tmp_dir_\u0026#39;) as dir: print(dir) 序列化Python对象 import pickle data = [1, 2, 3] with open(\u0026#39;somefile\u0026#39;, \u0026#39;wb\u0026#39;) as f: pickle.dump(data, f) print(pickle.dumps(data)) # Restore from a file f = open(\u0026#39;somefile\u0026#39;, \u0026#39;rb\u0026#39;) data = pickle.load(f) # Restore from a string data = pickle.loads(s) import time import threading class Countdown: def __init__(self, n): self.n = n self.thr = threading.Thread(target=self.run) self.thr.daemon = True self.thr.start() def run(self): while self.n \u0026gt; 0: print(\u0026#39;T-minus\u0026#39;, self.n) self.n -= 1 time.sleep(5) def __getstate__(self): return self.n def __setstate__(self, n): self.__init__(n) 数据编码和处理 读写CSV数据 import csv import re col_types = [str, float, str, str, float, int] with open(\u0026#39;stock.csv\u0026#39;) as f: f_csv = csv.reader(f) headers = [ re.sub(\u0026#39;[^a-zA-Z_]\u0026#39;, \u0026#39;_\u0026#39;, h) for h in next(f_csv) ] Row = namedtuple(\u0026#39;Row\u0026#39;, headers) for row in f_csv: # Apply conversions to the row items row = Row(convert(value) for convert, value in zip(col_types, row)) headers = [\u0026#39;Symbol\u0026#39;, \u0026#39;Price\u0026#39;, \u0026#39;Date\u0026#39;, \u0026#39;Time\u0026#39;, \u0026#39;Change\u0026#39;, \u0026#39;Volume\u0026#39;] rows = [{\u0026#39;Symbol\u0026#39;:\u0026#39;AA\u0026#39;, \u0026#39;Price\u0026#39;:39.48, \u0026#39;Date\u0026#39;:\u0026#39;6/11/2007\u0026#39;, \u0026#39;Time\u0026#39;:\u0026#39;9:36am\u0026#39;, \u0026#39;Change\u0026#39;:-0.18, \u0026#39;Volume\u0026#39;:181800}, {\u0026#39;Symbol\u0026#39;:\u0026#39;AIG\u0026#39;, \u0026#39;Price\u0026#39;: 71.38, \u0026#39;Date\u0026#39;:\u0026#39;6/11/2007\u0026#39;, \u0026#39;Time\u0026#39;:\u0026#39;9:36am\u0026#39;, \u0026#39;Change\u0026#39;:-0.15, \u0026#39;Volume\u0026#39;: 195500}, {\u0026#39;Symbol\u0026#39;:\u0026#39;AXP\u0026#39;, \u0026#39;Price\u0026#39;: 62.58, \u0026#39;Date\u0026#39;:\u0026#39;6/11/2007\u0026#39;, \u0026#39;Time\u0026#39;:\u0026#39;9:36am\u0026#39;, \u0026#39;Change\u0026#39;:-0.46, \u0026#39;Volume\u0026#39;: 935000}, ] with open(\u0026#39;stocks.csv\u0026#39;,\u0026#39;w\u0026#39;) as f: f_csv = csv.DictWriter(f, headers) f_csv.writeheader() f_csv.writerows(rows) 读写JSON数据 import json data = { \u0026#39;name\u0026#39; : \u0026#39;ACME\u0026#39;, \u0026#39;shares\u0026#39; : 100, \u0026#39;price\u0026#39; : 542.23 } json_str = json.dumps(data) data = json.loads(json_str) # Writing JSON data with open(\u0026#39;data.json\u0026#39;, \u0026#39;w\u0026#39;) as f: json.dump(data, f) # Reading data back with open(\u0026#39;data.json\u0026#39;, \u0026#39;r\u0026#39;) as f: data = json.load(f) 解析和修改XML \u0026gt;\u0026gt;\u0026gt; from xml.etree.ElementTree import parse, Element \u0026gt;\u0026gt;\u0026gt; doc = parse(\u0026#39;pred.xml\u0026#39;) \u0026gt;\u0026gt;\u0026gt; root = doc.getroot() \u0026gt;\u0026gt;\u0026gt; root \u0026lt;Element \u0026#39;stop\u0026#39; at 0x100770cb0\u0026gt; \u0026gt;\u0026gt;\u0026gt; # Remove a few elements \u0026gt;\u0026gt;\u0026gt; root.remove(root.find(\u0026#39;sri\u0026#39;)) \u0026gt;\u0026gt;\u0026gt; root.remove(root.find(\u0026#39;cr\u0026#39;)) \u0026gt;\u0026gt;\u0026gt; # Insert a new element after \u0026lt;nm\u0026gt;...\u0026lt;/nm\u0026gt; \u0026gt;\u0026gt;\u0026gt; root.getchildren().index(root.find(\u0026#39;nm\u0026#39;)) 1 \u0026gt;\u0026gt;\u0026gt; e = Element(\u0026#39;spam\u0026#39;) \u0026gt;\u0026gt;\u0026gt; e.text = \u0026#39;This is a test\u0026#39; \u0026gt;\u0026gt;\u0026gt; root.insert(2, e) \u0026gt;\u0026gt;\u0026gt; # Write back to a file \u0026gt;\u0026gt;\u0026gt; doc.write(\u0026#39;newpred.xml\u0026#39;, xml_declaration=True) \u0026gt;\u0026gt;\u0026gt; 与关系型数据库的交互 stocks = [ (\u0026#39;GOOG\u0026#39;, 100, 490.1), (\u0026#39;AAPL\u0026#39;, 50, 545.75), (\u0026#39;FB\u0026#39;, 150, 7.45), (\u0026#39;HPQ\u0026#39;, 75, 33.2), ] \u0026gt;\u0026gt;\u0026gt; import sqlite3 \u0026gt;\u0026gt;\u0026gt; db = sqlite3.connect(\u0026#39;database.db\u0026#39;) \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; c = db.cursor() \u0026gt;\u0026gt;\u0026gt; c.execute(\u0026#39;create table portfolio (symbol text, shares integer, price real)\u0026#39;) \u0026lt;sqlite3.Cursor object at 0x10067a730\u0026gt; \u0026gt;\u0026gt;\u0026gt; db.commit() \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; c.executemany(\u0026#39;insert into portfolio values (?,?,?)\u0026#39;, stocks) \u0026lt;sqlite3.Cursor object at 0x10067a730\u0026gt; \u0026gt;\u0026gt;\u0026gt; db.commit() \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; for row in db.execute(\u0026#39;select * from portfolio\u0026#39;): ... print(row) ... \u0026gt;\u0026gt;\u0026gt; min_price = 100 \u0026gt;\u0026gt;\u0026gt; for row in db.execute(\u0026#39;select * from portfolio where price \u0026gt;= ?\u0026#39;, (min_price,)): ... print(row) ... 编码和解码十六进制数 \u0026gt;\u0026gt;\u0026gt; # Initial byte string \u0026gt;\u0026gt;\u0026gt; s = b\u0026#39;hello\u0026#39; \u0026gt;\u0026gt;\u0026gt; # Encode as hex \u0026gt;\u0026gt;\u0026gt; import binascii \u0026gt;\u0026gt;\u0026gt; h = binascii.b2a_hex(s) \u0026gt;\u0026gt;\u0026gt; h b\u0026#39;68656c6c6f\u0026#39; \u0026gt;\u0026gt;\u0026gt; # Decode back to bytes \u0026gt;\u0026gt;\u0026gt; binascii.a2b_hex(h) b\u0026#39;hello\u0026#39; \u0026gt;\u0026gt;\u0026gt; 编码解码Base64数据 \u0026gt;\u0026gt;\u0026gt; # Some byte data \u0026gt;\u0026gt;\u0026gt; s = b\u0026#39;hello\u0026#39; \u0026gt;\u0026gt;\u0026gt; import base64 \u0026gt;\u0026gt;\u0026gt; # Encode as Base64 \u0026gt;\u0026gt;\u0026gt; a = base64.b64encode(s) \u0026gt;\u0026gt;\u0026gt; a b\u0026#39;aGVsbG8=\u0026#39; \u0026gt;\u0026gt;\u0026gt; # Decode from Base64 \u0026gt;\u0026gt;\u0026gt; base64.b64decode(a) b\u0026#39;hello\u0026#39; \u0026gt;\u0026gt;\u0026gt; 读写二进制数组数据 from struct import Struct def write_records(records, format, f): \u0026#39;\u0026#39;\u0026#39;Write a sequence of tuples to a binary file of structures.\u0026#39;\u0026#39;\u0026#39; record_struct = Struct(format) for r in records: f.write(record_struct.pack(*r)) # Example if __name__ == \u0026#39;__main__\u0026#39;: records = [ (1, 2.3, 4.5), (6, 7.8, 9.0), (12, 13.4, 56.7) ] with open(\u0026#39;data.b\u0026#39;, \u0026#39;wb\u0026#39;) as f: write_records(records, \u0026#39;\u0026lt;idd\u0026#39;, f) def read_records(format, f): record_struct = Struct(format) chunks = iter(lambda: f.read(record_struct.size), b\u0026#39;\u0026#39;) return (record_struct.unpack(chunk) for chunk in chunks) # Example if __name__ == \u0026#39;__main__\u0026#39;: with open(\u0026#39;data.b\u0026#39;,\u0026#39;rb\u0026#39;) as f: for rec in read_records(\u0026#39;\u0026lt;idd\u0026#39;, f): # Process rec ... the first character of the format string can be used to indicate the byte order, size and alignment of the packed data, according to the following table:\n   Character Byte order Size Alignment     @ native native native   = native standard none   \u0026lt; little-endian standard none   \u0026gt; big-endian standard none   ! network (= big-endian) standard none    If the first character is not one of these, '@' is assumed.\nFormat characters have the following meaning; the conversion between C and Python values should be obvious given their types. The ‘Standard size’ column refers to the size of the packed value in bytes when using standard size; that is, when the format string starts with one of '\u0026lt;', '\u0026gt;', '!' or '='. When using native size, the size of the packed value is platform-dependent.\n   Format C Type Python type Standard size Notes     x pad byte no value     c char bytes of length 1 1    b signed char integer 1 (1),(3)   B unsigned char integer 1 (3)   ? _Bool bool 1 (1)   h short integer 2 (3)   H unsigned short integer 2 (3)   i int integer 4 (3)   I unsigned int integer 4 (3)   l long integer 4 (3)   L unsigned long integer 4 (3)   q long long integer 8 (2), (3)   Q unsigned long long integer 8 (2), (3)   n ssize_t integer  (4)   N size_t integer  (4)   e (7) float 2 (5)   f float float 4 (5)   d double float 8 (5)   s char[] bytes     p char[] bytes     P void * integer  (6)    Changed in version 3.3: Added support for the 'n' and 'N' formats.\nChanged in version 3.6: Added support for the 'e' format.\n读取嵌套和可变长二进制数据 polys = [ [ (1.0, 2.5), (3.5, 4.0), (2.5, 1.5) ], [ (7.0, 1.2), (5.1, 3.0), (0.5, 7.5), (0.8, 9.0) ], [ (3.4, 6.3), (1.2, 0.5), (4.6, 9.2) ], ] 数据要被编码到一个以下列头部开始的二进制文件中去了：\n+------+--------+------------------------------------+ |Byte | Type | Description | +======+========+====================================+ |0 | int | 文件代码（0x1234，小端） | +------+--------+------------------------------------+ |4 | double | x 的最小值（小端） | +------+--------+------------------------------------+ |12 | double | y 的最小值（小端） | +------+--------+------------------------------------+ |20 | double | x 的最大值（小端） | +------+--------+------------------------------------+ |28 | double | y 的最大值（小端） | +------+--------+------------------------------------+ |36 | int | 三角形数量（小端） | +------+--------+------------------------------------+ 紧跟着头部是一系列的多边形记录，编码格式如下：\n+------+--------+-------------------------------------------+ |Byte | Type | Description | +======+========+===========================================+ |0 | int | 记录长度（N字节） | +------+--------+-------------------------------------------+ |4-N | Points | (X,Y) 坐标，以浮点数表示 | +------+--------+-------------------------------------------+ import struct import itertools def write_polys(filename, polys): # Determine bounding box flattened = list(itertools.chain(*polys)) min_x = min(x for x, y in flattened) max_x = max(x for x, y in flattened) min_y = min(y for x, y in flattened) max_y = max(y for x, y in flattened) with open(filename, \u0026#39;wb\u0026#39;) as f: f.write(struct.pack(\u0026#39;\u0026lt;iddddi\u0026#39;, 0x1234, min_x, min_y, max_x, max_y, len(polys))) for poly in polys: size = len(poly) * struct.calcsize(\u0026#39;\u0026lt;dd\u0026#39;) f.write(struct.pack(\u0026#39;\u0026lt;i\u0026#39;, size + 4)) for pt in poly: f.write(struct.pack(\u0026#39;\u0026lt;dd\u0026#39;, *pt)) def read_polys(filename): with open(filename, \u0026#39;rb\u0026#39;) as f: # Read the header header = f.read(40) file_code, min_x, min_y, max_x, max_y, num_polys = \\ struct.unpack(\u0026#39;\u0026lt;iddddi\u0026#39;, header) polys = [] for n in range(num_polys): pbytes, = struct.unpack(\u0026#39;\u0026lt;i\u0026#39;, f.read(4)) poly = [] for m in range(pbytes // 16): pt = struct.unpack(\u0026#39;\u0026lt;dd\u0026#39;, f.read(16)) poly.append(pt) polys.append(poly) return polys ","permalink":"https://jeremyxu2010.github.io/2017/10/py3_cookbook_notes_01/","tags":["python"],"title":"py3_cookbook_notes_01"},{"categories":["数据库开发"],"contents":"数据库事务备忘 以前对数据库事务的隔离级别概念不是很清楚，今天看到一篇文章，将这个事情讲得比较清楚，这里记录一下。\n数据库事务的特性  原子性（Atomicity）：原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 一致性（Consistency）：事务前后数据的完整性必须保持一致。在事务执行之前数据库是符合数据完整性约束的，无论事务是否执行成功，事务结束后的数据库中的数据也应该是符合完整性约束的。在某一时间点，如果数据库中的所有记录都能保证满足当前数据库中的所有约束，则可以说当前的数据库是符合数据完整性约束的。比如删部门表前应该删掉关联员工（已经建立外键），如果数据库服务器发生错误，有一个员工没删掉，那么此时员工的部门表已经删除，那么就不符合完整性约束了，所以这样的数据库也就性能太差啦！ 隔离性（Isolation）：事务的隔离性是指多个用户并发访问数据库时，一个用户的事务不能被其它用户的事务所干扰，多个并发事务之间数据要相互隔离。 持久性（Durability）：持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来即使数据库发生故障也不应该对其有任何影响。  事务隔离性问题 如果不考虑事务的隔离性，会出现以下问题：\n  脏读：指一个线程中的事务读取到了另外一个线程中未提交的数据。\n  不可重复读（虚读）：指一个线程中的事务读取到了另外一个线程中提交的update的数据。\n  幻读：指一个线程中的事务读取到了另外一个线程中提交的insert的数据。\n  将数据库设计为串行化程的数据库，让一张表在同一时间内只能有一个线程来操作。如果将数据库设计为这样，那数据库的效率太低了。所以数据库的设计并没有直接将数据库设计为串行化，而是为数据库提供多个隔离级别选项，使数据库的使用者可以根据使用情况自己定义到底需要什么样的隔离级别，不同隔离级别可以解决一些问题：\n   隔离级别 脏读（Dirty Read） 不可重复读（NonRepeatable Read） 幻读（Phantom Read）     未提交读（Read uncommitted） 可能 可能 可能   已提交读（Read committed） 不可能 可能 可能   可重复读（Repeatable read） 不可能 不可能 可能   可串行化（Serializable ） 不可能 不可能 不可能    级别越高，数据越安全，但性能越低。\n不可重复读与幻读的区别 不可重复读与幻读比较相似，都是在一个事务中多次读取到不同的数据，但两者还是有一些区别。\n不可重复读 所谓的虚读，也就是大家经常说的不可重复读，是指在数据库访问中，一个事务范围内两个相同的查询却返回了不同数据。这是由于查询时系统中其他事务修改的提交而引起的。比如事务T1读取某一数据，事务T2读取并修改了该数据，T1为了对读取值进行检验而再次读取该数据，便得到了不同的结果。 一种更易理解的说法是：在一个事务内，多次读同一个数据。在这个事务还没有结束时，另 一个事务也访问该同一数据。那么，在第一个事务的两次读数据之间。由于第二个事务的修改，那么第一个事务读到的数据可能不一样，这样就发生了在一个事务内 两次读到的数据是不一样的，因此称为不可重复读，即原始读取不可重复。\n幻读 所谓幻读，是指事务A读取与搜索条件相匹配的若干行。事务B以插入或删除行等方式来修改事务A的结果集，然后再提交。 幻读是指当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，比如这种修改涉及到表中的“全部数据行”。同时，第二个事务也 修改这个表中的数据，这种修改是向表中插入“一行新数据”。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一 样，一般解决幻读的方法是增加范围锁RangeS，锁定检索范围为只读，这样就避免了幻读。简单来说，幻读是由插入或者删除引起的。\n两者大致的区别在于不可重复读是由于另一个事务对数据的更改所造成的，而幻读是由于另一个事务插入或删除引起的。\n从总的结果来看, 似乎两者都表现为两次读取的结果不一致，但如果你从控制的角度来看, 两者的区别就比较大：\n  对于前者, 只需要锁住满足条件的记录\n  对于后者, 要锁住满足条件及其相近的记录\n  mysql事务相关操作 show engines; # 查看数据库支持的存储引擎 show variables like \u0026#39;%storage_engine%\u0026#39;; # 当前的存储引擎 select @@tx_isolation; # 查看当前的事务隔离级别，InnoDB的默认隔离级别是REPEATABLE-READ select @@global.tx_isolation; # 查看系统当前隔离级别 set [global/session] transaction isolation level xxxx; # 设置隔离级别 另外理论上在MySQL中InnoDB的默认隔离级别是REPEATABLE-READ，还是会有幻读的问题，但MySQL5.5以上版本InnoDB存储引擎通过多版本并发控制(MVCC，Multiversion Concurrency Control)机制解决了该问题。\n","permalink":"https://jeremyxu2010.github.io/2017/10/%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1%E5%A4%87%E5%BF%98/","tags":["mysql","transaction","isolation"],"title":"数据库事务备忘"},{"categories":["java开发"],"contents":"Java VisualVM使用备忘 一直觉得JDK带的新版诊断工具VisualVM功能都没有原来的jconsole强大，今天偶然翻到了VisualVM的github主页，看了下文档，发现简单配置下，功能还是很强大的。\n安装插件 默认带的功能看起来还不如jconsole，但其实装上插件就很强大了。不过我本机默认配置的插件更新地址还是java.net的，根本没法安装插件，在这里找到了对应版本的更新地址，比如我本机是JDK1.8.0_102自带的VisualVM，因此选择https://visualvm.github.io/archive/uc/8u40/updates.xml.gz，将其填到下图的位置：\n然后就可以安装插件了，这里有主要插件的描述，可以根据需要自行安装，我本机安装了以下这些插件：\n安装后，VisualVM的功能看起来就很强大了，比jconsole强不少了，还美观。\n连接远程JVM VisualVM默认是可以连接本机的JVM的，如果要连远程服务器上的JVM，则要在上面启动jstatd，启动方法如下：\n# 创建jstatd运行时的安全策略文件，注意要填写正确的tools.jar路径 echo \u0026#34;grant codebase \u0026#34;file:/Library/Java/Home/lib/tools.jar\u0026#34; { permission java.security.AllPermission; };\u0026#34; \u0026gt; jstatd.all.policy # 启动jstatd jstatd -J-Djava.security.policy=jstatd.all.policy 然后在VisualVM里填入远程服务器的IP地址，即可连接上该服务器上的JVM进行管理了。\n","permalink":"https://jeremyxu2010.github.io/2017/10/java-visualvm%E4%BD%BF%E7%94%A8%E5%A4%87%E5%BF%98/","tags":["java","visualvm","jstatd"],"title":"Java VisualVM使用备忘"},{"categories":["python开发"],"contents":"python开发小技巧 今天在工作中写了一个python脚本从数据库中导数据，其中用到了一些技巧，在这里记录一下。\n判断字符串仅包含英文 直接通过字符的ord来判断\ndef is_pure_english(check_str): return all(ord(c) \u0026lt; 128 for c in check_str) 判断字符串中包含某些语言的字符 根据字符的unicode范围判断是否包含某些语言的字符\ndef contains_invalid_lang_chs(check_str): check_str=check_str.strip() # 判断包含任何阿拉伯文、朝鲜文、日文平假名、日文片假名、日文片假名语音扩展、朝鲜文音节、俄文（西里尔字母、西里尔字母补充） return any((u\u0026#39;\\u0600\u0026#39; \u0026lt;= c \u0026lt;= u\u0026#39;\\u06FF\u0026#39;) or (u\u0026#39;\\u1100\u0026#39; \u0026lt;= c \u0026lt;= u\u0026#39;\\u11FF\u0026#39;) or (u\u0026#39;\\u3040\u0026#39; \u0026lt;= c \u0026lt;= u\u0026#39;\\u309F\u0026#39;) or (u\u0026#39;\\u30A0\u0026#39; \u0026lt;= c \u0026lt;= u\u0026#39;\\u30FF\u0026#39;) or (u\u0026#39;\\u31F0\u0026#39; \u0026lt;= c \u0026lt;= u\u0026#39;\\u31FF\u0026#39;) or (u\u0026#39;\\uAC00\u0026#39; \u0026lt;= c \u0026lt;= u\u0026#39;\\uD7AF\u0026#39;) or (u\u0026#39;\\u0400\u0026#39; \u0026lt;= c \u0026lt;= u\u0026#39;\\u052F\u0026#39;) for c in check_str) 完整的UNICODE编码表如下：\n   十进制 Unicode 编码 十六进制 Unicode 编码 字符数 编码分类（中文） 编码分类（英文）       起始 终止 起始 终止 (个)     0 127 0 007F 128 C0控制符及基本拉丁文 C0 Control and Basic Latin   128 255 80 00FF 128 C1控制符及拉丁文补充-1 C1 Control and Latin 1 Supplement   256 383 100 017F 128 拉丁文扩展-A Latin Extended-A   384 591 180 024F 208 拉丁文扩展-B Latin Extended-B   592 687 250 02AF 96 国际音标扩展 IPA Extensions   688 767 02B0 02FF 80 空白修饰字母 Spacing Modifiers   768 879 300 036F 112 结合用读音符号 Combining Diacritics Marks   880 1023 370 03FF 144 希腊文及科普特文 Greek and Coptic   1024 1279 400 04FF 256 西里尔字母 Cyrillic   1280 1327 500 052F 48 西里尔字母补充 Cyrillic Supplement   1328 1423 530 058F 96 亚美尼亚语 Armenian   1424 1535 590 05FF 112 希伯来文 Hebrew   1536 1791 600 06FF 256 阿拉伯文 Arabic   1792 1871 700 074F 80 叙利亚文 Syriac   1872 1919 750 077F 48 阿拉伯文补充 Arabic Supplement   1920 1983 780 07BF 64 马尔代夫语 Thaana   1984 2047 07C0 07FF 64 西非書面語言 N'Ko   2048 2143 800 085F 96 阿维斯塔语及巴列维语 Avestan and Pahlavi   2144 2175 860 087F 32 Mandaic Mandaic   2176 2223 880 08AF 48 撒马利亚语 Samaritan   2304 2431 900 097F 128 天城文书 Devanagari   2432 2559 980 09FF 128 孟加拉语 Bengali   2560 2687 0A00 0A7F 128 锡克教文 Gurmukhi   2688 2815 0A80 0AFF 128 古吉拉特文 Gujarati   2816 2943 0B00 0B7F 128 奥里亚文 Oriya   2944 3071 0B80 0BFF 128 泰米尔文 Tamil   3072 3199 0C00 0C7F 128 泰卢固文 Telugu   3200 3327 0C80 0CFF 128 卡纳达文 Kannada   3328 3455 0D00 0D7F 128 德拉维族语 Malayalam   3456 3583 0D80 0DFF 128 僧伽罗语 Sinhala   3584 3711 0E+00 0E7F 128 泰文 Thai   3712 3839 0E+00 0EFF 128 老挝文 Lao   3840 4095 0F00 0FFF 256 藏文 Tibetan   4096 4255 1000 109F 160 缅甸语 Myanmar   4256 4351 10A0 10FF 96 格鲁吉亚语 Georgian   4352 4607 1100 11FF 256 朝鲜文 Hangul Jamo   4608 4991 1200 137F 384 埃塞俄比亚语 Ethiopic   4992 5023 1380 139F 32 埃塞俄比亚语补充 Ethiopic Supplement   5024 5119 13A0 13FF 96 切罗基语 Cherokee   5120 5759 1400 167F 640 统一加拿大土著语音节 Unified Canadian Aboriginal Syllabics   5760 5791 1680 169F 32 欧甘字母 Ogham   5792 5887 16A0 16FF 96 如尼文 Runic   5888 5919 1700 171F 32 塔加拉语 Tagalog   5920 5951 1720 173F 32 Hanunóo Hanunóo   5952 5983 1740 175F 32 Buhid Buhid   5984 6015 1760 177F 32 Tagbanwa Tagbanwa   6016 6143 1780 17FF 128 高棉语 Khmer   6144 6319 1800 18AF 176 蒙古文 Mongolian   6320 6399 18B0 18FF 80 Cham Cham   6400 6479 1900 194F 80 Limbu Limbu   6480 6527 1950 197F 48 德宏泰语 Tai Le   6528 6623 1980 19DF 96 新傣仂语 New Tai Lue   6624 6655 1.9E+01 19FF 32 高棉语记号 Kmer Symbols   6656 6687 1A00 1A1F 32 Buginese Buginese   6688 6751 1A20 1A5F 64 Batak Batak   6784 6895 1A80 1AEF 112 Lanna Lanna   6912 7039 1B00 1B7F 128 巴厘语 Balinese   7040 7088 1B80 1BB0 49 巽他语 Sundanese   7104 7167 1BC0 1BFF 64 Pahawh Hmong Pahawh Hmong   7168 7247 1C00 1C4F 80 雷布查语 Lepcha   7248 7295 1C50 1C7F 48 Ol Chiki Ol Chiki   7296 7391 1C80 1CDF 96 曼尼普尔语 Meithei/Manipuri   7424 7551 1D00 1D7F 128 语音学扩展 Phonetic Extensions   7552 7615 1D80 1DBF 64 语音学扩展补充 Phonetic Extensions Supplement   7616 7679 1DC0 1DFF 64 结合用读音符号补充 Combining Diacritics Marks Supplement   7680 7935 1E+00 1EFF 256 拉丁文扩充附加 Latin Extended Additional   7936 8191 1F00 1FFF 256 希腊语扩充 Greek Extended   8192 8303 2000 206F 112 常用标点 General Punctuation   8304 8351 2070 209F 48 上标及下标 Superscripts and Subscripts   8352 8399 20A0 20CF 48 货币符号 Currency Symbols   8400 8447 20D0 20FF 48 组合用记号 Combining Diacritics Marks for Symbols   8448 8527 2100 214F 80 字母式符号 Letterlike Symbols   8528 8591 2150 218F 64 数字形式 Number Form   8592 8703 2190 21FF 112 箭头 Arrows   8704 8959 2200 22FF 256 数学运算符 Mathematical Operator   8960 9215 2300 23FF 256 杂项工业符号 Miscellaneous Technical   9216 9279 2400 243F 64 控制图片 Control Pictures   9280 9311 2440 245F 32 光学识别符 Optical Character Recognition   9312 9471 2460 24FF 160 封闭式字母数字 Enclosed Alphanumerics   9472 9599 2500 257F 128 制表符 Box Drawing   9600 9631 2580 259F 32 方块元素 Block Element   9632 9727 25A0 25FF 96 几何图形 Geometric Shapes   9728 9983 2600 26FF 256 杂项符号 Miscellaneous Symbols   9984 10175 2700 27BF 192 印刷符号 Dingbats   10176 10223 27C0 27EF 48 杂项数学符号-A Miscellaneous Mathematical Symbols-A   10224 10239 27F0 27FF 16 追加箭头-A Supplemental Arrows-A   10240 10495 2800 28FF 256 盲文点字模型 Braille Patterns   10496 10623 2900 297F 128 追加箭头-B Supplemental Arrows-B   10624 10751 2980 29FF 128 杂项数学符号-B Miscellaneous Mathematical Symbols-B   10752 11007 2A00 2AFF 256 追加数学运算符 Supplemental Mathematical Operator   11008 11263 2B00 2BFF 256 杂项符号和箭头 Miscellaneous Symbols and Arrows   11264 11359 2C00 2C5F 96 格拉哥里字母 Glagolitic   11360 11391 2C60 2C7F 32 拉丁文扩展-C Latin Extended-C   11392 11519 2C80 2CFF 128 古埃及语 Coptic   11520 11567 2D00 2D2F 48 格鲁吉亚语补充 Georgian Supplement   11568 11647 2D30 2D7F 80 提非纳文 Tifinagh   11648 11743 2D80 2DDF 96 埃塞俄比亚语扩展 Ethiopic Extended   11776 11903 2E+00 2E7F 128 追加标点 Supplemental Punctuation   11904 12031 2E+80 2EFF 128 CJK 部首补充 CJK Radicals Supplement   12032 12255 2F00 2FDF 224 康熙字典部首 Kangxi Radicals   12272 12287 2FF0 2FFF 16 表意文字描述符 Ideographic Description Characters   12288 12351 3000 303F 64 CJK 符号和标点 CJK Symbols and Punctuation   12352 12447 3040 309F 96 日文平假名 Hiragana   12448 12543 30A0 30FF 96 日文片假名 Katakana   12544 12591 3100 312F 48 注音字母 Bopomofo   12592 12687 3130 318F 96 朝鲜文兼容字母 Hangul Compatibility Jamo   12688 12703 3190 319F 16 象形字注释标志 Kanbun   12704 12735 31A0 31BF 32 注音字母扩展 Bopomofo Extended   12736 12783 31C0 31EF 48 CJK 笔画 CJK Strokes   12784 12799 31F0 31FF 16 日文片假名语音扩展 Katakana Phonetic Extensions   12800 13055 3200 32FF 256 封闭式 CJK 文字和月份 Enclosed CJK Letters and Months   13056 13311 3300 33FF 256 CJK 兼容 CJK Compatibility   13312 19903 3400 4DBF 6592 CJK 统一表意符号扩展 A CJK Unified Ideographs Extension A   19904 19967 4DC0 4DFF 64 易经六十四卦符号 Yijing Hexagrams Symbols   19968 40895 4E+00 9FBF 20928 CJK 统一表意符号 CJK Unified Ideographs   40960 42127 A000 A48F 1168 彝文音节 Yi Syllables   42128 42191 A490 A4CF 64 彝文字根 Yi Radicals   42240 42527 A500 A61F 288 Vai Vai   42592 42751 A660 A6FF 160 统一加拿大土著语音节补充 Unified Canadian Aboriginal Syllabics Supplement   42752 42783 A700 A71F 32 声调修饰字母 Modifier Tone Letters   42784 43007 A720 A7FF 224 拉丁文扩展-D Latin Extended-D   43008 43055 A800 A82F 48 Syloti Nagri Syloti Nagri   43072 43135 A840 A87F 64 八思巴字 Phags-pa   43136 43231 A880 A8DF 96 Saurashtra Saurashtra   43264 43391 A900 A97F 128 爪哇语 Javanese   43392 43487 A980 A9DF 96 Chakma Chakma   43520 43583 AA00 AA3F 64 Varang Kshiti Varang Kshiti   43584 43631 AA40 AA6F 48 Sorang Sompeng Sorang Sompeng   43648 43743 AA80 AADF 96 Newari Newari   43776 43871 AB00 AB5F 96 越南傣语 Vi?t Thái   43904 43936 AB80 ABA0 33 Kayah Li Kayah Li   44032 55215 AC00 D7AF 11184 朝鲜文音节 Hangul Syllables   55296 56319 D800 DBFF 1024 High-half zone of UTF-16 High-half zone of UTF-16   56320 57343 DC00 DFFF 1024 Low-half zone of UTF-16 Low-half zone of UTF-16   57344 63743 E000 F8FF 6400 自行使用區域 Private Use Zone   63744 64255 F900 FAFF 512 CJK 兼容象形文字 CJK Compatibility Ideographs   64256 64335 FB00 FB4F 80 字母表達形式 Alphabetic Presentation Form   64336 65023 FB50 FDFF 688 阿拉伯表達形式A Arabic Presentation Form-A   65024 65039 FE00 FE0F 16 变量选择符 Variation Selector   65040 65055 FE10 FE1F 16 竖排形式 Vertical Forms   65056 65071 FE20 FE2F 16 组合用半符号 Combining Half Marks   65072 65103 FE30 FE4F 32 CJK 兼容形式 CJK Compatibility Forms   65104 65135 FE50 FE6F 32 小型变体形式 Small Form Variants   65136 65279 FE70 FEFF 144 阿拉伯表達形式B Arabic Presentation Form-B   65280 65519 FF00 FFEF 240 半型及全型形式 Halfwidth and Fullwidth Form   65520 65535 FFF0 FFFF 16 特殊 Specials    执行SQL语句 创建一个生成器方法用于执行SQL语句\ndef sql_query(sql): try: conn = pymysql.connect(host=DB_IP, port=DB_PORT, user=DB_USER, password=DB_PASSWD, \\ db=DB_NAME, charset=\u0026#39;utf-8\u0026#39;, cursorclass=pymysql.cursors.DictCursor) with conn.cursor() as cursor: cursor.execute(sql) for row in cursor: yield row finally: conn.close() results=sql_query(\u0026#39;select * from users\u0026#39;) for row in results: print(row[\u0026#39;id\u0026#39;]) 将python依赖库打包一起分发 有时执行脚本的服务器，没有足够的权限，无法通过pip安装python依赖库，这时可以将依赖库打包起来随同脚本一起分发。\n比如现在有个脚本依赖于pymysql, openpyxl, 这时可以通过下面的命令下载好依赖库：\npip install --install-option=\u0026#34;--prefix=./tmp_libs\u0026#34; pymysql pip install --install-option=\u0026#34;--prefix=./tmp_libs\u0026#34; openpyxl mkdir libs cp -r ./tmp_libs/.../site-packages/* libs/ 最后修改脚本，在脚本开始执行前修改python的库路径：\nimport sys import os sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), \u0026#39;libs\u0026#39;)) 写入excel文件 可以通过openpyxl写excel文件\nworkbook=openpyxl.Workbook() sheet=workbook.active .... cell=sheet.cell(row=i, column=j, value=\u0026#39;xxxx\u0026#39;) cell.font=openpyxl.styles.Font(bold=True) .... workbook.save(\u0026#39;result.xlsx\u0026#39;) 更多openpyxl的用法参见其文档\n","permalink":"https://jeremyxu2010.github.io/2017/09/python%E5%BC%80%E5%8F%91%E5%B0%8F%E6%8A%80%E5%B7%A7/","tags":["python","unicode","pymysql","openpyxl"],"title":"python开发小技巧"},{"categories":["工作杂记"],"contents":"疑难问题之bsdiff 问题背景 项目中使用到了bsdiff命令进行增量包的生成，不过在使用中发现对于某些文件，bsdiff命令会卡住。\n诊断问题 刚开始以为是操作系统的问题，换了个全新的系统，按网上的教程从http://www.daemonology.net/bsdiff/下载bsdiff的源码，重新编译得到bsdiff，这里把原来卡住的两个文件重新试了一次，发现还是会卡住。\n看来这个是bsdiff本身存在问题，在网上搜索了下，终于发现有人遇过一样的问题。很奇怪，在国内也有很多人用bsdiff生成增量包，但却没人反馈这个问题。国外这个帖子里也写明了解决方案，那就是打上一些优化补丁。\n We should apply the following 4 patches from chromium - they are small, and, as part of Chromium, also presumably well tested: (1) https://chromium.googlesource.com/chromiumos/third_party/bsdiff/+/e2b85ce4ea246ca804ee2a9e18005c44e193d868 Replacing the custom suffix sorting implementation with -ldivsufsort. This brings down the time to generate a delta (for the proposed new format) for libreoffice-core 5.4~rc2-1-\u0026gt;5.4-1 from 93 seconds down to 50 seconds. It also just involves replacing code with a library call, so that\u0026#39;s nice as well. (2) https://chromium.googlesource.com/chromiumos/third_party/bsdiff/+/a055996c743add7a9558839276fd1e4994d16bd3 Speeds up a pathological case (3) https://chromium.googlesource.com/chromiumos/third_party/bsdiff/+/58146f74abd6b1b69693943195f37f4ac6a6acef Fixes a hang (4) https://chromium.googlesource.com/chromiumos/third_party/bsdiff/+/426e4aa1cbeb3c8a73002047d7a796ca8e5e17d4 Another pathological case where files differ by less than 8 bytes  又看了下，发现这些补丁都在google的chromiumous项目中，于是找到项目代码地址https://chromium.googlesource.com/chromiumos/third_party/bsdiff/。原来google早就发现了这个问题，并在它的项目内对其进行了优化，但不知为什么迟迟没有回馈开源社区。这样就好办了，直接编译google版的bsdiff命令出来就好了。\n编译google版本bsdiff命令 获取代码 wget https://cmake.org/files/v3.9/cmake-3.9.2.tar.gz # libdivsufsort编译要使用cmake git clone https://github.com/y-256/libdivsufsort.git # google版的bsdiff依赖这个 git clone https://chromium.googlesource.com/chromiumos/third_party/bsdiff 编译 我用的linux服务器没有root权限，安装稍微麻烦一点。\ntar xf cmake-3.9.2.tar.gz pushd cmake-3.9.2 ./bootstrap --prefix=$HOME/local make \u0026amp;\u0026amp; make install popd # 设置一系列环境变量 echo \u0026#34;export PATH=$HOME/local/bin:$PATHexport LD_LIBRARY_PATH=$HOME/local/lib:$LD_LIBRARY_PATHexport LIBRARY_PATH=$HOME/local/lib:$LIBRARY_PATHexport CPATH=$HOME/local/include:$CPATH\u0026#34; \u0026gt; $HOME/.bashrc source $HOME/.bashrc pushd libdivsufsort mkdir build pushd build cmake -DCMAKE_BUILD_TYPE=\u0026#34;Release\u0026#34; -DCMAKE_INSTALL_PREFIX=\u0026#34;$HOME/local\u0026#34; .. popd popd pushd bsdiff # 稍微修改下Makefile文件 sed -i -e \u0026#39;s/\\-ldivsufsort64//g\u0026#39; Makefile sed -i -e \u0026#34;s#PREFIX = /usr#PREFIX = $HOME/local#g\u0026#34; Makefile make \u0026amp;\u0026amp; make install # 为了不跟原来的命令重名，将新命令重命名为bsdiff2 mv $HOME/local/bin/bsdiff $HOME/local/bin/bsdiff2 效果 使用新的bsdiff2命令测试了下，目前生成增量包一切正常，再也没有卡住的现象了，而且占用的内存也比原来小不少，速度还快。\n","permalink":"https://jeremyxu2010.github.io/2017/09/%E7%96%91%E9%9A%BE%E9%97%AE%E9%A2%98%E4%B9%8Bbsdiff/","tags":["bsdiff","cmake"],"title":"疑难问题之bsdiff"},{"categories":["java开发"],"contents":"Spring Boot学习备忘 Spring Boot简化了基于Spring的应用开发，只需要\u0026quot;run\u0026quot;就能创建一个独立的，产品级别的Spring应用。工作即将使用到Spring Boot，这里将自学Spring Boot的一些操作经验记录一下。\nSpring Boot Cli 创建第一个Spring Boot应用有多种方式，我这里选用最简单的Spring Boot Cli方案。\n安装Spring Boot Cli 我是使用macOS系统的，已经安装了Java8、maven、OSX Homebrew，安装Spring Cli就变得很简单了。\nbrew tap pivotal/tap brew install springboot spring version #验证安装的Spring Boot版本 使用Spring Boot Cli 验证Spring Boot Cli是否可正常工作也比较简单，先写一个app.groovy, 内容如下：\n@RestController class ThisWillActuallyRun { @RequestMapping(\u0026#34;/\u0026#34;) String home() { \u0026#34;Hello World!\u0026#34; } } 然后执行下面的命令：\nspring run app.groovy 最后请求测试一下：\ncurl http://127.0.0.1:8080 使用Spring Boot Cli创建工程：\nspring init --groupId=personal.jeremyxu --artifactId=springboottest --name=springboottest --description=\u0026#34;Spring Boot Test Project\u0026#34; --dependencies=web mkdir springboottest unzip springboottest.zip -d springboottest spring init执行时可以有不少参数，可以通过执行spring init --list命令来查看，详细文档可以参看这里。\n工程创建后，简单地添加一个测试Controller，如下：\npackage personal.jeremyxu.springboottest.controller; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; /** * Created by jeremy on 2017/9/10. */ @RestController public class GreetingController { @RequestMapping(value = \u0026#34;/\u0026#34;) public String sayHello(){ return \u0026#34;Hello World!\u0026#34;; } } 再在SpringboottestApplication里加上@EnableAutoConfiguration，然后便可运行Spring Boot应用了：\ncd springboottest spring run . 还是通过命令测试一下：\ncurl http://127.0.0.1:8080 代码写完后，执行下面的命令打包：\ncd springboottest mkdir -p ./target spring jar ./target/springboottest.jar . 后面执行下面的命令就可以简单将工程运行起来了：\njava -jar ./target/springboottest.jar 编写测试用例 以前写代码时不怎么关注测试用例，其实测试用例还是很重要的，这里将上面的小例子补上测试用例，测试用例的写法可以参考这里。\nSpringboottestApplicationTests.java\nimport org.junit.Assert; import org.junit.Rule; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.boot.test.rule.OutputCapture; import org.springframework.test.context.junit4.SpringRunner; import org.springframework.util.StringUtils; import java.util.ArrayList; import java.util.Arrays; import java.util.List; @RunWith(SpringRunner.class) @SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT) public class SpringboottestApplicationTests { private static final String SPRING_STARTUP = \u0026#34;root of context hierarchy\u0026#34;; @Rule public OutputCapture outputCapture = new OutputCapture(); @Test public void contextLoads() { SpringboottestApplication.main(getArgs()); Assert.assertTrue(getOutput().contains(SPRING_STARTUP)); } private String[] getArgs(String... args) { List\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(Arrays.asList( \u0026#34;--spring.main.webEnvironment=false\u0026#34;, \u0026#34;--spring.main.showBanner=OFF\u0026#34;, \u0026#34;--spring.main.registerShutdownHook=false\u0026#34;)); if (args.length \u0026gt; 0) { list.add(\u0026#34;--spring.main.sources=\u0026#34; + StringUtils.arrayToCommaDelimitedString(args)); } return list.toArray(new String[list.size()]); } private String getOutput() { return this.outputCapture.toString(); } } GreetingControllerTests.java\nimport org.hamcrest.Matchers; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.autoconfigure.web.servlet.WebMvcTest; import org.springframework.http.MediaType; import org.springframework.test.context.junit4.SpringRunner; import org.springframework.test.web.servlet.MockMvc; import org.springframework.test.web.servlet.request.MockMvcRequestBuilders; import org.springframework.test.web.servlet.result.MockMvcResultMatchers; /** * Created by jeremy on 2017/9/10. */ @RunWith(SpringRunner.class) @WebMvcTest(GreetingController.class) public class GreetingControllerTests { @Autowired private MockMvc mvc; @Test public void sayHello() throws Exception { mvc.perform(MockMvcRequestBuilders.get(\u0026#34;/sayHello\u0026#34;) .accept(MediaType.TEXT_PLAIN)) .andExpect(MockMvcResultMatchers.status().isOk()) .andExpect(MockMvcResultMatchers.content().string(Matchers.equalTo(\u0026#34;Hello World!\u0026#34;))); } } 在IDEA里选中工程包，然后Run 'Tests in …' with Coverage, 然后就可以看到源代码的测试代码覆盖率为100%了，perfect!\n","permalink":"https://jeremyxu2010.github.io/2017/09/spring-boot%E5%AD%A6%E4%B9%A0%E5%A4%87%E5%BF%98/","tags":["java","spring","spring boot"],"title":"Spring Boot学习备忘"},{"categories":["java开发"],"contents":"Gearman使用范例 Gearman是一个分发任务的程序框架，可以用在各种场合，与Hadoop相比，Gearman更偏向于任务分发功能。它的任务分布非常简单，简单得可以只需要用脚本即可完成。Gearman最初用于LiveJournal的图片resize功能，由于图片resize需要消耗大量计算资源，因此需要调度到后端多台服务器执行，完成任务之后返回前端再呈现到界面。\n工程依赖配置 \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.gearman.jgs\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;java-gearman-service\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.7.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;net.sf.json-lib\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;json-lib\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.4\u0026lt;/version\u0026gt; \u0026lt;classifier\u0026gt;jdk15\u0026lt;/classifier\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;aliyun\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;aliyun private nexus\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://maven.aliyun.com/nexus/content/groups/public/\u0026lt;/url\u0026gt; \u0026lt;releases\u0026gt; \u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt; \u0026lt;/releases\u0026gt; \u0026lt;snapshots\u0026gt; \u0026lt;enabled\u0026gt;false\u0026lt;/enabled\u0026gt; \u0026lt;/snapshots\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;jfrog\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;jfrog private maven library\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;https://oss.jfrog.org/libs-snapshot/\u0026lt;/url\u0026gt; \u0026lt;releases\u0026gt; \u0026lt;enabled\u0026gt;false\u0026lt;/enabled\u0026gt; \u0026lt;/releases\u0026gt; \u0026lt;snapshots\u0026gt; \u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt; \u0026lt;/snapshots\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; Gearman服务端 /* * Create a Gearman instance */ Gearman gearman = Gearman.createGearman(); try { /* * Start a new job server. The resulting server will be running in * the local address space. * * Parameter 1: The port number to listen on * * throws IOException */ GearmanServer server = gearman .startGearmanServer(EchoWorker.ECHO_PORT); /* * Create a gearman worker. The worker poll jobs from the server and * executes the corresponding GearmanFunction */ } catch (IOException ioe) { /* * If an exception occurs, make sure the gearman service is shutdown */ gearman.shutdown(); // forward exception  throw ioe; } 这里注意有一个重载的public GearmanServer startGearmanServer(int port, GearmanPersistence persistence)，通过它可以将提交的任务持久化，即使Gearman服务端重启，提交的任务还是可以还原的\nGearman客户端与Worker端 Gearman里提交任务有两种方式：非backgroud提交方式、background提交方式。简单来说非backgroud提交方式是一种同步提交方式，客户端提交任务后保持一个长连接，通过这个长连接可以从执行的Function中获得中间任务数据；而background提交方式是一种异步提交方式，客户端提交任务后获得一个jobHandle, 后面都通过jobHandle获取执行Function的任务状态。\n非backgroud提交方式 System.out.println(EchoWorker.ECHO_FUNCTION_NAME); /* * Create a Gearman instance */ Gearman gearman = Gearman.createGearman(); /* * Create a new gearman client. * * The client is used to submit requests the job server. */ GearmanClient client = gearman.createGearmanClient(); /* * Create the job server object. This call creates an object represents * a remote job server. * * Parameter 1: the host address of the job server. * Parameter 2: the port number the job server is listening on. * * A job server receives jobs from clients and distributes them to * registered workers. */ GearmanServer server = gearman.createGearmanServer( EchoWorker.ECHO_HOST, EchoWorker.ECHO_PORT); /* * Tell the client that it may connect to this server when submitting * jobs. */ client.addServer(server); /* * Submit a job to a job server. * * Parameter 1: the gearman function name * Parameter 2: the data passed to the server and worker * * The GearmanJobReturn is used to poll the job\u0026#39;s result */ JSONObject json = new JSONObject(); json.put(\u0026#34;name\u0026#34;, \u0026#34;admin\u0026#34;); GearmanJobReturn jobReturn = client.submitJob( EchoWorker.ECHO_FUNCTION_NAME,json.toString().getBytes()); /* * Iterate through the job events until we hit the end-of-file */ byte[] jobHandle = null; while (!jobReturn.isEOF()) { // Poll the next job event (blocking operation)  GearmanJobEvent event = jobReturn.poll(); switch (event.getEventType()) { // success  case GEARMAN_JOB_SUCCESS: // Job completed successfully  // print the result  System.out.println(new String(event.getData())); break; case GEARMAN_SUBMIT_SUCCESS: // get job handle  jobHandle = event.getData(); break; case GEARMAN_JOB_DATA: // print job data  System.out.println(new String(event.getData())); break; // failure  case GEARMAN_SUBMIT_FAIL: // The job submit operation failed  case GEARMAN_JOB_FAIL: // The job\u0026#39;s execution failed  System.err.println(event.getEventType() + \u0026#34;: \u0026#34; + new String(event.getData())); } } /* * Close the gearman service after it\u0026#39;s no longer needed. (closes all * sub-services, such as the client) * * It\u0026#39;s suggested that you reuse Gearman and GearmanClient instances * rather recreating and closing new ones between submissions */ gearman.shutdown(); public static void main(String... args) { System.out.println(EchoWorker.ECHO_FUNCTION_NAME); registWorker(); } public static void registWorker(){ /* * Create a Gearman instance */ Gearman gearman = Gearman.createGearman(); /* * Create the job server object. This call creates an object represents * a remote job server. * * Parameter 1: the host address of the job server. * Parameter 2: the port number the job server is listening on. * * A job server receives jobs from clients and distributes them to * registered workers. */ GearmanServer server = gearman.createGearmanServer( EchoWorker.ECHO_HOST, EchoWorker.ECHO_PORT); /* * Create a gearman worker. The worker poll jobs from the server and * executes the corresponding GearmanFunction */ System.out.println(server.toString()); GearmanWorker worker = gearman.createGearmanWorker(); /* * Tell the worker how to perform the echo function */ worker.addFunction(EchoWorker.ECHO_FUNCTION_NAME, new EchoWorker()); /* * Tell the worker that it may communicate with the this job server */ boolean success = worker.addServer(server); System.out.println(success); } public byte[] work(String function, byte[] data, GearmanFunctionCallback callback) throws Exception { /* * The work method performs the gearman function. In this case, the echo * function simply returns the data it received */ System.out.println(new String(data)); System.out.println(\u0026#34;begin work\u0026#34;); callback.sendData(\u0026#34;some data\u0026#34;.getBytes()); System.out.println(\u0026#34;end work\u0026#34;); return \u0026#34;job result\u0026#34;.getBytes(); } backgroud提交方式 System.out.println(EchoWorker.ECHO_FUNCTION_NAME); /* * Create a Gearman instance */ Gearman gearman = Gearman.createGearman(); /* * Create a new gearman client. * * The client is used to submit requests the job server. */ GearmanClient client = gearman.createGearmanClient(); /* * Create the job server object. This call creates an object represents * a remote job server. * * Parameter 1: the host address of the job server. * Parameter 2: the port number the job server is listening on. * * A job server receives jobs from clients and distributes them to * registered workers. */ GearmanServer server = gearman.createGearmanServer( EchoWorker.ECHO_HOST, EchoWorker.ECHO_PORT); /* * Tell the client that it may connect to this server when submitting * jobs. */ client.addServer(server); /* * Submit a job to a job server. * * Parameter 1: the gearman function name * Parameter 2: the data passed to the server and worker * * The GearmanJobReturn is used to poll the job\u0026#39;s result */ JSONObject json = new JSONObject(); json.put(\u0026#34;name\u0026#34;, \u0026#34;admin\u0026#34;); GearmanJobReturn jobReturn = client.submitBackgroundJob( EchoWorker.ECHO_FUNCTION_NAME,json.toString().getBytes()); /* * Iterate through the job events until we hit the end-of-file */ byte[] jobHandle = null; while (!jobReturn.isEOF()) { // Poll the next job event (blocking operation)  GearmanJobEvent event = jobReturn.poll(); switch (event.getEventType()) { case GEARMAN_SUBMIT_SUCCESS: jobHandle = event.getData(); break; // failure  case GEARMAN_SUBMIT_FAIL: // The job submit operation failed  case GEARMAN_JOB_FAIL: // The job\u0026#39;s execution failed  System.err.println(event.getEventType() + \u0026#34;: \u0026#34; + new String(event.getData())); } } for (int i = 0; i \u0026lt; 10; i++) { GearmanJobStatus status = client.getStatus(jobHandle); System.out.println(String.format(\u0026#34;known: %b, running: %b, denominator: %d, numerator: %d\u0026#34;, status.isKnown(), status.isRunning(), status.getDenominator(), status.getNumerator())); Thread.sleep(2000L); } /* * Close the gearman service after it\u0026#39;s no longer needed. (closes all * sub-services, such as the client) * * It\u0026#39;s suggested that you reuse Gearman and GearmanClient instances * rather recreating and closing new ones between submissions */ gearman.shutdown(); public static void main(String... args) { System.out.println(EchoWorker.ECHO_FUNCTION_NAME); registWorker(); } public static void registWorker(){ /* * Create a Gearman instance */ Gearman gearman = Gearman.createGearman(); /* * Create the job server object. This call creates an object represents * a remote job server. * * Parameter 1: the host address of the job server. * Parameter 2: the port number the job server is listening on. * * A job server receives jobs from clients and distributes them to * registered workers. */ GearmanServer server = gearman.createGearmanServer( EchoWorker.ECHO_HOST, EchoWorker.ECHO_PORT); /* * Create a gearman worker. The worker poll jobs from the server and * executes the corresponding GearmanFunction */ System.out.println(server.toString()); GearmanWorker worker = gearman.createGearmanWorker(); /* * Tell the worker how to perform the echo function */ worker.addFunction(EchoWorker.ECHO_FUNCTION_NAME, new EchoWorker()); /* * Tell the worker that it may communicate with the this job server */ boolean success = worker.addServer(server); System.out.println(success); } public byte[] work(String function, byte[] data, GearmanFunctionCallback callback) throws Exception { /* * The work method performs the gearman function. In this case, the echo * function simply returns the data it received */ System.out.println(new String(data)); System.out.println(\u0026#34;begin work\u0026#34;); for (int i = 0; i \u0026lt; 10; i++) { callback.sendStatus(i, i); Thread.sleep(2000L); } System.out.println(\u0026#34;end work\u0026#34;); return null; } 参考 http://www.blogdaren.com/m/?post=1497\n","permalink":"https://jeremyxu2010.github.io/2017/09/gearman%E4%BD%BF%E7%94%A8%E8%8C%83%E4%BE%8B/","tags":["java","gearman","任务分派"],"title":"Gearman使用范例"},{"categories":["java开发"],"contents":"Java开发小技巧 平时开发中有一些小技巧，都不算很有技术含量，但在工作中运用这些技巧确实可以提高工作效率，这里把这些小技分享出来。\n参数验证 提供的API接口类方法如有参数，都要做参数校验，参数校验不通过明确抛出异常或对应的响应码。到处写if表达式判断代码，正常的业务逻辑会被这些校验代码干扰，这里介绍两个用得比较多的方案。\nCommons-lang的Validate String val1 = \u0026#34; \u0026#34;; Validate.notBlank(val1, \u0026#34;输入的参数val1=%s为空\u0026#34;, val1); String val2 = null; Validate.notNull(val2, \u0026#34;输入的参数val2为null\u0026#34;); String[] arr1 = new String[0]; Validate.notEmpty(arr1, \u0026#34;数组为空\u0026#34;); List\u0026lt;String\u0026gt; lst1 = new ArrayList\u0026lt;String\u0026gt;(); Validate.notEmpty(lst1, \u0026#34;数组为空\u0026#34;); String[] arr2 = new String[0]; int idx = 3; Validate.validIndex(arr2, idx, \u0026#34;索引超过数组范围\u0026#34;); short state1 = (short)1; short validState = 3; Validate.validState(state1==validState, \u0026#34;当前的状态不正确, state=%d\u0026#34;, state1); boolean valid = (3 == 4); Validate.isTrue(valid, \u0026#34;表达式不合法\u0026#34;); BeanValidation 如果输入参数是pojo对象，采用BeanValidation方案更优雅一些。\npublic static void main(String[] args) { ValidatorFactory factory = Validation.buildDefaultValidatorFactory(); Validator validator = factory.getValidator(); Pojo pojo = new Pojo(); pojo.setKey1(\u0026#34;abcdef\u0026#34;); pojo.setKey2(3.3); pojo.setTime1(new GregorianCalendar(2010, 3, 4).getTime()); Set\u0026lt;ConstraintViolation\u0026lt;Pojo\u0026gt;\u0026gt; violationSet = validator.validate(pojo); if(violationSet.size() \u0026gt; 0) { for (ConstraintViolation\u0026lt;Pojo\u0026gt; violation : violationSet) { System.out.println(violation); } } else { System.out.println(\u0026#34;校验成功\u0026#34;); } } private static class Pojo{ @NotNull(message = \u0026#34;key1为空\u0026#34;) @Size(min = 6, max = 20, message = \u0026#34;key1的长度不正确\u0026#34;) private String key1; @NotNull(message = \u0026#34;key2为空\u0026#34;) @DecimalMin(value = \u0026#34;0\u0026#34;, message = \u0026#34;值不能小于0\u0026#34;) @DecimalMax(value = \u0026#34;100\u0026#34;, message = \u0026#34;值不能大于100\u0026#34;) private Double key2; @NotNull(message = \u0026#34;time1为空\u0026#34;) @Past(message = \u0026#34;时间不能大于现在\u0026#34;) private Date time1; public String getKey1() { return key1; } public void setKey1(String key1) { this.key1 = key1; } public Double getKey2() { return key2; } public void setKey2(Double key2) { this.key2 = key2; } public Date getTime1() { return time1; } public void setTime1(Date time1) { this.time1 = time1; } } 重视Deprecated 调用标准库或第三方API时，一些接口上打上了Deprecated标记，而且一些还解释了准备废弃这个接口的原因及应该改用的写法。\n// bad java.net.URLEncoder#encode(java.lang.String); // good java.net.URLEncoder#encode(java.lang.String, java.lang.String); // bad java.net.URLDecoder#decode(java.lang.String); // good java.net.URLDecoder#decode(java.lang.String, java.lang.String); // bad java.util.Date#Date(int, int, int); // good java.net.URLDecoder#decode(java.lang.String, java.lang.String); // bad org.springframework.orm.hibernate3.support.HibernateDaoSupport#getSession().createSQLQuery(sql).executeUpdate(); // good org.springframework.orm.hibernate3.support.HibernateDaoSupport#getHibernateTemplate().execute(new HibernateCallback\u0026lt;Void\u0026gt;() { @Override public Void doInHibernate(Session session) throws HibernateException, SQLException { session.createSQLQuery(sql).executeUpdate(); return null; } }); Java文件操作 Java 7中引入了新的文件操作API，具有不少优点，新代码建议采用这套API操作文件。\n可参考Java NIO File操作。\n打印日志 平时打日志时有几个错误范例，打日志时要注意一下。\n拼接字符串 // bad Lg.info(\u0026#34;abc \u0026#34; + 3 + \u0026#34; def \u0026#34; + 4 + \u0026#34; xxx \u0026#34; + 5); // good Lg.info(String.format(\u0026#34;abc %d def %d xxx %d\u0026#34;, 3, 4, 5)) 打印异常堆栈至标准错误输出 // bad e.printStackTrace(); Lg.error(\u0026#34;执行出错\u0026#34;); // good Lg.error(\u0026#34;执行出错\u0026#34;, e); 打印过长字符串 // bad Lg.info(tooLongStr); // good if(Lg.isInfoEnabled()){ Lg.info(tooLongStr);\t} 资源清理 网络连接、数据库连接、会话、HttpClient的连接等使用完后一定要进行资源清理。\n以下是一些错误的示例：\n  private static Logger Lg = LoggerFactory.getLogger(ResourceCleanDemo.class); public static void main(String[] args) { Socket socket = null; BufferedReader reader = null; PrintWriter writer = null; try { socket = new Socket(\u0026#34;127.0.0.1\u0026#34;, 8080); reader = new BufferedReader(new InputStreamReader(socket.getInputStream(), StandardCharsets.UTF_8.name())); writer = new PrintWriter(new OutputStreamWriter(socket.getOutputStream(), StandardCharsets.UTF_8.name())); // do something  ... socket.close(); writer.close(); reader.close(); } catch (IOException e){ Lg.error(\u0026#34;出错\u0026#34;, e); } }   private static Logger Lg = LoggerFactory.getLogger(ResourceCleanDemo.class); public static void main(String[] args) { Socket socket = null; BufferedReader reader = null; PrintWriter writer = null; try { socket = new Socket(\u0026#34;127.0.0.1\u0026#34;, 8080); reader = new BufferedReader(new InputStreamReader(socket.getInputStream(), StandardCharsets.UTF_8.name())); writer = new PrintWriter(new OutputStreamWriter(socket.getOutputStream(), StandardCharsets.UTF_8.name())); // do something  ... } catch (IOException e){ Lg.error(\u0026#34;出错\u0026#34;, e); } finally { try { socket.close(); writer.close(); reader.close(); } catch (IOException e) { Lg.error(\u0026#34;出错\u0026#34;, e); } } }   正确的范例：\nprivate static Logger Lg = LoggerFactory.getLogger(ResourceCleanDemo.class); public static void main(String[] args) { Socket socket = null; BufferedReader reader = null; PrintWriter writer = null; try { socket = new Socket(\u0026#34;127.0.0.1\u0026#34;, 8080); reader = new BufferedReader(new InputStreamReader(socket.getInputStream(), StandardCharsets.UTF_8.name())); writer = new PrintWriter(new OutputStreamWriter(socket.getOutputStream(), StandardCharsets.UTF_8.name())); } catch (IOException e){ Lg.error(\u0026#34;出错\u0026#34;, e); } finally { IOUtils.closeQuietly(socket); IOUtils.closeQuietly(writer); IOUtils.closeQuietly(reader); } } HttpClient连接池 HttpClient库为了提高性能，是使用了连接池的，应尽量使用连接池特性。\n错误示例如下：\npublic static void main(String[] args) throws IOException { for (int i = 0; i \u0026lt; 10; i++) { CloseableHttpClient httpClient = HttpClients.createDefault(); CloseableHttpResponse resp = httpClient.execute(new HttpGet(\u0026#34;www.baidu.com\u0026#34;)); // do something  .... httpClient.close(); } } 正确示例如下：\nprivate static final CloseableHttpClient httpClient = HttpClients.createDefault(); public static void main(String[] args) throws IOException { for (int i = 0; i \u0026lt; 10; i++) { HttpGet httpGet = null; try { httpGet = new HttpGet(\u0026#34;www.baidu.com\u0026#34;); CloseableHttpResponse resp = httpClient.execute(httpGet); // do something  .... } catch (Exception e) { Lg.error(\u0026#34;出错\u0026#34;, e); } finally { httpGet.releaseConnection(); } } IOUtils.closeQuietly(httpClient); } 善用Spring的工具类 Spring中有一些已经写好的工具类，代码都比较简单，即可以学习下，本时工作中用一用也可以提高开发效率。\norg.springframework.dao.support.DataAccessUtils org.springframework.util.StringUtils org.springframework.util.CollectionUtils org.springframework.util.Base64Utils org.springframework.util.DigestUtils org.springframework.util.FileCopyUtils org.springframework.util.FileSystemUtils org.springframework.util.ReflectionUtils org.springframework.util.SocketUtils org.springframework.beans.BeanUtils Guava中一些有用的API // 把Throwable包装成RuntimeException抛出 com.google.common.base.Throwables#propagate(Throwable throwable); // 新集合类型 com.google.common.collect.Multiset com.google.common.collect.Multimap // Google Guava，详细用法参见http://jeremy-xu.oschina.io/2016/09/01/java%E4%B8%AD%E7%94%A8%E5%A5%BDcache/ com.google.common.cache.LoadingCache // 异步任务执行完毕后自动执行指定的回调方法 com.google.common.util.concurrent.ListenableFuture // 同步事件总线 com.google.common.eventbus.EventBus // 异步事件总线 com.google.common.eventbus.AsyncEventBus 可参考Google Guava官方教程（中文版）\n这里举几个例子：\nThrowables抛出异常 public void someBusinessMethod(String param1){ try { System.out.println(Integer.parseInt(param1)); } catch (NumberFormatException e) { Throwables.propagate(e); } } ListenableFuture示例 private static final ListeningExecutorService service = MoreExecutors.listeningDecorator(Executors.newFixedThreadPool(10)); public static void main(String[] args) { ListenableFuture\u0026lt;Integer\u0026gt; future = service.submit(new Callable\u0026lt;Integer\u0026gt;() { @Override public Integer call() throws Exception { Thread.sleep(10000L); return new Integer(0); } }); Futures.addCallback(future, new FutureCallback\u0026lt;Integer\u0026gt;() { @Override public void onSuccess(Integer result) { System.out.println(String.format(\u0026#34;success, result: %d\u0026#34;, result)); } @Override public void onFailure(Throwable t) { t.printStackTrace(); } }); } 事件总线 private static EventBus eventBus = new EventBus(\u0026#34;globalEventBus\u0026#34;); public static void main(String[] args) { eventBus.register(new CustomEventHandler()); eventBus.register(new DeadEventHandler()); eventBus.post(new CustomEvent()); } private static class CustomEventHandler{ @Subscribe public void handleCustomEvent(CustomEvent event){ System.out.println(event); } } private static class DeadEventHandler{ @Subscribe public void handleDeadEvent(DeadEvent deadEvent){ System.out.println(deadEvent); } } private static class CustomEvent{ } 重试逻辑 经常写代码实现业务的重试逻辑，可考虑spring-retry，用法可参考Retrying_Library_For_Java\n工具技巧   java -XX:+PrintFlagsFinal ：打印出几乎所有的JVM支持的参数以及他们的默认值\n  -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address={port}：jvm开启jdwp，可运程调试\n  -Dcom.sun.management.jmxremote=true -Djava.rmi.server.hostname=ip−Dcom.sun.management.jmxremote.port={port} -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false ：jvm开启jmx，可使用jconsole连接\n  mybatis-generator的配置文件中加入\u0026lt;property name=\u0026quot;addRemarkComments\u0026quot; value=\u0026quot;true\u0026quot; /\u0026gt;，可数据库表的备注可自动作为实体属性的注释\n  IntelliJ IDEA的几个重要的重构功能：Rename、Change Signature…、Move…、Copy…、Extract Constant…、Extract Method…、Extract Interface…、Pull Member Up…。\n​\n  ","permalink":"https://jeremyxu2010.github.io/2017/09/java%E5%BC%80%E5%8F%91%E5%B0%8F%E6%8A%80%E5%B7%A7/","tags":["java","nio","validate","log","guava","spring-retry"],"title":"Java开发小技巧"},{"categories":["机器学习"],"contents":"核（Kernels） SVM算法的原理如下：\n$$ min \\frac 1 2 ||w||^2 \\\\ s. t. y^{(i)}(W^TX^{(i)} + b) \u0026gt;= 1 $$\n上述式子的对偶问题如下： $$ max\\sum_i\\alpha_i - \\frac 1 2 \\sum_i \\sum_j y^{(i)}y^{(j)} \\alpha_i \\alpha_j \u0026lt;X^{(i)}, X^{(j)}\u0026gt; \\\\ s. t. \\alpha_i\u0026gt;=0, \\sum_iy_i\\alpha_i=0 \\\\ W = \\sum_i\\alpha_iy^{(i)}X^{(i)} $$\n软间隔SVM SMO算法 ","permalink":"https://jeremyxu2010.github.io/2017/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B_%E7%AC%94%E8%AE%B008/","tags":["机器学习","andrew ng"],"title":"机器学习课程_笔记08"},{"categories":["机器学习"],"contents":"自己的数学知识丢太久了，这一课看了好几篇，最后结合视频及网上的分析文档终于看懂了，汗。。。\n最优间隔分类器(optimal margin classifier) 如果训练集是线性可分的， 就是说用超平面可以分隔正负样本. 我们要找到最大的几何间隔. 我们可以转化为下面的优化问题：\n​ 即，找到一个超平面，在将正负样本分开的同时，使超平面到正负样本间的距离尽可能大。\n由于w和b可随意缩放，约束条件||w||=1，使得函数间隔等于几何间隔。但是这个约束本身是一个非凸性约束。（非凸性：是指系统有多个稳定的平衡态。）要求解的参数w在一个球体表面，如果想得到一个凸优化问题，必须保证如梯度下降算法这种局部最优值搜索算法不会找到局部最优值，而非凸性约束不能满足这个条件，所以需要改变优化问题。因此转化为更好的一个问题：\n我们的目标变成要最大化,并且去掉了约束条件||w=1||，但是仍然是非凸性的.\n因此，加上规模的限制，对训练集的函数间隔设置为1：至此，我们得到最终的最优间隔分类器：\n此时，我们的优化问题变为一个凸二次目标函数。\n原始/对偶优化问题（KKT）（primal/dual optimization problem） 拉格朗日二元性 考虑下式：\n即最小化函数f(w)，并满足约束条件hi(w)=0，可以将hi写成0向量**，**我们可以通过拉格朗日乘数法的方法解决：\n1、创建拉格朗日算子：\n即等于原始目标函数加限制函数的线性组合，其中参数β称为拉格朗日乘数。\n2、对下式求偏导数置为0，即可求出解w和β：\n原始问题 拉格朗日乘数法的一般形式，也称为原始问题**。**\n考虑下式：\n创建拉格朗日算子：\n此时α和β为拉格朗日乘数，定义：\n上式中的“p”表示“原始问题”（primal），\n如果w违反了约束条件，即**，**那么上式变成：\n​ 分析上式，若gi(w)\u0026gt;0，那么只要使αi无穷大，θp(w)就会无穷大；若hi(w)≠0，只要使βi相应取无穷大（hi(w)\u0026gt;0）或无穷小（hi(w)\u0026lt;0），θp(w)也会无穷大。\n反之，若w满足约束条件，那么θp(w) = f(w)，所以可得：\n那么，求min f(w)就是求下式的值，定义为p*：\n​ ​ 对偶问题 与上面原始问题有略微差别，我们定义：\n对其取最大值，即给出对偶优化问题，定义为d*：\n​ 显然，我们有：\n​ 在某些条件下，会有，因此我们可以通过解决原始问题来解决对偶问题.\n原始问题和对偶问题获得相同解的条件：\n 令f为凸函数（凸函数的hessian 矩阵是半正定的，H\u0026gt;=0，即开口朝上的碗状函数） 假设hi为仿射函数（(affine，和线性是一样的，只不过是加了截距），即 假设gi是严格可执行的，即存在w，使得对于所有i，gi(w)\u0026lt;0  在上述条件下，存在w*，α*，β*，其中w*是原始问题的解，α*，β*是对偶问题的解，并且：\n此外，还要满足以下条件：\n这些条件被称为KKT条件。（KKT是三个人名的缩写），如果满足KKT条件，那么就是原始问题和对偶问题的解。\n其中，称为KKT对偶补充条件。即就是：\n如果αi\u0026gt;0 ，那么 gi(w*)=0，但是一般来说αi\u0026gt;0 \u0026lt;=\u0026gt; gi(w*)=0。\n当gi(w*)=0，称gi(w*)为活动约束。\n使用对偶方法求解SVM最化间隔分类问题 前面，我们有了最优间隔分类器如下：\n约束条件可以写为：\n通过KKT条件，αi\u0026gt;0 =\u0026gt; gi(w,b)=0 =\u0026gt; y(i)(wTx(i)+b)=1，即函数间隔为1\n给出例子如下图：\n图中的圈和叉即正负样本，实线即w,b确定的分割的超平面，最小的间隔是离决定边界最近的点，上图中有三个看出有三个样本的函数间隔为1，其他样本的函数间隔大于1，虚线即为函数间隔为1的点所构成的线。\n过KKT条件，这些函数间隔为1的样本对应的拉格朗日乘数一般不等于0， (因为根据KKT对偶补充条件，只有，函数边界才等于 1).。这三个点被称为支持向量（support vector），由此可见，支持向量的数量比训练样本数量小很多**。**\n所以，总结为：αi\u0026gt;0。这个函数间隔为1的样本称为支持向量。因为支撑向量数量很少，所以多数的αi=0，那么反推可得，αi=0，对应的样本不是支撑向量。\n对最优间隔优化问题构建拉格朗日算子，有：\n​ 由于这个问题只有不等式约束，所以没有β。\n对w求偏导并设为0**：**\n推出：\nw就是输入特征向量的线性组合。对b求偏导：\n将w代入拉格朗日算子，得到：\n​ 根据对b求偏导的结果，最后一项为0，得到：\n将上式表示为W(α)，对偶问题就是：\n运用梯度下降法或极大似然法解上面这个对偶问题，即可求出参数α。求出α后，代入\n即可求出w。\n求出α和w后，容易求出b，因为w决定了超平面的斜率，那么根据最优间隔，将α和w代入原始问题，就容易求出b了，如下式：\n再得到：\n​ $w^Tx + b = \\sum_{i=1}^m\\alpha_iy^{(i)}\u0026lt;x^{(i)}, x\u0026gt; + b = 0$定义出了超平面，而函数间隔为1的样本对应的拉格朗日乘数$\\alpha_i$才不等于0，所以这个公式的直观理解就是，找到最差的样本（离得最近的正负样本，也就是支持向量），接着，就只需要计算x和支持向量的内积就可以求出超平面的位置。\n","permalink":"https://jeremyxu2010.github.io/2017/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B_%E7%AC%94%E8%AE%B007/","tags":["机器学习","andrew ng"],"title":"机器学习课程_笔记07"},{"categories":["机器学习"],"contents":"多项式事件模型 面的这种基本的朴素贝叶斯模型叫做多元伯努利事件模型，该模型有多种扩展，一种是每个分量的多值化，即将$P(X_i|y)$由伯努利分布扩展到多项式分布；还有一种是将连续变量值离散化。例如以房屋面积为例：\n   Living area(sq. feet) \u0026lt;400 400-800 800-1200 1200-1600 \u0026gt;1600     $X_i$ 1 2 3 4 5    还有一种，与多元伯努利有较大区别的朴素贝叶斯模型，就是多项式事件模型。\n多项式事件模型改变了特征向量的表示方法：\n在多元伯努利模型中，特征向量的每个分量代表词典中该index上的词语是否在文本中出现过，其取值范围为{0,1}，特征向量的长度为词典的大小。\n而在多项式事件模型中，特征向量中的每个分量的值是文本中处于该分量位置的单词在词典中的索引，其取值范围是{1,2,\u0026hellip;,|V|}，|V|是词典的大小，特征向量的长度为文本中单词的数量。\n例如：在多元伯努利模型下，一篇文本的特征向量可能如下：\n​ 在多项式事件模型下，这篇文本的特征向量为：\n​ 一篇文本产生的过程是：\n1、确定文本类别\n2、以相同的多项式分布在各个位置上生成词语。\n例如：x1是由服从p(x1|y)的多项式分布产生的，x2是独立与x1的并且来自于同一个多项式分布，同样的，产生x3,x4，一直到xn。\n因此，所有的这个信息的概率是$\\Pi_{i=1}^nP(X_i|y)$\n模型的参数为： $$ \\phi_y = P(y) \\\\ \\phi_{i|y=1} = P(X_j=i|y=1) \\\\ \\phi_{i|y=0} = P(X_j=i|y=0) $$ 参数在训练集上的极大似然函数：\n​ 参数的最大似然估计为： ​ 应用laplace平滑，分子加1，分母加|V|，得到： ​ 对于式子：\n​ 分子的意思是对训练集合中的所有垃圾邮件中词k出现的次数进行求和。\n分母的含义是对训练样本集合进行求和，如果其中的一个样本是垃圾邮件（y=1），那么就把它的长度加起来，所以分母的含义是训练集合中所有垃圾邮件的词语总长。\n所以这个比值的含义就是在所有垃圾邮件中，词k所占的比例。 注意这个公式与多元伯努利的不同在于：这里针对整体样本求的φk|y=1 ，而多远伯努利里面针对每个特征求的φxj=1|y=1 ，而且这里的特征值维度不一定是相同的。\n举例说明多项式事件模型 假设邮件中有a,b,c三个词，他们在词典的位置分别是1,2,3,第一封里面内容为a,b，第二封为b,a;第三封为a,c,b,第四封为c,c,c,a。Y=1是垃圾邮件。 因此，我们有： 那么，我们可得： 假如有一封信的邮件，内容为b,c。那么它的特征向量为{2,3},我们可得： 那么该邮件为垃圾邮件概率是0.6。\n先验分布、后验分布、似然估计的概念 用“瓜熟蒂落”这个因果例子，从概率（probability）的角度说一下，\n先验概率，就是常识、经验所透露出的“因”的概率，即瓜熟的概率。应该很清楚。\n后验概率，就是在知道“果”之后，去推测“因”的概率，也就是说，如果已经知道瓜蒂脱落，那么瓜熟的概率是多少。后验和先验的关系可以通过贝叶斯公式来求。也就是：\nP（瓜熟 | 已知蒂落）=P（瓜熟）×P（蒂落 | 瓜熟）/ P（蒂落）\n似然函数，是根据已知结果去推测固有性质的可能性（likelihood），是对固有性质的拟合程度，所以不能称为概率。在这里就是说，不要管什么瓜熟的概率，只care瓜熟与蒂落的关系。如果蒂落了，那么对瓜熟这一属性的拟合程度有多大。似然函数，一般写成L（瓜熟 | 已知蒂落），和后验概率非常像，区别在于似然函数把瓜熟看成一个肯定存在的属性，而后验概率把瓜熟看成一个随机变量。\n再扯一扯似然函数和条件概率的关系。似然函数就是条件概率的逆反。意为：\nL（瓜熟 | 已知蒂落）= C × P（蒂落 | 瓜熟），C是常数。具体来说，现在有1000个瓜熟了，落了800个，那条件概率是0.8。那我也可以说，这1000个瓜都熟的可能性是0.8C。\n注意，之所以加个常数项，是因为似然函数的具体值没有意义，只有看它的相对大小或者两个似然值的比率才有意义，后面还有例子。\n同理，如果理解上面的意义，分布就是一“串”概率。\n先验分布：现在常识不但告诉我们瓜熟的概率，也说明了瓜青、瓜烂的概率\n后验分布：在知道蒂落之后，瓜青、瓜熟、瓜烂的概率都是多少\n似然函数：在知道蒂落的情形下，如果以瓜青为必然属性，它的可能性是多少？如果以瓜熟为必然属性，它的可能性是多少？如果以瓜烂为必然属性，它的可能性是多少？似然函数不是分布，只是对上述三种情形下各自的可能性描述。\n那么我们把这三者结合起来，就可以得到：后验分布 正比于 先验分布 × 似然函数。先验就是设定一种情形，似然就是看这种情形下发生的可能性，两者合起来就是后验的概率。\n至于似然估计：\n就是不管先验和后验那一套，只看似然函数，现在蒂落了，可能有瓜青、瓜熟、瓜烂，这三种情况都有个似然值（L（瓜青）：0.6、L（瓜熟）：0.8、L（瓜烂）：0.7），我们采用最大的那个，即瓜熟，这个时候假定瓜熟为必然属性是最有可能的。\n神经网络 对于之前学习的分类算法，我们的目标都是求解一条直线，这条直线将数据进行分类，但如果数据并不是线性可分的话，这些模型的性能会变差。针对非线性分类的问题，出现了很多分类算法，神经网络是其中最早出现的一种。\n例如，下图使用Logistic模型分类，得到的是图中的直线，但这条直线并不是很合理，我们希望得到图中的曲线：\n假设特征向量为{x0,x1,x2,x3}sigmoid代表计算节点,output是函数输出对于Logistic模型来说，过程如图：\nSigmoid计算节点含有参数θ，其函数形式为：\n但对于神经网络来说，过程如图：\n特征向量输入到多个sigmoid单元，然后这些单元再输入到一个sigmoid单元，这些中间节点叫做隐藏层，神经网络可以有多个隐藏层.。\n其中的参数分别为：\n​ 求解其中的参数，需要使用成本函数：\n​ 然后通过梯度下降方法求得参数值，在神经网络模型中，梯度下降算法有一个专有的名字叫做：反向传播算法。\n神经网络算法的特点：\n 不知道隐藏层计算的东西的意义。 有很多的局部最优解，需要通过多次随机设定初始值然后运行梯度下降算法获得全局最优值。  支持向量机(SVM) 在了解支持向量机之前，我们需要知道函数间隔和几何间隔。\n首先，我们先定义新的标记：\n1、用g(z)∈{-1,1}代替y(x)∈{0,1}。 2、目标函数从变为。\n​ （这里b代替了的角色，w代替的角色，ω和b可以确定唯一的一个超平面）\n点(x(i),y(i))到由ω，b决定的超平面的函数间隔是：\n** **从上面的十字可以看出：如果，为了使函数间隔很大，需要是一个很大的正数。如果为了使函数间隔很大，需要是一个很大的负数.。如果，则我们的预测结果是正确的。因此，函数间隔越大，说明预测结果越是确定正确的。\n如果我们用2w代替w，用2b代替b，那么由于，不会对有任何改变，也就是说只是取决于符号而跟数量没有关系.。但是用（2w,2b）代替（w,b）会使得函数间隔间隔增大两倍。\n超平面与整个训练集合的函数间隔是：\n​ 为了解决这函数间隔无意义增大的问题**，就有了几何间隔的定义，几何间隔定义如下：**\n​ 用下图说明几何间隔的问题：\n​ 上图中，w垂直于分隔超平面，训练样本A，它到分隔线 的距离是，也就是线段AB的长度. 是单位向量（unit-length vector）, B点表示为为：，在分隔线上的所有点满足因此有：\n​ 解到： ​ 由上式可知：当||w||等于1，几何间隔等于函数间隔. 但几何间隔是不会随着参数的调整而变化的。\n超平面与整个训练集合的几何间隔是： ​ 有了几何间隔和函数间隔，使得我们的分类结果不仅能保证正确性，还可以保证分类结果的确定性。\n","permalink":"https://jeremyxu2010.github.io/2017/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B_%E7%AC%94%E8%AE%B006/","tags":["机器学习","andrew ng"],"title":"机器学习课程_笔记06"},{"categories":["java开发"],"contents":"最近在github上看到一个得了不少星的项目Retrying library for Python，果然还是人家比较有想法，这些重试的逻辑是可以包装为一个库供别人使用的。想到平时自己在写Java代码时，经常还手工写些代码实现重试逻辑，真的挺low的。那么Java里是否有类似的函数库呢？简单搜索了下，发现了两个选择：guava-retrying、 spring-retry。简单比较了下，功能都差不多，但很明显spring-retry更强大一些，支持三种用法：API形式、Annotation形式、XML形式。个人还是比较喜欢Annotation形式的用法，看起来很简单，下面写一个简单的示例。\nNotStableService.java\nimport org.springframework.retry.annotation.Backoff; import org.springframework.retry.annotation.EnableRetry; import org.springframework.retry.annotation.Recover; import org.springframework.retry.annotation.Retryable; import org.springframework.stereotype.Service; import java.security.SecureRandom; import java.util.Random; /** * Created by jeremy on 2017/6/14. */ @EnableRetry @Service public class NotStableService { private Random random = new SecureRandom(); @Retryable( value = {RuntimeException.class}, maxAttempts = 8, backoff= @Backoff(delay = 1000, maxDelay = 32000, multiplier = 2) ) public Integer executeNotStableMethod(){ // 下面模拟该方法的执行不太稳定的状态，比如调用第三方不太稳定的服务接口等场景  int result = random.nextInt(10); System.out.println(\u0026#34;Got \u0026#34; + result); if(result \u0026lt; 7) { throw new RuntimeException(\u0026#34;服务执行失败\u0026#34;); } else { return result; } } @Recover public void recover(RuntimeException e){ System.out.println(\u0026#34;尝试多次，仍然失败\u0026#34;); } } 简单写个测试案例：\nNotStableServiceTests.java\nimport org.junit.Assert; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.test.context.junit4.SpringRunner; @RunWith(SpringRunner.class) @SpringBootTest(webEnvironment=SpringBootTest.WebEnvironment.RANDOM_PORT) public class NotStableServiceTests { @Autowired private NotStableService notStableService; @Test public void executeNotStableMethod(){ Assert.assertNotNull(notStableService.executeNotStableMethod()); } } 测试执行结果：\n2017-06-15 00:06:44.451 INFO 24549 --- [ main] p.j.service.NotStableServiceTests : Started NotStableServiceTests in 15.068 seconds (JVM running for 16.511) Got 3 Got 3 Got 2 Got 2 Got 6 Got 9 可以看到确实重试了好几次，最终得到了合适的结果。\n本想好好讲一下spring-retry的详细用法的，不过在网上找资料时，发现已有一位兄弟写了很详细的文档，我这里就不赘述了。\n以下内容转载自转载自http://iyiguo.net/blog/2016/01/19/spring-retry-common-case/，《Spring Retry 常用示例》的一切权利归作者@Leeyee所有。\n重试策略 连续重试N次 SimpleRetryPolicy可以实现指定次数的重试。只需要设置 maxAttempts 参数即可。其默认重试次数是3次。该策略为创建RetryTemplate对象时默认的重试策略。具体使用如下：\nSimpleRetryPolicy simpleRetryPolicy = new SimpleRetryPolicy(); simpleRetryPolicy.setMaxAttempts(4); 规定时间内连续重试 TimeoutRetryPolicy可以实现指定时间内的重试。超时时间通过参数 timeout 进行设置。默认超时时间1s。使用方式如下：\n// all spend 1s TimeoutRetryPolicy timeoutRetryPolicy = new TimeoutRetryPolicy(); timeoutRetryPolicy.setTimeout(2000L); 组合重试 CompositeRetryPolicy实现了重试策略的组合。通过其 policies 字段，可以为其添加多个重试策略。组合策略执行的过程中，所有策略只要有一个达成终止条件，那么该重试结束。我们可以用组合重试策略实现一些相对比较复杂的重试。比如我们要实现在指定时间1s内重试3次，每次重试间隔0.2秒，就可以使用以下方法：\nCompositeRetryPolicy compositeRetryPolicy = new CompositeRetryPolicy(); SimpleRetryPolicy simpleRetryPolicy = new SimpleRetryPolicy(); TimeoutRetryPolicy timeoutRetryPolicy = new TimeoutRetryPolicy(); FixedBackOffPolicy fixedBackOffPolicy = new FixedBackOffPolicy(); fixedBackOffPolicy.setBackOffPeriod(200); // 每次重试间隔200ms  compositeRetryPolicy.setPolicies(new RetryPolicy[]{simpleRetryPolicy,\ttimeoutRetryPolicy,}); 通过该方法，如果重试总耗时超过1s，重试次数不超过3次，那么重试终止；如果未超过1s，但重试次数已达到3次，那么重试终止！（重试等待的设置可以看下文）\n异常分类重试 有可能存在这样一种场景，比如在进行HTTP请求时，有可能因为网络原因导致请求超时，也有可能在拿到HTTP响应结果后的业务处理中发生异常，针对这两种异常我们可能需要不同的异常重试机制。这时就可以通过ExceptionClassifierRetryPolicy对异常分类进行分类：\nExceptionClassifierRetryPolicy retryPolicy = new ExceptionClassifierRetryPolicy(); Map\u0026lt;Class\u0026lt;? extends Throwable\u0026gt;, RetryPolicy\u0026gt; policyMap = Maps.newHashMap(); policyMap.put(NullPointerException.class, new SimpleRetryPolicy()); policyMap.put(ArithmeticException.class, new TimeoutRetryPolicy()); retryPolicy.setPolicyMap(policyMap); 上述示例中，我们针对重试业务抛出的空指针异常使用SimpleRetryPolicy策略，而对于算术异常采用TimeoutRetryPolicy策略。实际的重试过程中，这两中情况有可能交替出现，但不管如何，只要有一个重试策略达到终止状态，则整个重试调用终止。\n等待策略（BackOff） 重试策略RetryPolicy只是实现了基本的重试功能，也就是核心的循环逻辑，形如以下的代码：\ndo ... while 那么每次重试之间的相关场景该如何处理呢？为此，Spring Retry 将重试间可能有的重试等待策略抽像成了BackoffPolicy接口，并提供了一些简单的实现。\n在使用RetryTemplate时，可以通过setBackOffPolicy方法进行设置。\n指定时间等待 首先FixedBackOffPolicy应该是最常用的重试间隔1策略！通过该类，可以指定重试之间需要等待的时间。FixedBackOffPolicy 有两个基本属性：backOffPeriod 和 sleeper。可以通过backOffPeriod 直接设定间隔实现，当然也可以通过sleeper属性，实现自己的重试间隔方法。同时，backOffPeriod实质上还是调用了Sleeper实现（sleeper.sleep(backOffPeriod);）。\nFixedBackOffPolicy fixedBackOffPolicy = new FixedBackOffPolicy(); fixedBackOffPolicy.setBackOffPeriod(1500);\t指数级等待 ExponentialBackOffPolicy类提供了指数级重试间隔的实现。通过该类，可以使重试之间的等待按指数级增长。其中\n initialInterval属性为初始默认间隔，默认值是100毫秒； maxInterval属性为最大默认间隔。当实际计算出的间隔超过该值时，使用该值。默认为30秒； multiplier为乘数。默认2，当其等于1时，其行为同FixedBackOffPolicy为固定时间间隔。建议不要使用1，会造成重试过快！  FixedBackOffPolicy的时间间隔计算公式是：\nMath.min(maxInterval, Math.pow(initialInterval, multiplier)) 源码为：\npublic synchronized long getSleepAndIncrement() {\tlong sleep = this.interval;\tif (sleep \u0026gt; maxInterval) {\tsleep = maxInterval;\t} else {\tthis.interval = getNextInterval();\t}\treturn sleep; } protected long getNextInterval() {\treturn (long)(this.interval * this.multiplier); } 指数级随机等待 ExponentialRandomBackOffPolicy继承自ExponentialBackOffPolicy，只是重写了获取重试时间间隔的方法。在获取重试间隔后，在加上一些随机的时间。具体实现可参看源码：\n@Overridepublic synchronized long getSleepAndIncrement() {\tlong next = super.getSleepAndIncrement();\tnext = (long)(next*(1 + r.nextFloat()*(getMultiplier()-1)));\treturn next;} 指定范围内的随机等待 UniformRandomBackOffPolicy允许给定最大，最小等待时间，然后让每次的重试在其之间进行随机等待。参数minBackOffPeriod和maxBackOffPeriod的默认值分别为500ms和1500ms，具体的计算方式是：\nprotected void doBackOff() throws BackOffInterruptedException {\ttry {\tlong delta = maxBackOffPeriod==minBackOffPeriod ? 0 : random.nextInt((int) (maxBackOffPeriod - minBackOffPeriod)); sleeper.sleep(minBackOffPeriod + delta ); }\tcatch (InterruptedException e) {\tthrow new BackOffInterruptedException(\u0026#34;Thread interrupted while sleeping\u0026#34;, e);\t} } 基于注解的重试 最后我们了解下如何使用注解实现重试机制。最基本的，我们需要以下这几个注解：\n @EnableRetry：能否重试。注解类的，其proxyTargetClass属性为true时，使用CGLIB代理。默认使用标准JAVA注解。当类中有@Retryable注释的方法时，对该方法生成代理。 @Retryable：注解需要被重试的方法。include 指定处理的异常类。默认所有异常maxAttempts 最大重试次数。默认3次backoff 重试等待策略。默认使用@Backoff注解 @Backoff：重试等待策略。不设置参数时，默认使用FixedBackOffPolicy，重试等待1000ms只设置delay()属性时，使用FixedBackOffPolicy，重试等待指定的毫秒数当设置delay()和maxDealy()属性时，重试等待在这两个值之间均态分布使用delay()，*maxDealy()和multiplier()属性时，使用ExponentialBackOffPolicy当设置multiplier()属性不等于0时，同时也设置了random()*属性时，使用ExponentialRandomBackOffPolicy @Recover: 用于方法。用于@Retryable失败时的“兜底”处理方法。@Recover注释的方法参数为@Retryable异常类，返回值应与重试方法返回相同，否则无法识别！因此可以针对可能异常设置多个@Recover方法进行“兜底”处理。  对于@Backoff可以具体查看下参数不同时源码是如何处理的2：\nprivate BackOffPolicy getBackoffPolicy(Backoff backoff) {\tlong min = backoff.delay() == 0 ? backoff.value() : backoff.delay();\tlong max = backoff.maxDelay();\tif (backoff.multiplier() \u0026gt; 0) {\tExponentialBackOffPolicy policy = new ExponentialBackOffPolicy(); if (backoff.random()) {\tpolicy = new ExponentialRandomBackOffPolicy(); }\tpolicy.setInitialInterval(min); policy.setMultiplier(backoff.multiplier()); policy.setMaxInterval(max \u0026gt; min ? max : ExponentialBackOffPolicy.DEFAULT_MAX_INTERVAL); if (sleeper != null) {\tpolicy.setSleeper(sleeper);\t}\treturn policy;\t}\tif (max \u0026gt; min) {\tUniformRandomBackOffPolicy policy = new UniformRandomBackOffPolicy();\tpolicy.setMinBackOffPeriod(min);\tpolicy.setMaxBackOffPeriod(max);\tif (sleeper != null) {\tpolicy.setSleeper(sleeper);\t}\treturn policy;\t}\tFixedBackOffPolicy policy = new FixedBackOffPolicy(); policy.setBackOffPeriod(min);\tif (sleeper != null) {\tpolicy.setSleeper(sleeper);\t}\treturn policy; } 最后给出一个使用示例，供大家参考下：\n@Service@EnableRetry() public class AnnoService {\tpublic Logger logger = LoggerFactory.getLogger(AnnoService.class); @Retryable(maxAttempts = 5, backoff = @Backoff(random = true)) public String someService() {\tint random = (int) (Math.random() * 10); if (random \u0026lt; 4) {\tlogger.info(\u0026#34;random={} Null Pointer Excep\u0026#34;, random);\tthrow new NullPointerException();\t} else if (random \u0026lt; 9) {\tlogger.info(\u0026#34;random={} Arithmetic Excep\u0026#34;, random);\tthrow new ArithmeticException();\t}\tlogger.info(\u0026#34;random={} ok !!!!\u0026#34;, random);\treturn \u0026#34;ok\u0026#34;;\t}\t@Recover\tpublic String recover(NullPointerException ne) { logger.info(\u0026#34;{}\u0026#34;, \u0026#34;NullPointerException\u0026#34;); return \u0026#34;null pointer recover\u0026#34;;\t}\t@Recover\tpublic String recover(ArithmeticException ne) { logger.info(\u0026#34;{}\u0026#34;, \u0026#34;ArithmeticException\u0026#34;); return \u0026#34;ArithmeticException recover\u0026#34;;\t} } public class Main {\tpublic static void main(String[] args) throws Exception { ApplicationContext context = new AnnotationConfigApplicationContext(\u0026#34;com.leeyee.spring.retry.*\u0026#34;); AnnoService annoService = context.getBean(AnnoService.class); String result = annoService.someService(); System.out.println(result);\t} } ","permalink":"https://jeremyxu2010.github.io/2017/06/retrying_library_for_java/","tags":["java","spring","retry"],"title":"Retrying_Library_For_Java"},{"categories":["机器学习"],"contents":"生成学习算法 logistic回归的执行过程就是要搜索这样的一条直线，能够将两类数据分隔开。\n判别学习算法描述为以下公式： $$ Learns \\quad P(y|X) \\quad or \\quad learns \\quad h_\\Theta(X) \\in \\{0, 1\\} \\quad directly. $$\n所以logistics回归是判别学习算法的一个例子。\n一个生成学习算法给定所属的类的情况下显示某种特定特征的概率。其计算公式如下： $$ P(y=1|X) = \\frac {P(X|y=1)P(y)} {P(X)} \\\\ P(X) = p(y=0|X)P(X) + P(y=1|X)P(X) $$ 一个生成学习算法一开始是对$P(X|y)$进行建模，而不是对$P(y|X)$。\n高斯判别分析 推导过程： $$ 假设 \\quad X \\in \\mathbb R^n, \\quad 并且是连续的 \\\\ 假设 \\quad P(X|y) \\quad 服从高斯分布 \\\\ 随机变量z \\sim N(\\mu, \\Sigma), \\quad 这里\\mu是均值，\\Sigma是协方差 = E[(X-\\mu)(X-\\mu)^T] \\\\ 那么概率密度函数为P(z) = \\frac 1 {(2\\pi)^{\\frac n 2}|\\Sigma|^{\\frac 1 2}} exp(- \\frac 1 2 (X-\\mu)^T\\Sigma^{-1}(X-\\mu)) \\\\ P(y) = \\phi^y(1-\\phi)^{1-y} \\\\ P(X|y=0) = \\frac 1 {(2\\pi)^{\\frac n 2}|\\Sigma|^{\\frac 1 2}}exp(- \\frac 1 2 (X-\\mu_0)^T\\Sigma^{-1}(X-\\mu_0)) \\\\ P(X|y=1) = \\frac 1 {(2\\pi)^{\\frac n 2}|\\Sigma|^{\\frac 1 2}}exp(- \\frac 1 2 (X-\\mu_1)^T\\Sigma^{-1}(X-\\mu_1)) \\\\ 参数的对数似然性公式为 \\quad \\ell(\\phi, \\mu_0, \\mu_1, \\Sigma) = log\\Pi_{i=1}^mP(X^{(i)}, y^{(i)}) \\\\ = log\\Pi_{i=1}^mP(X^{(i)}| y^{(i)})P(y^{(i)})，这个是Joint \\quad likelihood\\\\ 这里对比logistics回归里的参数对数似然性公式为\\quad \\ell(\\Theta) = log\\Pi_{i=1}^mP(y^{(i)|X^{(i)}}; \\Theta)，这个是Conditional \\quad likelihood\\\\ 最大化\\ell得出\\\\ \\phi = \\frac {\\Sigma_i y^{(i)}} m = \\frac {\\Sigma_i \\mathbb 1 \\{y^{(i) = 1}\\}} m \\\\ \\mu_0 = \\frac {\\Sigma_i^m \\mathbb 1 \\{y^{(i) = 0}\\} X{(i)}} {\\Sigma_i^m \\mathbb 1 \\{y^{(i) = 0}\\}} \\\\ \\mu_1 = \\frac {\\Sigma_i^m \\mathbb 1 \\{y^{(i) = 1}\\} X{(i)}} {\\Sigma_i^m \\mathbb 1 \\{y^{(i) = 1}\\}} \\\\ \\Sigma = \\cdots \\\\ 预测\\underset{y}{\\operatorname{argmax}}P(y|X) = \\frac {\\underset{y}{\\operatorname{argmax}}P(X|y)P(y)} {P(X)} = \\underset{y}{\\operatorname{argmax}}P(X|y)P(y) \\\\ 这里P(X) = P(X|y=0)P(y=0) + P(X|y=1)P(y=1) \\\\ 如果P(y)是均匀分布的P(y=0)=P(y=1)，则上述公式就推导得到\\underset{y}{\\operatorname{argmax}}P(X|y) $$\n生成学习算法与判别学习算法的对比 这里有几个结论：\n 如果$X|y$服从高斯分布，那么$P(y=1|X)$的后验分布函数将是一个logistics函数。 如果$P(X|y=1) \\sim Poisson(\\lambda_1)，P(X|y=0) \\sim Poisson(\\lambda_0)$，那么$P(y=1|X)$的后验分布函数将是一个logistics函数。 如果$P(X|y=1)、P(X|y=0)$服从某个相同的指数分布族，那么的后验分布函数将是一个logistics函数。  因此$X|y$服从高斯分布或泊松分布是比$y|X$服从logistics分布更强的假设。\n如果$P(X|y)$服从高斯分布的假设假设或大概成立，那么高斯判别算法的表现将会更好，将会优于logistic回归，因为高斯判别算法利用了更多的关于数据的信息。相反如果不确定$P(X|y)$的分布情况，那么logistic回归的表现可能会更好。\n高斯判别分析为了拟合出一个还不错的模型，通常需要更少的数据。而logistic回归算法做了更弱的假设，与高斯判别分析相比，为了拟合出模型它需要多的样本。\n朴素贝叶斯方法 这里先讲了一个创建特征向量$X$来表示某一封邮件的办法。\n假设$X \\in \\{0, 1\\}^n，n=50000$，现在要对$P(X|y)$建模，则$X$有$2^{50000}$个可能，如果使用多项式分布的softmax回归，则需要得到$2^{50000}-1$个参数，这样计算量太大了。\n朴素贝叶斯方法，推导过程如下：\n假设$X_i$是条件独立的，因此有$P(X_1, X_2, \\cdots, X_n|y) = P(X_1|y)P(X_2|y) \\cdots P(X_n|y) = \\prod_{i=1}^nP(X_i|y)$。\n而在伯努利分布里，参数定义如下：\n$\\phi_{i|y=1} = P(X_i=1|y=1)，\\quad \\phi_{i|y=0} = P(X_i=1|y=0)，\\quad \\phi_y=P(y=1)$\n因此就可以写出Joint参数似然性:\n$\\ell(\\phi_y, \\phi_{i|y=1}, \\phi_{i|y=0}) = \\prod_{i=1}^mP(X^{(i)}, y^{(i)})$，之后进行极大似然估计，就得到了\n$\\phi_{j|y=1} = \\frac {\\sum_{i=1}^m \\mathbb 1 \\{X_j^{(i)}, y^{(i)}=1\\}} {\\sum_{i=1}^m \\mathbb 1 \\{y^{(i)} = 1\\}}$\n$\\phi_{j|y=0} = \\frac {\\sum_{i=1}^m \\mathbb 1 \\{X_j^{(i)}, y^{(i)}=0\\}} {\\sum_{i=1}^m \\mathbb 1 \\{y^{(i)} = 0\\}}$\n$\\phi_y = \\frac {\\sum_{i=1}^m\\{y^{(i)} = 1\\}} {m}$\n又因为贝叶斯公式，得到\n$P(y=1|X)=\\frac {P(X|y=1)P(y=1)} {P(X|y=1)P(y=1) + P(X|y=0)P(y=0)} = \\frac {(\\prod_{i=1}^nP(X_i|y=1))P(y=1)} {(\\prod_{i=1}^nP(X_i|y=1))P(y=1) + (\\prod_{i=1}^nP(X_i|y=0))P(y=0)}$\n最后将上述的$\\phi_{j|y=1}，\\phi_{j|y=0}，\\phi_y$代入上式，即可得到预测结果。\n这里讲朴素贝叶斯讲得比较复杂，如果想比较简单地理解，推荐看看阮一峰的一篇文章-朴素贝叶斯分类器的应用。\nLaplace平滑 为了避免一些没有见过的事件，算法认为这些事件不可能发生，于是可以使用Laplace平滑改进此问题。方法如下：\n如果$y \\in \\{1, 2, \\cdots, k\\}$，那么$P(y=j)=\\frac {\\sum_{i=1}^m \\mathbb 1 \\{y^{(i)} = j\\} + 1} {m+k}$\n","permalink":"https://jeremyxu2010.github.io/2017/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B_%E7%AC%94%E8%AE%B005/","tags":["机器学习","andrew ng"],"title":"机器学习课程_笔记05"},{"categories":["机器学习"],"contents":"牛顿方法 首先假设存在一个函数$f(\\Theta)$，然后算法的目标是找到一个$\\Theta$，使得$f(\\Theta)=0$。\n牛顿方法的一次迭代： $$ \\Theta^{(t+1)}= \\Theta^{(t)} - \\frac {f(\\Theta^{(t)})} {f'(\\Theta^{(t)}) } $$ 持续地迭代下去，就可以得到$f(\\Theta)=0$。\n同样的，假设现在存在一个函数$\\ell(\\Theta)$，也就是对数似然率，目标是找到一个$\\Theta$，使得$\\ell(\\Theta)$最大化。可以容易想到$\\ell(\\Theta)$的一阶导数$\\ell'(\\Theta)$为0时，$\\ell(\\Theta)$即达到最大化了。\n同样运用牛顿方法，其一次迭代： $$ \\Theta^{(t+1)} = \\Theta^{(t)} - \\frac {\\ell'(\\Theta^{(t)})} {\\ell''(\\Theta^{(t)})} $$ 事实证明牛顿方法是一个收敛速度非常快的算法，它的收敛速度用术语可以描述为二次收敛。如果不考虑常量因子，牛顿方法的每一次迭代都会使你正在逼近的解的有效数字的数目加倍。当实现牛顿方法时，对于logistic回归来说通常会在十几次迭代之后收敛。\n一般化的牛顿方法中，$\\Theta$是一个向量，因而一般化的算法是这样的： $$ \\Theta^{(t+1)} = \\Theta^{(t)} - H^{-1} \\ell'(\\Theta) $$ 其中$H​$是Hessian矩阵，该矩阵中元素定义如下： $$ H_{ij} = \\frac {\\partial^2 \\ell} {\\partial \\Theta_i \\partial \\Theta_j} $$ 通常情况下你会看到算法收敛一般情况下会执行十几次迭代，和梯度上升比起来算法收敛所需要的迭代次数要少得多。牛顿方法的缺点是每一次迭代你都需要重新计算一次Hession矩阵的逆，Hessian矩阵是一个(n+1)*(n+1)的矩阵，如果你要处理的问题中有大量的特征，将会花费很大的代价，但是对于规模较小 特征数量合理的问题，这通常情况下会是一个很快的算法。\n广义线性模型 在线性回归中，服从高斯分布 $$ P(y|X; \\Theta) \\sim N(\\mu, \\sigma^2) $$ 在logistics回归中，服从伯努利分布 $$ P(y|X;\\Theta) \\sim B(\\phi) $$ 上述两种分布只是都是一类分布的特例，这类分布被称为指数分布族。\n指数分布族可以写成以下的形式： $$ P(y;η)=b(y)exp(η^T T(y)-a(η)) $$ 其中η被称为分布的自然参数，T(y)被称为充分统计量。\n通常情况下，我们经常见到的许多例子里，包括伯努利分布和高斯分布，$T(y)=y$。\n我们固定这三个函数a、b和T，那么这个公式就定义了一个概率分布的集合，它以η为参数，定义了一类概率分布。\n这里推导一下分啥说伯努利分布、高斯分布都是指数分布族的特例。\n推导伯努利分布 $$ P(y=1;\\phi) = \\phi \\\\ P(y;\\phi) = \\phi^y (1 - \\phi)^{1-y} \\\\ = exp(log\\phi^y(1-\\phi)^{1-y}) \\\\ = exp(ylog\\phi + (1-y)log(1-\\phi)) \\\\ = exp(log \\frac \\phi {1-\\phi} y + log(1-\\phi)) \\\\ = b(y)exp(η^T T(y)-a(η)) \\quad where \\quad b(y)=1, \\quad η= log \\frac \\phi {1-\\phi}, \\quad T(y)=y, \\quad a(η)=-log(1-\\phi) $$\n感觉上面的推导像在硬拼，呵呵。\n推导高斯分布 $$ N(\\mu, \\sigma^2) \\quad assume \\quad \\sigma=1 \\\\ N(\\mu, \\sigma^2) = \\frac 1 {\\sqrt{2\\pi}} (- \\frac 1 2 (y - \\mu)^2) \\\\ = \\frac 1 {\\sqrt{2\\pi}} exp(- \\frac 1 2 y^2)exp(\\mu y - \\frac 1 2 \\mu^2) \\\\ = b(y)exp(η^T T(y)-a(η)) \\quad where \\quad b(y)=\\frac 1 {\\sqrt{2\\pi}} exp(- \\frac 1 2 y^2), \\quad η= \\mu, \\quad T(y)=y, \\quad a(η)=\\frac 1 2 \\mu^2 = \\frac 1 2 η^2 $$\n感觉上面的推导像在硬拼，呵呵。\n指数分布族推导出一个广义线性模型 广义线性模型通常被简写为GLM。\n三个假设，也可以将它们看成是设计决策，这可以使我生成广义线性模型： $$ Assume: \\quad y|X;\\Theta \\quad \\sim ExpFamily(η) \\\\ Give \\quad X, goal \\quad is \\quad to \\quad output \\quad ExpFamily[T(y)|X] , \\quad Want \\quad h(X) = ExpFamily[T(y)|X] \\\\ η=\\Theta^T X $$ 下面将伯努利分布推导出对应的广义线性模型 $$ h_\\Theta(X) = ExpFamily[T(y)|X; \\Theta] = P(y=1|X;\\Theta) = P(y=1;\\phi) \\\\ =\\phi \\\\ = \\frac 1 {1+e^{-\\mu}} \\\\ = \\frac 1 {1+e^{-\\Theta^T X}} $$ 这里 $$ g(η) = ExpFamily[y;η] = \\frac 1 {1+ e^{-η}} $$ $g(η)$将自然参数η与y的期望值联系起来，这个函数被称为正则响应函数，而$g^{-1}$被称为正则关联函数。\n多项式分布 多项式分布是指在k可能取值上的分布。\n推导过程： $$ y \\in {1, \\cdots, k} \\\\ Parameters: \\quad \\phi_1,\\phi_2,\\cdots,\\phi_k \\\\ P(y=i) = \\phi_i \\\\ \\sum_{i=1}^k P(y=i) = 1 \\\\ \\phi_k = 1- \\sum_{i=1}^{k-1} \\phi_i \\\\ Assume \\quad Parameters: \\quad \\phi_1,\\phi_2,\\cdots,\\phi_{k-1} \\\\ T(1) = \\begin{bmatrix}1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, \\quad T(k-1) = \\begin{bmatrix}0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix}, \\quad T(k) = \\begin{bmatrix}0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\in \\mathbb R^{k-1} \\\\ \\mathbb{1}\\{True\\} =1, \\mathbb{1}\\{False\\} =0 \\\\ T(y)_i = \\mathbb{1}\\{y=i\\} \\\\ P(y) = \\phi^{\\mathbb{1}\\{y=1\\}}\\phi^{\\mathbb{1}\\{y=2\\}}\\cdots\\phi^{\\mathbb{1}\\{y=k\\}}\\\\ =\\phi^{T(y)_{1}}\\phi^{T(y)_{2}}\\cdots\\phi^{T(y)_{k-1}}\\phi^{1-\\sum_{j=1}^{k-1}T(y)_j} \\\\ = b(y)exp(η^T T(y)-a(η)) \\quad where \\quad b(y)=1, \\quad η= \\begin{bmatrix} log(\\frac {\\phi_1} {\\phi_k}) \\\\ \\vdots \\\\ log(\\frac {\\phi_{k-1}} {\\phi_k}) \\end{bmatrix} \\in \\mathbb R^{k-1}, \\quad T(y)_i = \\mathbb{1}\\{y=i\\}, \\quad a(η)=-log(\\phi_k) $$ 这样就将概率分布从多项式分布的形式转化成了指数分布族的形式。\n这里将η定义为$\\Theta$的函数，求解这个式，最后得到这个结果： $$ \\phi_i= \\frac {e^η_i} {1+ \\sum_{j=1}^{k-1} e^{η_j}} \\quad Here \\quad (i=1, \\cdots, k-1) \\\\ = \\frac {e^{\\Theta_i^T X}} {1+ \\sum_{j=1}^{k-1} e^{\\Theta_j^T X}} $$ 学习算法的假设函数就可以推导为： $$ h_\\Theta(X) = ExpFamily[T(y)|X;\\Theta] \\\\ = ExpFamily\\left[ \\begin{matrix} \\mathbb 1\\{j=1\\} \\\\ \\vdots \\\\ \\mathbb 1\\{j=k-1\\} \\end{matrix} \\arrowvert X; \\Theta \\right] \\\\ = \\begin{bmatrix} \\phi_1 \\\\ \\vdots \\\\ \\phi_{k-1} \\end{bmatrix} \\\\ = \\begin{bmatrix} \\frac {e^{\\Theta_1^T X}} {1+ \\sum_{j=1}^{k-1} e^{\\Theta_j^T X}} \\\\ \\vdots \\\\ \\frac {e^{\\Theta_{k-1}^T X}} {1+ \\sum_{j=1}^{k-1} e^{\\Theta_j^T X}} \\end{bmatrix} $$ 上述算法就叫softmax回归，是logistics回归的推广。\nsoftmax回归的求解过程就可以归纳如下： $$ Assume \\quad y \\in \\{1, \\cdots, k\\} \\\\ You \\quad Have: \\quad (X^{(1)}, y^{(1)}), \\cdots, (X^{(m)}, y^{(m)}) \\\\ L(\\Theta) = \\Pi_{i=1}^m P(y^{(i)}|X^{(i)}; \\Theta) \\\\ = \\Pi_{i=1}^m \\phi_1^{\\mathbb 1 \\{y^{(i)} = 1\\}} \\cdots \\phi_k^{\\mathbb 1 \\{y^{(i)} = k\\}} \\quad Here \\quad \\begin{bmatrix} \\phi_1 \\\\ \\vdots \\\\ \\phi_{k-1} \\end{bmatrix} = \\begin{bmatrix} \\frac {e^{\\Theta_1^T X}} {1+ \\sum_{j=1}^{k-1} e^{\\Theta_j^T X}} \\\\ \\vdots \\\\ \\frac {e^{\\Theta_{k-1}^T X}} {1+ \\sum_{j=1}^{k-1} e^{\\Theta_j^T X}} \\end{bmatrix} $$ 然后使用极大似然估计法求出$\\Theta$。\n","permalink":"https://jeremyxu2010.github.io/2017/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B_%E7%AC%94%E8%AE%B004/","tags":["机器学习","andrew ng"],"title":"机器学习课程_笔记04"},{"categories":["机器学习"],"contents":"局部加权回归 线性回归算法里的成本函数：\n$J(\\Theta) = \\frac 1 2 \\sum_{i=1}^m(h_\\Theta(X^{(i)})-y^{(i)})^2$\n正规方程解出的参数解析表达式：\n$\\Theta = (X^TX)^{-1}X^Ty$\n由于使用了过小的特征集合使得模型过于简单，在这种情形下数据中的某些非常明显的模式没有被成功地拟合出来，我们将其称之为:欠拟合(underfitting)。\n由于使用了过大的特征集合使得模型过于复杂，算法拟合出的结果仅仅反映了所给的特定数据的特质，我们可以称之为过拟合。\n在特征选择中存在上述两类问题。\n这里讲到一类非参数学习算法，可以缓解对于选取特征的需求，就是局部加权回归算法。\n这个算法可以让我们不必太担心对于特征的选择。\n算法示意 $$ Fit \\quad \\Theta \\quad To \\quad Minimize \\\\ \\sum_i w^{(i)}(y{(i)} - \\Theta^TX{(i)})^2 \\quad where \\quad w^{(i)} = exp(- \\frac {(X^{(i)} - x)^2} {2 \\tau^2}) \\\\ Then \\quad Return \\quad \\Theta^Tx \\\\ If \\quad |X^{(i) - x}| \\quad small, \\quad then \\quad w^{(i)} \\approx 1 \\\\ If \\quad |X^{(i) - x}| \\quad large, \\quad then \\quad w^{(i)} \\approx 0 $$ $\\tau$称为波长函数， $\\tau$较小，则权值随距离下降得快， $\\tau$较大，则权值随距离下降得慢。\n线性回归的概率解释 $$ Assume \\quad y^{(i)} = \\Theta^TX^{(i)} + \\varepsilon^{(i)} \\\\ \\varepsilon^{(i)} = error \\\\ \\varepsilon^{(i)} \\sim N(0, \\sigma^2) \\\\ P(\\varepsilon^{(i)}) = \\frac 1 {\\sqrt {2\\pi} \\sigma} exp(- \\frac {( \\varepsilon^{(i)} )^2} {2\\sigma^2}) $$ 其中$\\varepsilon^{(i)}$代表误差项，它表示了一种我们没有捕获到的特征，或者你也可以把它看成一种随机的噪声。\n然后可以得到\n$$ P(y^{(i)} | X^{(i)}; \\Theta) \\\\ = \\frac 1 {\\sqrt {2\\pi} \\sigma} exp(- \\frac {(y^{(i)} - \\Theta^TX^{(i)})^2} {2\\sigma^2}) \\\\ \\sim N(\\Theta^TX^{(i)}, \\sigma^2) $$ $\\varepsilon^{(i)}$s 是独立同分布的。\n然后定义\n$$ L(\\Theta) = P(\\overrightarrow y|X; \\Theta) \\\\ = \\Pi_{i=1}^m P(y^{(i)} | X^{(i)}; \\Theta) \\\\ = \\Pi_{i=1}^m \\frac 1 {\\sqrt {2\\pi} \\sigma} exp(- \\frac {(y^{(i)} - \\Theta^TX^{(i)})^2} {2\\sigma^2}) $$ 这个就是参数$\\Theta$的似然性。\n算法的目标也就变为：\n$$ Choose \\quad \\Theta \\quad To \\quad Maximize \\quad L(\\Theta) = P(\\overrightarrow y|X; \\Theta) $$ 再定义\n$$ \\ell(\\Theta) = logL(\\Theta) = log\\Pi_{i=1}^m \\frac 1 {\\sqrt {2\\pi} \\sigma} exp(- \\frac {(y^{(i)} - \\Theta^TX^{(i)})^2} {2\\sigma^2}) \\\\ = mlog\\frac 1 {\\sqrt {2\\pi} \\sigma} + \\sum_{i=1}^m - \\frac {(y^{(i) - \\Theta^TX^{(i)}})^2} {2\\sigma^2} $$ 然后最大化$\\ell(\\Theta)$就变成了最小化$ \\frac {\\sum_{i=1}^m(y^{(i) - \\Theta^TX^{(i)}})^2} 2=J(\\Theta)$\n于是推导出了线性回归算法里的成本函数。\nlogistic回归 推导过程 $$ y \\in \\{0, 1\\} \\\\ h_\\Theta(X) \\in [0, 1] \\\\ Choose \\quad h_\\Theta(X) = g(\\Theta^TX) = \\frac 1 {1 + e ^{-\\Theta^TX}} \\\\ g(Z) = \\frac 1 {1 + e^{-Z}} $$ 上述$g(Z)$公式就叫做sigmoid函数，也叫logistic函数。\n同样使用概率解释下logistic回归函数。\n$$ P(y|X;\\Theta） = h_\\Theta(X)^y(1-h_\\Theta(X))^{1-y} \\\\ L(\\Theta) = P(\\overrightarrow y|X;\\Theta）= \\Pi_i P(y^{(i)}|X^{(i)};\\Theta）\\\\ = \\Pi_i h_\\Theta(X^{(i)})^{y^{(i)}}(1-h_\\Theta(X^{(i)}))^{1-y^{(i)}} \\\\ \\ell(\\Theta) = logL(\\Theta) = \\sum_{i=1}^m y^{(i)} logh(X_\\Theta^{(i)}) + (1-y^{(i)})log(1-h_\\Theta(X^{(i)})) $$ 采用梯度上升算法来最大化$\\ell(\\Theta)$\n$$ \\Theta := \\Theta + \\alpha \\nabla_\\Theta \\ell(\\Theta) \\\\ \\frac \\partial {\\partial \\Theta_j} = \\sum_{i=1}^m(y^{(i)} - h_\\Theta(X^{(i)}))X_j^{(i)} $$ 最后得到logistic回归的更新$\\Theta​$的过程为\n$$ \\Theta_j := \\Theta_j - \\alpha \\sum_{i=1}^m(h_\\Theta(X^{(i)}))X_j^{(i)} $$\n感知器算法 这个跟logistic算法很相似，更新$\\Theta$的过程为\n$$ \\Theta_j := \\Theta_j - \\alpha \\sum_{i=1}^m(h_\\Theta(X^{(i)}))X_j^{(i)} $$\n","permalink":"https://jeremyxu2010.github.io/2017/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B_%E7%AC%94%E8%AE%B003/","tags":["机器学习","andrew ng"],"title":"机器学习课程_笔记03"},{"categories":["机器学习"],"contents":"线性回归 首先展示了一段视频，介绍了Dean Pomerleau利用监督学习让一辆汽车可以自动行驶。\n使用的符号    符号 代表的含义     m 训练样本的数目   X 输入变量，通常也可以称为特征   y 输出变量，有时也称为目标变量   (X, y) 表示一个样本   ($X^{(i)}$, $y^{(i)}$) 表示第i个样本   h 假设（hypothesis）函数   n 特征的个数    推导过程 首先是单个特征的线性假设函数 $h(x)=\\theta_0 + \\theta_1x$\n多个特征的线性假设函数\n$h(X)=h_\\Theta(X)=\\theta_0+\\theta_1X_1+\\theta_2X_2$\n为了便利，定义\n$X_0=1$\n则有\n$h\\Theta(X) = \\sum_{i=0} ^n \\Theta_i X_i = \\Theta^TX$\n$\\Theta$被称为学习算法的参数，利用训练集合选择或学习得到合适的参数值是学习算法的任务。\n为了进行预测，一件可以做的事是尝试让学习算法的预测在训练数据上尽可能准确。\n那么就得到了线性回归算法里的成本函数。\n$J(\\Theta) = \\frac 1 2 \\sum_{i=1}^m(h_\\Theta(X^{(i)})-y^{(i)})^2$\n我们要做的是要使上述函数的值最小化。\n下面有两个方法可以帮助选取$\\Theta$以使上述函数的值最小化。\n梯度下降（Gradient Descent Algorithm） 这是一个搜索算法，基本的想法是先给参数向量一个初始值，然后不断地改变参数向量使得不断减小，直到我们找到了一个使得取到了最小值，这个算法称之为梯度下降。\n算法推导 $\\frac \\partial {\\partial\\Theta_i}J(\\Theta)$ 这个导数即为梯度在$\\theta_i$上下降最陡的方向。\n因此更新$\\Theta$的过程可以总结为以下公式：\n$\\Theta_i := \\Theta_i - \\alpha \\frac \\partial {\\partial\\Theta_i}J(\\Theta)$\n其中$\\alpha$为学习速度参数，它控制了算法朝着最陡峭的方向下降的时候迈的步子有多大。$\\alpha$值设的过小，算法向着最陡峭方向下降时，每次迈很小的一步，这样它会花很长时间去收敛。值设的过大，算法可能会越过最小值，因为步子迈的太大了。\n$$ \\begin{align} \\frac \\partial {\\partial\\Theta_i}J(\\Theta) \u0026amp; = \\frac \\partial {\\partial\\Theta_i} \\frac 1 2 (h_\\Theta(X) - y)^2 \\\\ \u0026amp; = 2 \\ast \\frac 1 2 (h_\\Theta(X) - y) \\frac \\partial {\\partial\\Theta_i}(h_\\Theta(X) - y) \\\\ \u0026amp; = (h_\\Theta(X) - y) \\frac \\partial {\\partial\\Theta_i}(\\Theta_0X_0 + … + \\Theta_nX_n -y) \\\\ \u0026amp; = (h_\\Theta(X) -y ) X_i \\end{align} $$ 最后得到更新$\\Theta$的过程可以总结为以下公式：\n$\\Theta_i := \\Theta_i - \\alpha(h_\\Theta(X) -y) * X_i$\n批梯度下降 算法的过程就是重复以下过程，直接最后收敛。\n$\\Theta_i := \\Theta_i - \\alpha \\sum_{j=1}^m(h_\\Theta(X^{(j)}) -y^{(j)}) * X_i^{(j)}=\\Theta_i - \\alpha \\frac \\partial {\\partial\\Theta_i} J(\\Theta)$\n在批梯度下降算法中每一次迭代都需要遍历整个训练集合。当训练数据集过大时，就不太适合了，而应该使用随机梯度下降算法\n随机梯度下降（增量梯度下降） 伪代码示意：\nfor j=1 to m {\n​\t$\\Theta_i := \\Theta_i - \\alpha(h_\\Theta(X^{(j)}) - y^{(j)}) \\ast X_i^{(j)}$ for all i\n}\n一直重复上述过程，直至最后收敛。\n这个算法可能会在全局最小值附近一直徘徊，通常得到的参数值能够很接近全局最小值，这已经足够了。这个算法通常比批梯度下降算法快得多，尤其是当你有一个大规模训练集合的时候。\n正规方程组 对于最小二乘回归问题或者普通的最小二乘问题，实际上存在着方法可以直接给出参数向量的解析表达式，这样为了求参数的值就不需要进行迭代了，这个就是正规方程组。\n定义一些符号    符号 代表的含义     $\\nabla_\\Theta J=\\begin{bmatrix}\\frac {\\partial J} {\\partial \\Theta_0} \\\\ \\vdots \\\\ \\frac {\\partial J} {\\partial \\Theta_n} \\\\ \\end{bmatrix} \\in \\mathbb R^{n+1}$ J是一个关于参数数组的函数，定义J的梯度关于的导数   $\\Theta := \\Theta - \\alpha \\nabla_\\Theta J \\quad \\Theta \\in \\mathbb R^{n+1},\\nabla_\\Theta J \\in \\mathbb R^{n+1}$ 梯度下降更新过程   $f(A) \\in \\mathbb R \\quad A \\in \\mathbb R^{m*n}$ f是一个将m*n的矩阵映射为实数的函数   $\\nabla_Af(A) =\\begin{bmatrix}\\frac {\\partial f} {\\partial A_{11}} \u0026amp; \\cdots \u0026amp; \\frac {\\partial f} {\\partial A_{1n}} \\\\ \\vdots \u0026amp; \\cdots \u0026amp; \\vdots \\\\ \\frac {\\partial f} {\\partial A_{m1}} \u0026amp; \\cdots \u0026amp; \\frac {\\partial f} {\\partial A_{mn}} \\\\ \\end{bmatrix}$ f对于A的导数   $trA=\\sum_{i=0}^nA_{ii} \\quad if \\quad A \\in \\mathbb R^{n*n}$ 方阵的迹    一些事实  $trAB = trBA$ $trABC = trCAB = trBCA$ $f(A) = trAB \\quad then \\quad \\nabla_AtrAB=B^T$ $trA = trA^T$ $tr\\alpha = \\alpha \\quad if \\quad a \\in \\mathbb R$ $\\nabla_AtrABA^TC = CAB + C^TAB^T$  证明正规方程过程中将用到上面所述的事实。\n推导过程 定义 $$ X = \\begin{bmatrix} (X^{(1)})^T \\\\ \\vdots \\\\ (X^{(m)})^T \\end{bmatrix} $$ 则\n$$ X\\Theta = \\begin{bmatrix}(X^{(i)})^T\\Theta \\\\ \\vdots \\\\ (X^{(m)})^T\\Theta \\end{bmatrix} = \\begin{bmatrix}h_\\Theta(X^{(1)} \\\\ \\vdots \\\\ h_\\Theta(X^{(m)} \\end{bmatrix} $$ 再定义 $$ Y = \\begin{bmatrix} y^{(1)} \\\\ \\vdots \\\\ y^{(m)}\\end{bmatrix} $$ 然后得到 $$ X\\Theta \\ast y = \\begin{bmatrix} h(X^{(1)} - y^{(1)}) \\\\ \\vdots \\\\ h(X^{(m)} - y^{(m)}) \\end{bmatrix} $$ 又因为\n$$ Z^TZ=\\sum_iZ_i^2 \\quad if \\quad Z \\in \\mathbb R^{m*1} $$ 所以\n$$ \\frac 1 2 (X\\Theta - y) ^ T (X\\Theta - y) = \\frac 1 2 \\sum_{i=1}^m(h(X^{(i)}) - y)^2 = J(\\Theta) $$ 然后算法的目标(算法尽量收敛)是\n$$ \\nabla_\\Theta J(\\Theta) \\approx \\overrightarrow 0 $$ 所以有\n$$ \\begin {align} \\nabla_\\Theta \\frac 1 2 (X\\Theta - y )^T(X\\Theta - y ) \u0026amp; = \\frac 1 2 \\nabla_\\Theta tr ( \\Theta^T X^T X \\Theta - \\Theta^T X^T y - y^T X \\Theta + y^T y) \\\\ \u0026amp; = \\frac 1 2 \\left[ \\nabla_\\Theta tr \\Theta \\Theta^TX^TX - \\nabla_\\Theta tr y^T X \\Theta - \\nabla_\\Theta y^T X \\Theta \\right] \\\\ \u0026amp; = \\frac 1 2 \\left[ X^T X \\Theta + X^T X \\Theta - X^Ty - X^Ty \\right] \\\\ \u0026amp; = X^TX\\Theta - X^Ty \\\\ \u0026amp; = 0 \\end {align} $$ 最后就有 $$ X^TX\\Theta = X^Ty $$ 上述公式被称为正规方程组。我们现在可以给出这个关于的方程组解的解析表达式了： $$ \\Theta = (X^TX)^{-1}X^Ty $$ 上述公式要求$X^TX$可逆，如果$X^TX$不可逆，可以用伪逆最小化的方法来解决这个问题。\n","permalink":"https://jeremyxu2010.github.io/2017/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B_%E7%AC%94%E8%AE%B002/","tags":["机器学习","andrew ng"],"title":"机器学习课程_笔记02"},{"categories":["机器学习"],"contents":"最近放了一个长假，计划系统地学习下机器学习的基本知识，途径主要是看andrew ng大牛的斯坦福大学公开课-机器学习课程视频，当然在看的过程中为了加深理解，会记下笔记，此篇为第一篇笔记。\n机器学习的定义 非正式定义：\nArthur Samuel (1959). Machine Learning: Field of study that gives computers the ability to learn without being explicitly programmed. 在不直接针对问题进行编程的情况下赋予计算机学习能力的一个研究领域。 更现代的定义：\nTom Mitchell (1998). Well-posed Learning Problem: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. 对于一个计算机程序来说，给它一个任务T和一个性能测量方法P，如果在经验E的影响下，P对T的测量结果得到了改进，那么就说该程序从E中学习。 课程的4个部分 监督学习（Unsuperized Learning） 我们在\u0026quot;监督\u0026quot;问题的算法，这里给算法提供了一组\u0026quot;标准答案\u0026rdquo;，之后我们希望算法去学习标准输入和标准答案之间的联系，以尝试对于我们的其他输入给我们提供更为标准的答案。\n 预测Portland Oregon地区房屋价格，需要预测的变量是连续的 。（回归问题） 预测一个肿瘤是否为恶性，这里算法处理的是一些离散值（分类问题） 把数据映射到无限维空间（支持向量机）  学习理论（Learning Theory） 学习理论分析为什么学习型算法是有效的，这样我们才可以让算法尽可能高效地工作。\n 通过定理证明来找出什么时候你能达到高的精度 了解什么样的算法能很好地近似不同的函数 试图了解一些诸如需要多少训练数据这样的问题  学习理论可以使人能够真正地将机器学习的理念应用到解决实际问题当中。\n无监督学习（Unsuperized Learning） 给一组数据，不告诉关于数据的任何正确答案，然后算法在这组数据中寻找一些有趣的结构。\n 聚类问题将数据聚成几类 按照基因在试验中体现出的形状对这些单独的基因进行分组 聚类算法来进行图像处理，对像素进行聚类 用图像聚类的结果创建这个世界的3D模型 计算机集群组织、社会网络分析、市场划分、航天数据的分析、鸡尾酒会问题 文本处理、理解功能分级和机械数据（ICA算法）  强化学习(Reinforcement Learning) 用在你不需要进行一次决策的情形中，你通常会在一段时间内做出一系列的决策。在强化学习背后的基本概念是一个称为回报函数的概念。强化学习的关键是需要找到一种方式来定义一个好的行为和一个坏的行为，之后就是需要一个学习型算法来尽可能地获得更多的回报和更少的惩罚。\n 机器人领域 网页爬取  ","permalink":"https://jeremyxu2010.github.io/2017/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B_%E7%AC%94%E8%AE%B001/","tags":["机器学习","andrew ng"],"title":"机器学习课程_笔记01"},{"categories":["工具"],"contents":"为何要用mathjax 在书写数值计算类文章，特别是机器学习相关算法时，难免需要插入复杂的数学公式。一种是用图片在网页上展示，另外一种是使用 MathJax 来展示复杂的数学公式。它直接使用 Javascript 使用矢量字库或 SVG 文件来显示数学公式。优点是效果好，比如在 Retina 屏幕上也不会变得模糊。并且可以直接把公式写在 Markdown 文章里。\nhexo支持MathJax 我是使用Typora书写markdown文档的，它自身就支持MathJax了，就不用特别的想办法支持MathJax了。\n最好写好的markdown文档要hexo-next主题渲染出来，它支持MathJax的方法很简单，还是简单记录一下，直接在_config.yml文件里加入以下代码段就可以了。\n# MathJax Support mathjax: enable: true per_page: false cdn: //cdn.bootcss.com/mathjax/2.4.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML 但默认的hexo使用的markdown渲染引擎与mathjax有些冲突，建议还是换用hexo-renderer-pandoc作为markdown的渲染引擎。\n安装方法也很简单：\nbrew install pandoc yarn remove hexo-renderer-marked yarn add hexo-renderer-pandoc LaTex简明教程 先看个例子\n$$ J(\\theta) = \\frac 1 2 \\sum_{i=1}^m (h_\\theta(x^{(i)} - y^{(i)}))^2 $$ 上面的LaTex 格式书写的数学公式经过 MathJax 展示后效果如下：\n$$ J(\\theta) = \\frac 1 2 \\sum_{i=1}^m (h_\\theta(x^{(i)} - y^{(i)}))^2 $$ 这个公式是线性回归算法里的成本函数。\n规则 关于在 Markdown 书写 LaTex 数学公式有几个规则常用规则需要记住：\n行内公式 行内公式使用 $ 号作为公式的左右边界，如 $h(x) = \\theta_0 + \\theta_1 x$，示例如下：\n梯度递减公式： $ \\theta_i = \\theta_i - \\alpha\\frac\\partial{\\partial\\theta_i}J(\\theta) $\n行内公式 公式需要独立显示一行时，使用 $$ 来作为公式的左右边界\n常用LaTex代码 需要记住的几个常用的符号，这样书写起来会快一点\n   编码 说明 示例 代码     \\frac 分子分母之间的横线 $\\frac1 x$  $\\frac1x$   _ 用下划线来表示下标 $x_i$  $x_i$   ^ 次方运算符来表示上标 $x^i$  $x^i$   \\sum 累加器，上下标用上面介绍的编码来书写 $\\sum$  $\\sum$   \\alpha 希腊字母 alpha $y := \\alpha x$  $y := \\alpha x$   \\theta 希腊字母theta $\\theta$ $\\theta$   \\pi 希腊字母pi $\\pi$ $\\pi$   \\delta 希腊字母delta $\\delta$ $\\delta$   \\Delta 希腊字母Delta $\\Delta$ $\\Delta$   \\prod 连乘积符号 $\\prod$ $\\prod$   \\int 积分符号 $ \\int$ $\\int$   \\nabla 希腊字母nabla $\\nabla$ $\\nabla$   \\in 属于 $\\in$ $\\in$   \\partial 希腊字母partial $\\partial$ $partial$   \\begin{bmatrix}a\u0026amp;b\\c\u0026amp;d\\\\end{bmatrix} 矩阵符号 $\\begin{bmatrix}a\u0026amp;b\\c\u0026amp;d\\\\end{bmatrix}$ $\\begin{bmatrix}a\u0026amp;b\\\\c\u0026amp;d\\\\\\end{bmatrix}$   \\mathbb R 实数 $\\mathbb R$ $\\mathbb R$   \\left(A + B\\right) 随着公式大小缩放的左右括号 $\\left(A + B\\right)$ $\\left(A + B\\right)$   A \\quad B 加入一些空隙 $A \\quad B$ $A \\quad B$   \\sqrt[2]{x} 根式 $ \\sqrt[2]{x}$ $\\sqrt[2]{x}$    记住这几个就差不多了，完整的符号列表要看这里，倒回去看一下线性回归算法的成本函数的公式及其 LaTex 代码，对着练习个10分钟基本就可以掌握常用公式的写法了。要特别注意公式里空格和 {} 的运用规则。基本原则是，空格可加可不加，但如果会引起歧义，最好加上空格。{} 是用来组成群组的。比如写一个分式时，分母是一个复杂公式时，可以用 {} 包含起来，这样整个复杂公式都会变成分母了。\n几个非常有用的资源  这是一篇质量很高的介绍 MathJax 的中文博客文章，需要注意的是如果是用 markdown 编写 MathJax 公式，当公式里需要两个斜杠 \\ 时要写四个斜杠 \\。因为 \\ 会被 markdown 转义一次。   Github 上有个在线 Markdown MathJax 编辑器，可以在这里练习，平时写公式时也可以在这里先写好再拷贝到文章里 这是 LaTex 完整教程，包含完整的 LaTex 数学公式的内容，包括更高级的格式控制等 这是一份PDF 格式的 MathJax 支持的数学符号表，当需要书写复杂数学公式时，一些非常特殊的符号的转义字符可以从这里查到 别人整理出的一份技巧  好啦，这样差不多就可以写出优美的数学公式啦。\n","permalink":"https://jeremyxu2010.github.io/2017/06/%E4%BD%BF%E7%94%A8mathjax/","tags":["hexo","markdwon","mathjax"],"title":"使用mathjax"},{"categories":["机器学习"],"contents":"今天做一个业务功能时，需要自动登录第三方系统，虽然第三方系统已经给我方分配了用户名及密码，但登录时必须必须输入验证码，如此就很难做到自动化登录了。因为前一段时间研究过机器学习，觉得可以使用keras, tensorflow之类的深度学习框架解决验证码识别的问题。\n生成训练数据 机器学习一般都需要比较多的训练数据，怎么得到训练数据呢？主要有以下方法：\n 手动（累死人系列） 破解验证码生成机制，自动生成无限多的训练数据 打入敌人内部（卧底+不要脸+不要命+多大仇系列）  第1个方法太耗人力，当然依赖打码兔之类的技术也可以完成，但也比较费钱，第3个方法太不实际，于是只能从第2个方法入手。检查了下，发现这个第三方网站做得挺随意的，验证码的地址就是http://xxx.xxx.com/kaptcha.jpg。从事多年java开发，一看就知道是使用kaptcha库生成的验证码。进一步研究发现就是直接采用kaptcha的默认配置生成的验证码，这样就比较好办了，直接生成一批验证码出来。代码如下：\nGenKaptcha.java\nimport com.google.code.kaptcha.Producer; import com.google.code.kaptcha.util.Config; import javax.imageio.ImageIO; import java.awt.image.BufferedImage; import java.io.IOException; import java.nio.file.Files; import java.nio.file.Path; import java.nio.file.Paths; import java.util.Properties; /** * Created by jeremy on 2017/5/7. */ public class GenKaptcha { public static void main(String[] args) throws IOException { ImageIO.setUseCache(false); Config config = new Config(new Properties()); Producer kaptchaProducer = config.getProducerImpl(); Path p = Paths.get(\u0026#34;pics\u0026#34;); if(!Files.exists(p)){ Files.createDirectory(p); } genPics(kaptchaProducer, 5000); } private static void genPics(Producer kaptchaProducer, int count) throws IOException { for (int i=0; i\u0026lt;count; i++) { String capText = kaptchaProducer.createText(); BufferedImage bi = kaptchaProducer.createImage(capText); Path p = Paths.get(\u0026#34;pics\u0026#34;, i + \u0026#34;_\u0026#34; + capText + \u0026#34;.jpg\u0026#34;); if(!Files.exists(p)){ Files.createFile(p); } ImageIO.write(bi, \u0026#34;jpg\u0026#34;, Files.newOutputStream(p)); } } } 对训练数据预处理 有了训练数据还需要进行简单的预处理\n验证码的向量化 验证码是形如ncn34之类的字符串，而机器学习时用到的标签也必须是向量，因此写两个方法，分别完成验证码字符串的向量化及反向量化。我们知道一个字符很容易向量化，采用one-hot encoding, 那么一个字符串向量化可以简单地把字符one-hot encoding得到的向量拼起来。\n# 验证码的可选字符是从kaptcha得到的默认值 captcha_chars = \u0026#39;abcde2345678gfynmnpwx\u0026#39; char_idx_mappings = {} idx_char_mappings = {} for idx, c in enumerate(list(captcha_chars)): char_idx_mappings[c] = idx idx_char_mappings[idx] = c MAX_CAPTCHA = 5 CHAR_SET_LEN = len(captcha_chars) # 验证码转化为向量 def text2vec(text): text_len = len(text) if text_len \u0026gt; MAX_CAPTCHA: raise ValueError(\u0026#39;验证码最长%d个字符\u0026#39;%MAX_CAPTCHA) vector = np.zeros(MAX_CAPTCHA*CHAR_SET_LEN) for i, c in enumerate(text): idx = i * CHAR_SET_LEN + char_idx_mappings[c] vector[idx] = 1 return vector # 向量转化为验证码 def vec2text(vec): text = [] vec[vec\u0026lt;0.5] = 0 char_pos = vec.nonzero()[0] for i, c in enumerate(char_pos): char_idx = c % CHAR_SET_LEN text.append(idx_char_mappings[char_idx]) return \u0026#39;\u0026#39;.join(text) 图片灰度化 验证码识别这个场景里，图片的色彩并不能帮助识别，因此可以将图片灰度化以减少计算压力。\n# 将图片灰度化以减少计算压力 def preprocess_pics(): for (dirpath, dirnames, filenames) in os.walk(pics_dir): for filename in filenames: if filename.endswith(\u0026#39;.jpg\u0026#39;): with open(pics_dir + \u0026#39;/\u0026#39; + filename, \u0026#39;rb\u0026#39;) as f: image = Image.open(f) # 直接使用convert方法对图片进行灰度操作 image = image.convert(\u0026#39;L\u0026#39;) with open(processed_pics_dir + \u0026#39;/\u0026#39; + filename, \u0026#39;wb\u0026#39;) as of: image.save(of) 提供取得训练数据的方法 为了便于在模型训练时取得训练数据，提供工具方法供外部取得数据\nimg_idx_filename_mappings = {} img_idx_text_mappings = {} img_idxes = [] # 首先遍历目录，根据文件名初始化idx-\u0026gt;filename, idx-\u0026gt;text的映射，同时初始化idx列表 for (dirpath, dirnames, filenames) in os.walk(processed_pics_dir): for filename in filenames: if filename.endswith(\u0026#39;.jpg\u0026#39;): idx = int(filename[0:filename.index(\u0026#39;_\u0026#39;)]) text = filename[int(filename.index(\u0026#39;_\u0026#39;)+1):int(filename.index(\u0026#39;.\u0026#39;))] img_idx_filename_mappings[idx] = filename img_idx_text_mappings[idx] = text img_idxes.append(idx) # 为避免频繁读入文件，将images及labels缓存起来 sample_idx_image_mappings = {} sample_idx_label_mappings = {} # 提供给外部取得一批训练数据的接口 def get_batch_data(batch_size): images = [] labels = [] target_idxes = random.sample(img_idxes, batch_size) for target_idx in target_idxes: image = None if target_idx in sample_idx_image_mappings: image = sample_idx_image_mappings[target_idx] else: with open(processed_pics_dir + \u0026#39;/\u0026#39; + img_idx_filename_mappings[target_idx], \u0026#39;rb\u0026#39;) as f: image = Image.open(f) # 对数据正则化，tensorflow处理时更高效 image = np.array(image)/255 sample_idx_image_mappings[target_idx] = image label = None if target_idx in sample_idx_label_mappings: label = sample_idx_label_mappings[target_idx] else: label = text2vec(img_idx_text_mappings[target_idx]) sample_idx_label_mappings[target_idx] = label images.append(image) labels.append(label) x = np.array(images) y = np.array(labels) return (x, y) 构造深度学习模型 以前直接玩过tensorflow，写起来真的很费劲，这回换keras使使，它相当于tensorflow的API简易封装，这次一用就喜欢上它了。直接上代码。\nfrom pathlib import Path from keras.models import Sequential from keras.layers import Dense, InputLayer from keras.layers.core import Reshape, Dropout, Flatten from keras.layers.convolutional import Conv2D from keras.layers.pooling import MaxPooling2D from keras.layers import Input, concatenate from keras.models import Model import kaptcha_data model = Sequential() # 首先对输入数据reshape一下，因为输入的数据是(-1, kaptcha_data.IMAGE_HEIGHT, kaptcha_data.IMAGE_WIDTH), 要把它变为(-1, kaptcha_data.IMAGE_HEIGHT, kaptcha_data.IMAGE_WIDTH, 1)这样才能方便后面卷积层处理 model.add(InputLayer(input_shape=(kaptcha_data.IMAGE_HEIGHT, kaptcha_data.IMAGE_WIDTH))) model.add(Reshape((kaptcha_data.IMAGE_HEIGHT, kaptcha_data.IMAGE_WIDTH, 1))) # 三组卷积逻辑，每组包括两个卷积层及一个池化层 for i in range(3): model.add(Conv2D(4*2**i, 3, strides=(1, 1), padding=\u0026#39;same\u0026#39;, use_bias=True)) model.add(Conv2D(8*2**i, 5, strides=(1, 1), padding=\u0026#39;same\u0026#39;, use_bias=True)) model.add(MaxPooling2D(pool_size=2, strides=2, padding=\u0026#39;same\u0026#39;)) # 马上要接上全连接层，要将数据展平 model.add(Flatten()) # 全连接层，输出维数是kaptcha_data.MAX_CAPTCHA * kaptcha_data.CHAR_SET_LEN image_input = Input(shape=(kaptcha_data.IMAGE_HEIGHT, kaptcha_data.IMAGE_WIDTH)) encoded_image = model(image_input) encoded_softmax = [] for i in range(kaptcha_data.MAX_CAPTCHA): encoded_softmax.append(Dense(kaptcha_data.CHAR_SET_LEN, use_bias=True, activation=\u0026#39;softmax\u0026#39;)(encoded_image)) output = concatenate(encoded_softmax) model = Model(inputs=[image_input], outputs=output) # 编译模型，损失函数使用categorical_crossentropy， 优化函数使用adadelta，每一次epoch度量accuracy model.compile(loss=\u0026#39;categorical_crossentropy\u0026#39;, optimizer=\u0026#39;adadelta\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) # 模型可视化 # from keras.utils import plot_model # plot_model(model, to_file=captcha_preprocess.base_dir + \u0026#39;/captcha_recognition_model.png\u0026#39;) # 加载之前模型的权值 if Path(kaptcha_data.base_dir + \u0026#39;/kaptcha_recognition.h5\u0026#39;).is_file(): model.load_weights(kaptcha_data.base_dir + \u0026#39;/kaptcha_recognition.h5\u0026#39;) batch_size = 64 epoch = 0 while True: print(\u0026#34;epoch {}...\u0026#34;.format(epoch + 1)) (x_batch, y_batch) = kaptcha_data.get_batch_data(batch_size) train_result = model.train_on_batch(x=x_batch, y=y_batch) print(\u0026#39;loss: %.6f, accuracy: %.6f\u0026#39; % (train_result[0], train_result[1])) if epoch % 50 == 0: # 保存模型的权值 model.save_weights(kaptcha_data.base_dir + \u0026#39;/kaptcha_recognition.h5\u0026#39;) # 当准确率大于0.5时，说明学习到的模型已经可以投入实际使用，停止计算 if train_result[1] \u0026gt; 0.5: break epoch += 1 验证结果 家里的电脑没有GPU支持，计算起来比较慢，但经过3000多次迭代后，还是达到了0.3以前的准确率。\nepoch 3502... loss: 8.298573, accuracy: 0.312500 模型计算好了后，以后要计算某张图片的验证码就简单了。\ndef get_single_image(filename): images = [] with open(filename, \u0026#39;rb\u0026#39;) as f: image = Image.open(f) image = image.convert(\u0026#39;L\u0026#39;) images.append(np.array(image)/255) return np.array(images) # 计算某一张图片的验证码 predicts = model.predict(kaptcha_data.get_single_image(kaptcha_data.base_dir + \u0026#39;/pics/0_ncn34.jpg\u0026#39;), batch_size=1) print(\u0026#39;predict: %s\u0026#39; % kaptcha_data.vec2text(predicts[0])) 总结 有了机器学习以后，以前认为很难突破的障碍变得越来越容易突破了，比如文字验证码，理论上说像kaptcha之类的纯粹文字验证码，对机器学习来说真的太容易破解了。另外在平时工作中如正在要用验证码，一定要设置别人不容易猜出来的规则，绝对不能直接用默认的。\n","permalink":"https://jeremyxu2010.github.io/2017/05/%E4%BD%BF%E7%94%A8keras%E7%A0%B4%E8%A7%A3%E9%AA%8C%E8%AF%81%E7%A0%81/","tags":["keras","tensorflow","python"],"title":"使用keras破解验证码"},{"categories":["工作杂记"],"contents":"今天一段时间一直在忙工作上的事，并没有系统地学习研究某一个具体的问题，但回顾这一个月的工作，发现还是有一些经验可以记录一下的。但这些经验没法系统地整理起来，因此只能算是开发中的杂项了。\n杂项一：httpclient典型用法  基础用法 HttpClient httpClient = HttpClientBuilder.create().build(); HttpPost postMethod = null; try { postMethod = new HttpPost(reqUrl); postMethod.setConfig(RequestConfig.custom().setConnectTimeout(2000).setSocketTimeout(5000).build()); Map\u0026lt;String, Object\u0026gt; params = new HashMap(); params.put(\u0026#34;param1\u0026#34;, param1); params.put(\u0026#34;param2\u0026#34;, param2); postMethod.setEntity(new StringEntity(JSON.json(params), ContentType.APPLICATION_JSON)); HttpResponse response = httpClient.execute(postMethod); if (response.getStatusLine().getStatusCode() == 200) { HttpEntity resEntity = response.getEntity(); JSONObject parsedJsonObj = (JSONObject) JSON.parse(EntityUtils.toString(resEntity, \u0026#34;UTF-8\u0026#34;)); //process parsedJsonObj  if (resEntity != null) { try { EntityUtils.consume(resEntity); } catch (Exception ignore) { } } } } catch (Exception e) { logger.error(\u0026#34;请求失败\u0026#34;, e); } finally { if (postMethod != null) { postMethod.releaseConnection(); } } 上面这段代码还是太麻烦了，实际编码中可以将上述代码封装成函数，只需要传入reqUrl,params, JsonResponseProcessHandler就可以了。\n 拼接请求url List\u0026lt;NameValuePair\u0026gt; params = new ArrayList\u0026lt;NameValuePair\u0026gt;(); params.add(new BasicNameValuePair(\u0026#34;param1\u0026#34;, param1)); params.add(new BasicNameValuePair(\u0026#34;param2\u0026#34;, param2)); URIBuilder uriBuilder = new URIBuilder(\u0026#34;http://exmaple.com\u0026#34;).setPath(\u0026#34;/req_path\u0026#34;).addParameters(params); HttpPost postMethod = new HttpPost(uriBuilder.build().toString());  流式续传文件 RandomAccessFile raf = new RandomAccessFile(file, \u0026#34;r\u0026#34;); InputStream fileIn = Channels.newInputStream(raf.getChannel().position(offset)); InputStreamEntity reqEntity = new InputStreamEntity(fileIn, fileSize - offset, ContentType .APPLICATION_OCTET_STREAM); postMethod.setEntity(reqEntity); HttpResponse response = httpClient.execute(postMethod);  定制httpclient的连接管理器 PoolingHttpClientConnectionManager connectionManager = new PoolingHttpClientConnectionManager(); // 每个主机最大的连接数(如果会大量向同一主机发送大量http请求，需加大此值) connectionManager.setDefaultMaxPerRoute(10); // 总共最大的连接数 connectionManager.setMaxTotal(100); httpClient = HttpClientBuilder.create().setConnectionManager(connectionManager).build();   杂项二：jdk6升级jdk7改造   jpeg编码代码改造\njdk6下的代码\nJPEGImageEncoder encoder = JPEGCodec.createJPEGEncoder(out); JPEGEncodeParam param = encoder.getDefaultJPEGEncodeParam(bufferedImage); param.setQuality(quality, true); encoder.setJPEGEncodeParam(param); encoder.encode(bufferedImage); jdk7下的代码\nImageWriter imageWriter = (ImageWriter)ImageIO.getImageWritersBySuffix(\u0026#34;jpeg\u0026#34;).next(); ImageOutputStream ios = ImageIO.createImageOutputStream(out); imageWriter.setOutput(ios); JPEGImageWriteParam jpegParams = (JPEGImageWriteParam) imageWriter.getDefaultWriteParam(); jpegParams.setCompressionMode(JPEGImageWriteParam.MODE_EXPLICIT); jpegParams.setCompressionQuality(quality); IIOMetadata imageMetaData = imageWriter.getDefaultImageMetadata(new ImageTypeSpecifier(bufferedImage), null); imageWriter.write(imageMetaData, new IIOImage(bufferedImage, null, null), null);   自定义DataSource代码改造\njdk6下的代码\npublic class CustomDataSource extends AbstractRoutingDataSource { @Override public Object determineCurrentLookupKey() { ... } } jdk7下的代码\npublic class CustomDataSource extends AbstractRoutingDataSource { @Override public Object determineCurrentLookupKey() { ... } @Override public Logger getParentLogger() throws SQLFeatureNotSupportedException { throw new java.sql.SQLFeatureNotSupportedException(\u0026#34;getParentLogger not supported\u0026#34;); } }   pom升级\npom中加入代码编译级别的配置\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.3.2\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;source\u0026gt;1.7\u0026lt;/source\u0026gt; \u0026lt;target\u0026gt;1.7\u0026lt;/target\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt;   杂项三：简易的python程序分发 工作中使用python写了一部分与NLP相关的代码，但主程序是部署在Tomcat里的java程序，于是需要想办法分发python程序，同时完成java程序与python程序的交互。\n 自带python程序的依赖库 python程序依赖于一些第三方python库，但很难让运维提前使用pip安装第三方python库，研究了下，可以采用以下简易方法。  在目录下新建一个libs目录，将jieba, snownlp等第三方库放到libs目录下。 修改python程序入口，在最开始加入以下代码。 import sys import os sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), \u0026#39;libs\u0026#39;))    python程序作为简单的http伺服 为了方便java与python交互，将python程序包装为http伺服，以供java程序交互，这里没有用任何其它第三方http框架。 import threading from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer import urlparse server = None def stop_server(): global server if server is not None: server.shutdown() class CustomRequestHandler(BaseHTTPRequestHandler): def do_GET(self): # GET请求访问http://127.0.0.1:8333/?op=stop即可停止http伺服器 params = urlparse.parse_qs(urlparse.urlparse(self.path).query) if params.has_key(\u0026#39;op\u0026#39;) and params[\u0026#39;op\u0026#39;][0] == \u0026#39;stop\u0026#39;: thread = threading.Thread(None, stop_server) thread.start() else: self.send_response(200) self.send_header(\u0026#39;Content-type\u0026#39;, \u0026#39;text/html\u0026#39;) self.end_headers() self.wfile.write(\u0026#34;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;It Works!\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#34;) def do_POST(self): content_length = int(self.headers[\u0026#39;Content-Length\u0026#39;]) post_data = self.rfile.read(content_length) # process post data self.send_response(200) self.send_header(\u0026#39;Content-type\u0026#39;, \u0026#39;text/plain\u0026#39;) self.end_headers() self.wfile.write(\u0026#39;\\n\u0026#39;.join(respLines)) def log_message(self, format, *args): pass def run(server_class=HTTPServer, handler_class=CustomRequestHandler, port=8333): global httpd server_address = (\u0026#39;\u0026#39;, port) server = server_class(server_address, handler_class) server.serve_forever() if __name__ == \u0026#39;__main__\u0026#39;: from sys import argv if len(argv) == 2: run(port=int(argv[1])) else: run()  java程序里启动与停止python伺服 private void startPythonService() { CommandUtil.executeCommand(new String[]{\u0026#34;python\u0026#34;, PYTHON_SCRIPT_PATH, String.valueOf(SERVER_PORT)}); } private void stopPythonService() { HttpGet getMethod = null; try { getMethod = new HttpGet(\u0026#34;http://127.0.0.1:\u0026#34; + SERVER_PORT + \u0026#34;/?op=stop\u0026#34;); HttpResponse response = httpClient.execute(getMethod); } catch (Exception e) { logger.debug(\u0026#34;停止python服务的状态失败\u0026#34;,e); } finally { if(getMethod != null) { getMethod.releaseConnection(); } } }   杂项四：统计一组数据中的出现次数最多的topN数据 Multiset\u0026lt;String\u0026gt; nameCounter = HashMultiset.create(); for(String name : names) { nameCounter.add(name); } Iterable\u0026lt;String\u0026gt; top5Names = Iterables.limit(Multisets.copyHighestCountFirst(nameCounter).asList(), 5); 杂项五：python操作excel文件  从文件加载excel文件 # 加载xlsx文件 workbook = openpyxl.load_workbook(xlsx_file_path) # 获取活跃的sheet sheet = workbook.active # 获取所有的sheet名称 sheet_names = sheet.get_sheet_names() # 获取最二个sheet another_sheet = workbook[sheet_names[1]] # 修改sheet的名称 another_sheet.title = \u0026#39;AnotherSheet\u0026#39; # 获取最大的行数 row_count = sheet.max_row # 获取最大的列数 column_count = sheet.max_column  遍历数据 for i in range(1,101): for j in range(1,101): print(ws.cell(row=i, column=j).value) for row in ws.iter_rows(min_row=1, max_col=3, max_row=2): for cell in row: print(cell.value)  访问单元格的数据 #设置单元格数据 sheet.cell(row=2, column=3, value=\u0026#39;xdfdf\u0026#39;) sheet[\u0026#39;C2\u0026#39;].value = \u0026#39;xdfdf\u0026#39; #获取单元格数据 cell_value = sheet.cell(row=2, column=3).value cell_value = sheet[\u0026#39;C2\u0026#39;].value  写入文件 workbook.save(xlsx_save_path)  修改单元格的字段 cell.font = openpyxl.styles.Font(bold = True)   还有一些更高级的用法参见https://openpyxl.readthedocs.io/en/default/\n杂项六：python里操作mysql数据库 写了一个工具操作mysql数据库的工具方法如下：\ndef sql_query_generator(sql): try: conn = pymysql.connect(host=DB_IP, user=DB_USER, password=DB_PASSWD, \\ db=DB_NAME, charset=\u0026#39;utf8\u0026#39;, cursorclass=pymysql.cursors.DictCursor) with conn.cursor() as cursor: cursor.execute(sql) for row in cursor: yield row finally: conn.close() 使用起来也比较简单：\nquery_generator = sql_query_generator(\u0026#39;select * from user;\u0026#39;) for user in query_generator: print(user) 杂项七：python里计算两个字符串的相似度 import difflib print(difflib.SequenceMatcher(None, \u0026#39;hello world\u0026#39;, \u0026#39;hello\u0026#39;).ratio()) # 也可以用Levenshtein import Levenshtein print(Levenshtein.ratio(\u0026#39;hello world\u0026#39;, \u0026#39;hello\u0026#39;)) ","permalink":"https://jeremyxu2010.github.io/2017/04/%E5%BC%80%E5%8F%91%E5%B0%8F%E6%8A%80%E5%B7%A7%E5%A4%87%E5%BF%98/","tags":["java","python"],"title":"开发小技巧备忘"},{"categories":["java开发"],"contents":"背景 今天在用mybatis写一些单表查询操作业务逻辑时，发现一个简单的查询至少要写三行，如下所示：\nDemoCriteria criteria = new DemoCriteria(); criteria.createCriteria().andFiled1EqualTo(filed1Value); List\u0026lt;Demo\u0026gt; demos = demoMapper.selectByCriteria(criteria); 这样写很累啊，于是想了下能否在一行里搞定呢？\n分析 打开DemoCriteria.java，这样找到createCriteriaInternal这个方法：\nprotected Criteria createCriteriaInternal() { Criteria criteria = new Criteria(); return criteria; } 这里我应该可以将DemoCriteria对象的引用转入Criteria对象，而Criteria对象的大部分方法已经支持链式操作，这样就可以在一行完成查询操作，如下面代码示例：\nprotected Criteria createCriteriaInternal() { Criteria criteria = new Criteria(); return criteria; } public static class Criteria extends GeneratedCriteria { private DemoCriteria topCriteria; protected Criteria() { super(); } protected Criteria(DemoCriteria topCriteria) { super(); this.topCriteria = topCriteria; } public DemoCriteria getTopCriteria() { return this.topCriteria; } } // 使用代码示例方法 List\u0026lt;Demo\u0026gt; demos = demoMapper.selectByCriteria(new DemoCriteria().createCriteria().andFiled1EqualTo(filed1Value)getTopCriteria()); 编写mybatis-generator插件 因为工程中的Example类都是用mybatis-generator生成出来的，而mybatis-generator并没有自带插件完成这件事，因此自己动手写了个插件，如下代码：\npackage personal.jeremyxu2010.mybatis.plugins; import org.mybatis.generator.api.IntrospectedTable; import org.mybatis.generator.api.PluginAdapter; import org.mybatis.generator.api.dom.java.Field; import org.mybatis.generator.api.dom.java.FullyQualifiedJavaType; import org.mybatis.generator.api.dom.java.InnerClass; import org.mybatis.generator.api.dom.java.JavaVisibility; import org.mybatis.generator.api.dom.java.Method; import org.mybatis.generator.api.dom.java.Parameter; import org.mybatis.generator.api.dom.java.TopLevelClass; import java.util.List; /** * @Description: * @Author: jeremyxu * @Created Date: 2017/3/20 * @Created Time: 9:31 * @Version:1.0 */ public class ModelExampleBuilderPlugin extends PluginAdapter { public boolean validate(List\u0026lt;String\u0026gt; warnings) { return true; } public boolean modelExampleClassGenerated(TopLevelClass topLevelClass, IntrospectedTable introspectedTable) { for (Method method : topLevelClass.getMethods()) { if(\u0026#34;createCriteriaInternal\u0026#34;.equals(method.getName())){ method.getBodyLines().clear(); method.addBodyLine(\u0026#34;Criteria criteria = new Criteria(this);\u0026#34;); //$NON-NLS-1$  method.addBodyLine(\u0026#34;return criteria;\u0026#34;); //$NON-NLS-1$  } } for (InnerClass innerClass : topLevelClass.getInnerClasses()) { if(new FullyQualifiedJavaType(\u0026#34;Criteria\u0026#34;).equals(innerClass.getType())){ Field filed = new Field(\u0026#34;topCriteria\u0026#34;, topLevelClass.getType()); filed.setVisibility(JavaVisibility.PRIVATE); innerClass.addField(filed); Method constructMethod = new Method(); constructMethod.setVisibility(JavaVisibility.PROTECTED); constructMethod.setName(\u0026#34;Criteria\u0026#34;); //$NON-NLS-1$  constructMethod.addParameter(new Parameter(topLevelClass.getType(), \u0026#34;topCriteria\u0026#34;)); constructMethod.setConstructor(true); constructMethod.addBodyLine(\u0026#34;super();\u0026#34;); //$NON-NLS-1$  constructMethod.addBodyLine(\u0026#34;this.topCriteria = topCriteria;\u0026#34;); //$NON-NLS-1$  innerClass.addMethod(constructMethod); Method getMethod = new Method(); getMethod.setVisibility(JavaVisibility.PUBLIC); getMethod.setName(\u0026#34;getTopCriteria\u0026#34;); //$NON-NLS-1$  getMethod.setReturnType(topLevelClass.getType()); getMethod.setConstructor(false); getMethod.addBodyLine(\u0026#34;return this.topCriteria;\u0026#34;); //$NON-NLS-1$  innerClass.addMethod(getMethod); } } return true; } } 代码很简单，就不另外说明了。\n然后在mybatis-generator的配置文件里加入\u0026lt;plugin type=\u0026quot;personal.jeremyxu2010.mybatis.plugins.ModelExampleBuilderPlugin\u0026quot;\u0026gt;\u0026lt;/plugin\u0026gt;就可以了。\n这里值得注意的是PluginAdapter里提供了很多方法供插件来覆盖，开发者可根据自己的需要修改生成的domain object、domain example object、mapper class、mapper xml file，编写插件可参考这里。\n最后安利一下自己常用的一些mybatis-generator插件，见这里。\n","permalink":"https://jeremyxu2010.github.io/2017/03/%E7%BC%96%E5%86%99mybatis-generator%E6%8F%92%E4%BB%B6/","tags":["mybatis","mybatis-generator"],"title":"编写mybatis-generator插件"},{"categories":["机器学习"],"contents":"上一篇使用tensorflow完成一个卷积神经网络，但当时写的代码虽然可以工作，还比较零乱，并且并没有经过参数调优，最终得到的模型准确率也并不是很高。本周花了些时间将代码进行了重构，并且对某些地方进行了调整了，目前得到的准确率就比较高了。\n神经网络 神经网络的概念 神经网络只是一个很酷的名词，媒体用来夸大其词的，其实没有生物神经那么高级。神经网络归根结底就是计算图谱，或者说数据流图谱。其实就是一串链在一起的函数，这些函数的操作对象是各种维度的矩阵。\n下面这段我总结出来的话很重要，很重要，很重要。\n在tensorflow里定义的计算图谱有一个很重要的特征，所有的矩阵运算都是可求导的。这样如果有大量可训练的数据，利用反向传播法，通过微积分就可以不断优化更新计算图谱里的各种权重值和偏值，最终即可训练出一个比较好的计算模型来。\n这个视频讲得很通俗易懂，初入门的同学都可以看看。\n卷积神经网络 概念 发现之前自己对卷积神经网络的理解并不是很清楚，这次重新学习终于将其理解清楚了。卷积神经网络可以总结为是一个“矩阵滑窗”乘法，主要用来在视觉识别里进行特征提取。\n这个视频将卷积神经网络提取特征的原理讲得非常清楚了。\n重构版卷积神经网络实现 花了点时间将上一篇实现的卷积神经网络重写了，见下面的代码：\ndemo3.py\n\u0026#39;\u0026#39;\u0026#39;卷积神经网络重构版本\u0026#39;\u0026#39;\u0026#39; from __future__ import print_function, division import os import stat from pathlib import Path import tensorflow as tf import numpy as np from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(\u0026#34;data/mnist/\u0026#34;, one_hot=True) print(\u0026#39;Original train data\u0026#39;, mnist.train.images.shape, mnist.train.labels.shape) print(\u0026#39;Original test data\u0026#39;, mnist.test.images.shape, mnist.test.labels.shape) ckpt_dir = \u0026#39;/ckpt/demo3\u0026#39; ckpt_filename = \u0026#39;default.ckpt\u0026#39; logs_dir = \u0026#39;/logs/demo3\u0026#39; def reformat(samples, labels): \u0026#39;\u0026#39;\u0026#39;对原始数据进行格式化 \u0026#39;\u0026#39;\u0026#39; samples = samples.reshape(samples.shape[0], 28, 28, 1) # (sampleNum, 28, 28, 1) return samples, labels def rm_dirs(top): \u0026#39;\u0026#39;\u0026#39;递归删除目录\u0026#39;\u0026#39;\u0026#39; if Path(top).is_dir(): for root, dirs, files in os.walk(top, topdown=False): for name in files: filename = os.path.join(root, name) os.chmod(filename, stat.S_IWUSR) os.remove(filename) for name in dirs: os.rmdir(os.path.join(root, name)) os.rmdir(top) train_samples, train_labels = reformat(mnist.train.images, mnist.train.labels) test_samples, test_labels = reformat(mnist.test.images, mnist.test.labels) print(\u0026#39;Train data\u0026#39;, train_samples.shape, train_labels.shape) print(\u0026#39;Test data\u0026#39;, test_samples.shape, test_labels.shape) class Network(): def __init__(self, train_batch_size, test_batch_size, image_size, num_channels, conv_kernel_size, conv1_feature_num, conv2_feature_num, hidden1_num, keep_prob, num_labels): \u0026#39;\u0026#39;\u0026#39;计算图谱的构造函数 \u0026#39;\u0026#39;\u0026#39; self.train_batch_size = train_batch_size self.test_batch_size = test_batch_size # 计算图谱中各种层的关键参数 self.image_size = image_size self.num_channels = num_channels self.conv_kernel_size = conv_kernel_size self.conv1_feature_num = conv1_feature_num self.conv2_feature_num = conv2_feature_num self.hidden1_num = hidden1_num self.keep_prob = keep_prob self.num_labels = num_labels # 计算图谱相关op self.graph = tf.Graph() self.session = None self.tf_train_samples = None self.tf_train_labels = None self.tf_test_samples = None self.tf_test_labels = None self.train_prediction = None self.test_prediction = None self.saver = None self.train_accuracy = None self.train_merged_summary = None self.loss = None self.optimizer = None self.test_accuracy = None self.test_merged_summary = None self.summary_writer = None # 各种权值及偏值 self.w_conv1 = None self.b_conv1 = None self.w_conv2 = None self.b_conv2 = None self.w_fc1 = None self.b_fc1 = None self.w_fc2 = None self.b_fc2 = None self.fc_weights = [] self.fc_biases = [] # 定义计算图谱 self.define_graph() def get_batch(self, samples, labels, batchSize): \u0026#39;\u0026#39;\u0026#39;这个函数是一个迭代器/生成器，用于每一次只得到 batchSize 这么多的数据用于 for loop， just like range() function\u0026#39;\u0026#39;\u0026#39; if len(samples) != len(labels): raise Exception(\u0026#39;Length of samples and labels must equal\u0026#39;) stepStart = 0 # initial step i = 0 while stepStart \u0026lt; len(samples): stepEnd = stepStart + batchSize if stepEnd \u0026lt; len(samples): yield i, samples[stepStart:stepEnd], labels[stepStart:stepEnd] i += 1 stepStart = stepEnd def weight_variable(self, shape): \u0026#39;\u0026#39;\u0026#39;定义方法用以产生带稍许噪音的权值\u0026#39;\u0026#39;\u0026#39; initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial) def bias_variable(self, shape): \u0026#39;\u0026#39;\u0026#39;定义方法用以产生带稍许噪音的偏值\u0026#39;\u0026#39;\u0026#39; initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) def add_layer(self, inputs, weights, biases, activation_function=None): \u0026#39;\u0026#39;\u0026#39;定义工具方法，用以创建隐藏层\u0026#39;\u0026#39;\u0026#39; wx_plus_b = tf.add(tf.matmul(inputs, weights), biases) if activation_function is None: outputs = wx_plus_b else: outputs = activation_function(wx_plus_b, ) return outputs def conv2d(self, inputs, weights): \u0026#39;\u0026#39;\u0026#39;定义工具方法，创建卷积层 \u0026#39;\u0026#39;\u0026#39; return tf.nn.conv2d(inputs, weights, strides=[1, 1, 1, 1], padding=\u0026#39;SAME\u0026#39;) def max_pool_2x2(self, inputs): \u0026#39;\u0026#39;\u0026#39;定义工具方法，创建池化层 \u0026#39;\u0026#39;\u0026#39; return tf.nn.max_pool(inputs, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\u0026#39;SAME\u0026#39;) def define_graph(self): \u0026#39;\u0026#39;\u0026#39;定义计算图谱 \u0026#39;\u0026#39;\u0026#39; with self.graph.as_default(): # 这里只是定义图谱中的各种传入变量 self.tf_train_samples = tf.placeholder( tf.float32, shape=(self.train_batch_size, self.image_size, self.image_size, self.num_channels) ) self.tf_train_labels = tf.placeholder( tf.float32, shape=(self.train_batch_size, self.num_labels) ) self.tf_test_samples = tf.placeholder( tf.float32, shape=(self.test_batch_size, self.image_size, self.image_size, self.num_channels) ) self.tf_test_labels = tf.placeholder( tf.float32, shape=(self.test_batch_size, self.num_labels) ) # 这里定义图谱中的各种权值及偏值 with tf.name_scope(\u0026#34;conv1_var\u0026#34;): self.w_conv1 = self.weight_variable([self.conv_kernel_size, self.conv_kernel_size, self.num_channels, self.conv1_feature_num]) self.b_conv1 = self.bias_variable([self.conv1_feature_num]) with tf.name_scope(\u0026#34;conv2_var\u0026#34;): self.w_conv2 = self.weight_variable([self.conv_kernel_size, self.conv_kernel_size, self.conv1_feature_num, self.conv2_feature_num]) self.b_conv2 = self.bias_variable([self.conv2_feature_num]) with tf.name_scope(\u0026#34;h_layer1_var\u0026#34;): self.w_fc1 = self.weight_variable([(self.image_size//(2*2))*(self.image_size//(2*2))*self.conv2_feature_num, self.hidden1_num]) self.b_fc1 = self.bias_variable([self.hidden1_num]) with tf.name_scope(\u0026#34;h_layer2_var\u0026#34;): self.w_fc2 = self.weight_variable([self.hidden1_num, self.num_labels]) self.b_fc2 = self.bias_variable([self.num_labels]) self.fc_weights.append(self.w_fc1) self.fc_weights.append(self.w_fc2) self.fc_biases.append(self.b_fc1) self.fc_biases.append(self.b_fc2) # Add ops to save and restore variables. self.saver = tf.train.Saver() # 这里定义图谱的运算 def model(samples): \u0026#39;\u0026#39;\u0026#39;定义图谱的运算 \u0026#39;\u0026#39;\u0026#39; # 第一层卷积层及池化层 with tf.name_scope(\u0026#34;conv1\u0026#34;): h_conv1 = tf.nn.relu(self.conv2d(samples, self.w_conv1) + self.b_conv1) # (samples, 28, 28, 32) h_pool1 = self.max_pool_2x2(h_conv1) # (samples, 14, 14, 32) # 第二层卷积及池化层 with tf.name_scope(\u0026#34;conv2\u0026#34;): h_conv2 = tf.nn.relu(self.conv2d(h_pool1, self.w_conv2) + self.b_conv2) # (samples, 14, 14, 64) h_pool2 = self.max_pool_2x2(h_conv2) # (samples, 7, 7, 64) # 将四维的输出reshape为二维数据，为连接全连接层作准备 h_pool2_flat = tf.reshape(h_pool2, [-1, (self.image_size//(2*2))*(self.image_size//(2*2))*self.conv2_feature_num]) # (samples, 7*7*64) # 添加一个隐藏层 with tf.name_scope(\u0026#34;h_layer1\u0026#34;): h_fc1 = self.add_layer(h_pool2_flat, self.w_fc1, self.b_fc1, tf.nn.relu) # (samples, 1024) # 添加一个按比率随机drop层 with tf.name_scope(\u0026#34;drop_layer\u0026#34;): h_fc1_drop = tf.nn.dropout(h_fc1, self.keep_prob) # (samples, 1024) # 使用softmax回归模型计算出预测的y，这个是用来分类处理的 with tf.name_scope(\u0026#34;h_layer2\u0026#34;): h_fc2 = tf.add(tf.matmul(h_fc1_drop, self.w_fc2), self.b_fc2) with tf.name_scope(\u0026#34;softmax_layer\u0026#34;): return tf.nn.softmax(h_fc2) # (samples, 10) def calc_accuracy(predictions, labels): \u0026#39;\u0026#39;\u0026#39;计算预测的正确率 \u0026#39;\u0026#39;\u0026#39; correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1)) return tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) * 100 def apply_regularization(): \u0026#39;\u0026#39;\u0026#39;对全连接层的weights与biases进行regularization \u0026#39;\u0026#39;\u0026#39; regularation_param = 0.01 regularation = 0.0 for weights, biases in zip(self.fc_weights, self.fc_biases): regularation += tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases) return regularation_param * regularation with tf.name_scope(\u0026#34;train\u0026#34;): self.train_prediction = model(self.tf_train_samples) self.train_accuracy = calc_accuracy(self.train_prediction, self.tf_train_labels) self.train_merged_summary = tf.summary.merge([tf.summary.scalar(\u0026#39;train_accuracy\u0026#39;, self.train_accuracy)]) # 损失 self.loss = -tf.reduce_sum(self.tf_train_labels * tf.log(self.train_prediction)) self.loss += apply_regularization() # 逐渐降低学习速率 batch = tf.Variable(0, trainable=False) starter_learning_rate = 0.001 learning_rate = tf.train.exponential_decay(starter_learning_rate, batch * self.train_batch_size, self.train_batch_size * 10, 0.95, staircase=True) # 优化 self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss, global_step=batch) with tf.name_scope(\u0026#34;test\u0026#34;): # Predictions for the training, validation, and test data. self.test_prediction = model(self.tf_test_samples) self.test_accuracy = calc_accuracy(self.test_prediction, self.tf_test_labels) self.test_merged_summary = tf.summary.merge([tf.summary.scalar(\u0026#39;test_accuracy\u0026#39;, self.test_accuracy)]) def train(self): \u0026#39;\u0026#39;\u0026#39;训练 \u0026#39;\u0026#39;\u0026#39; if self.session is None: self.session = tf.Session(graph=self.graph) with self.session as session: session.run(tf.global_variables_initializer()) # 从磁盘上还原计算图谱参数 if Path(os.getcwd() + ckpt_dir + \u0026#34;/\u0026#34; + ckpt_filename + \u0026#34;.index\u0026#34;).is_file(): self.saver.restore(session, os.getcwd() + ckpt_dir + \u0026#34;/\u0026#34; + ckpt_filename) print(\u0026#34;Model restored.\u0026#34;) if self.summary_writer is None: self.summary_writer = tf.summary.FileWriter(os.getcwd() + logs_dir, self.graph) ### 训练 print(\u0026#39;Start Training\u0026#39;) for i, samples, labels in self.get_batch(train_samples, train_labels, batchSize=self.train_batch_size): _, train_summary, loss, accuracy = session.run( [self.optimizer, self.train_merged_summary, self.loss, self.train_accuracy], feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels} ) self.summary_writer.add_summary(train_summary, i) if i % 50 == 0: print(\u0026#39;Batch at step %d\u0026#39; % i) print(\u0026#39;Batch loss: %f\u0026#39; % loss) print(\u0026#39;Batch accuracy: %.1f%%\u0026#39; % accuracy) # 将训练得到的计算图谱参数写入磁盘 if not Path(os.getcwd() + ckpt_dir).is_dir(): os.makedirs(os.getcwd() + ckpt_dir) save_path = self.saver.save(session, os.getcwd() + ckpt_dir + \u0026#34;/\u0026#34; + ckpt_filename) print(\u0026#34;Model saved in file: %s\u0026#34; % save_path) self.session = None def test(self): \u0026#39;\u0026#39;\u0026#39;测试 \u0026#39;\u0026#39;\u0026#39; if self.session is None: self.session = tf.Session(graph=self.graph) with self.session as session: session.run(tf.global_variables_initializer()) # 从磁盘上还原计算图谱参数 if Path(os.getcwd() + ckpt_dir + \u0026#34;/\u0026#34; + ckpt_filename + \u0026#34;.index\u0026#34;).is_file(): self.saver.restore(session, os.getcwd() + ckpt_dir + \u0026#34;/\u0026#34; + ckpt_filename) print(\u0026#34;Model restored.\u0026#34;) if self.summary_writer is None: self.summary_writer = tf.summary.FileWriter(os.getcwd() + logs_dir, self.graph) print(\u0026#39;Start Testing\u0026#39;) accuracies = [] for i, samples, labels in self.get_batch(test_samples, test_labels, batchSize=self.test_batch_size): test_summary, accuracy = self.session.run( [self.test_merged_summary, self.test_accuracy], feed_dict={self.tf_test_samples: samples, self.tf_test_labels: labels} ) accuracies.append(accuracy) self.summary_writer.add_summary(test_summary, i) print(\u0026#39;Test accuracy: %.1f%%\u0026#39; % accuracy) print(\u0026#39;Average accuracy:\u0026#39;, np.average(accuracies)) print(\u0026#39;Standard deviation:\u0026#39;, np.std(accuracies)) self.session = None if __name__ == \u0026#39;__main__\u0026#39;: rm_dirs(os.getcwd() + logs_dir) net = Network(100, 300, 28, 1, 5, 32, 64, 1024, 0.5, 10) net.train() net.test() 这里的代码注释得比较清楚了，与上一篇的主要区别有以下几点：\n  将计算图谱的定义写在一个Network类里了，同时这个类提供了train与test方法进行训练与测试\n  Network类抽取了计算图谱里的参数，这样在主函数里就可以很方便的修改\n  对原始数据预处理的逻辑从计算图谱抽离出来了，保持计算图谱功能的单一性\n  整个代码框架整理的比较合理，以后其它类型的神经网络可以照此模式书写\n  优化点 为了提升学习效率及准备度，上述重构版本还采用了几项技术，这里简要记录一下。\n 使用了dropout层  为了预防训练出的模型过拟合，这里采用了dropout层，用以随机丢弃一些元素，代码示例如下：\n# 添加一个按比率随机drop层 with tf.name_scope(\u0026#34;drop_layer\u0026#34;): h_fc1_drop = tf.nn.dropout(h_fc1, self.keep_prob) # (samples, 1024)  对全连接层的权值及偏值进行了regularization  为了控制全连接层的权值及偏值，使这些权值及偏值均衡地分布于真实权值及偏值的周围，对全连接层的权值及偏值进行了regularization。这里有一个讲解regularization作用的视频。\n代码示例如下：\ndef apply_regularization(): \u0026#39;\u0026#39;\u0026#39;对全连接层的weights与biases进行regularization \u0026#39;\u0026#39;\u0026#39; regularation_param = 0.01 regularation = 0.0 for weights, biases in zip(self.fc_weights, self.fc_biases): regularation += tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases) return regularation_param * regularation self.loss += apply_regularization()  使用了exponential_decay将学习速度递减  为了减小机器学习在后期的摇摆，使用了exponential_decay将学习速度递减。这里有一个详细讲解在训练过程中要将学习速度递减原理的视频。代码示例如下：\n# 逐渐降低学习速率 batch = tf.Variable(0, trainable=False) starter_learning_rate = 0.001 learning_rate = tf.train.exponential_decay(starter_learning_rate, batch * self.train_batch_size, self.train_batch_size * 10, 0.95, staircase=True) # 优化 self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss, global_step=batch)  使用tf.train.Saver保存或加载训练的参数  为了利用上一次训练的结果，使用了tf.train.Saver保存或加载训练的参数。代码示例如下：\n# Add ops to save and restore variables. self.saver = tf.train.Saver() # 从磁盘上还原计算图谱参数 if Path(os.getcwd() + ckpt_dir + \u0026#34;/\u0026#34; + ckpt_filename + \u0026#34;.index\u0026#34;).is_file(): self.saver.restore(session, os.getcwd() + ckpt_dir + \u0026#34;/\u0026#34; + ckpt_filename) print(\u0026#34;Model restored.\u0026#34;) # 将训练得到的计算图谱参数写入磁盘 if not Path(os.getcwd() + ckpt_dir).is_dir(): os.makedirs(os.getcwd() + ckpt_dir) save_path = self.saver.save(session, os.getcwd() + ckpt_dir + \u0026#34;/\u0026#34; + ckpt_filename) print(\u0026#34;Model saved in file: %s\u0026#34; % save_path)  使用tensorboard可视化准确率  使用tensorboard可视化观测准确率的变化。tensorboard的用法可参考这里。代码示例如下：\nself.train_accuracy = calc_accuracy(self.train_prediction, self.tf_train_labels) self.train_merged_summary = tf.summary.merge([tf.summary.scalar(\u0026#39;train_accuracy\u0026#39;, self.train_accuracy)]) if self.summary_writer is None: self.summary_writer = tf.summary.FileWriter(os.getcwd() + logs_dir, self.graph) for i, samples, labels in self.get_batch(train_samples, train_labels, batchSize=self.train_batch_size): _, train_summary, loss, accuracy = session.run( [self.optimizer, self.train_merged_summary, self.loss, self.train_accuracy], feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels} ) self.summary_writer.add_summary(train_summary, i) 总结 经过这几天的重新学习，终于更进一步理解了tensorflow神经网络的概念，同时整理了一个较合理的代码框架结构，以后其它类型的神经网络可以照此模式书写。\n","permalink":"https://jeremyxu2010.github.io/2017/03/tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_03/","tags":["tensorflow","python"],"title":"tensorflow学习笔记_03"},{"categories":["机器学习"],"contents":"背景 最近项目中有一个需求，希望分析用户对某些商品的评论，以推测用户对这些商品的情感倾向，从而为运营人员管理这些商品提供依据。\n这个问题属于自然语言处理的范畴，国外有很多这方面的论文。但我不是搞学术的，得想办法快速解决这个问题。\n从网上看到一哥们通过微博分析女朋友的情绪，他的方案里包括分词的选择、情绪分析词典的选择、情绪值的计算等，但因为自己实现的效果比较差，最后废弃了自己的方案，直接选择了腾讯文智的情感分析收费服务。\n因为最近研究过tensorflow，也了解到使用tensorflow参照word2vec完成了词向量后，使用训练好的词向量，应该可以很容易进行语句的情绪分类。这里海航的一个工程师做了个方案。\n一直在想有没有简单点的方案了，搜索了多天，还真被我发现一个简单的方案-snownlp。\nsnownlp的使用 snownlp的文档写得很简单，如下：\nfrom snownlp import SnowNLP s = SnowNLP(u\u0026#39;这个东西真心很赞\u0026#39;) s.words # [u\u0026#39;这个\u0026#39;, u\u0026#39;东西\u0026#39;, u\u0026#39;真心\u0026#39;, # u\u0026#39;很\u0026#39;, u\u0026#39;赞\u0026#39;] s.tags # [(u\u0026#39;这个\u0026#39;, u\u0026#39;r\u0026#39;), (u\u0026#39;东西\u0026#39;, u\u0026#39;n\u0026#39;), # (u\u0026#39;真心\u0026#39;, u\u0026#39;d\u0026#39;), (u\u0026#39;很\u0026#39;, u\u0026#39;d\u0026#39;), # (u\u0026#39;赞\u0026#39;, u\u0026#39;Vg\u0026#39;)] s.sentiments # 0.9769663402895832 positive的概率 s.pinyin # [u\u0026#39;zhe\u0026#39;, u\u0026#39;ge\u0026#39;, u\u0026#39;dong\u0026#39;, u\u0026#39;xi\u0026#39;, # u\u0026#39;zhen\u0026#39;, u\u0026#39;xin\u0026#39;, u\u0026#39;hen\u0026#39;, u\u0026#39;zan\u0026#39;] s = SnowNLP(u\u0026#39;「繁體字」「繁體中文」的叫法在臺灣亦很常見。\u0026#39;) s.han # u\u0026#39;「繁体字」「繁体中文」的叫法 # 在台湾亦很常见。\u0026#39; text = u\u0026#39;\u0026#39;\u0026#39;自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。\u0026#39;\u0026#39;\u0026#39; s = SnowNLP(text) s.keywords(3) # [u\u0026#39;语言\u0026#39;, u\u0026#39;自然\u0026#39;, u\u0026#39;计算机\u0026#39;] s.summary(3) # [u\u0026#39;因而它是计算机科学的一部分\u0026#39;, # u\u0026#39;自然语言处理是一门融语言学、计算机科学、 # 数学于一体的科学\u0026#39;, # u\u0026#39;自然语言处理是计算机科学领域与人工智能 # 领域中的一个重要方向\u0026#39;] s.sentences s = SnowNLP([[u\u0026#39;这篇\u0026#39;, u\u0026#39;文章\u0026#39;], [u\u0026#39;那篇\u0026#39;, u\u0026#39;论文\u0026#39;], [u\u0026#39;这个\u0026#39;]]) s.tf s.idf s.sim([u\u0026#39;文章\u0026#39;])# [0.3756070762985226, 0, 0] 我这个场景主要用到sentiments得到某句话的情感倾向值，如下：\nfrom snownlp import SnowNLP s = SnowNLP(u\u0026#39;这个东西真心很赞\u0026#39;) print(s.sentiments) # 得到这句话的情感倾向值，取值范围为0~1.0，0为负面评价的极限值，1.0为正面评价的极限值 文档中也说明\n 情感分析（现在训练数据主要是买卖东西时的评价，所以对其他的一些可能效果不是很好，待解决）\n 幸好它还提供了自己训练情感的方式：\nfrom snownlp import sentiment sentiment.train(\u0026#39;neg.txt\u0026#39;, \u0026#39;pos.txt\u0026#39;) sentiment.save(\u0026#39;sentiment.marshal\u0026#39;) 这样训练好的文件就存储为sentiment.marshal了，之后修改snownlp/sentiment/__init__.py里的data_path指向刚训练好的文件即可\nsnownlp在项目中的应用 实际在项目中应用时，我选择了snownlp的一个fork项目，因为这个fork项目使用了jieba进行分词，同时加强了繁体中文的支持。\n贴一下requirements.txt文件:\njieba==0.38 -e git+https://github.com/david30907d/snownlp.git@8af8237#egg=snownlp 计算某个商品的情感倾向均值方法比较简单，就是取到每条评论的情感倾向值，加起来平均即可。\n实现时有几点要注意一下：\n 某个商品的评论数太少，比如不足5条，这样统计出的均值可能不具代表性，因此忽略对这些商品的分析 某个商品的评论数太多，多于200条，为了加快分析过程，随机取100条评论进行分析  使用matplotlib观测数据的分布 为了更直观地观测数据的分布，我这里还使了matplotlib进行图形显示，如下代码：\nimport matplotlib import matplotlib.pyplot as plt def showPlot(goodsStats): \u0026#34;\u0026#34;\u0026#34;显示情感均值分布图\u0026#34;\u0026#34;\u0026#34; x = [] y = [] idx = 1 goodsSize = len(goodsStats) zhfont1 = matplotlib.font_manager.FontProperties(fname=\u0026#39;C:/Windows/Fonts/msyh.ttf\u0026#39;) for value in goodsStats.values(): x.append(idx) y.append(value[\u0026#39;sentimentsAvg\u0026#39;]) idx += 1 plt.plot(x, y, \u0026#39;bo\u0026#39;, label=(str(goodsSize) + \u0026#34;Goods Sentiments Average\u0026#34;)) plt.ylabel(\u0026#34;Sentiments Avarage\u0026#34;) plt.ylim(0, 1.0) plt.axis([0, goodsSize, 0, 1.0]) plt.title(str(goodsSize) + \u0026#34;个商品的情感均值分布图\u0026#34;, fontproperties=zhfont1) plt.legend() plt.show() ","permalink":"https://jeremyxu2010.github.io/2017/03/%E4%BD%BF%E7%94%A8snownlp%E8%BF%9B%E8%A1%8C%E8%AF%84%E8%AE%BA%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/","tags":["python","matplotlib","snownlp"],"title":"使用snownlp进行评论情感分析"},{"categories":["web开发"],"contents":"WEB应用程序基本架构 胖服务端 这个架构的特点：\n 后台良好的分层模型 页面由后台输出至浏览器，一般采用JSP、PHP等动态页面技术处理页面的动态内容  一些改进：\n 引入AJAX，局部更新数据，避免整页面刷新 后端使用模板技术，帮助输出页面 前端使用模板技术，帮助构造html页面片断 前端形成了一些CSS框架，如bootstrap 前端形成了一些JS工具方法或常用组件，如jQuery, jQuery插件, ExtJS, YUI等  胖客户端 这个架构的特点：\n 后端跟上面一样良好的分层模型，但成了仅提供API接口的API Server 前端处理与显现相关的大部分逻辑，包括页面路由、数据请求、组件数据绑定、业务逻辑串联等  胖客户端架构的优点  分离前后端关注点，前端负责界面显示，后端负责数据存储和计算，各司其职，不会把前后端的逻辑混杂在一起 前端页面组件化，提高代码重复利用率，简化了开发，适合大型的项目 减轻服务器压力，服务器只用出数据就可以，不用管展示逻辑和页面合成，吞吐能力会提高几倍 同一套后端程序代码，不用修改就可以用于Web界面、手机、平板等多种客户端  前端负责的逻辑这么复杂了，为了便于管理，自然要进行必要的分层。\n前端架构模式 前端架构模式－MVC  用户可以向 View 发送指令（DOM 事件），再由 View 直接要求 Model 改变状态。 用户也可以直接向 Controller 发送指令（改变 URL 触发 hashChange 事件），再由 Controller 发送给 View。 Controller 非常薄，只起到路由的作用，而 View 非常厚，业务逻辑都部署在 View。所以，Backbone 索性取消了 Controller，只保留一个 Router（路由器） 。  典型代表：backbone之类的前端框架\n前端架构模式－MVP  MVP 模式将 Controller 改名为 Presenter，同时改变了通信方向。 各部分之间的通信，都是双向的。 View 与 Model 不发生联系，都通过 Presenter 传递。 View 非常薄，不部署任何业务逻辑，称为\u0026quot;被动视图\u0026rdquo;（Passive View），即没有任何主动性，而 Presenter非常厚，所有逻辑都部署在那里。  这个在Android开发中用得比较多。\n前端架构模式－MVVM MVVM 模式将 Presenter 改名为 ViewModel，基本上与 MVP 模式完全一致。唯一的区别是，它采用双向绑定（data-binding）：View的变动，自动反映在 ViewModel，反之亦然。这种双向绑定功能一般借助于ReactJS、VueJS、AngularJS之类的UI框架。\nReactJS介绍 简介 React (有时叫 React.js 或 ReactJS) 是一个为数据提供渲染为 HTML 的视图的开源 JavaScript 库。React 视图通常采用包含以自定义 HTML 标记规定的其他组件的组件渲染。React 为程序员提供了一种子组件不能直接影响外层组件 (\u0026ldquo;data flows down\u0026rdquo;) 的模型，数据改变时对 HTML 文档的有效更新，和现代单页应用中组件之间干净的分离。它由 Facebook, Instagram 和一个由个人开发者和企业组成的社群维护，它于 2013 年 5 月在 JSConf US 开源。\n原理 在Web开发中，我们总需要将变化的数据实时反应到UI上，这时就需要对DOM进行操作，而复杂或频繁的DOM操作通常是性能瓶颈产生的原因。\nReact为此引入了虚拟DOM（Virtual DOM）的机制：在浏览器端用Javascript实现了一套DOM API。基于React进行开发时所有的DOM构造都是通过虚拟DOM进行，每当数据变化时，React都会重新构建整个DOM树，然后React将当前整个DOM树和上一次的DOM树进行对比，得到DOM结构的区别，然后仅仅将需要变化的部分进行实际的浏览器DOM更新。而且React能够批处理虚拟DOM的刷新，在一个事件循环（Event Loop）内的两次数据变化会被合并。\n尽管每一次都需要构造完整的虚拟DOM树，但是因为虚拟DOM是内存数据，性能是极高的，而对实际DOM进行操作的仅仅是Diff部分，因而能达到提高性能的目的。这样，在保证性能的同时，开发者将不再需要关注某个数据的变化如何更新到一个或多个具体的DOM元素，而只需要关心在任意一个数据状态下，整个界面是如何Render的。\n这里有一个更通俗的解释\n如果对虚拟DOM的工作方式感兴趣，可以看这里\n特点  简单  仅仅只要表达出你的应用程序在任一个时间点应该长的样子，然后当底层的数据变了，React 会自动处理所有用户界面的更新。\n 响应式 (Declarative)  数据变化后，React 概念上与点击“刷新”按钮类似，但仅会更新变化的部分。\n 构建可组合的组件  React 易于构建可复用的组件。事实上，通过 React 你唯一要做的事情就是构建组件。得益于其良好的封装性，组件使代码复用、测试和关注分离（separation of concerns）更加简单。\n 学习一次，到处都可以使  React并没有依赖其它的技术栈，因此可以在老旧项目中使用ReactJS开发新功能，不需要重写存在的代码。React可以在浏览器端或服务端进行渲染，甚至借助于React Native，可在移动设备中渲染。\n关键概念  渲染函数  ReactDOM.render是 React 的最基本方法，用于将模板转为HTML语言，并插入指定的DOM节点。用于将模板转为HTML语言，并插入指定的 DOM 节点\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;script src=\u0026quot;../build/react.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;../build/react-dom.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;../build/browser.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026quot;root\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script type=\u0026quot;text/babel\u0026quot;\u0026gt; ReactDOM.render( \u0026lt;h1\u0026gt;Hello, world!\u0026lt;/h1\u0026gt;, document.getElementById('root') ); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  JSX语法  HTML语言直接写在JavaScript语言之中，不加任何引号，这就是JSX的语法，它允许HTML与JavaScript的混写。\nJSX的规则是：遇到HTML标签（以\u0026lt;开头），就用HTML规则解析；遇到代码块（以{开头），就用 JavaScript 规则解析。\n这种写法虽然将模板直接写到JavaScript中了，但带来很多灵活，不需要去学特定的标签语法，会JS就成。比如下面的代码：\nvar names = ['Alice', 'Emily', 'Kate']; ReactDOM.render( \u0026lt;div\u0026gt; { names.map(function (name) { return \u0026lt;div\u0026gt;Hello, {name}!\u0026lt;/div\u0026gt; }) } \u0026lt;/div\u0026gt;, document.getElementById('example') );  组件  React 允许将代码封装成组件（component），然后像插入普通 HTML 标签一样，在网页中插入这个组件。所有组件类都必须有自己的render方法，用于输出组件。组件的用法与原生的HTML标签完全一致，可以任意加入属性。组件的属性可以在组件类的this.props对象上获取。\nclass HelloWorld extends React.Component{ render(){ return \u0026lt;h1\u0026gt;Hello, {this.props.name}\u0026lt;/h1\u0026gt; } } class Container extends React.Component{ render(){ return \u0026lt;HelloWorld name=\u0026#34;world\u0026#34;\u0026gt;\u0026lt;/HelloWorld\u0026gt; } } ReactDOM.render( \u0026lt;Container /\u0026gt;, document.getElementById(\u0026#39;root\u0026#39;) );  组件状态  组件免不了要与用户互动，React将组件看成是一个状态机，一开始有一个初始状态，然后用户互动，导致状态变化，从而触发重新渲染UI。\nclass LikeButton extends React.Component{ constructor(props){ super(props); this.state = {liked: false}; } handleClick() { this.setState({liked: !this.state.liked}); } render() { var text = this.state.liked ? \u0026#39;like\u0026#39; : \u0026#39;haven\\\u0026#39;t liked\u0026#39;; return ( \u0026lt;p onClick={() =\u0026gt; this.handleClick()}\u0026gt; You {text} this. Click to toggle. \u0026lt;/p\u0026gt; ); } } ReactDOM.render( \u0026lt;LikeButton /\u0026gt;, document.getElementById(\u0026#39;example\u0026#39;) );  组件的生命周期  组件的生命周期分成三个状态：\nMounting：已插入真实 DOM Updating：正在被重新渲染 Unmounting：已移出真实 DOM React 为每个状态都提供了两种处理函数，will 函数在进入状态之前调用，did 函数在进入状态之后调用，三种状态共计五种处理函数。\ncomponentWillMount() componentDidMount() componentWillUpdate(object nextProps, object nextState) componentDidUpdate(object prevProps, object prevState) componentWillUnmount() 比如说实际编码过程中，我们经常会在componentDidMount方法加入逻辑：发出AJAX请求，请求后台数据后修改组件状态。\n简单示例 更多示例代码见 https://facebook.github.io/react\n我自己写的一个SSM+ReactJS+Redux工程示例：http://git.oschina.net/jeremy-xu/ssm-scaffold\nReact简单的教程 http://www.ruanyifeng.com/blog/2015/03/react.html http://wiki.jikexueyuan.com/project/react/\n缺点  尽管可以省掉编译过程体验ReactJS的特性，但要完全发挥它的优点，还得依赖webpack之类的前端打包工具 JSX语法，在javascript代码里写标签，很难让人接受 相对于VueJS来说组件封装不够彻底，CSS部分还在外部文件里 由于整个页面都是JS渲染起来的，产生SEO问题，现在可以通过Prerender等技术解决一部分 初次加载耗时相对增多，现在可以通过服务端渲染解决一部分 有一定门槛，对前端开发人员技能水平要求较高  适用场景 一些后台管理、UI交互特别复杂、频繁操作DOM的页面\n一些小坑   文档虽多，但因为历史原因，找到的文档有的是ES5语法，有的是ES6语法，造成了一些混乱。推荐使用ES6语法，多参考官方文档。同时也读一下两种语法的对照表\n  如果要支持IE8，有一些额外操作要做，参考这里\n  即使是HTML标准标签，在React里也变成React的组件了，要拿到组件对应的DOM对象，需用ReactDOM.findDOMNode(componentInstance)或ReactDOM.findDOMNode(this.refs.compRef)\n  React里的事件是模拟事件SyntheticEvent，它不是原生的DOM事件，支持的属性与方法见这里\n  ES6语法中，组件的方法this回归JavaScript的本意。这样当指定事件回调方法时，this很有可能指定的是触发事件的组件。可以用ES6里的箭头函数来解决这个问题。\n  ReactJS在老旧项目中的应用 限制  要与现有前端页面技术无缝衔接 没有前端编译工具 没有前端模块依赖工具，全凭script标签引入  目前的方案  将常用的JS库文件（ReactJS库、组件库、工具库）一起使用script标签引入 将用ReactJS书写的代码保存在单独的文件里 使用babel在前端实时将ES6的ReactJS代码编译为ES5（这个导致页面初次渲染更慢了）  比如一个实际的例子：\ntest.jsp\n\u0026lt;c:set var=\u0026quot;ctx\u0026quot; value=\u0026quot;${pageContext.request.contextPath}\u0026quot;/\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026quot;root\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;!-- 喜欢使用lodash里一些工具方法，高效且实用 --\u0026gt; \u0026lt;script type=\u0026quot;text/javascript\u0026quot; src=\u0026quot;${ctx}/libs/lodash.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- React的库文件 --\u0026gt; \u0026lt;script type=\u0026quot;text/javascript\u0026quot; src=\u0026quot;${ctx}/libs/react.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026quot;text/javascript\u0026quot; src=\u0026quot;${ctx}/libs/react-dom.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- 阿里出品，强大的React组件库 --\u0026gt; \u0026lt;script type=\u0026quot;text/javascript\u0026quot; src=\u0026quot;${ctx}/libs/antd.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- 时间日期工具库 --\u0026gt; \u0026lt;script type=\u0026quot;text/javascript\u0026quot; src=\u0026quot;${ctx}/libs/moment.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- Promise风格发送AJAX请求，避免javascript callback hell http://callbackhell.com/ --\u0026gt; \u0026lt;script type=\u0026quot;text/javascript\u0026quot; src=\u0026quot;${ctx}/libs/axios.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- 使用babel将ES6的代码在浏览器端翻译为ES5代码 --\u0026gt; \u0026lt;script type=\u0026quot;text/javascript\u0026quot; src=\u0026quot;${ctx}/libs/babel/browser.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- 声明一个JS变量保存webapp上下文，以后发送AJAX请求时会用到 --\u0026gt; \u0026lt;script type=\u0026quot;text/javascript\u0026quot;\u0026gt;var __CTX_PATH__='${ctx}';\u0026lt;/script\u0026gt; \u0026lt;!-- 引入业务JSX文件 --\u0026gt; \u0026lt;script type=\u0026quot;text/babel\u0026quot; src=\u0026quot;${ctx}/scripts/business1.jsx\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; business1.jsx\nconst React = window.React; const ReactDOM = window.ReactDOM; const axios = window.axios; const antd = window.antd; const _ = window._; class Demo1 extends React.Component{ render(){ return \u0026lt;h1\u0026gt;TODO\u0026lt;/h1\u0026gt; } } ReactDOM.render( \u0026lt;Demo1 /\u0026gt;, document.getElementById(\u0026#39;root\u0026#39;) ); 其它相关技术资料  语法类  ES6教程 http://es6.ruanyifeng.com/\n 编译工具类  Gulp教程 http://www.gulpjs.com.cn/docs/ Webpack教程 http://zhaoda.net/webpack-handbook/module-system.html Browserify文档 https://github.com/substack/node-browserify#usage Babeljs使用教程 https://babeljs.io/docs/setup/#installation\n 实用工具函数库  lodash文档 http://lodashjs.com/docs/\n 避免js callback hell利器  Promise教程 http://es6.ruanyifeng.com/#docs/promise Axios教程 https://www.kancloud.cn/yunye/axios/234845\n 前端路由类  React-Router教程 https://react-guide.github.io/react-router-cn/\n React组件类  Antd文档 https://ant.design/docs/react/introduce-cn\n 前端数据流框架  Redux教程 http://cn.redux.js.org/docs/basics/index.html\n 优秀的对标者  VueJS文档 https://cn.vuejs.org/v2/guide/ Vuex文档 https://vuex.vuejs.org/zh-cn/ element组件文档 http://element.eleme.io/#/zh-CN/component/installation\n","permalink":"https://jeremyxu2010.github.io/2017/03/%E5%89%8D%E7%AB%AFreactjs%E6%8A%80%E6%9C%AF%E4%BB%8B%E7%BB%8D/","tags":["reactjs","javascript"],"title":"前端ReactJS技术介绍"},{"categories":["java开发"],"contents":"最近要做一个新的项目，项目涉及的业务还比较复杂，表相当多。项目使用的技术框架为SSM。于是决定使用mybatis-generator来生成DAO层大部分代码。使用的过程中遇到一些问题，这里小计一下。\n实体对象属性为枚举 为了避免硬编码，希望生成的实体对象有的属性尽量使用枚举。\n可以先定义一个枚举。\nUserStatus.java\npublic enum UserState implements CodeTypeEnum\u0026lt;UserState\u0026gt; { ENABLED((byte)0), DISABLED((byte)1); private final Byte code; UserState(Byte code) { this.code = code; } @Override public Byte getCode(){ return this.code; } } 然后在MBG的配置文件中加入\n\u0026lt;table tableName=\u0026#34;user\u0026#34; escapeWildcards=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;columnOverride column=\u0026#34;user_status\u0026#34; javaType=\u0026#34;personal.jeremyxu.entity.enums.UserState\u0026#34; /\u0026gt; \u0026lt;/table\u0026gt; 还需要给枚举定义TypeHandler，TypeHandler的代码比较简单，这里为了以后其它枚举能复用，写了一个范式化的TypeHandler\nCodeTypeHandler.java\npublic class CodeTypeHandler \u0026lt;E extends CodeTypeEnum\u0026gt; extends BaseTypeHandler\u0026lt;E\u0026gt; { private Map\u0026lt;Byte, E\u0026gt; enumMap = new HashMap\u0026lt;Byte, E\u0026gt;(); public CodeTypeHandler(Class\u0026lt;E\u0026gt; type) { E[] enums = type.getEnumConstants(); if (enums == null) { throw new IllegalArgumentException(type.getSimpleName() + \u0026#34; does not represent an enum type.\u0026#34;); } for(E e : enums){ enumMap.put(e.getCode(), e); } } @Override public void setNonNullParameter(PreparedStatement ps, int i, E parameter, JdbcType jdbcType) throws SQLException { ps.setByte(i, parameter.getCode()); } @Override public E getNullableResult(ResultSet rs, String columnName) throws SQLException { Byte code = rs.getByte(columnName); return enumMap.get(code); } @Override public E getNullableResult(ResultSet rs, int columnIndex) throws SQLException { Byte code = rs.getByte(columnIndex); return enumMap.get(code); } @Override public E getNullableResult(CallableStatement cs, int columnIndex) throws SQLException { Byte code = cs.getByte(columnIndex); return enumMap.get(code); } } 然后其它具体某个枚举的TypeHandler就可以这么写了\nUserStateTypeHandler.java\npublic class UserStateTypeHandler extends CodeTypeHandler\u0026lt;UserState\u0026gt; { public UserStateTypeHandler() { super(UserState.class); } } 最后在MyBatis配置里添加TypeHandler的注册\n\u0026lt;bean id=\u0026#34;sqlSessionFactory\u0026#34; class=\u0026#34;org.mybatis.spring.SqlSessionFactoryBean\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;dataSource\u0026#34; ref=\u0026#34;dataSource\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;mapperLocations\u0026#34; value=\u0026#34;classpath:personal/jeremyxu/mapper/*.xml\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;typeHandlersPackage\u0026#34; value=\u0026#34;personal.jeremyxu.entity.enums.handlers\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; 定制MBG生成的代码 MBG提供大量的参数用来定制生成的代码，还提供插件机制，方便其它开发者开发插件来定制生成的代码。\n我这里的配置如下：\n\u0026lt;context id=\u0026#34;default\u0026#34; targetRuntime=\u0026#34;MyBatis3\u0026#34;\u0026gt; ... \u0026lt;!--为生成的实体类添加equals，hashCode方法 --\u0026gt; \u0026lt;plugin type=\u0026#34;org.mybatis.generator.plugins.EqualsHashCodePlugin\u0026#34; /\u0026gt; \u0026lt;!--为生成的实体类添加toString方法 --\u0026gt; \u0026lt;plugin type=\u0026#34;org.mybatis.generator.plugins.ToStringPlugin\u0026#34; /\u0026gt; \u0026lt;!--修改生成的Example类的类名，将其中的Example修改为Criteria --\u0026gt; \u0026lt;plugin type=\u0026#34;org.mybatis.generator.plugins.RenameExampleClassPlugin\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;searchString\u0026#34; value=\u0026#34;Example$\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;replaceString\u0026#34; value=\u0026#34;Criteria\u0026#34; /\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;!--修改生成的Mapper类中的方法名或参数名，将方法中的Example修改为Criteria，参数中的example修改为criteria --\u0026gt; \u0026lt;plugin type=\u0026#34;personal.jeremyxu2010.mybatis.plugins.RenameExampleClassAndMethodsPlugin\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;classMethodSearchString\u0026#34; value=\u0026#34;Example\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;classMethodReplaceString\u0026#34; value=\u0026#34;Criteria\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;parameterSearchString\u0026#34; value=\u0026#34;example\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;parameterReplaceString\u0026#34; value=\u0026#34;criteria\u0026#34; /\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;!--使生成的Example类支持setOffset, setLimit方法，以便分页 --\u0026gt; \u0026lt;plugin type=\u0026#34;personal.jeremyxu2010.mybatis.plugins.MySQLLimitPlugin\u0026#34; /\u0026gt; \u0026lt;!--将生成的Example类放到filters包下，不跟实体类混在一起 --\u0026gt; \u0026lt;plugin type=\u0026#34;personal.jeremyxu2010.mybatis.plugins.CreateSubPackagePlugin\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;exampleSubPackage\u0026#34; value=\u0026#34;filters\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;exampleClassSuffix\u0026#34; value=\u0026#34;\u0026#34; /\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;commentGenerator\u0026gt; \u0026lt;!--生成的注释中不带时间戳 --\u0026gt; \u0026lt;property name=\u0026#34;suppressDate\u0026#34; value=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;!--将数据库中列的注释生成到实体的属性注释里，这个很重要 --\u0026gt; \u0026lt;property name=\u0026#34;addRemarkComments\u0026#34; value=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;/commentGenerator\u0026gt; \u0026lt;javaModelGenerator targetPackage=\u0026#34;${modelPackage}\u0026#34; targetProject=\u0026#34;${targetProject}\u0026#34;\u0026gt; \u0026lt;!--是否对model添加构造函数 --\u0026gt; \u0026lt;property name=\u0026#34;constructorBased\u0026#34; value=\u0026#34;false\u0026#34; /\u0026gt; \u0026lt;!--是否允许子包，即targetPackage.schemaName.tableName --\u0026gt; \u0026lt;property name=\u0026#34;enableSubPackages\u0026#34; value=\u0026#34;false\u0026#34; /\u0026gt; \u0026lt;!--建立的Model对象是否 不可改变 即生成的Model对象不会有 setter方法，只有构造方法 --\u0026gt; \u0026lt;property name=\u0026#34;immutable\u0026#34; value=\u0026#34;false\u0026#34; /\u0026gt; \u0026lt;!--是否对类CHAR类型的列的数据进行trim操作 --\u0026gt; \u0026lt;property name=\u0026#34;trimStrings\u0026#34; value=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;/javaModelGenerator\u0026gt; \u0026lt;!--escapeWildcards设置为true可以帮助抵御SQL注入 --\u0026gt; \u0026lt;table tableName=\u0026#34;user\u0026#34; escapeWildcards=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;columnOverride column=\u0026#34;user_status\u0026#34; javaType=\u0026#34;personal.jeremyxu.entity.enums.UserState\u0026#34; /\u0026gt; \u0026lt;/table\u0026gt; ... \u0026lt;/context\u0026gt; 生成数据库中的TEXT字段 在表的配置中添加columnOverride即可，如下\n\u0026lt;columnOverride column=\u0026#34;text_column\u0026#34; jdbcType=\u0026#34;VARCHAR\u0026#34; /\u0026gt; 参考项目 这里提供了一个示例工程对上述说到的内容作了演示，地址在这里。\n","permalink":"https://jeremyxu2010.github.io/2017/03/mybatis-generator%E4%BD%BF%E7%94%A8%E5%A4%87%E5%BF%98/","tags":["java","mybatis","spring"],"title":"mybatis-generator使用备忘"},{"categories":["机器学习"],"contents":"上一篇笔记采用一个线性关系的神经层处理了MNIST的训练数据，最后得到一个准确率一般的神经网络。但其实对于这种图像识别的场景，tensorflow里还可以使用卷积神经网络技术进行准确率更高的机器学习。\n卷积与池化 卷积是一个数学上的概念，简单说就是拿卷积核从原始图像里提取特征映射，将一张图片转化为多张包含特征映射的图片。理解卷积可以读一下这篇帖子，里面除了很抽象的数学定义外，还有一些便于理解的示例。 池化主要用来浓缩卷积层的输出结果并创建一个压缩版本的信息并输出。\n示例程序 学习卷积神经网络，我也参照官方的代码写了个小例子，如下。\ndemo2.py\nimport tensorflow as tf # 下载mnist并加载MNIST的训练数据 import tensorflow.examples.tutorials.mnist.input_data as input_data mnist = input_data.read_data_sets(\u0026#34;MNIST_data/\u0026#34;, one_hot=True) # 定义两个方法，用以产生带稍许噪音的权值与偏值 def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial) def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) # 定义工具方法，用以创建隐藏层 def add_layer(inputs, Weights, biases, activation_function=None): # add one more layer and return the output of this layer Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases) if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b, ) return outputs # 定义工具方法，创建卷积层 def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\u0026#39;SAME\u0026#39;) # 定义工具方法，创建池化层 def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\u0026#39;SAME\u0026#39;) # 定义两个外部传入的张量 x = tf.placeholder(tf.float32, [None, 784]) y = tf.placeholder(tf.float32, [None, 10]) # drop层使用到的保留比率 keep_prob = tf.placeholder(tf.float32) # 将原始二维数据reshape为四维数据 x_image = tf.reshape(x, [-1, 28, 28, 1]) # (samples, 28, 28, 1) # 第一层卷积层及池化层 W_conv1 = weight_variable([5, 5, 1, 32]) b_conv1 = bias_variable([32]) h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # (samples, 28, 28, 32) h_pool1 = max_pool_2x2(h_conv1) # (samples, 14, 14, 32) # 第二层卷积及池化层 W_conv2 = weight_variable([5, 5, 32, 64]) b_conv2 = bias_variable([64]) h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) # (samples, 14, 14, 64) h_pool2 = max_pool_2x2(h_conv2) # (samples, 7, 7, 64) # 将四维的输出reshape为二维数据 h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64]) # (samples, 7*7*64) # 添加一个隐藏层 W_fc1 = weight_variable([7*7*64, 1024]) b_fc1 = bias_variable([1024]) h_fc1 = add_layer(h_pool2_flat, W_fc1, b_fc1, tf.nn.relu) # (samples, 1024) # 添加一个按比率随机drop层 h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) # (samples, 1024) # 使用softmax回归模型计算出预测的y，这个是用来分类处理的 W_fc2 = weight_variable([1024, 10]) b_fc2 = bias_variable([10]) prediction_y = tf.nn.softmax(tf.add(tf.matmul(h_fc1_drop, W_fc2), b_fc2)) # (samples, 10) # 使用交叉熵计算预测的y与实际的y的损失 loss = -tf.reduce_sum(y*tf.log(prediction_y)) # 使用AdamOptimizer以0.0001的学习速率最小化交叉熵 train_step = tf.train.AdamOptimizer(1e-4).minimize(loss) # 计算预测的y与实际的y是否匹配，返回为[True, False, True, True...] correct_prediction = tf.equal(tf.argmax(prediction_y, 1), tf.argmax(y, 1)) # 计算上一步计算出来的张量的平均值 accuracy = tf.reduce_mean(tf.cast(correct_prediction, \u0026#34;float\u0026#34;)) with tf.Session() as sess: # 初始化所有变量 sess.run(tf.global_variables_initializer()) # 循环训练1000次，每次从MNIST的训练数据中随机抽出100条进行训练 for i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 0.5}) if i % 10 == 0: # 使用MNIST的测试数据计算模型的准确率 batch_xs, batch_ys = mnist.test.next_batch(100) print(sess.run(accuracy, \\ feed_dict={x: batch_xs, y: batch_ys, keep_prob: 0.5})) 代码注释得很清楚了，为了便于理解，多个层之间转换时，我也将张量的shape标示出来了。\n总结 本篇作为tensorflow入门的一个较复杂的例子，其中涉及了较多数学知识，理解起来还是挺困难的。后面我会尝试用tensorboard等工具将神经网络以可视化的方式呈现出来，这样可能容易理解一点。看到一句话，原以为深度学习工程师很高大上，原来大家都是这么干活的。\n 话说，深度学习工程师50%的时间在调参数，49%的时间在对抗过/欠拟合，剩下1%时间在修改网上down下来的程序。\n 参考 http://wiki.jikexueyuan.com/project/tensorflow-zh/tutorials/deep_cnn.html http://www.jianshu.com/p/3b611043cbae https://www.zhihu.com/question/22298352?rf=21686447 https://www.zhihu.com/question/38098038\n","permalink":"https://jeremyxu2010.github.io/2017/03/tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_02/","tags":["tensorflow","python"],"title":"tensorflow学习笔记_02"},{"categories":["机器学习"],"contents":"最近看到一个有趣的项目pix2pix-tensorflow。大概功能是用户在网页上画一只猫的轮廓，然后它就可以输出与这个轮廓很相似的猫的清晰图片。出于好奇，就想研究一下这个项目是如何实现的，于是跳入了tensorflow机器学习这个坑。\ntensorflow是什么  TensorFlow是一个开源软件库，用于各种感知和语言理解任务的机器学习。目前被50个团队用于研究和生产许多Google商业产品，如语音识别、Gmail、Google 相册和搜索，其中许多产品曾使用过其前任软件DistBelief。TensorFlow最初由Google Brain团队开发，用于Google的研究和生产，于2015年11月9日在Apache 2.0开源许可证下发布。\n  TensorFlow是Google Brain的第二代机器学习系统，2015年11月9日，参考实现作为开源软件发布。虽然参考实现运行在单台设备，TensorFlow可以运行在多个CPU和GPU（和可选的CUDA扩展）。它运行在64位Linux或macOS桌面或服务器系统，以及在移动计算平台上，包括Android和iOS。TensorFlow的计算用有状态的数据流图表示。许多Google团队已从DistBelief迁移到TensorFlow进行研究和生产。这个库的算法源于Google需要指导称为神经网络的计算机系统，类似人类学习和推理的方法，以便派生出新的应用程序承担以前仅人类能胜任的角色和职能；TensorFlow的名字来源于这类神经网络对多维数组执行的操作。这些多维数组被称为“张量”，但这个概念并不等同于张量的数学概念。其目的是训练神经网络检测和识别模式和相互关系。\n tensorflow安装 安装过程很简单，就是普通的python库安装方法，这里重点说一下windows下安装tensorflow的方法，玩其它系统的用户看看官方文档肯定能搞定安装。\n 安装64位3.5.x版的python，安装包从这里下载 安装cpu版本或gpu版本(如果有NV的显卡的话)的tensorflow  C:\\\u0026gt; pip3 install --upgrade tensorflow 或\npip3 install --upgrade tensorflow-gpu 写个hello world入门程序 学什么东西，先上个hello world入门程序\nhelloworld.py\nimport tensorflow as tf greeting = tf.constant(\u0026#34;hello world!\u0026#34;, dtype=tf.string) with tf.Session() as sess: print(sess.run(greeting)) 然后执行它\npython helloworld.py b\u0026#39;hello world!\u0026#39; 正常输出hello world!了。\n这个小程序逻辑很简单，解释一下：先用tensorflow定义图的结构，就定义了一个tf.string的常量greeting，然后在打开的tensorflow会话里运行并得到这个张量的值，最后用print打印出来。\n一个入门的例子 先上代码：\ndemo1.py\nimport tensorflow as tf # 下载mnist并加载MNIST的训练数据 import tensorflow.examples.tutorials.mnist.input_data as input_data mnist = input_data.read_data_sets(\u0026#34;MNIST_data/\u0026#34;, one_hot=True) # 定义两个外部传入的张量 x = tf.placeholder(tf.float32, [None, 784]) y = tf.placeholder(\u0026#34;float\u0026#34;, [None, 10]) # 定义要训练学习的变量 W = tf.Variable(tf.zeros([784, 10])) b = tf.Variable(tf.zeros([10])) # 使用softmax回归模型计算出预测的y prediction_y = tf.nn.softmax(tf.matmul(x, W) + b) # 使用交叉熵计算预测的y与实际的y的损失 loss = -tf.reduce_sum(y*tf.log(prediction_y)) # 使用梯度下降算法以0.01的学习速率最小化交叉熵 train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) # 计算预测的y与实际的y是否匹配，返回为[True, False, True, True...] correct_prediction = tf.equal(tf.argmax(prediction_y, 1), tf.argmax(y, 1)) # 计算上一步计算出来的张量的平均值 accuracy = tf.reduce_mean(tf.cast(correct_prediction, \u0026#34;float\u0026#34;)) with tf.Session() as sess: # 在图中初始化所有变量 sess.run(tf.global_variables_initializer()) # 循环训练1000次，每次从MNIST的训练数据中随机抽出100条进行训练 for i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys}) if i % 10 == 0: # 使用MNIST的测试数据计算模型的准确率 batch_xs, batch_ys = mnist.test.next_batch(100) print(sess.run(accuracy, \\ feed_dict={x: batch_xs, y: batch_ys})) 代码里的注释写得比较清楚，就不一一解释了。下面对关键点作个说明。\n入门例子关键点分析   tensorflow的程序一般分为如下几个部分\n 定义包含n个层的tensorflow神经网络的模型，这个模型一般会描述逻辑如何将输入计算为预测的输出 定义损失函数，损失函数为预测的输出与实际输出的差距 定义用何种方法优化减小预测的损失 迭代地输入训练数据，用以训练模型 训练的过程中定期检测模型的准确率    定义的模型如果要从外部传入张量，一般写法如下：\n  # 定义外部传入的张量 parma1 = tf.placeholder(tf.float32, [None, 784]) ... with tf.Session() as sess: ... # 在图中运行时传入张量 sess.run(val1, feed_dict={parma1: param_value})  定义的模型如果使用了变量，一般写法如下：  # 定义变量 val1 = tf.placeholder(tf.float32, [None, 784]) ... # 初始化所有变量 init = tf.global_variables_initializer() with tf.Session() as sess: # 在图中初始化所有变量 sess.run(init) ...  一个神经网络层一般形式如下  l1_output = tf.nn.softmax(tf.matmul(l1_input,W) + b) 其中W为权值，b为偏置，tf.nn.softmax是用来分类的。有可能还会有激励函数，毕竟并不是所有关系都是线性的，激励函数就是用来将线性关系掰弯的，tensorflow里完成此类功能的激励函数有很多，见这里。\n 一个训练step一般形式如下  train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) 其中tf.train.GradientDescentOptimizer为优化函数，tensorflow里自带的优化函数挺多的，见这里，loss为损失函数。\n总结 本篇作为tensorflow入门的一个笔记，后面我会再说一说tensorflow里的CNN与RNN。\n参考 https://www.tensorflow.org/install/install_windows https://www.tensorflow.org/get_started/mnist/beginners https://www.tensorflow.org/versions/r0.10/api_docs/python\n","permalink":"https://jeremyxu2010.github.io/2017/02/tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_01/","tags":["tensorflow","python"],"title":"tensorflow学习笔记_01"},{"categories":["golang开发"],"contents":"前段时间换了个环境写代码，但公司由于信息安全限制，要求大部分人员使用windows系统，于是我只能硬着头皮用windows。用windows的过程中发现一个很不便的地方，以前用类Unix系统，可以很方便写脚本完成一些小任务，但在windows里就变得很麻烦。解决方案有好几种：\n 使用cygwin之类的bash环境模拟器。但涉及windows命令与cygwin里的命令互操作时，会出现一些问题，解决起来很麻烦。 使用微软的powershell写脚本。不太想学一门新的类bash脚本语言。 装上python，写python脚本。这似乎是个好办法，可对python语言不太熟。  最后想了下，之前用过Go，可以用它来写小工具，试了试还挺好使的，下面举几个小例子。\n自动生成hexo博客静态文件 package main import ( \u0026#34;log\u0026#34; \u0026#34;os/exec\u0026#34; \u0026#34;time\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; ) // 此命令工具用于辅助hexo进行博客写作 // 原理： // 1. 根据source目录下的文件提交变化自动重新生成博客静态文件 // 2. 启动hexo服务器，端口为5000 func main() { blogDir := filepath.Join(`W:\\`, `gits`, `blog`) //首先生成一次，然后取得当前的commitID \tgenerateBlog(blogDir) lastCommitID := getLastCommitID(blogDir) //运行hexo本地服务器 \tgo func() { runBlog(blogDir) }() //不停地检查当前commitID是否与保存的是否一致，如不一致，则重新生成 \tfor { newCommitID := getLastCommitID(blogDir) if newCommitID != lastCommitID { generateBlog(blogDir) lastCommitID = newCommitID } time.Sleep(10 * time.Second) } } func runBlog(blogDir string) { cmd := exec.Command(`hexo`, `server`, `-s`, `-p`, `5000`) cmd.Dir = blogDir cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr err := cmd.Run() if err != nil { log.Fatal(err) } } func getLastCommitID(blogDir string) string { cmd := exec.Command(`git`, `rev-parse`, `HEAD`) cmd.Dir = filepath.Join(blogDir, `source`) out, err := cmd.Output() if err != nil { log.Fatal(err) } return string(out) } func generateBlog(blogDir string) { cmd := exec.Command(`hexo`, `generate`) cmd.Dir = blogDir cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr err := cmd.Run() if err != nil { log.Fatal(err) } } 部署hexo博客 package main import ( \u0026#34;path/filepath\u0026#34; \u0026#34;os/exec\u0026#34; \u0026#34;os\u0026#34; \u0026#34;log\u0026#34; ) // 此命令工具用于将hexo部署至服务器 func main() { blogDir := filepath.Join(`W:\\`, `gits`, `blog`) deployBlog(blogDir) } func deployBlog(blogDir string) { cmd := exec.Command(`hexo`, `deploy`) cmd.Dir = blogDir cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr err := cmd.Run() if err != nil { log.Fatal(err) } } 杀Tomcat package main import ( \u0026#34;os/exec\u0026#34; \u0026#34;log\u0026#34; \u0026#34;bufio\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;io\u0026#34; ) // 此命令工具杀掉意外未死的tomcat进程 // idea有时候意外退出，此时开发用的tomcat服务器还在运行 func main() { cmd := exec.Command(`netstat`, `-ano`) output, err := cmd.StdoutPipe() if err != nil { log.Fatal(err) } go func() { cmd.Run() }() processOutput(output) } func processOutput(output io.ReadCloser) { scanner := bufio.NewScanner(output) for scanner.Scan() { line := scanner.Text() if hasTomcatPort(line) { parts := strings.Fields(line) tomcatPid := parts[len(parts) -1] log.Println(tomcatPid) killCmd := exec.Command(`taskkill`, `/F`, `/PID`, tomcatPid) killCmd.Run() } } } func hasTomcatPort(line string) bool { return strings.Contains(line, `LISTENING`) \u0026amp;\u0026amp; (strings.Contains(line, `8080`) || strings.Contains(line, `1099`)) } 总结 Go语言很精练，用来写这些小工具很合适。同时也可以经常用Go写点小东西练练手，以免长时间不用忘记了Go的玩法。\n","permalink":"https://jeremyxu2010.github.io/2017/02/%E9%87%87%E7%94%A8go%E7%BC%96%E5%86%99%E5%B0%8F%E5%B7%A5%E5%85%B7/","tags":["golang","hexo","tomcat"],"title":"采用go编写小工具"},{"categories":["工具"],"contents":"今天在用hexo写博文时遇到一个涉及中文的302问题，记录一下。\n当访问链接为http://middle3.dev.vivo.xyz:4000/authors/测试时，控制台报错：\nTypeError: The header content contains invalid characters at ServerResponse.OutgoingMessage.setHeader (_http_outgoing.js:358:11) at /home/hexo/blog/node_modules/hexo-server/lib/middlewares/route.js:27:11 at call (/home/hexo/blog/node_modules/connect/index.js:239:7) at next (/home/hexo/blog/node_modules/connect/index.js:183:5) at /home/hexo/blog/node_modules/hexo-server/lib/middlewares/header.js:9:5 at call (/home/hexo/blog/node_modules/connect/index.js:239:7) at next (/home/hexo/blog/node_modules/connect/index.js:183:5) at Function.handle (/home/hexo/blog/node_modules/connect/index.js:186:3) at Server.app (/home/hexo/blog/node_modules/connect/index.js:51:37) at emitTwo (events.js:106:13) at Server.emit (events.js:191:7) at HTTPParser.parserOnIncoming [as onIncoming] (_http_server.js:546:12) at HTTPParser.parserOnHeadersComplete (_http_common.js:99:23) 跟踪了下hexo-server/lib/middlewares/route.js的代码如下：\nvar url = route.format(decodeURIComponent(req.url)); var data = route.get(url); var extname = pathFn.extname(url); // When the URL is `foo/index.html` but users access `foo`, redirect to `foo/`. if (!data) { if (extname) return next(); res.statusCode = 302; res.setHeader(\u0026#39;Location\u0026#39;, root + url + \u0026#39;/\u0026#39;); res.end(\u0026#39;Redirecting\u0026#39;); return; } 这里url是decode出来的字符串，如果字符串里包含中文，然后调用res.setHeader方法即会报上面的错，解决方法也比较简单，加入一行即可：\nif (!data) { if (extname) return next(); url = encodeURI(url); res.statusCode = 302; res.setHeader(\u0026#39;Location\u0026#39;, root + url + \u0026#39;/\u0026#39;); res.end(\u0026#39;Redirecting\u0026#39;); return; } 进一步查了下，hexo-server的git版本是修复了这个问题的，见这里，但hexo依赖的hexo-server@0.2.0版本却没有修复这个问题，估计很多非英语用户都会遇到这个问题，真坑爹！\n","permalink":"https://jeremyxu2010.github.io/2017/02/%E4%BF%AE%E5%A4%8Dhexo%E6%B6%89%E5%8F%8A%E4%B8%AD%E6%96%87%E7%9A%84302%E9%97%AE%E9%A2%98/","tags":["nodejs"],"title":"修复hexo涉及中文的302问题"},{"categories":["java开发"],"contents":"项目中使用hibernate进行数据库查询，但由于项目历时较长，经手的人较多，DAO层的代码风格很不致，这里将一些常见的场景进行归纳，并给出推荐的写法。\n 根据ID查询实体  // 不推荐 Demo demo = getSession().createQuery(\u0026#34;from Demo where id=?\u0026#34;).setLong(0, id).uniqueResult(); // 推荐 Demo demo = getHibernateTemplate().get(Demo.class, id);  根据某些条件查询  // 不推荐 List\u0026lt;Demo\u0026gt; demos = getSession().createQuery(hql).setLong(0, param1).setParameter(1, param2).list(); // 推荐 List\u0026lt;Demo\u0026gt; demos = getHibernateTemplate().find(hql, param1, param2);  根据某个条件查询唯一的返回值  // 不推荐 Demo demo = getSession().createQuery(hql).setLong(0, param1).setParameter(1, param2).uniqueResult(); // 推荐 Demo demo = DataAccessUtils.uniqueResult((List\u0026lt;Demo\u0026gt;)getHibernateTemplate().find(hql, param1, param2));  删除、保存、更新实体  // 不推荐 getSession().delete(demo); getSession().save(demo); getSession().saveOrUpdate(demo); getSession().update(demo); // 推荐 getHibernateTemplate().delete(demo); getHibernateTemplate().save(demo); getHibernateTemplate().saveOrUpdate(demo); getHibernateTemplate().update(demo);  执行更新操作  // 不推荐 getSession().createQuery(hql).setLong(0, param1).setParameter(1, param2).executeUpdate(); // 推荐 getHibernateTemplate().bulkUpdate(hql, param1, param2);  执行SQL  // 不推荐 getSession().createSQLQuery(sql).setLong(0, param1).setParameter(1, param2).executeUpdate(); // 推荐 getHibernateTemplate().execute(new HibernateCallback\u0026lt;Void\u0026gt;() { @Override public Void doInHibernate(Session session) throws HibernateException, SQLException { session.createSQLQuery(sql).setLong(0, param1).setParameter(1, param2).executeUpdate(); return null; } });  查询数目  // 不推荐 Long count = (Long)getSession().createQuery(\u0026#34;select count(*) from Demo where param1=? and param2=?\u0026#34;).setParameter(0, param1).setParameter(1, param2).uniqueResult(); // 推荐 long count = DataAccessUtils.longResult(getHibernateTemplate().find(\u0026#34;select count(*) from Demo where param1=? and param2=?\u0026#34;, param1, param2));  分页查询  // 不推荐 Query query = getSession().createQuery(hql).setParameter(0, param1).setParameter(1, param2); query.setFirstResult(offset); query.setMaxResults(limit); List\u0026lt;Demo\u0026gt; demos = query.list(); // 推荐 List\u0026lt;Demo\u0026gt; demos = getHibernateTemplate().executeFind(new HibernateCallback\u0026lt;List\u0026lt;Demo\u0026gt;\u0026gt;() { @Override public List\u0026lt;Demo\u0026gt; doInHibernate(Session session) throws HibernateException, SQLException { Query query = session.createSQLQuery(hql).setLong(0, param1).setParameter(1, param2); query.setFirstResult(offset); query.setMaxResults(limit); return query.list(); } });  使用Criteria  // 不推荐 Criteria criteria = getSession().createCriteria(Demo.class); criteria.add(Restrictions.eq(\u0026#34;param1\u0026#34;, param1)); List\u0026lt;Demo\u0026gt; demos = criteria.list(); // 推荐 DetachedCriteria criteria = DetachedCriteria.forClass(Demo.class) .add(Restrictions.eq(\u0026#34;param1\u0026#34;, param1)); List\u0026lt;Demo\u0026gt; demos = getHibernateTemplate().findByCriteria(criteria);  使用Criteria加分页功能  Criteria criteria = getSession().createCriteria(Demo.class); criteria.add(Restrictions.eq(\u0026#34;param1\u0026#34;, param1)); List\u0026lt;Demo\u0026gt; demos = criteria.list(); // 推荐 DetachedCriteria criteria = DetachedCriteria.forClass(Demo.class) .add(Restrictions.eq(\u0026#34;param1\u0026#34;, param1)); List\u0026lt;Demo\u0026gt; demos = getHibernateTemplate().findByCriteria(criteria, offset, pageSize); ","permalink":"https://jeremyxu2010.github.io/2017/02/hibernate%E6%9F%A5%E8%AF%A2%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BC%98%E5%8C%96%E5%86%99%E6%B3%95/","tags":["java","hibernate","spring"],"title":"hibernate查询的一些优化写法"},{"categories":["java开发"],"contents":"今天在工作中，发现用Hibernate实现的DAO类中存在两种获取hibernate会话的方式，如下：\n@Repository(\u0026#34;demoDao\u0026#34;) public class DemoDaoImpl extends HibernateDaoSupport implements DemoDao{ //通过getSession方法获取  @Override public Demo method1(final Integer param) { Session session = this.getSession(); ... } //通过getHibernateTemplate().execute方法获取  @Override public Demo method2(final Integer param) { return getHibernateTemplate().execute(new HibernateCallback\u0026lt;Demo\u0026gt;() { @Override public AppPzTestStats doInHibernate(Session session) throws HibernateException, SQLException { ... } }); } } 研究了下，这两种方式还有有区别的，先看看javadoc。\n/** * Obtain a Hibernate Session, either from the current transaction or * a new one. The latter is only allowed if the * {@link org.springframework.orm.hibernate3.HibernateTemplate#setAllowCreate \u0026#34;allowCreate\u0026#34;} * setting of this bean\u0026#39;s {@link #setHibernateTemplate HibernateTemplate} is \u0026#34;true\u0026#34;. * \u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Note that this is not meant to be invoked from HibernateTemplate code * but rather just in plain Hibernate code.\u0026lt;/b\u0026gt; Either rely on a thread-bound * Session or use it in combination with {@link #releaseSession}. * \u0026lt;p\u0026gt;In general, it is recommended to use HibernateTemplate, either with * the provided convenience operations or with a custom HibernateCallback * that provides you with a Session to work on. HibernateTemplate will care * for all resource management and for proper exception conversion. * @return the Hibernate Session * @throws DataAccessResourceFailureException if the Session couldn\u0026#39;t be created * @throws IllegalStateException if no thread-bound Session found and allowCreate=false * @see org.springframework.orm.hibernate3.SessionFactoryUtils#getSession(SessionFactory, boolean) * @deprecated as of Spring 3.2.7, in favor of {@link HibernateTemplate} usage */ @Deprecated protected final Session getSession() throws DataAccessResourceFailureException, IllegalStateException { return getSession(this.hibernateTemplate.isAllowCreate()); } /** * Execute the action specified by the given action object within a * {@link org.hibernate.Session}. * \u0026lt;p\u0026gt;Application exceptions thrown by the action object get propagated * to the caller (can only be unchecked). Hibernate exceptions are * transformed into appropriate DAO ones. Allows for returning a result * object, that is a domain object or a collection of domain objects. * \u0026lt;p\u0026gt;Note: Callback code is not supposed to handle transactions itself! * Use an appropriate transaction manager like * {@link HibernateTransactionManager}. Generally, callback code must not * touch any {@code Session} lifecycle methods, like close, * disconnect, or reconnect, to let the template do its work. * @param action callback object that specifies the Hibernate action * @return a result object returned by the action, or {@code null} * @throws org.springframework.dao.DataAccessException in case of Hibernate errors * @see HibernateTransactionManager * @see org.hibernate.Session */ \u0026lt;T\u0026gt; T execute(HibernateCallback\u0026lt;T\u0026gt; action) throws DataAccessException; 从文档上看，getSession的方式得到的Session需要由程序员自行调用releaseSession方法进行session的释放，而且getSession方法已经不推荐使用了。官方更推荐使用hibernateTemplate配合HibernateCallback的方案。这种方案由hibernate负责处理资源的管理及异常的转换。\n另外看到网上一哥们的分析，于是更坚信了要使用hibernateTemplate配合HibernateCallback的方案。一搜项目，竟然有700多处都是老写法，看来得花一番功夫将所有这些代码改成推荐的方案了。\n","permalink":"https://jeremyxu2010.github.io/2017/02/%E4%B8%A4%E7%A7%8D%E8%8E%B7%E5%8F%96hibernate%E4%BC%9A%E8%AF%9D%E7%9A%84%E5%8C%BA%E5%88%AB/","tags":["java","hibernate"],"title":"两种获取hibernate会话的区别"},{"categories":["web开发"],"contents":"今天的主要工作都在用react.js写一些前端界面，中间遇到了一些问题，这里解决这些问题的过程记录一下。\n使用echarts图表组件 由于项目比较老旧，并没有使用webpack、gulp之类的前端编译工具进行编译，而是直接将依赖的javascript库引入，如下所示：\n\u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;reactHolder\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;${ctx}/script/lodash/lodash.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;${ctx}/script/react/react.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;${ctx}/script/react/react-dom.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;${ctx}/script/antd/antd.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;${ctx}/script/moment/moment.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;${ctx}/script/axios/axios.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!--将应用上下文路径存入js变量 --\u0026gt; \u0026lt;c:set var=\u0026#34;ctx\u0026#34; value=\u0026#34;${pageContext.request.contextPath}\u0026#34;/\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var __CTX_PATH__=\u0026#39;${ctx}\u0026#39;; \u0026lt;/script\u0026gt; \u0026lt;!--将ES6语法的js翻译为ES5 --\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;${ctx}/script/babel-core/browser.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/babel\u0026#34; src=\u0026#34;${ctx}/script/app1.jsx\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; 由于项目需求，需要使用图表组件，自然想到使用echarts, 找了下echarts的reactjs包装npm库，找到echarts-for-react。可看了下echarts-for-react的源码，它虽然是用webpack编译的，但并没有使用umd格式打包这个库，这样如果不用webpack或browserify等工具编译，这个库无法直接引入jsp页面使用。想了下，可以手动将echarts-for-react编译打包为umd格式，于是将其源码clone下来，简单改了改webpack.config.js, 以打出umd格式的库文件，如下：\n ... entry: './src/echarts-for-react.js', output: { path: './dist', filename: 'echarts-for-react.min.js', library: 'ReactEcharts', libraryTarget: 'umd', umdNamedDefine: true }, externals: [ { \u0026quot;react\u0026quot;: { root: 'React', commonjs2: 'react', commonjs: 'react', amd: 'react' } }, 'echarts' ], ... 最后引入jsp文件。\n... \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;${ctx}/script/echarts/echarts.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;${ctx}/script/echarts-for-react/echarts-for-react.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; ... 在jsx文件里使用方法如下：\nconst ReactEcharts = window.ReactEcharts; class Demo extends React.Component{ getChartOptions(){ // echarts的options格式可参考http://echarts.baidu.com/option.html return { ... } } render(){ return \u0026lt;ReactEcharts option={this.getChartOptions()} notMerge={true} /\u0026gt; } } 将页面中的Table导出为Excel 页面中已经使用了antd的Table组件，但希望将这些Table导出为Excel文件，同时又懒得添加后台接口，搜索了下，找到一个excellentexport库，它的文档里写到可以这样使用它：\n\u0026lt;table id=\u0026quot;datatable\u0026quot;\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;100\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;200\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;300\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;400\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;500\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;600\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;a download=\u0026quot;somedata.xls\u0026quot; href=\u0026quot;#\u0026quot; onclick=\u0026quot;return ExcellentExport.excel(this, 'datatable', 'Sheet Name Here');\u0026quot;\u0026gt;Export to Excel\u0026lt;/a\u0026gt; \u0026lt;a download=\u0026quot;somedata.csv\u0026quot; href=\u0026quot;#\u0026quot; onclick=\u0026quot;return ExcellentExport.csv(this, 'datatable');\u0026quot;\u0026gt;Export to CSV\u0026lt;/a\u0026gt; 但在ReactJS里要用它还是有点技巧的，最后示例代码如下：\nconst antd = window.antd; const {Table} = antd; const ExcellentExport = window.ExcellentExport; const $ = window.$; class Demo extends React.Component{ exportExcel(anchor, refName, sheetName){ let tableEl = $('.ant-table-body \u0026gt; table', ReactDOM.findDOMNode(this.refs[refName])); if(tableEl.length \u0026gt; 0){ ExcellentExport.excel(anchor, tableEl[0], sheetName); } } render(){ return ( \u0026lt;div\u0026gt; \u0026lt;a download=\u0026quot;Demo表格.xls\u0026quot; href=\u0026quot;#\u0026quot; onClick={(e) =\u0026gt; {this.exportExcel(e.target, 'demoTable', 'Demo页');}}\u0026gt;导出Excel文件\u0026lt;/a\u0026gt; \u0026lt;Table ref=\u0026quot;demoTable\u0026quot; .../\u0026gt; \u0026lt;/div\u0026gt; ) } } ","permalink":"https://jeremyxu2010.github.io/2017/02/%E8%AE%B0%E5%BD%95%E8%A7%A3%E5%86%B3%E5%87%A0%E4%B8%AA%E5%89%8D%E7%AB%AF%E5%B0%8F%E9%97%AE%E9%A2%98%E7%9A%84%E8%BF%87%E7%A8%8B/","tags":["react"],"title":"记录解决几个前端小问题的过程"},{"categories":["golang开发"],"contents":"看到有人用go语言开发api服务，分发打包的程序只需要分发一个可执行文件就可以了，真的好方便，于是我也来试一试。\n依赖管理 go语言的第三方包依赖管理一直比较混乱，官方并没有给出推荐的依赖管理工具。有人推荐使用godep或govendor，docker开源项目使用的又好像是trash。参考这里,经过一番对比，我最终选择了glide，原因很简单，它跟npm之类很像，对于我来说很容易上手。\n#我习惯将一些工具命令装到一个独立的地方 set GOPATH=W:\\go_tools go get -v github.com/Masterminds/glide #记得要将W:\\go_tools\\bin路径加入到系统的PATH变量里去 应用框架 搜索了一下，最终选定了比较热门的beego，这里使用它的命令行工具bee帮助创建工程。\n 安装bee  set GOPATH=W:\\go_tools go get -v github.com/beego/bee  创建工程  #我的GOPATH是W:\\workspace\\go_projs cd W:\\workspace\\go_projs\\src bee api apitest  安装第三方依赖  cd W:\\workspace\\go_projs\\src\\apitest glide init glide install  运行  cd W:\\workspace\\go_projs\\src\\apitest bee run 然后就可以使用浏览器访问http://127.0.0.1:8080/v1/user/。\n 打包  cd W:\\workspace\\go_projs\\src\\apitest go build -o apitest.exe main.go 这样打出的apitest.exe就可以分发了，超方便啊。\n总结 相对于java那一套，使用golang开发api服务分发程序真的很方便，就一个可执行文件就OK了，以后做点小项目可以用golang来整了。\n参考 https://github.com/golang/go/wiki/PackageManagementTools https://github.com/Masterminds/glide https://beego.me/docs/install/bee.md\n","permalink":"https://jeremyxu2010.github.io/2017/02/%E4%BD%BF%E7%94%A8go%E5%BC%80%E5%8F%91api%E6%9C%8D%E5%8A%A1/","tags":["golang","api"],"title":"使用go开发api服务"},{"categories":["devops"],"contents":"上周末使用docker做了一个简化应用分发的小例子，但今天在最新版本的docker上一运行就出错，研究了好半天，终于将这些坑都填过去了，这里记录一下。\n挂载目录用户权限问题 我是将dockerfiles相关文件放在windows系统上的，然后通过virtualbox虚拟机的共享文件夹功能将目录共享给Linux的，这样在Linux下就会看到这些文件的用户组是vboxsf, 这些文件的权限为770。\n[jeremy@centos7-local dockerfiles]$ ls -l 总用量 1 -rwxrwx--- 1 root vboxsf 688 2月 3 11:53 docker-compose.yml drwxrwx--- 1 root vboxsf 0 2月 2 09:51 initdb drwxrwx--- 1 root vboxsf 0 2月 3 11:28 tools drwxrwx--- 1 root vboxsf 0 2月 2 18:05 wars 这时使用-v将目录挂载到docker容器\ndocker run --name=test -v `pwd`/wars:/var/lib/jetty/webapps -p 8080:8080 -d jetty:9 这时目录挂载过去后权限就很不对了，如下\nroot@15ba64dfbe33:/var/lib/jetty# ls -l total 0 drwxr-xr-x 3 jetty jetty 17 Feb 3 01:57 lib drwxr-xr-x 2 jetty jetty 6 Jan 18 00:38 resources drwxr-xr-x 2 jetty jetty 193 Feb 3 01:57 start.d drwxrwx--- 1 root 984 0 Feb 2 10:05 webapps 可以看到webapps目录的用户组为984, 文件权限是770。而jetty是以jetty用户运行的，自然就无法读取webapps目录下的内容。\n查了下，解决这个问题有四种办法：\n 在宿主机上创建与容器中需要的用户及用户组，创建的用户及用户组的ID必须与容器中的一致。在运行docker run -v ...命令前，将要挂载的目录权限设置正确。 将要挂载的目录设置为容器中存在的用户及用户组，比如设置为root用户，在宿主机与容器中都存在root用户与root用户组，而且root用户与root用户组的ID是一致的。 修改容器中用户及用户组的ID，使宿主机上的用户及用户组ID在容器内可被识别，有网友写了一个脚本来完成这件事。 运行docker run -v ...命令时，使用--user及--group更改容器运行进程的用户及用户组。同样要求指定的用户在容器里是存在的，一般来说也就只能使用root了。  这几种方法都有缺点，还是很麻烦。也在关注是否有其它更好的办法。\ndepends_on失效了 在docker-compose.yml里使用depends_on指定了web服务依赖于db服务，但web服务还没等db服务就绪就启动了，最终web服务启动失败。\n查了下文件，发现官方文档有这么一句话：\n Note: depends_on will not wait for db and redis to be “ready” before starting web - only until they have been started. If you need to wait for a service to be ready, see Controlling startup order for more on this problem and strategies for solving it.\n 最后参考这里, 使用wait-for-it方案解决了问题。\nversion: '2' services: ssm-mysql: image: 'mysql' volumes: - ./initdb:/docker-entrypoint-initdb.d environment: - MYSQL_DATABASE=ssm-db - MYSQL_ROOT_PASSWORD=123456 ssm-web: image: 'jetty:9' depends_on: - ssm-mysql links: - ssm-mysql volumes: - ./wars:/var/lib/jetty/webapps - ./tools:/tools entrypoint: [\u0026quot;/tools/wait-for-it.sh\u0026quot;, \u0026quot;ssm-mysql:3306\u0026quot;, \u0026quot;-s\u0026quot;, \u0026quot;-t\u0026quot;, \u0026quot;60\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;/docker-entrypoint.sh\u0026quot;] ports: - \u0026quot;8080:8080\u0026quot; environment: - MYSQL_PORT_3306_TCP_ADDR=ssm-mysql - MYSQL_PORT_3306_TCP_PORT=3306 - MYSQL_ENV_MYSQL_DATABASE=ssm-db - MYSQL_ENV_MYSQL_ROOT_PASSWORD=123456 使用docker的-p选项不监听端口 直接使用docker的-p选项，发现docker宿主机并不监听指定的端口，在docker宿主机上可以访问该端口，但外部就无法访问该端口了。\n[jeremy@centos7-local dockerfiles]$ docker run --name=test -p 8080:8080 -d jetty:9 dbb672d6c3bec87bd9048911798f7d3941e6681ebdd60e0bb54332b8a083ae3d #宿主机并不监听8080端口 [jeremy@centos7-local dockerfiles]$ lsof -i :8080 # 但在docker宿主机上wget可访问8080，外部就无法访问8080了 [jeremy@centos7-local dockerfiles]$ wget http://127.0.0.1:8080 --2017-02-03 21:01:43-- http://127.0.0.1:8080/ 正在连接 127.0.0.1:8080... 已连接。 # 发现原来是docker-proxy这个东东在工作 [jeremy@centos7-local dockerfiles]$ ps -ef|grep docker ... root 3190 3050 0 20:58 ? 00:00:00 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 8080 -container-ip 172.17.0.2 -container-port 8080 ... 查阅官方文档后，发现dockerd存在--userland-proxy这个选项。\n \u0026ndash;userland-proxy\ttrue\tUse userland proxy for loopback traffic\n 于是给dockerd加上--userland-proxy=false选项，然后问题解决了。这个选项应该是为安全性考虑的吧，默认只允许docker宿主机访问-p出来的端口，外部要想访问则需要配置相应的iptables规则。默认如果是这样也太不易用了。\n参考 https://docs.docker.com/engine/installation/linux/centos/ https://docs.docker.com/engine/installation/linux/linux-postinstall/ https://docs.docker.com/engine/admin/systemd/ https://github.com/schmidigital/permission-fix https://docs.docker.com/compose/compose-file/ https://docs.docker.com/compose/startup-order/ https://github.com/vishnubob/wait-for-it https://docs.docker.com/engine/reference/commandline/dockerd/\n","permalink":"https://jeremyxu2010.github.io/2017/02/docker%E6%8E%92%E9%9B%B7%E8%AE%B0/","tags":["docker"],"title":"docker排雷记"},{"categories":["容器编排"],"contents":"boot2docker中未安装docker-compose，同时无法自动挂载我在宿主机上共享的目录。研究了下，终于找到办法了，记录如下：\nsudo mkdir -p /var/lib/boot2docker/bin sudo curl -sL https://github.com/docker/compose/releases/download/1.10.0/docker-compose-`uname -s`-`uname -m` -o /var/lib/boot2docker/bin/docker-compose sudo chmod +x /var/lib/boot2docker/bin/docker-compose sudo ln -sf /var/lib/boot2docker/bin/docker-compose /usr/local/bin/docker-compose echo 'Writing to bootlocal.sh to make docker-compose available on every boot...' cat \u0026lt;\u0026lt;SCRIPT | sudo tee -a /var/lib/boot2docker/bootlocal.sh \u0026gt; /dev/null # docker-compose sudo ln -sf /var/lib/boot2docker/bin/docker-compose /usr/local/bin/docker-compose # automount SSDHOME mountOptions='defaults,iocharset=utf8' if grep -q '^docker:' /etc/passwd; then mountOptions=\u0026quot;${mountOptions},uid=$(id -u docker),gid=$(id -g docker)\u0026quot; fi try_mount_share() { dir=\u0026quot;$1\u0026quot; name=\u0026quot;${2:-$dir}\u0026quot; mkdir -p \u0026quot;$dir\u0026quot; 2\u0026gt;/dev/null if ! mount -t vboxsf -o \u0026quot;$mountOptions\u0026quot; \u0026quot;$name\u0026quot; \u0026quot;$dir\u0026quot; 2\u0026gt;/dev/null; then rmdir \u0026quot;$dir\u0026quot; 2\u0026gt;/dev/null || true while [ \u0026quot;$(dirname \u0026quot;$dir\u0026quot;)\u0026quot; != \u0026quot;$dir\u0026quot; ]; do dir=\u0026quot;$(dirname \u0026quot;$dir\u0026quot;)\u0026quot; rmdir \u0026quot;$dir\u0026quot; 2\u0026gt;/dev/null || break done return 1 fi return 0 } try_mount_share /SSDHOME 'SSDHOME' SCRIPT sudo chmod +x /var/lib/boot2docker/bootlocal.sh ","permalink":"https://jeremyxu2010.github.io/2017/01/boot2docker%E4%B8%AD%E5%AE%89%E8%A3%85docker-compose/","tags":["docker"],"title":"boot2docker中安装docker-compose"},{"categories":["devops"],"contents":"最近做了个小的Java Web脚手架工程。工程项目虽小，但算是一个很典型的Java Web项目，依赖于数据库，Java写的后端代码，JavaScript写的前端代码。本来写了一个说明，告诉用户如何将这个工程跑起来，很自然想到有好几步：\n 安装前后端编译工具 安装数据库，并初始化数据库结构 根据数据库的具体信息，修改项目中的配置文件 编译前端代码 编译后端代码，最终形成war包 将war包部署至应用服务器  想了下，真的好麻烦。突然想到可以使用docker简化应用的分发，于是有了以下尝试，这里记录一下。\n改造工程 原来加载mysql连接信息配置文件的方式改造了一下，以适应在docker引擎中引用mysql。\ndb.xml\n... \u0026lt;bean id=\u0026#34;propertyConfigurer\u0026#34; class=\u0026#34;org.springframework.beans.factory.config.PropertyPlaceholderConfigurer\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;locations\u0026#34;\u0026gt; \u0026lt;list\u0026gt; \u0026lt;value\u0026gt;classpath:jdbc.properties\u0026lt;/value\u0026gt; \u0026lt;value\u0026gt;classpath:env.properties\u0026lt;/value\u0026gt; \u0026lt;value\u0026gt;file:///external/env_overwrite.properties\u0026lt;/value\u0026gt; \u0026lt;/list\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;ignoreResourceNotFound\u0026#34; value=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;searchSystemEnvironment\u0026#34; value=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;systemPropertiesModeName\u0026#34; value=\u0026#34;SYSTEM_PROPERTIES_MODE_OVERRIDE\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; ... jdbc.properties\njdbc_url=jdbc:mysql://${MYSQL_PORT_3306_TCP_ADDR}:${MYSQL_PORT_3306_TCP_PORT}/${MYSQL_ENV_MYSQL_DATABASE}?useUnicode=true jdbc_user=root jdbc_password=${MYSQL_ENV_MYSQL_ROOT_PASSWORD} jdbc_driverClass=com.mysql.jdbc.Driver env.properties\nMYSQL_PORT_3306_TCP_ADDR=127.0.0.1 MYSQL_PORT_3306_TCP_PORT=3306 MYSQL_ENV_MYSQL_DATABASE=ssm-db MYSQL_ENV_MYSQL_ROOT_PASSWORD=123456 这里设置systemPropertiesModeName属性为SYSTEM_PROPERTIES_MODE_OVERRIDE，这样在解析一个占位符的时候，会先用系统属性来尝试，如果系统属性里没有才会用env.properties文件里定义的。\ndocker相关配置 项目下新建了dockerfiles目录，该目录下有一个docker-compose.yml文件，另外一个initdb目录下放数据库初始化脚本， 一个wars目录下放项目最后打的war包。\nproj - dockerfiles - initdb - initdb.sql - wars - proj.war - docker-compose.yml - src - main - frontend - java - resources - webapp - pom.xml docker-compose.yml\nversion: '2' services: ssm-mysql: image: 'mysql' volumes: - ./initdb:/docker-entrypoint-initdb.d environment: - MYSQL_DATABASE=ssm-db - MYSQL_ROOT_PASSWORD=123456 ssm-web: image: 'jetty:9-alpine' depends_on: - ssm-mysql links: - ssm-mysql volumes: - ./wars:/var/lib/jetty/webapps ports: - \u0026quot;8080:8080\u0026quot; environment: - MYSQL_PORT_3306_TCP_ADDR=ssm-mysql - MYSQL_PORT_3306_TCP_PORT=3306 - MYSQL_ENV_MYSQL_DATABASE=ssm-db - MYSQL_ENV_MYSQL_ROOT_PASSWORD=123456 docker-compose.yml文件里定义了两个docker service, ssm-mysql是数据库服务，ssm-web是Web容器服务。\n这里遇到了一坑，本来一个容器link另一个容器时，会从另一个容器得到一些环境变量，所以ssm-web服务的环境变量声明原本是不需要的，但去掉之后发现ssm-web服务跑不起来，好像是根本没有读到原本应该得到的环境变量。查了下原因，最后原因如下：\n links with environment variables: As documented in the environment variables reference, environment variables created by links have been deprecated for some time. In the new Docker network system, they have been removed. You should either connect directly to the appropriate hostname or set the relevant environment variable yourself, using the link hostname:\n web: links: - db environment: - DB_PORT=tcp://db:5432 看来以后还是不能依赖于links带来的变量。\n改造pom.xml文件 最后稍微改造了下pom.xml文件\npom.xml\n... \u0026lt;!--打war包前安装npm依赖及编译前端代码 --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.codehaus.mojo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;exec-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.3.2\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;install_npm_dependences\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;prepare-package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;exec\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;executable\u0026gt;yarn\u0026lt;/executable\u0026gt; \u0026lt;workingDirectory\u0026gt;src/main/frontend\u0026lt;/workingDirectory\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;build_frontend\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;prepare-package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;exec\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;executable\u0026gt;npm\u0026lt;/executable\u0026gt; \u0026lt;workingDirectory\u0026gt;src/main/frontend\u0026lt;/workingDirectory\u0026gt; \u0026lt;arguments\u0026gt; \u0026lt;argument\u0026gt;run\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt;build\u0026lt;/argument\u0026gt; \u0026lt;/arguments\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;!--将编译后的前端代码也打入war包 --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-war-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.0\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;webResources\u0026gt; \u0026lt;resource\u0026gt; \u0026lt;!--this is relative to the pom.xml directory --\u0026gt; \u0026lt;directory\u0026gt;src/main/frontend_react/build\u0026lt;/directory\u0026gt; \u0026lt;!--the default value is ** --\u0026gt; \u0026lt;includes\u0026gt; \u0026lt;include\u0026gt;**/*\u0026lt;/include\u0026gt; \u0026lt;/includes\u0026gt; \u0026lt;!--there\u0026#39;s no default value for this --\u0026gt; \u0026lt;excludes\u0026gt; \u0026lt;exclude\u0026gt;asset-manifest.json\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;**/*.css.map\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;**/*.js.map\u0026lt;/exclude\u0026gt; \u0026lt;/excludes\u0026gt; \u0026lt;/resource\u0026gt; \u0026lt;/webResources\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;!--将最后打出的war包拷贝至dockerfiles/wars目录 --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-antrun-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.8\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;target\u0026gt; \u0026lt;copy file=\u0026#34;${basedir}/target/${project.artifactId}.war\u0026#34; tofile=\u0026#34;${basedir}/dockerfiles/wars/${project.artifactId}.war\u0026#34; /\u0026gt; \u0026lt;/target\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;run\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; ... 总结 像上述这样改造后，分发项目就变得很简单了。\n 在工程根目录下执行mvn package完成war的构建 在dockerfiles目录下执行docker-compose up 使用浏览器访问http://${docker_host_ip}:8080  进一步想，其实很多依赖组件较多的项目都可以考虑这样分发。记得以前做的一个项目依赖了mysql, mongodb, redis, mq, zookeeper，当时每个新加入团队的成员至少要花大半天来搭建开发环境，如果工程这样组织的话，一个新人就能很快将项目运行起来。\n本工程源码地址：http://git.oschina.net/jeremy-xu/ssm-scaffold\n参考 http://javablog.blog.163.com/blog/static/20971116420127109200710/ https://docs.docker.com/compose/compose-file/ https://yeasy.gitbooks.io/docker_practice/content/compose/install.html https://hub.docker.com/_/mysql/ https://hub.docker.com/_/jetty/\n","permalink":"https://jeremyxu2010.github.io/2017/01/%E5%88%86%E5%8F%91javaweb%E9%A1%B9%E7%9B%AE%E4%B9%8Bdocker%E6%96%B9%E6%A1%88/","tags":["java","web","docker"],"title":"分发JavaWeb项目之docker方案"},{"categories":["工具"],"contents":"今天尝试将以前创建的一个前端项目改为webpack编译，该项目使用了VueJS v2.0，原来是编写gulp脚本完成构建的。很自然就直接用vue-cli来搞定这个事了。\n使用vue-cli创建项目 因为以前用过webpack，而vue-cli创建的项目底层其实还是使用webpack构建的，所以使用起来还是很简单的。\n# 使用yarn，这个命令是跟npm兼容的，但速度快很多，而且可以保证依赖包版本的一致性，强烈推荐 yarn install --global vue-cli vue-cli webpack vue-demo cd vue-demo # 安装项目依赖 yarn install # 启动开发服务器 yarn run dev 发现问题 但我在开发过程中发现问题了，在IDE中修改了vue文件，webpack开发服务器并不会重新编译对应的模块，更不会reload浏览器页面，webpack的watch选项失效了。\n在网上搜索了下原因，发现webpack的一个issue项。尝试按该问题中的说明在vue-demo/build/dev-server.js的23行加入watchOptions.polling选项，发现问题真的解决了。\nvar devMiddleware = require(\u0026#39;webpack-dev-middleware\u0026#39;)(compiler, { publicPath: webpackConfig.output.publicPath, quiet: true, // Reportedly, this avoids CPU overload on some systems.  // https://github.com/facebookincubator/create-react-app/issues/293  watchOptions: { poll: true } }) 深究问题 watchOptions.polling选项是控制webpack如何检测文件变动的，webpack默认是采用监听文件系统变动事件来感知文件变动的，如果开启这个选项，则会定时询问文件系统是否有文件变动。现在开启这个选项，则功能正常，不开启功能不正常？而vue-cli的广大使用者并没有报告存在该问题。\n个人感觉不应该是webpack的这个功能有问题，还是应该是环境问题。继续翻查资料，终于在webpack的官方文档中找到说明https://webpack.github.io/docs/troubleshooting.html#watching。这里说得很清楚，watch功能不起作用一般来说就是这几个原因。\n而我现在的开发操作系统是Windows，那么就只剩下2个可能原因了。\n windows路径问题 IDE的safe write特性干扰  试了一下终于发现是IDE的safe write特性这个问题造成的。IDE的这个特性是为了安全地写文件，它会先将文件写到一个临时文件里，然后最后一个原子move操作将文件move到目标位置。但这样webpack检测文件变动的原来逻辑就不工作了。代码见webpack/lib/node/NodeWatchFileSystem.js。\n/* MIT License http://www.opensource.org/licenses/mit-license.php Author Tobias Koppers @sokra */ var Watchpack = require(\u0026#34;watchpack\u0026#34;); function NodeWatchFileSystem(inputFileSystem) { this.inputFileSystem = inputFileSystem; this.watcherOptions = { aggregateTimeout: 0 }; this.watcher = new Watchpack(this.watcherOptions); } module.exports = NodeWatchFileSystem; NodeWatchFileSystem.prototype.watch = function watch(files, dirs, missing, startTime, options, callback, callbackUndelayed) { if(!Array.isArray(files)) throw new Error(\u0026#34;Invalid arguments: \u0026#39;files\u0026#39;\u0026#34;); if(!Array.isArray(dirs)) throw new Error(\u0026#34;Invalid arguments: \u0026#39;dirs\u0026#39;\u0026#34;); if(!Array.isArray(missing)) throw new Error(\u0026#34;Invalid arguments: \u0026#39;missing\u0026#39;\u0026#34;); if(typeof callback !== \u0026#34;function\u0026#34;) throw new Error(\u0026#34;Invalid arguments: \u0026#39;callback\u0026#39;\u0026#34;); if(typeof startTime !== \u0026#34;number\u0026#34; \u0026amp;\u0026amp; startTime) throw new Error(\u0026#34;Invalid arguments: \u0026#39;startTime\u0026#39;\u0026#34;); if(typeof options !== \u0026#34;object\u0026#34;) throw new Error(\u0026#34;Invalid arguments: \u0026#39;options\u0026#39;\u0026#34;); if(typeof callbackUndelayed !== \u0026#34;function\u0026#34; \u0026amp;\u0026amp; callbackUndelayed) throw new Error(\u0026#34;Invalid arguments: \u0026#39;callbackUndelayed\u0026#39;\u0026#34;); var oldWatcher = this.watcher; this.watcher = new Watchpack(options); if(callbackUndelayed) this.watcher.once(\u0026#34;change\u0026#34;, callbackUndelayed); this.watcher.once(\u0026#34;aggregated\u0026#34;, function(changes) { if(this.inputFileSystem \u0026amp;\u0026amp; this.inputFileSystem.purge) { this.inputFileSystem.purge(changes); } var times = this.watcher.getTimes(); callback(null, changes.filter(function(file) { return files.indexOf(file) \u0026gt;= 0; }).sort(), changes.filter(function(file) { return dirs.indexOf(file) \u0026gt;= 0; }).sort(), changes.filter(function(file) { return missing.indexOf(file) \u0026gt;= 0; }).sort(), times, times); }.bind(this)); this.watcher.watch(files.concat(missing), dirs, startTime); if(oldWatcher) { oldWatcher.close(); } return { close: function() { this.watcher.close(); }.bind(this), pause: function() { this.watcher.pause(); }.bind(this) }; }; 最终关闭了IDE的safe write特性。\n总结 研究这个坑的原因花了一个多小时，在此记录一下。\n","permalink":"https://jeremyxu2010.github.io/2017/01/webpack%E7%9A%84watch%E9%80%89%E9%A1%B9%E4%B8%8D%E5%B7%A5%E4%BD%9C%E5%8E%9F%E5%9B%A0%E5%88%86%E6%9E%90/","tags":["webpack","nodejs"],"title":"webpack的watch选项不工作原因分析"},{"categories":["web开发"],"contents":"以往启动一个Web项目，总要从一个现存的项目将gulpfile.js, package.json拷贝至新项目，然后根据需要修改这两个文件，确实挺麻烦的。今天在github上看到一个评分还比较高的项目create-react-app 。细细看了下它的文档，发现facebook通过这个项目将react的前端项目标准化了，约定大于配置，通过这个工具创建项目方便多了，这里记录一下以备忘。\n创建项目 执行以下命令：\n#安装create-react-app命令 npm install -g create-react-app #创建一个名为demo1的前端项目 create-react-app demo1 cd demo1 #这里直接启动了开发服务器 npm start 它会自动打开浏览器，并访问http://127.0.0.1:3000。\n如果修改工程src目录下的文件，则会自动编译，并刷新浏览器。如果出现编译错误，终端及浏览器上均会有提示。\n开发设置 在我实际工作中，一般是用java做后台的，因此要配置前端页面的API都代理至后端的Java Web服务器。\n修改package.json文件，加入代理设置\n\u0026quot;proxy\u0026quot;: \u0026quot;http://127.0.0.1:8080\u0026quot; 配置SpringMVC处理后台的api请求\n\u0026lt;context-param\u0026gt; \u0026lt;param-name\u0026gt;contextConfigLocation\u0026lt;/param-name\u0026gt; \u0026lt;param-value\u0026gt;classpath:applicationContext.xml\u0026lt;/param-value\u0026gt; \u0026lt;/context-param\u0026gt; \u0026lt;listener\u0026gt; \u0026lt;listener-class\u0026gt;org.springframework.web.context.ContextLoaderListener\u0026lt;/listener-class\u0026gt; \u0026lt;/listener\u0026gt; \u0026lt;servlet\u0026gt; \u0026lt;servlet-name\u0026gt;springmvc\u0026lt;/servlet-name\u0026gt; \u0026lt;servlet-class\u0026gt;org.springframework.web.servlet.DispatcherServlet\u0026lt;/servlet-class\u0026gt; \u0026lt;load-on-startup\u0026gt;1\u0026lt;/load-on-startup\u0026gt; \u0026lt;init-param\u0026gt; \u0026lt;param-name\u0026gt;contextConfigLocation\u0026lt;/param-name\u0026gt; \u0026lt;param-value\u0026gt;classpath:spring/mvc.xml\u0026lt;/param-value\u0026gt; \u0026lt;/init-param\u0026gt; \u0026lt;/servlet\u0026gt; \u0026lt;servlet-mapping\u0026gt; \u0026lt;servlet-name\u0026gt;springmvc\u0026lt;/servlet-name\u0026gt; \u0026lt;url-pattern\u0026gt;/api/*\u0026lt;/url-pattern\u0026gt; \u0026lt;/servlet-mapping\u0026gt; 实际开发时，先启动Java Web服务器，再执行npm start启动Web开发服务器，然后就可以开发了。\nJava Web应用启用browserHistory 如果前端使用了browserHistory, 则后台还需处理TryFiles的逻辑，TryFilesFilter在web.xml里的配置如下：\n\u0026lt;filter\u0026gt; \u0026lt;filter-name\u0026gt;TryFiles\u0026lt;/filter-name\u0026gt; \u0026lt;filter-class\u0026gt;personal.jeremyxu.filter.TryFilesFilter\u0026lt;/filter-class\u0026gt; \u0026lt;init-param\u0026gt; \u0026lt;param-name\u0026gt;files\u0026lt;/param-name\u0026gt; \u0026lt;param-value\u0026gt;$path /index.html\u0026lt;/param-value\u0026gt; \u0026lt;/init-param\u0026gt; \u0026lt;init-param\u0026gt; \u0026lt;param-name\u0026gt;excludes\u0026lt;/param-name\u0026gt; \u0026lt;param-value\u0026gt;/api\u0026lt;/param-value\u0026gt; \u0026lt;/init-param\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;filter-mapping\u0026gt; \u0026lt;filter-name\u0026gt;TryFiles\u0026lt;/filter-name\u0026gt; \u0026lt;url-pattern\u0026gt;/*\u0026lt;/url-pattern\u0026gt; \u0026lt;/filter-mapping\u0026gt; TryFilesFilter的代码见这里\n重新组织工程目录结构 在我实际工作中，一般是用java做后台的，因而希望直接将前端代码放到maven的webapp工程里，所以我一般是下面这样组织目录结构的。\ndemo1/ src/ main/ frontend/ src/ js/ index.js actions/ components/ constants/ i18n/ reducers/ utils/ css/ public/ index.html static/ package.json .gitignore java webapp WEB-INF pom.xml 创建一个frontend的目录与java目录平级，这里放置前端源代码。\n然后修改maven项目的pom.xml文件，确保打war包能自动编译前端代码，并将编译后的文件打入war包里。\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.codehaus.mojo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;exec-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.3.2\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;phase\u0026gt;prepare-package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;exec\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;executable\u0026gt;npm\u0026lt;/executable\u0026gt; \u0026lt;workingDirectory\u0026gt;src/main/frontend\u0026lt;/workingDirectory\u0026gt; \u0026lt;arguments\u0026gt; \u0026lt;argument\u0026gt;run\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt;build\u0026lt;/argument\u0026gt; \u0026lt;/arguments\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-war-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.0\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;webResources\u0026gt; \u0026lt;resource\u0026gt; \u0026lt;!--this is relative to the pom.xml directory --\u0026gt; \u0026lt;directory\u0026gt;src/main/frontend/build\u0026lt;/directory\u0026gt; \u0026lt;!--the default value is ** --\u0026gt; \u0026lt;includes\u0026gt; \u0026lt;include\u0026gt;**/*\u0026lt;/include\u0026gt; \u0026lt;/includes\u0026gt; \u0026lt;!--there\u0026#39;s no default value for this --\u0026gt; \u0026lt;excludes\u0026gt; \u0026lt;exclude\u0026gt;asset-manifest.json\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;**/*.css.map\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;**/*.js.map\u0026lt;/exclude\u0026gt; \u0026lt;/excludes\u0026gt; \u0026lt;/resource\u0026gt; \u0026lt;/webResources\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; 总结 用create-react-app快速创建前端项目确实很方便，省去了用gulp、webpack写编译脚本的麻烦，约定大于配置的思想贯彻得挺好的，以后创建新项目就靠你了。\n项目完整源码见这里。\n","permalink":"https://jeremyxu2010.github.io/2017/01/%E4%BD%BF%E7%94%A8create-react-app%E7%AE%80%E5%8C%96%E5%89%8D%E7%AB%AF%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%BB%BA%E7%AB%8B/","tags":["react","nodejs"],"title":"使用create-react-app简化前端项目的建立"},{"categories":["java开发"],"contents":"最近在做一个老旧项目，项目后台使用了hibernate。以前虽说也用过hibernate，但用得不够深入，一般最多两个表关联查询一下，比较简单。\n但今天在项目有一个需求，要求5个表进行关联查询，这样hibernate试了很久，发现还是搞不定。于是尝试在hibernate里直接使用SQL。在这个地方遇到了坑，卡了很久。最终解决了问题，这里记录一下。\nHibernate里使用SQL StringBuilder sql = new StringBuilder(); //这里开始拼装sql语句 //创建SQLQuery对象 SQLQuery sqlQuery = getSession().createSQLQuery(sql.toString()); //调用addScalar， 说明取结果集里的哪些字段， 字段被映射为哪种类型 sqlQuery.addScalar(\u0026#34;column1\u0026#34;, Hibernate.LONG); sqlQuery.addScalar(\u0026#34;column2\u0026#34;, Hibernate.STRING); sqlQuery.addScalar(\u0026#34;column3\u0026#34;, Hibernate.STRING); //设置取的结果集行数 sqlQuery.setFirstResult(...); sqlQuery.setMaxResults(...); //设置将对象转化为Cto对象， 注意Cto对象的各属性类型要与addScalar里指明的一致 sqlQuery.setResultTransformer(Transformers.aliasToBean(TestCto.class)); //返回TestCto的List列表 return sqlQuery.list(); 上述代码中的说明很详细了，就不解释了。\n总结 hibernate里使用SQL真心很累，还是MyBatis大法好。\n","permalink":"https://jeremyxu2010.github.io/2017/01/hibernate%E9%87%8C%E4%BD%BF%E7%94%A8sqlquery/","tags":["java","hibernate","sql"],"title":"Hibernate里使用SQLQuery"},{"categories":["工具"],"contents":"来到一个新的环境，发现周围好多同事都是用word写技术文档的，觉得有必要将markdown这么好的东西介绍给大家。同时为了方便各位技术同仁写技术博文，推荐一下hexo，真的很方便。\nmarkdown简介 Markdown 是一种「电子邮件」风格的「标记语言」，很适合写技术文档。总结下来，它有如下优点：\n 纯文本，所以兼容性极强，可以用所有文本编辑器打开。 让你专注于文字而不是排版。 格式转换方便，Markdown 的文本你可以轻松转换为 html、电子书等。 Markdown 的标记语法有极好的可读性。  Markdown的语法很简单，这里介绍一些常用的。\n标题 这是最为常用的格式，在平时常用的的文本编辑器中大多是这样实现的：输入文本、选中文本、设置标题格式。\n而在 Markdown 中，你只需要在文本前面加上 # 即可，同理、你还可以增加二级标题、三级标题、四级标题、五级标题和六级标题，总共六级，只需要增加 # 即可，标题字号相应降低。例如：\n# 一级标题 ## 二级标题 ### 三级标题 #### 四级标题 ##### 五级标题 ###### 六级标题 注：# 和一级标题之间要保留一个字符的空格，这是最标准的 Markdown 写法。\n列表 列表格式也很常用，在 Markdown 中，你只需要在文字前面加上 - 就可以了，例如：\n- 文本1 - 文本2 - 文本3 上面那个是有序列表，如果希望用有序列表，也可以在文字前面加上1. 2. 3. 就可以了，例如：\n1. 文本1 2. 文本2 3. 文本3 注：-、1.和文本之间要保留一个字符的空格。\n链接和图片 在 Markdown 中，插入链接不需要其他按钮，你只需要使用 [显示文本](链接地址) 这样的语法即可，例如：\n[简书](http://www.jianshu.com) 在 Markdown 中，插入图片不需要其他按钮，你只需要使用 ![](图片链接地址) 这样的语法即可，例如：\n![图片alt描述](http://ww4.sinaimg.cn/bmiddle/aa397b7fjw1dzplsgpdw5j.jpg) 注：插入图片的语法和链接的语法很像，只是前面多了一个!。\n引用 在我们写作的时候经常需要引用他人的文字，这个时候引用这个格式就很有必要了，在 Markdown 中，你只需要在你希望引用的文字前面加上\u0026gt;就好了，例如：\n\u0026gt; 一盏灯， 一片昏黄； 一简书， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。 注：\u0026gt; 和文本之间要保留一个字符的空格。\n粗体和斜体 Markdown 的粗体和斜体也非常简单，用两个*包含一段文本就是粗体的语法，用一个*包含一段文本就是斜体的语法。例如：\n*一盏灯*， 一片昏黄；**一简书**， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。 其中一盏灯是斜体，一简书是粗体。\n代码引用 需要引用代码时，如果引用的语句只有一段，不分行，可以用`将语句包起来。 如果引用的语句为多行，可以将```置于这段代码的首行和末行。\n表格 表格的语法也很简单，如下\n| Tables | Are | Cool | | ------------- |:-------------:| -----:| | col 3 is | right-aligned | $1600 | | col 2 is | centered | $12 | | zebra stripes | are neat | $1 | 基本语法就这么多了，很简单吧。一些高级语法见官方文档。\n写markdown的工具 语法介绍完了，下面就说一下写markdown的工具，市面上markdown编辑器很多，但因为我是一个开发人员，电脑里intellij idea会常年打开，因此就直接用idea写markdown了。idea的直接还挺好，在IDE里新建一个md文档，直接打开就可以了，而且默认分为两块面板，左边写markdown, 右边就直接显示markdown最终的显示效果，爽歪歪啊。\n写博文工具 万事俱备，开始写博文了，我习惯使用hexo。为啥选它，因为它真的很简单，只有几步而已。\n安装hexo 前提条件电脑上需要先安装NodeJS，如何安装可自行百度。\n安装hexo\nnpm install -g hexo-cli 创建博客目录 hexo init blog cd blog npm install 新建完成后，指定文件夹的目录如下：\n. ├── _config.yml ├── package.json ├── scaffolds ├── source | ├── _drafts | └── _posts └── themes 因为source目录下才是博文的源目录，我一般将它归入到git版本管理里。\ncd source git init git add . git commit -m \u0026quot;first commit\u0026quot; git add remote origin .... git push -u master 使用idea编辑博文 在idea里新建一个Static Web的Module，Module的路径就指定为hexo的source目录，然后就可以在idea里进行博文的编辑了。\n编辑博文的一点小规范  直接在_posts目录下新建md文件即是创建了一篇新的博文，如下图。   博文最上面使用Front-matter指定博文的一些元信息，如下面。  Front-matter的详细语法见这里。\n 为确保博客不依赖于某个域名，以后可切换域名，博文中引用的图片（如引用外部站点图片，则指明外部站点的完整URL）全部使用相对于根的URL，见下面所示。   为避免两篇博文的图片冲突，建议引用图片时，按博文的日期将图片放在不同的目录下。  运行博客 直接在博客目录下运行hexo server即可运行博客，使用浏览器访问http://127.0.0.1:4000即可看到博客的效果。\n博客自定义   hexo的配置文件_config.yml中有好几个配置项挺重要的，需设置合理。这些属性有title、subtitle、description、author、language、timezone、url、highlight。具体配置说明见这里。\n  为了便于被搜索引擎索引，可以使用hexo-generator-baidu-sitemap和hexo-generator-sitemap生成百度和Google的sitemap文件。具体用法见https://github.com/coneycode/hexo-generator-baidu-sitemap和https://github.com/hexojs/hexo-generator-sitemap。\n  为了便于RSS阅读，可以使用hexo-generator-feed生成rss的feed。具体用法见https://github.com/hexojs/hexo-generator-feed。\n  默认主题太没个性了不是，可以到主题库里选择一个有个性的主题，主题库地址在这里。\n  博文要让人可以评论，可集成多说的评论系统，配置说明见这里。\n  如果想将博客通过git部署到github或oschina，可参考我之前的一篇博文。\n  总结 这是篇工具使用说明，好像没什么可说明的。\n","permalink":"https://jeremyxu2010.github.io/2017/01/%E4%BD%BF%E7%94%A8hexo%E5%86%99%E5%8D%9A%E6%96%87/","tags":["hexo","nodejs"],"title":"使用hexo写博文"},{"categories":["web开发"],"contents":"最近参与了一个历时4-5年的项目，项目是一个后台管理系统，访问量并不高，但经常根据业务方的一些特殊需求，在原有代码添加功能。项目所采用的技术架构还十分老旧，后台采用Struts + Spring + Hibernate， 前台直接使用JSP, 辅以struts与jstl的一些标签。\n说实话，自从接受前端MVVM模式后，很久不再使用原始的JSP做前端了，实在是不习惯JSP这种杂乱无章的书写模式。\n但项目目前还有线上跑着，维护工作还得继续，同时小组长还告诉我在未完全了解全部业务之前，千万不要尝试进行大面积重构。唉，说实话，我很怀疑这么乱的代码，我最终能完全理解业务。。。\n想了下，最终还是想到办法使用原有的React技术栈完成前端工作，这里将方法写出来，供其它遇到这类问题的小伙伴参考一下。\nstruts的改造 struts的action方法仅完成两种用途，一是页面URL跳转，一是返回ajax数据。具体实现如下：\n@Component(\u0026#34;testAction\u0026#34;) @Scope(\u0026#34;prototype\u0026#34;) public class TestAction { private static final String JSON_RESULT = \u0026#34;jsonResult\u0026#34;; private Map\u0026lt;String, Object\u0026gt; jsonResultMap = Maps.newHashMap(); ... //这类action方法主要负责页面的跳转  public String gotoPage1(){ return \u0026#34;page1\u0026#34;; } //这类action方法主要负责以json的格式返回ajax数据  public String loadTestData(){ try { HttpServletRequest request = ServletActionContext.getRequest(); String param1 = request.getParameter(\u0026#34;param1\u0026#34;); //做业务操作  jsonResultMap.put(\u0026#34;testData\u0026#34;, testData); jsonResultMap.put(\u0026#34;success\u0026#34;, true); } catch (Exception e){ jsonResultMap.put(\u0026#34;errMsg\u0026#34;, e.getMessage()); jsonResultMap.put(\u0026#34;success\u0026#34;, false); } return JSON_RESULT; } ... } 对应的struts配置\n\u0026lt;package name=\u0026#34;test\u0026#34; namespace=\u0026#34;/test\u0026#34; extends=\u0026#34;json-default\u0026#34;\u0026gt; \u0026lt;action name=\u0026#34;*\u0026#34; class=\u0026#34;testAction\u0026#34; method=\u0026#34;{1}\u0026#34;\u0026gt; \u0026lt;result name=\u0026#34;page1\u0026#34;\u0026gt;/test/page1.jsp\u0026lt;/result\u0026gt; \u0026lt;result name=\u0026#34;jsonResult\u0026#34; type=\u0026#34;json\u0026#34;\u0026gt; \u0026lt;param name=\u0026#34;root\u0026#34;\u0026gt;jsonResultMap\u0026lt;/param\u0026gt; \u0026lt;param name=\u0026#34;contentType\u0026#34;\u0026gt;text/html;charset=UTF-8\u0026lt;/param\u0026gt; \u0026lt;/result\u0026gt; \u0026lt;/action\u0026gt; \u0026lt;/package\u0026gt; 前端jsp的改造 前端jsp页面引用一些常用CSS, JS资源，然后主要使用React来渲染页面，代码如下：\npage1.jsp\n\u0026lt;%@ page language=\u0026#34;java\u0026#34; pageEncoding=\u0026#34;UTF-8\u0026#34; contentType=\u0026#34;text/html; charset=UTF-8\u0026#34; %\u0026gt; \u0026lt;%@ taglib uri=\u0026#34;http://java.sun.com/jstl/core_rt\u0026#34; prefix=\u0026#34;c\u0026#34; %\u0026gt; \u0026lt;c:set var=\u0026#34;ctx\u0026#34; value=\u0026#34;${pageContext.request.contextPath}\u0026#34;/\u0026gt; \u0026lt;!DOCTYPE html PUBLIC \u0026#34;-//W3C//DTD XHTML 1.0 Transitional//EN\u0026#34; \u0026#34;http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\u0026#34;\u0026gt; \u0026lt;html xmlns=\u0026#34;http://www.w3.org/1999/xhtml\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026#34;Content-Type\u0026#34; content=\u0026#34;text/html; charset=utf-8\u0026#34;/\u0026gt; \u0026lt;title\u0026gt;title\u0026lt;/title\u0026gt; \u0026lt;link href=\u0026#34;${ctx}/css/bootstrap/bootstrap.min.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34;/\u0026gt; \u0026lt;link href=\u0026#34;${ctx}/css/sb-admin-2/sb-admin-2.min.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34;/\u0026gt; \u0026lt;link href=\u0026#34;${ctx}/css/antd/antd.min.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34;/\u0026gt; \u0026lt;link href=\u0026#34;${ctx}/css/module_common.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34;/\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;reactHolder\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;${ctx}/script/lodash/lodash.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;${ctx}/script/react/react.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;${ctx}/script/react/react-dom.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;${ctx}/script/antd/antd.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;${ctx}/script/moment/moment.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;${ctx}/script/axios/axios.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;${ctx}/script/babel-core/browser.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var __CTX_PATH__=\u0026#39;${ctx}\u0026#39;; \u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/babel\u0026#34; src=\u0026#34;${ctx}/script/TODO.jsx\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 这里在外部的jsx文件书写主要的页面渲染逻辑，jsx文件可使用ES6语法进行书写，将由babel5实时翻译为ES5代码（本项目为后台管理系统，可以忍受实时翻译的性能开销）。代码如下：\nTODO.jsx代码\nconst React = window.React; const ReactDOM = window.ReactDOM; const _ = window._; const axios = window.axios; const antd = window.antd; class TODO extends React.Component{ render(){ return \u0026lt;h1\u0026gt;TODO\u0026lt;/h1\u0026gt;; } } ReactDOM.render(\u0026lt;TODO/\u0026gt;, document.getElementById(\u0026#39;reactHolder\u0026#39;)); 前端jsx文件的引用 开发中可能会将一些公共方法抽取出来放到一个单独的文件中，而js(x)文件的加载都是异步的，无法保证依赖性。这里可采用umd方式封闭JS模块的方案，如下代码：\n;(function(f) { // CommonJS  if (typeof exports === \u0026#34;object\u0026#34; \u0026amp;\u0026amp; typeof module !== \u0026#34;undefined\u0026#34;) { module.exports = f(); // RequireJS  } else if (typeof define === \u0026#34;function\u0026#34; \u0026amp;\u0026amp; define.amd) { define([], f); // \u0026lt;script\u0026gt;  } else { var g; if (typeof window !== \u0026#34;undefined\u0026#34;) { g = window; } else if (typeof global !== \u0026#34;undefined\u0026#34;) { g = global; } else if (typeof self !== \u0026#34;undefined\u0026#34;) { g = self; } else { g = this; } g.Common = f(); } })(function(){ const util1 = function(){ ... }; const util2 = function(){ ... }; return { util1, util2 } }); 引用方可以这样写：\n;(function(f) { // CommonJS  if (typeof exports === \u0026#34;object\u0026#34; \u0026amp;\u0026amp; typeof module !== \u0026#34;undefined\u0026#34;) { module.exports = f(require(\u0026#39;common\u0026#39;)); // RequireJS  } else if (typeof define === \u0026#34;function\u0026#34; \u0026amp;\u0026amp; define.amd) { define([\u0026#39;common\u0026#39;], f); // \u0026lt;script\u0026gt;  } else { var g; if (typeof window !== \u0026#34;undefined\u0026#34;) { g = window; } else if (typeof global !== \u0026#34;undefined\u0026#34;) { g = global; } else if (typeof self !== \u0026#34;undefined\u0026#34;) { g = self; } else { g = this; } g.App1 = f(g.Common); } })(function(Common){ const app1Module1 = function(){ ... }; const app1Module2 = function(){ ... }; return { app1Module1, app1Module2 } }); 当然这么写还是有很大缺点，由于没有引入cmd，amd等JS模块化方案，这里是污染全局变量了。对于老旧项目来说，没有上requirejs或browserify、webpack打包方案，目前也只能这么干了。\n总结 虽然维护老旧项目很累，但能采用以前的技术栈写前端代码，这已经很幸福了。\n","permalink":"https://jeremyxu2010.github.io/2017/01/%E5%85%8D%E7%BC%96%E8%AF%91%E5%9C%A8jsp%E4%B8%AD%E7%9B%B4%E6%8E%A5%E5%86%99react%E4%BB%A3%E7%A0%81/","tags":["JSP","react"],"title":"免编译在JSP中直接写react代码"},{"categories":["web开发"],"contents":"之前项目中使用的webpack进行前端代码的编译，但一直不太喜欢webpack的那种玩法。使用webpack编写编译脚本时就是按webpack的规则进行各种配置，必须完全遵守它的条条框框，明明是自己写nodejs代码进行编译，但完全可控感。之前就听说过gulp+browserify的组合，这次就尝试使用这个东东重写编译脚本。话不多说，直接上最后的成果。\n前端依赖package.json package.json\n{ \u0026#34;name\u0026#34;: \u0026#34;ssm-scaffold-frontend\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;SSM scaffold frontend, powered by React and Redux\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;gulp\u0026#34;: \u0026#34;./node_modules/.bin/gulp\u0026#34;, \u0026#34;gulp-production\u0026#34;: \u0026#34;./node_modules/.bin/gulp production\u0026#34; }, \u0026#34;author\u0026#34;: \u0026#34;Jeremy Xu\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;ISC\u0026#34;, \u0026#34;dependencies\u0026#34;: { \u0026#34;antd\u0026#34;: \u0026#34;^2.6.1\u0026#34;, \u0026#34;axios\u0026#34;: \u0026#34;^0.15.3\u0026#34;, \u0026#34;bootstrap\u0026#34;: \u0026#34;^3.3.7\u0026#34;, \u0026#34;intl\u0026#34;: \u0026#34;^1.2.5\u0026#34;, \u0026#34;lodash\u0026#34;: \u0026#34;^4.17.2\u0026#34;, \u0026#34;react\u0026#34;: \u0026#34;^15.4.2\u0026#34;, \u0026#34;react-dom\u0026#34;: \u0026#34;^15.4.2\u0026#34;, \u0026#34;react-intl\u0026#34;: \u0026#34;^2.2.2\u0026#34;, \u0026#34;react-redux\u0026#34;: \u0026#34;^5.0.1\u0026#34;, \u0026#34;react-router\u0026#34;: \u0026#34;^3.0.0\u0026#34;, \u0026#34;redux\u0026#34;: \u0026#34;^3.6.0\u0026#34;, \u0026#34;redux-thunk\u0026#34;: \u0026#34;^2.1.0\u0026#34; }, \u0026#34;devDependencies\u0026#34;: { \u0026#34;babel-cli\u0026#34;: \u0026#34;^6.18.0\u0026#34;, \u0026#34;babel-plugin-transform-decorators-legacy\u0026#34;: \u0026#34;^1.3.4\u0026#34;, \u0026#34;babel-plugin-transform-object-rest-spread\u0026#34;: \u0026#34;^6.20.2\u0026#34;, \u0026#34;babel-preset-es2015\u0026#34;: \u0026#34;^6.18.0\u0026#34;, \u0026#34;babel-preset-react\u0026#34;: \u0026#34;^6.16.0\u0026#34;, \u0026#34;babelify\u0026#34;: \u0026#34;^7.3.0\u0026#34;, \u0026#34;browserify\u0026#34;: \u0026#34;^13.1.1\u0026#34;, \u0026#34;del\u0026#34;: \u0026#34;^2.2.2\u0026#34;, \u0026#34;errorify\u0026#34;: \u0026#34;^0.3.1\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;^7.1.1\u0026#34;, \u0026#34;gulp\u0026#34;: \u0026#34;^3.9.1\u0026#34;, \u0026#34;gulp-clean-css\u0026#34;: \u0026#34;^2.3.2\u0026#34;, \u0026#34;gulp-concat\u0026#34;: \u0026#34;^2.6.1\u0026#34;, \u0026#34;gulp-connect\u0026#34;: \u0026#34;^5.0.0\u0026#34;, \u0026#34;gulp-connect-reproxy\u0026#34;: \u0026#34;^0.0.98\u0026#34;, \u0026#34;gulp-css-image-rev\u0026#34;: \u0026#34;^1.0.4\u0026#34;, \u0026#34;gulp-htmlmin\u0026#34;: \u0026#34;^3.0.0\u0026#34;, \u0026#34;gulp-if-else\u0026#34;: \u0026#34;^1.0.3\u0026#34;, \u0026#34;gulp-less\u0026#34;: \u0026#34;3.3.0\u0026#34;, \u0026#34;gulp-run-sequence\u0026#34;: \u0026#34;^0.3.2\u0026#34;, \u0026#34;gulp-sourcemaps\u0026#34;: \u0026#34;^1.9.1\u0026#34;, \u0026#34;gulp-uglify\u0026#34;: \u0026#34;^2.0.0\u0026#34;, \u0026#34;gulp-util\u0026#34;: \u0026#34;^3.0.7\u0026#34;, \u0026#34;gulp-version-number\u0026#34;: \u0026#34;^0.1.5\u0026#34;, \u0026#34;less\u0026#34;: \u0026#34;^2.7.1\u0026#34;, \u0026#34;merge-stream\u0026#34;: \u0026#34;^1.0.1\u0026#34;, \u0026#34;vinyl-buffer\u0026#34;: \u0026#34;^1.0.0\u0026#34;, \u0026#34;vinyl-source-stream\u0026#34;: \u0026#34;^1.1.0\u0026#34;, \u0026#34;watchify\u0026#34;: \u0026#34;^3.8.0\u0026#34; } } 可以看到该项目前端主要依赖react、redux、react-router、antd、 bootstrap、axios、lodash、react-intl，这算一个比较常见的选择。前端编译js时使用了gulp、babel、browserify、babelify，编译css时使用了less，使用gulp-connect作为开发服务器。\ngulp编译脚本 gulpfile.js\nconst gulp = require(\u0026#39;gulp\u0026#39;); const browserify = require(\u0026#39;browserify\u0026#39;); const errorify = require(\u0026#39;errorify\u0026#39;); const babelify = require(\u0026#34;babelify\u0026#34;); const source = require(\u0026#39;vinyl-source-stream\u0026#39;); const connect = require(\u0026#39;gulp-connect\u0026#39;); const Reproxy = require(\u0026#34;gulp-connect-reproxy\u0026#34;); const fs = require(\u0026#39;fs\u0026#39;); const path = require(\u0026#39;path\u0026#39;); const del = require(\u0026#39;del\u0026#39;); const concat = require(\u0026#39;gulp-concat\u0026#39;); const merge = require(\u0026#39;merge-stream\u0026#39;); const less = require(\u0026#39;gulp-less\u0026#39;); const gutil = require(\u0026#39;gulp-util\u0026#39;); const ifElse = require(\u0026#39;gulp-if-else\u0026#39;); const _ = require(\u0026#39;lodash\u0026#39;); const watchify = require(\u0026#39;watchify\u0026#39;); const runSequence = require(\u0026#39;gulp-run-sequence\u0026#39;); const imageRev = require(\u0026#39;gulp-css-image-rev\u0026#39;); const version = require(\u0026#39;gulp-version-number\u0026#39;); const buffer = require(\u0026#39;vinyl-buffer\u0026#39;); const uglify = require(\u0026#39;gulp-uglify\u0026#39;); const cleanCSS = require(\u0026#39;gulp-clean-css\u0026#39;); const sourcemaps = require(\u0026#39;gulp-sourcemaps\u0026#39;); const htmlmin = require(\u0026#39;gulp-htmlmin\u0026#39;); const srcPath = \u0026#39;./src\u0026#39;; const buildPath = \u0026#39;../webapp\u0026#39;; const buildJsPath = buildPath + \u0026#39;/js\u0026#39;; const buildImgPath = buildPath + \u0026#39;/img\u0026#39;; const buildCssPath = buildPath + \u0026#39;/css\u0026#39;; const jsEntryPath = srcPath + \u0026#39;/js/pages\u0026#39;; const cssEntryPath = srcPath + \u0026#39;/css/pages\u0026#39;; const htmlEntryPath = srcPath + \u0026#39;/html\u0026#39;; const versionConfig = { \u0026#39;value\u0026#39;: \u0026#39;%MDS%\u0026#39;, \u0026#39;append\u0026#39;: { \u0026#39;key\u0026#39;: \u0026#39;v\u0026#39;, \u0026#39;to\u0026#39;: [\u0026#39;css\u0026#39;, \u0026#39;js\u0026#39;,\u0026#39;image\u0026#39;], }, }; // Environment setup. var env = { production: false }; // Environment task. gulp.task(\u0026#34;set-production\u0026#34;, function(){ env.production = true; }); gulp.task(\u0026#34;clean\u0026#34;, function(cb){ del([ path.join(buildPath, \u0026#39;*.html\u0026#39;), path.join(buildPath, \u0026#39;js\u0026#39;, \u0026#39;**/*\u0026#39;), path.join(buildPath, \u0026#39;css\u0026#39;, \u0026#39;**/*\u0026#39;), path.join(buildPath, \u0026#39;img\u0026#39;, \u0026#39;**/*\u0026#39;) ], {force: true}).then(function(){ cb(); }); }); function getPageNames() { return fs.readdirSync(jsEntryPath) .filter(function(file) { return fs.statSync(path.join(jsEntryPath, file)).isDirectory(); }); } var pageNames = getPageNames(); var buildjs_tasks = []; var buildjs_funcs = []; var buildcss_tasks = []; gulp.task(\u0026#34;prepare\u0026#34;, [\u0026#34;clean\u0026#34;], function(){ let uglifyFunc = function(){ return uglify({mangle: {except: [\u0026#39;require\u0026#39; ,\u0026#39;import\u0026#39;, \u0026#39;export\u0026#39;, \u0026#39;exports\u0026#39; ,\u0026#39;module\u0026#39;]}}); }; pageNames.forEach(function(pageName) { // 在这里添加自定义 browserify 选项  let customOpts = { entries: [path.join(jsEntryPath, pageName, \u0026#39;index.js\u0026#39;)], debug: !env.production, plugin: [watchify, errorify] }; let opts = _.assign({}, watchify.args, customOpts); let b = browserify(opts); b.exclude(\u0026#39;lodash\u0026#39;); b.exclude(\u0026#39;react\u0026#39;); b.exclude(\u0026#39;react-dom\u0026#39;); b.exclude(\u0026#39;intl\u0026#39;); b.exclude(\u0026#39;react-intl\u0026#39;); b.exclude(\u0026#39;react-intl-zh\u0026#39;); b.exclude(\u0026#39;react-intl-en\u0026#39;); b.exclude(\u0026#39;react-router\u0026#39;); b.exclude(\u0026#39;redux\u0026#39;); b.exclude(\u0026#39;redux-thunk\u0026#39;); b.exclude(\u0026#39;react-redux\u0026#39;); b.exclude(\u0026#39;antd\u0026#39;); b.exclude(\u0026#39;axios\u0026#39;); // 在这里加入变换操作  b.transform(babelify.configure({ presets: [\u0026#34;es2015\u0026#34;, \u0026#34;react\u0026#34;], plugins: [\u0026#34;transform-decorators-legacy\u0026#34;, \u0026#34;transform-object-rest-spread\u0026#34;] })); let buildjs = function(){ return b.bundle() .pipe(source(pageName + \u0026#39;_bundle.js\u0026#39;)) .pipe(buffer()) .pipe(ifElse(env.production, uglifyFunc)) // write to output  .pipe(gulp.dest(buildJsPath)); }; gulp.task(\u0026#39;buildjs_\u0026#39; + pageName, buildjs); // 这样你就可以运行 `gulp js` 来编译文件了  buildjs_tasks.push(\u0026#39;buildjs_\u0026#39; + pageName); buildjs_funcs.push({b: b, func : buildjs}); gulp.task(\u0026#39;buildcss_\u0026#39; + pageName, function(){ return gulp.src(path.join(cssEntryPath, pageName, \u0026#39;index.less\u0026#39;)) .pipe(imageRev()) .pipe(ifElse(!env.production, function(){ return sourcemaps.init(); })) .pipe(less({ paths: [ path.join(srcPath, \u0026#39;css\u0026#39;) ] })) .pipe(ifElse(!env.production, function(){ return sourcemaps.write(); })) .pipe(concat(pageName + \u0026#39;_bundle.css\u0026#39;)) .pipe(ifElse(env.production, function(){ return cleanCSS({keepSpecialComments: 0}) })) // write to output  .pipe(gulp.dest(buildCssPath)); }); buildcss_tasks.push(\u0026#39;buildcss_\u0026#39; + pageName); }); let b = browserify({ plugin: [errorify] }); b.require(\u0026#39;./node_modules/lodash/lodash\u0026#39;+ (env.production ? \u0026#39;.min\u0026#39;:\u0026#39;\u0026#39;) + \u0026#39;.js\u0026#39;, {expose: \u0026#39;lodash\u0026#39;}); b.require(\u0026#39;./node_modules/react/dist/react\u0026#39;+ (env.production ? \u0026#39;.min\u0026#39;:\u0026#39;\u0026#39;) + \u0026#39;.js\u0026#39;, {expose: \u0026#39;react\u0026#39;}); b.require(\u0026#39;./node_modules/react-dom/dist/react-dom\u0026#39;+ (env.production ? \u0026#39;.min\u0026#39;:\u0026#39;\u0026#39;) + \u0026#39;.js\u0026#39;, {expose: \u0026#39;react-dom\u0026#39;}); b.require(\u0026#39;./node_modules/intl/dist/intl\u0026#39;+ (env.production ? \u0026#39;.min\u0026#39;:\u0026#39;\u0026#39;) + \u0026#39;.js\u0026#39;, {expose: \u0026#39;intl\u0026#39;}); b.require(\u0026#39;./node_modules/react-intl/dist/react-intl\u0026#39;+ (env.production ? \u0026#39;.min\u0026#39;:\u0026#39;\u0026#39;) + \u0026#39;.js\u0026#39;, {expose: \u0026#39;react-intl\u0026#39;}); b.require(\u0026#39;./node_modules/react-intl/locale-data/zh.js\u0026#39;, {expose: \u0026#39;react-intl-zh\u0026#39;}); b.require(\u0026#39;./node_modules/react-intl/locale-data/en.js\u0026#39;, {expose: \u0026#39;react-intl-en\u0026#39;}); b.require(\u0026#39;./node_modules/react-router/umd/ReactRouter\u0026#39;+ (env.production ? \u0026#39;.min\u0026#39;:\u0026#39;\u0026#39;) + \u0026#39;.js\u0026#39;, {expose: \u0026#39;react-router\u0026#39;}); b.require(\u0026#39;./node_modules/redux/dist/redux\u0026#39;+ (env.production ? \u0026#39;.min\u0026#39;:\u0026#39;\u0026#39;) + \u0026#39;.js\u0026#39;, {expose: \u0026#39;redux\u0026#39;}); b.require(\u0026#39;./node_modules/redux-thunk/dist/redux-thunk\u0026#39;+ (env.production ? \u0026#39;.min\u0026#39;:\u0026#39;\u0026#39;) + \u0026#39;.js\u0026#39;, {expose: \u0026#39;redux-thunk\u0026#39;}); b.require(\u0026#39;./node_modules/react-redux/dist/react-redux\u0026#39;+ (env.production ? \u0026#39;.min\u0026#39;:\u0026#39;\u0026#39;) + \u0026#39;.js\u0026#39;, {expose: \u0026#39;react-redux\u0026#39;}); b.require(\u0026#39;./node_modules/antd/dist/antd\u0026#39;+ (env.production ? \u0026#39;.min\u0026#39;:\u0026#39;\u0026#39;) + \u0026#39;.js\u0026#39;, {expose: \u0026#39;antd\u0026#39;}); b.require(\u0026#39;./node_modules/axios/dist/axios\u0026#39;+ (env.production ? \u0026#39;.min\u0026#39;:\u0026#39;\u0026#39;) + \u0026#39;.js\u0026#39;, {expose: \u0026#39;axios\u0026#39;}); let jsVendorBundleTask = b.bundle() .pipe(source(\u0026#39;vendor_bundle.js\u0026#39;)) .pipe(buffer()) .pipe(ifElse(env.production, uglifyFunc)) // write to output  .pipe(gulp.dest(buildJsPath)); let cssVendorBundleTask = gulp.src([ \u0026#39;./node_modules/bootstrap/dist/css/bootstrap\u0026#39;+ (env.production ? \u0026#39;.min\u0026#39;:\u0026#39;\u0026#39;) + \u0026#39;.css\u0026#39;, path.join(srcPath, \u0026#39;css\u0026#39;, \u0026#39;common\u0026#39;, \u0026#39;sb-admin-2\u0026#39;+ (env.production ? \u0026#39;.min\u0026#39;:\u0026#39;\u0026#39;) + \u0026#39;.css\u0026#39;), path.join(srcPath, \u0026#39;vendors\u0026#39;, \u0026#39;antd\u0026#39;, \u0026#39;antd\u0026#39;+ (env.production ? \u0026#39;.min\u0026#39;:\u0026#39;\u0026#39;) + \u0026#39;.css\u0026#39;)]) .pipe(concat(\u0026#39;vendor_bundle.css\u0026#39;)) .pipe(ifElse(env.production, function(){ return cleanCSS({keepSpecialComments: 0}) })) // write to output  .pipe(gulp.dest(buildCssPath)); let cssFontTask = gulp.src([path.join(srcPath, \u0026#39;vendors\u0026#39;, \u0026#39;antd\u0026#39;, \u0026#39;font_*\u0026#39;)]) .pipe(gulp.dest(buildCssPath)); return merge([]).add(jsVendorBundleTask).add(cssVendorBundleTask).add(cssFontTask); }); gulp.task(\u0026#39;buildAllJs\u0026#39;, buildjs_tasks, function(cb){ cb(); gutil.log(\u0026#34;All Javascript is built.\u0026#34;) }); gulp.task(\u0026#39;buildAllCss\u0026#39;, buildcss_tasks, function (cb) { cb(); gutil.log(\u0026#34;All Stylesheet is built.\u0026#34;); }); gulp.task(\u0026#34;image\u0026#34;, function(){ gulp.src([path.join(srcPath, \u0026#39;img\u0026#39;, \u0026#39;**/*\u0026#39;)]) .pipe(gulp.dest(path.join(buildPath, \u0026#39;img\u0026#39;))); }); gulp.task(\u0026#39;html\u0026#39;, function(){ let pageNames = getPageNames(); let tasks = pageNames.map(function(pageName) { return gulp.src(htmlEntryPath + \u0026#39;/\u0026#39; + pageName + \u0026#39;.html\u0026#39;) .pipe(version(versionConfig)) .pipe(ifElse(env.production, function(){ return htmlmin({collapseWhitespace: true}); })) .pipe(gulp.dest(buildPath)); }); return merge([]).add(tasks); }); gulp.task(\u0026#39;livereload\u0026#39;, [\u0026#39;html\u0026#39;], function () { return gulp.src([]) .pipe(connect.reload()); }); gulp.task(\u0026#39;webserver\u0026#39;, function() { connect.server({ livereload: true, port: \u0026#34;7777\u0026#34;, root: buildPath, //action请求直接反向代理至后端应用服务器  middleware: function (connect, options) { options.rule = [/\\/api\\//]; options.server = \u0026#34;127.0.0.1:8080\u0026#34;; var proxy = new Reproxy(options); return [proxy]; } }); }); gulp.task(\u0026#39;watch\u0026#39;, function() { buildjs_funcs.forEach(function(o){ o.b.on(\u0026#39;update\u0026#39;, o.func); // 当任何依赖发生改变的时候，运行打包工具  o.b.on(\u0026#39;log\u0026#39;, gutil.log); // 输出编译日志到终端  }); pageNames.forEach(function(pageName) { gulp.watch(path.join(cssEntryPath, pageName, \u0026#39;**/*.less\u0026#39;), [\u0026#39;buildcss_\u0026#39; + pageName]); }); gulp.watch([path.join(srcPath, \u0026#39;img\u0026#39;, \u0026#39;**/*\u0026#39;)], [\u0026#39;image\u0026#39;]); gulp.watch([path.join(srcPath, \u0026#39;html\u0026#39;, \u0026#39;**/*\u0026#39;)], [\u0026#39;html\u0026#39;]); gulp.watch([path.join(buildJsPath, \u0026#39;**/*_bundle.js\u0026#39;), path.join(buildCssPath, \u0026#39;**/*_bundle.css\u0026#39;), path.join(buildImgPath, \u0026#39;**/*\u0026#39;)], [\u0026#39;livereload\u0026#39;]); }); gulp.task(\u0026#39;default\u0026#39;, function(cb){ runSequence(\u0026#39;prepare\u0026#39;, [\u0026#39;buildAllJs\u0026#39;, \u0026#39;buildAllCss\u0026#39;, \u0026#39;image\u0026#39;], \u0026#39;html\u0026#39;, \u0026#39;webserver\u0026#39;, \u0026#39;watch\u0026#39;, function(){ cb(); gutil.log(\u0026#34;Default task is done.\u0026#34;); }); }); gulp.task(\u0026#39;production\u0026#39;, function(cb){ runSequence(\u0026#39;set-production\u0026#39;, \u0026#39;prepare\u0026#39;, [\u0026#39;buildAllJs\u0026#39;, \u0026#39;buildAllCss\u0026#39;, \u0026#39;image\u0026#39;], \u0026#39;html\u0026#39;, function(){ cb(); gutil.log(\u0026#34;Production task is done.\u0026#34;); process.exit(0); }); }); 这个gulpfile.js看着很长，但其实分成几个task任务，每个task任务的责任还是比较清楚。\nset-production：这个task负责设置当前编译的环境级别，是开发级别的编译，还是生产级别的编译。 clean：这个task负责清理工作。 prepare：这个task最复杂了，主要包括两个部分，一是按页面分别定义了编译各页面的js与css任务。二是编译出引用的第三方公共的js、css、font资源。 buildAllJs：这个task负责编译所有的页面的js。 buildAllCss：这个task负责编译所有的页面的css。 image：这个task是对图片进行处理，目前仅仅是拷贝到编译目录。 html：这个task对html进行处理，这里会对引用的js, css加入version number以避免浏览器缓存。 livereload：这个task用来通知浏览器刷新。 webserver: 这个task启动一个开发web服务器，这里使用Reproxy将api请求代理至后端应用服务器。 watch：这个task启用监听源代码中的文件变更，当发现文件变更时，进行相应的编译处理。同时监听编译目录下的文件变更，当发现变更时，通过浏览器刷新页面。 default：这是default任务，还是将上述多个任务串进来。 production：这个是生产级别编译，也就是说仅编译，但不启动开发web服务器，也不监听文件变更。当执行完就退出node进程。\n这样一分析，整个gulpfile.js就比较简单了。gulp的用法还是比较简单的，可参考中文文档。\n其它 最后分享一下我做了一个工程脚手架，前端使用react+redux, 前端编译使用gulp+browerify+babel, 后端使用springmvc+spring+MyBatis，项目地址http://git.oschina.net/jeremy-xu/ssm-scaffold。\n","permalink":"https://jeremyxu2010.github.io/2016/12/gulp%E6%9B%BF%E6%8D%A2webpack/","tags":["gulp","react","redux"],"title":"gulp替换webpack"},{"categories":["java开发"],"contents":"问题由来 今天运行工程时，发现停止tomcat时，发现控制台会报一些错误。\n十二月 09, 2016 9:25:14 上午 org.apache.coyote.AbstractProtocol stop 信息: Stopping ProtocolHandler [\u0026quot;http-apr-8080\u0026quot;] 十二月 09, 2016 9:25:14 上午 org.apache.catalina.loader.WebappClassLoaderBase loadClass 信息: Illegal access: this web application instance has been stopped already. Could not load org.apache.zookeeper.server.ZooTrace. The eventual following stack trace is caused by an error thrown for debugging purposes as well as to attempt to terminate the thread which caused the illegal access, and has no functional impact. java.lang.IllegalStateException at org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1747) at org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1705) at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1128) 十二月 09, 2016 9:25:14 上午 org.apache.catalina.loader.WebappClassLoaderBase loadClass 信息: Illegal access: this web application instance has been stopped already. Could not load org.apache.log4j.spi.ThrowableInformation. The eventual following stack trace is caused by an error thrown for debugging purposes as well as to attempt to terminate the thread which caused the illegal access, and has no functional impact. java.lang.IllegalStateException at org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1747) at org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1705) at org.apache.log4j.spi.LoggingEvent.\u0026lt;init\u0026gt;(LoggingEvent.java:165) at org.apache.log4j.Category.forcedLog(Category.java:391) at org.apache.log4j.Category.log(Category.java:856) at org.slf4j.impl.Log4jLoggerAdapter.error(Log4jLoggerAdapter.java:576) at org.apache.zookeeper.ClientCnxn$1.uncaughtException(ClientCnxn.java:414) at java.lang.Thread.dispatchUncaughtException(Thread.java:1986) 分析原因 框架时使用CuratorFramework连接zookeeper的，在spring bean销毁时也正确的关闭了zookeeper连接。\ncuratorFramework.close(); 但跟踪代码发现curatorFramework关闭时会调用org.apache.curator.CuratorZookeeperClient#close，之后会org.apache.curator.ConnectionState#close，再之后会调到org.apache.curator.HandleHolder#closeAndClear，再之后会调到org.apache.curator.HandleHolder#internalClose，再之后会调到org.apache.zookeeper.ZooKeeper#close，再之后会调到org.apache.zookeeper.ClientCnxn#close，再之后再调到org.apache.zookeeper.ClientCnxn#disconnect，再之后会调到org.apache.zookeeper.ClientCnxn.SendThread#close。\norg/apache/zookeeper/ClientCnxn.java:1311\nvoid close() { state = States.CLOSED; clientCnxnSocke.wakeupCnxn(); } 这样仅仅只是修改了SendThread线程内部的变量，并没有等SendThread完全退出。 这样就存在spring bean销毁了，但SendThread线程还活着的场景。spring容器退出后，tomcat将该web应用标识为stopped，该web应用的classloader也不再可用。这时SendThread线程执行时要从该web应用的classloader里加载类时，就会报上面的错。\n解决方案 这个问题本质上应该是zookeeper-3.4.8.jar的bug, 关闭zookeeper时，并没有等待SendThread线程完全退出。但项目中不太好直接修改zookeeper的源码，因此从封装的框架层面解决此问题。\npublic synchronized void destroy() { try { if (nodepath != null) { curatorFramework.delete().forPath(nodepath); LOGGER.info(\u0026#34;ZK Close,Path:{}\u0026#34;, nodepath); } } catch (Exception e) { LOGGER.error(\u0026#34;Couldn\u0026#39;t Delete Registry Node\u0026#34;, e); } try { curatorFramework.close(); //等待zookeeper的相关线程完全退出  synchronized (curatorFramework){ curatorFramework.wait(500L); } } catch (Exception e){ LOGGER.error(\u0026#34;Close Registry Error\u0026#34;, e); } } ","permalink":"https://jeremyxu2010.github.io/2016/12/%E8%A7%A3%E5%86%B3zookeeper%E5%AF%BC%E8%87%B4tomcat%E5%81%9C%E6%AD%A2%E6%97%B6%E6%8A%A5%E5%BC%82%E5%B8%B8%E7%9A%84%E9%97%AE%E9%A2%98/","tags":["zookeeper"],"title":"解决zookeeper导致tomcat停止时报异常的问题"},{"categories":["java开发"],"contents":"问题由来 今天运行工程时，发现停止tomcat时，java进程并不会退出，而是必须kill -9杀掉tomcat进程。 问题出现时将线程dump出来后，发现有一个非daemon的线程仍在运行。\n\u0026quot;Hashed wheel timer #1\u0026quot; prio=6 tid=0x000000000ee73800 nid=0x750 waiting on condition [0x000000001383e000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:503) at org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:401) at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) at java.lang.Thread.run(Thread.java:745) 分析原因 com.alibaba.dubbo.remoting.transport.netty.NettyClient创建了一个NioClientSocketChannelFactory，而NioClientSocketChannelFactory在构造时又会创建一个NioClientBossPool，NioClientBossPool在构造时又会创建一个HashedWheelTimer，而HashedWheelTimer创建时使用的是Executors.defaultThreadFactory()，这个线程工厂创建的线程是非daemon的，因此必须调用NioClientSocketChannelFactory的releaseExternalResources方法才可以优雅地停止这些非daemon线程。而dubbo出于规避netty的一个bug\n// 因ChannelFactory的关闭有DirectMemory泄露， // 采用静态化规避 https://issues.jboss.org/browse/NETTY-424 private static final ChannelFactory channelFactory = new NioClientSocketChannelFactory(Executors.newCachedThreadPool(new NamedThreadFactory(\u0026#34;NettyClientBoss\u0026#34;, true)), Executors.newCachedThreadPool(new NamedThreadFactory(\u0026#34;NettyClientWorker\u0026#34;, true)), Constants.DEFAULT_IO_THREADS); , 所以注释掉了NettyClient里的doClose方法里的逻辑\n@Override protected void doClose() throws Throwable { /*try { bootstrap.releaseExternalResources(); } catch (Throwable t) { logger.warn(t.getMessage()); }*/ } 而采用注册shutdownhook的方式进行资源的释放\nstatic { Runtime.getRuntime().addShutdownHook(new Thread(new Runnable() { public void run() { if (logger.isInfoEnabled()) { logger.info(\u0026#34;Run shutdown hook of netty client now.\u0026#34;); } try { channelFactory.releaseExternalResources(); } catch (Throwable t) { logger.warn(t.getMessage()); } } }, \u0026#34;DubboShutdownHook-NettyClient\u0026#34;)); } 但这个方案其实并不能释放Netty的资源，正常关闭java进程时，因为有非daemon线程存在，所以shutdownhook并不会执行，这就是个死循环。\n解决方案 最后使用反射解决了此问题。\n//先释放dubbo所占用的资源 ProtocolConfig.destroyAll(); //用反射释放NettyClient所占用的资源, 以避免不能优雅shutdown的问题 releaseNettyClientExternalResources(); private void releaseNettyClientExternalResources() { try { Field field = NettyClient.class.getDeclaredField(\u0026#34;channelFactory\u0026#34;); field.setAccessible(true); ChannelFactory channelFactory = (ChannelFactory) field.get(NettyClient.class); channelFactory.releaseExternalResources(); field.setAccessible(false); LOGGER.info(\u0026#34;Release NettyClient\u0026#39;s external resources\u0026#34;); } catch (Exception e){ LOGGER.error(\u0026#34;Release NettyClient\u0026#39;s external resources error\u0026#34;, e); } } ","permalink":"https://jeremyxu2010.github.io/2016/12/%E8%A7%A3%E5%86%B3dubbo%E5%AF%BC%E8%87%B4tomcat%E6%97%A0%E6%B3%95%E4%BC%98%E9%9B%85shutdown%E7%9A%84%E9%97%AE%E9%A2%98/","tags":["dubbo","tomcat","netty"],"title":"解决dubbo导致tomcat无法优雅shutdown的问题"},{"categories":["java开发"],"contents":"之前在项目中使用过dubbo，但很久没有再用，以致都忘了它的用法，今天看到当当网开源的一个项目dubbox, 觉得挺实用的。这里先将dubbo的概念及用法记录下来以备忘。\n快速启动 如果是在一个进程内不同的组件调用，一般在spring里是如下配置的：\n\u0026lt;bean id=“xxxService” class=“com.xxx.XxxServiceImpl” /\u0026gt; \u0026lt;!--xxxAction注入xxxService --\u0026gt; \u0026lt;bean id=“xxxAction” class=“com.xxx.XxxAction”\u0026gt; \u0026lt;property name=“xxxService” ref=“xxxService” /\u0026gt; \u0026lt;/bean\u0026gt; 但如果跨进程或跨主机，就不同这么简单的调用了，即会引入RPC框架。dubbo作为一个完善的RPC框架提供的解决方案还算比较简单。\n准备注册中心 这里可以使用zookeeper、redis、multicast、simple四种方式，这里先采用官方首推的zookeeper。\ntar xf zookeeper-3.4.9.tar.gz mv zookeeper-3.4.9 zookeeper cd zookeeper cp conf/zoo_sample.cfg conf/zoo.cfg vim conf/zoo.cfg #修改dataDir ./bin/zkServer.sh start #启动zkServer 当然，为了避免单点故障，在生产环境应该部署zkServer集群。\n准备公共接口 DemoService.java\npublic interface DemoService { String sayHello(String name); } 服务提供方实现与配置  服务提供方实现公共接口  DemoServiceImpl.java\npublic class DemoServiceImpl implements DemoService{ @Override public String sayHello(String name) { return \u0026#34;hello, \u0026#34; + name; } }  将服务为dubbo的方式暴露出来  remote-provider.xml\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:dubbo=\u0026#34;http://code.alibabatech.com/schema/dubbo\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd\u0026#34;\u0026gt; \u0026lt;!--提供方应用信息，用于计算依赖关系 --\u0026gt; \u0026lt;dubbo:application name=\u0026#34;hello-world-app\u0026#34; organization=\u0026#34;personal.jeremyxu\u0026#34; owner=\u0026#34;jeremyxu\u0026#34;/\u0026gt; \u0026lt;dubbo:registry address=\u0026#34;zookeeper://127.0.0.1:2181\u0026#34; /\u0026gt; \u0026lt;bean id=\u0026#34;demoService\u0026#34; class=\u0026#34;personal.jeremyxu.impl.DemoServiceImpl\u0026#34; /\u0026gt; \u0026lt;!--和本地服务一样实现远程服务 --\u0026gt; \u0026lt;dubbo:service interface=\u0026#34;personal.jeremyxu.api.DemoService\u0026#34; ref=\u0026#34;demoService\u0026#34; /\u0026gt; \u0026lt;!--增加暴露远程服务配置 --\u0026gt; \u0026lt;/beans\u0026gt;  服务提供方启动器  ServiceProviderApp.java\npublic class ServiceProviderApp { public static void main( String[] args ) throws IOException { Properties props = System.getProperties(); props.setProperty(\u0026#34;dubbo.spring.config\u0026#34;, \u0026#34;classpath:remote-provider.xml\u0026#34;); Main.main(args); } } 服务消费方实现与配置  使用dubbo的方式引入远程服务  remote-consumer.xml\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:dubbo=\u0026#34;http://code.alibabatech.com/schema/dubbo\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd\u0026#34;\u0026gt; \u0026lt;!--提供方应用信息，用于计算依赖关系 --\u0026gt; \u0026lt;dubbo:application name=\u0026#34;consumer-of-hello-world-app\u0026#34; organization=\u0026#34;personal.jeremyxu\u0026#34; owner=\u0026#34;jeremyxu\u0026#34; /\u0026gt; \u0026lt;dubbo:registry address=\u0026#34;zookeeper://127.0.0.1:2181\u0026#34; /\u0026gt; \u0026lt;!--生成远程服务代理，可以和本地bean一样使用demoService --\u0026gt; \u0026lt;dubbo:reference id=\u0026#34;demoService\u0026#34; interface=\u0026#34;personal.jeremyxu.api.DemoService\u0026#34; /\u0026gt; \u0026lt;/beans\u0026gt;  服务消费方启动器  ServiceConsumerApp.java\npublic class ServiceConsumerApp { public static void main(String[] args) throws InterruptedException { ApplicationContext appCtx = new ClassPathXmlApplicationContext(\u0026#34;remote-consumer.xml\u0026#34;); DemoService demoService = (DemoService)appCtx.getBean(\u0026#34;demoService\u0026#34;); for(int i=0; i\u0026lt;100000; i++) { System.out.println(demoService.sayHello(\u0026#34;xxj\u0026#34;)); Thread.sleep(2000L); } } } 依赖的jar包 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-context\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.3.3.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;dubbo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.101tec\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;zkclient\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.10\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 先后启动服务提供方与服务消费方，即可正常运行。\n高级特性 可以看到快速启动还是比较容易的，但dubbo可没这么简单，它还有不少高级特性，参见这里。\ndubbo在逻辑上分了很多层，每个层里都有好几个实现策略可以选择，参见这里。可能会根据实现场景配置注册中心、协议、NIO框架、序列化框架、ProxyFactory动态代理实现、集群策略、负载均衡策略、路由策略、服务运行容器。\n换用dubbox dubbox相对于dubbox来说，新增了一些很有用的特性：\n  支持REST风格远程调用（HTTP + JSON/XML)：基于非常成熟的JBoss RestEasy框架，在dubbo中实现了REST风格（HTTP + JSON/XML）的远程调用，以显著简化企业内部的跨语言交互，同时显著简化企业对外的Open API、无线API甚至AJAX服务端等等的开发。事实上，这个REST调用也使得Dubbo可以对当今特别流行的“微服务”架构提供基础性支持。 另外，REST调用也达到了比较高的性能，在基准测试下，HTTP + JSON与Dubbo 2.x默认的RPC协议（即TCP + Hessian2二进制序列化）之间只有1.5倍左右的差距，详见文档中的基准测试报告。\n  支持基于Kryo和FST的Java高效序列化实现：基于当今比较知名的Kryo和FST高性能序列化库，为Dubbo默认的RPC协议添加新的序列化实现，并优化调整了其序列化体系，比较显著的提高了Dubbo RPC的性能，详见文档中的基准测试报告。\n  支持基于Jackson的JSON序列化：基于业界应用最广泛的Jackson序列化库，为Dubbo默认的RPC协议添加新的JSON序列化实现。\n  支持基于嵌入式Tomcat的HTTP remoting体系：基于嵌入式tomcat实现dubbo的HTTP remoting体系（即dubbo-remoting-http），用以逐步取代Dubbo中旧版本的嵌入式Jetty，可以显著的提高REST等的远程调用性能，并将Servlet API的支持从2.5升级到3.1。（注：除了REST，dubbo中的WebServices、Hessian、HTTP Invoker等协议都基于这个HTTP remoting体系）。\n  升级Spring：将dubbo中Spring由2.x升级到目前最常用的3.x版本，减少版本冲突带来的麻烦。\n  升级ZooKeeper客户端：将dubbo中的zookeeper客户端升级到最新的版本，以修正老版本中包含的bug。\n  支持完全基于Java代码的Dubbo配置：基于Spring的Java Config，实现完全无XML的纯Java代码方式来配置dubbo\n  注：dubbox和dubbo 2.x是兼容的，没有改变dubbo的任何已有的功能和配置方式（除了升级了spring之类的版本）\n因为dubbox与dubbo 2.x是兼容的，因此不用修改代码，可直接替换dubbox，步骤如下：\n编译dubbox git clone https://github.com/dangdangdotcom/dubbox.git cd dubbox #确保JAVA_HOME指向JDK7 mvn -Dmaven.test.skip=true clean package install 修改pom.xml依赖 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;dubbo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.8.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 使用dubbox的特性 换用dubbox必然是为了使用dubbox的一些新增特性，这里我参考在Dubbo中使用高效的Java序列化（Kryo和FST），为dubbo换用的kryo序列化方法。\n添加maven依赖 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.esotericsoftware.kryo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kryo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.24.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;de.javakaffee\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kryo-serializers\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.26\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 配置使用kryo序列化实现 在服务提供者及服务消费者的dubbo配置中增加以下配置：\n\u0026lt;dubbo:protocol serialization=\u0026#34;kryo\u0026#34;/\u0026gt; 经过对比，可以发现RPC调用的平均处理时间降低了不少。\n配置Monitor dubbo的核心概念里有一个Monitor，这个组件负责统计服务的调用频率和调用时间，它也是作为一个Dubbo服务部署的。\n添加maven依赖 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;dubbo-monitor-simple\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.8.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 创建monitor服务的配置 simple-monitor.xml\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:dubbo=\u0026#34;http://code.alibabatech.com/schema/dubbo\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd\u0026#34;\u0026gt; \u0026lt;!--提供方应用信息，用于计算依赖关系 --\u0026gt; \u0026lt;dubbo:application name=\u0026#34;simple-monitor\u0026#34; organization=\u0026#34;personal.jeremyxu\u0026#34; owner=\u0026#34;jeremyxu\u0026#34;/\u0026gt; \u0026lt;dubbo:registry address=\u0026#34;zookeeper://127.0.0.1:2181\u0026#34; /\u0026gt; \u0026lt;bean id=\u0026#34;monitorService\u0026#34; class=\u0026#34;com.alibaba.dubbo.monitor.simple.SimpleMonitorService\u0026#34; \u0026gt; \u0026lt;property name=\u0026#34;statisticsDirectory\u0026#34; value=\u0026#34;/tmp/monitor/statistics\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;chartsDirectory\u0026#34; value=\u0026#34;/tmp/monitor/charts\u0026#34;/\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;dubbo:service interface=\u0026#34;com.alibaba.dubbo.monitor.MonitorService\u0026#34; ref=\u0026#34;monitorService\u0026#34;/\u0026gt; \u0026lt;dubbo:reference id=\u0026#34;registryService\u0026#34; interface=\u0026#34;com.alibaba.dubbo.registry.RegistryService\u0026#34; /\u0026gt; \u0026lt;/beans\u0026gt; 创建Monitor服务的启动器 MonitorApp.java\npublic class MonitorApp { public static void main( String[] args ) throws IOException { Properties props = System.getProperties(); props.setProperty(\u0026#34;dubbo.spring.config\u0026#34;, \u0026#34;classpath:simple-monitor.xml\u0026#34;); props.setProperty(\u0026#34;dubbo.container\u0026#34;, \u0026#34;spring,jetty,registry\u0026#34;); props.setProperty(\u0026#34;dubbo.registry.address\u0026#34;, \u0026#34;zookeeper://127.0.0.1:2181\u0026#34;); props.setProperty(\u0026#34;dubbo.jetty.port\u0026#34;, \u0026#34;8889\u0026#34;); props.setProperty(\u0026#34;dubbo.jetty.directory\u0026#34;, \u0026#34;/tmp/monitor\u0026#34;); Main.main(args); } } 修改服务提供方与消费方的dubbo配置文件 在服务提供方与消费方的dubbo配置文件中添加Monitor相关配置\n\u0026lt;dubbo:monitor protocol=\u0026#34;registry\u0026#34;/\u0026gt; 最后访问http://${monitor_ip}:8889, 即可通过网页看到服务的状态，还可以以图形化的方式查看服务的调用频率和调用时间。\ndubbo管理控制台 dubbo还提供了图形化的管理控制台用以管理dubbo内部的服务。部署方法可参考这里\n总结 dubbo作为一个服务治理框架，提供的功能还是比较完备的，选项也很丰富，在微服务架构体系中可以得到较多应用。\n","permalink":"https://jeremyxu2010.github.io/2016/12/dubbo%E8%B5%B7%E6%AD%A5/","tags":["dubbox","微服务","rpc"],"title":"dubbo起步"},{"categories":["java开发"],"contents":"最近看了本书《Redis中文文档》，这本书写得挺好，讲了Redis的方方面面，在这里记录一下以备忘。\n相关概念 键空间通知 这个用得比较少，暂时不记录了。\n事务 大多数NOSQL数据库并不支持事务，可Redis提供有限的事务支持。之所以说是有限的事务支持，是因为客户端成功在开启事务之后执行 EXEC，在执行EXEC的过程中如果 Redis 服务器因为某些原因被管理员杀死，或者遇上某种硬件故障，那么可能只有部分事务命令会被成功写入到磁盘中。这时Redis 在重新启动时发现 AOF 文件出了这样的问题，那么它会退出，并汇报一个错误。使用redis-check-aof 程序可以修复这一问题：它会移除 AOF 文件中不完整事务的信息，确保服务器可以顺利启动。\nMULTI 、 EXEC 、 DISCARD 和 WATCH 是 Redis 事务的基础。\n事务使用范例：\n\u0026gt; MULTI OK \u0026gt; INCR foo QUEUED \u0026gt; INCR bar QUEUED \u0026gt; EXEC 1) (integer) 1 2) (integer) 1 发布与订阅 SUBSCRIBE 、 UNSUBSCRIBE 和 PUBLISH 三个命令实现了发布与订阅信息泛型（Publish/Subscribe messaging paradigm）， 在这个实现中， 发送者（发送信息的客户端）不是将信息直接发送给特定的接收者（接收信息的客户端）， 而是将信息发送给频道（channel）， 然后由频道将信息转发给所有对这个频道感兴趣的订阅者。\n发送者无须知道任何关于订阅者的信息， 而订阅者也无须知道是那个客户端给它发送信息， 它只要关注自己感兴趣的频道即可。\n发布与订阅使用范例：\n\u0026gt; SUBSCRIBE first second 1) \u0026quot;subscribe\u0026quot; 2) \u0026quot;first\u0026quot; 3) (integer) 1 1) \u0026quot;subscribe\u0026quot; 2) \u0026quot;second\u0026quot; 3) (integer) 2 #另一个客户端执行PUBLISH命令 \u0026gt; PUBLISH second Hello #前一个客户端则会收到消息 1) \u0026quot;message\u0026quot; 2) \u0026quot;second\u0026quot; 3) \u0026quot;hello\u0026quot; 复制 Redis 支持简单且易用的主从复制（master-slave replication）功能， 该功能可以让从服务器(slave server)成为主服务器(master server)的精确复制品。\n复制功能可以单纯地用于数据冗余（data redundancy）， 也可以通过让多个从服务器处理只读命令请求来提升扩展性（scalability）。另外由于从服务器是主服务器的精确复制品，于是在Redis集群里，从服务器可以很方便地接管主服务器，以达到自动故障迁移的目的。\n配置一个从服务器非常简单， 只要在配置文件中增加以下的这一行就可以了：\n\u0026gt; slaveof 192.168.1.1 6379 OK 通信协议 通信协议一般实现Redis客户端时会用到，日常使用倒是不会用到它，这里暂时不记录它了。\n持久化 相对于memcache来说，Redis一大优势是存在它里的数据是可以持久化的，即使重启Redis，数据依旧还在。\nRedis 提供了多种不同级别的持久化方式：\n RDB 持久化可以在指定的时间间隔内生成数据集的时间点快照（point-in-time snapshot）。 AOF 持久化记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。 AOF 文件中的命令全部以 Redis 协议的格式来保存，新命令会被追加到文件的末尾。 Redis 还可以在后台对 AOF 文件进行重写（rewrite），使得 AOF 文件的体积不会超出保存数据集状态所需的实际大小。 Redis 还可以同时使用 AOF 持久化和 RDB 持久化。 在这种情况下， 当 Redis 重启时， 它会优先使用 AOF 文件来还原数据集， 因为 AOF 文件保存的数据集通常比 RDB 文件所保存的数据集更完整。 你甚至可以关闭持久化功能，让数据只在服务器运行时存在。  # 手动让Redis进行数据集RDB持久化 \u0026gt; BGSAVE # 开启AOF持久化 \u0026gt; CONFIG SET appendonly yes # 手动让Redis对AOF文件进行重建 \u0026gt; BGREWRITEAOF Sentinel哨兵 Redis 的 Sentinel 系统用于管理多个 Redis 服务器（instance）， 该系统执行以下三个任务：\n 监控（Monitoring）： Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。 提醒（Notification）： 当被监控的某个 Redis 服务器出现问题时， Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。 自动故障迁移（Automatic failover）： 当一个主服务器不能正常工作时， Sentinel 会开始一次自动故障迁移操作， 它会将失效主服务器的其中一个从服务器升级为新的主服务器， 并让失效主服务器的其他从服务器改为复制新的主服务器； 当客户端试图连接失效的主服务器时， 集群也会向客户端返回新主服务器的地址， 使得集群可以使用新主服务器代替失效服务器。  Redis Sentinel 是一个分布式系统， 你可以在一个架构中运行多个 Sentinel 进程（progress）， 这些进程使用流言协议（gossip protocols)来接收关于主服务器是否下线的信息， 并使用投票协议（agreement protocols）来决定是否执行自动故障迁移， 以及选择哪个从服务器作为新的主服务器。\n集群 Redis 集群是一个可以在多个 Redis 节点之间进行数据共享的设施。 Redis 集群不支持那些需要同时处理多个键的 Redis 命令， 因为执行这些命令需要在多个 Redis 节点之间移动数据， 并且在高负载的情况下， 这些命令将降低 Redis 集群的性能， 并导致不可预测的行为。 Redis 集群通过分区（partition）来提供一定程度的可用性（availability）： 即使集群中有一部分节点失效或者无法进行通讯， 集群也可以继续处理命令请求。 Redis 集群提供了以下两个好处：\n 将数据自动切分（split）到多个节点的能力。 当集群中的一部分节点失效或者无法进行通讯时， 仍然可以继续处理命令请求的能力。  Redis 集群使用数据分片（sharding）而非一致性哈希（consistency hashing）来实现： 一个 Redis 集群包含 16384 个哈希槽（hash slot）， 数据库中的每个键都属于这 16384 个哈希槽的其中一个， 集群使用公式 CRC16(key) % 16384 来计算键 key 属于哪个槽， 其中 CRC16(key) 语句用于计算键 key 的 CRC16 校验和 。集群中的每个节点负责处理一部分哈希槽。\n#创建集群 ./redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 \\ 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 #对集群进行重新分片 ./redis-trib.rb reshard 127.0.0.1:7000 #接着回答几个问题，包括要移动的哈希槽的数量、目标节点的 ID、源节点信息，最后yes回车后，Redis集群就会开始重新分片操作 #检查集群是否正常 ./redis-trib.rb check 127.0.0.1:7000 #列出集群节点信息 ./redis-cli -p 7000 cluster nodes Redis命令 上一节主要是概括Redis的一些核心概念，Redis的部署运维时需了解这些概念。Redis与memcache最大的不同在于Redis拥有更多的数据结构和并支持更丰富的数据操作，而这些数据结构是通过一系列命令来完成的。\n键的通用命令 DEL、DUMP、RESTORE、EXISTS、EXPIRE、EXPIREAT、TTL、PERSIST、PEXPIRE、PEXPIREAT、PTTL、MIGRAGE、MOVE、OBJECT、RANDOMKEY、KEYS、RENAME、RENAMENX、SORT、TYPE、SCAN\nString相关命令 APPEND、BITCOUNT、BITOP、DECR、DECRBY、GET、GETBIT、GETRANGE、GETSET、INCR、INCRBY、INCRBYFLOAT、MGET、MSET、MSETNX、PSETEX、SET、SETBIT、SETEX、SETNX、SETRANGE、STRLEN\nList相关命令 BLPOP、BRPOP、BRPOPLPUSH、LINDEX、LINSERT、LLEN、LPOP、LPUSH、LRANGE、LREM、LSET、LTRIM、RPOP、RPOPLPUSH、RPUSH、RPUSHX\nHash相关命令 HDEL、HEXISTS、HGET、HGETALL、HINCRBY、HINCRBYFLOAT、HKEYS、HLEN、HMGET、HMSET、HSET、HSETNX、HVALS、HSCAN\nSet相关命令 SADD、SCARD、SDIFF、SDIFFSTORE、SINTER、SINTERSTORE、SISMEMBER、SMEMBERS、SMOVE、SPOP、SRANDMEMBER、SREM、SUNION、SUNIONSTORE、SSCAN\nSortedSet相关命令 ZADD、ZCARD、ZCOUNT、ZINCRBY、ZRANGE、ZRANGEBYSCORE、ZRANK、ZREM、ZREMRANGEBYRANK、ZREMRANGEBYSCORE、ZREVRANGE、ZREVRANGEBYSCORE、ZREVRANK、ZSCORE、ZUNIONSTORE、ZINTERSTORE、ZSCAN\n发布订阅相关命令 PSUBSCRIBE、PUBLISH、PUBSUB、PUNSUBSCRIBE、SUBSCRIBE、UNSUBSCRIBE\n事务相关命令 DISCARD、EXEC、MULTI、UNWATCH、WATCH\nScript相关命令 EVAL、EVALSHA、SCRIPT EXISTS、SCRIPT FLUSH、SCRIPT KILL、SCRIPT LOAD\nConnection相关命令 AUTH、ECHO、PING、QUIT、SELECT\nServer相关命令 BGREWRITEAOF、BGSAVE、CLIENT GETNAME、CLIENT KILL、CLIENT LIST、CLIENT SETNAME、CONFIG GET、CONFIG RESETSTAT、CONFIG REWRITE、CONFIG SET、DBSIZE、DEBUG OBJECT、DEBUG SEGFAULT、FLUSHALL、FLUSHDB、INFO、LASTSAVE、MONITOR、SAVE、SHUTDOWN、SLAVEOF、SLOWLOG、TIME\n虽然这里的命令看着很多，可大部分命令的方式很一致，实在不太清楚也可以查看在线文档。\n实际运用场景 显示最新的项目列表 直接用DB的实现方法可能是这样的：\nSELECT * FROM foo WHERE ... ORDER BY time DESC LIMIT 10 可随着表里的数据越来越多，这个方案性能越来越差。\n使用Redis可以这样设计：\n假设我们的一个Web应用想要列出用户贴出的最新20条评论。在最新的评论边上我们有一个“显示全部”的链接，点击后就可以获得更多的评论。数据库中的每条评论都有一个唯一的递增的ID字段。我们可以使用分页来制作主页和评论页，使用Redis的模板，每次新评论发表时，我们会将它的ID添加到一个Redis列表：\n#将ID添加到一个Redis列表 LPUSH latest.comments \u0026lt;ID\u0026gt; #Redis只需要保存最新的5000条评论 LTRIM latest.comments 0 5000 每次我们需要获取最新评论的项目范围时，我们调用一个函数来完成（使用伪代码）：\nFUNCTION get_latest_comments(start, num_items): id_list = redis.lrange(\u0026quot;latest.comments\u0026quot;,start,start+num_items - 1) IF id_list.length \u0026lt; num_items RETURN SQL_DB(\u0026quot;SELECT ... ORDER BY time DESC LIMIT num_items OFFSET start\u0026quot;) END RETURN SQL_DB(\u0026quot;SELECT ... where id in id_list\u0026quot;) END 过滤 有些时候你想要给不同的列表附加上不同的过滤器。如果过滤器的数量受到限制，你可以简单的为每个不同的过滤器使用不同的Redis列表。毕竟每个列表只有5000条项目，但Redis却能够使用非常少的内存来处理几百万条项目。\n使用Redis可以这样设计：\n假设每次往DB插入新记录后，我们根据过滤条件将记录的ID插入多个Redis列表里：\n#将ID添加到一个Redis列表 LPUSH keyword1.posts \u0026lt;ID\u0026gt; #Redis只需要保存最新的5000条评论 LTRIM keyword1.posts 0 5000 #将ID添加到一个Redis列表 LPUSH keyword2.posts \u0026lt;ID\u0026gt; #Redis只需要保存最新的5000条评论 LTRIM keyword2.posts 0 5000 #将ID添加到一个Redis列表 LPUSH keyword3.posts \u0026lt;ID\u0026gt; #Redis只需要保存最新的5000条评论 LTRIM keyword4.posts 0 5000 以后要查询某个过滤条件的记录就很方便了：\nFUNCTION get_posts_filter_by_keyword(keyword): id_list = redis.lrange(keyword + \u0026quot;.posts\u0026quot;,0,-1) RETURN SQL_DB(\u0026quot;SELECT ... where id in id_list\u0026quot;) END 排行榜相关 一个很普遍的需求是各种数据库的数据并非存储在内存中，因此在按得分排序以及实时更新这些几乎每秒钟都需要更新的功能上数据库的性能不够理想。\n在某个用户获得新得分时，则执行下面的命令：\nZINCRBY sorceboard \u0026lt;score\u0026gt; \u0026lt;userID\u0026gt; 要获取前100名高分用户就很简单：\nZREVRANGE sorceboard 0 99。 获取某个用户的排名也很简单：\nZRANK sorceboard \u0026lt;userID\u0026gt; 计数 记录某用户在单位时间内登录失败的次数，以后可根据失败的次数决定某些业务逻辑：\nINCR loginFailed:\u0026lt;userID\u0026gt; EXPIRE loginFailed:\u0026lt;userID\u0026gt; 60 限速 限制某API被请求的频率：\nFUNCTION LIMIT_API_CALL(ip): current = GET(ip) IF current != NULL AND current \u0026gt; 10 THEN ERROR \u0026quot;too many requests per second\u0026quot; ELSE value = INCR(ip) IF value == 1 THEN EXPIRE(value,1) END PERFORM_API_CALL() END 特定时间内的特定项目 另一项对于其他数据库很难，但Redis做起来却轻而易举的事就是统计在某段特点时间里有多少特定用户访问了某个特定资源。比如我想要知道某些特定的注册用户或IP地址，他们到底有多少访问了某篇文章。\n每次我获得一次新的页面浏览时我只需要这样做：\nSADD page:day_20161112:\u0026lt;pageID\u0026gt; \u0026lt;userID\u0026gt; 想知道特定用户的数量吗？只需要使用\nSCARD page:day_20161112:\u0026lt;pageID\u0026gt; 需要测试某个特定用户是否访问了这个页面？只需要使用\nSISMEMBER page:day_20161112:\u0026lt;pageID\u0026gt; \u0026lt;userID\u0026gt; 订阅与发布 Redis的Pub/Sub非常非常简单，运行稳定并且快速。支持模式匹配，能够实时订阅与取消频道。一些可靠性要求没那么高的事件订阅与发布是可以用Redis的Pub/Sub代替MQ方案的。\n队列 现代的互联网应用大量地使用了消息队列（Messaging）。消息队列不仅被用于系统内部组件之间的通信，同时也被用于系统跟其它服务之间的交互。消息队列的使用可以增加系统的可扩展性、灵活性和用户体验。非基于消息队列的系统，其运行速度取决于系统中最慢的组件的速度（注：短板效应）。而基于消息队列可以将系统中各组件解除耦合，这样系统就不再受最慢组件的束缚，各组件可以异步运行从而得以更快的速度完成各自的工作。\nlist push和list pop这样的Redis命令能够很方便的执行队列操作了，但能做的可不止这些：比如Redis还有list pop的变体命令，能够在列表为空时阻塞队列。\n一些可靠性要求没那么高的事件订阅与发布是可以用Redis的List方案代替MQ方案的。\n缓存 基本上memcache可以搞定的事儿，redis都可以搞定，而且redis重启后，数据还是持久的。因此memcache可以退休了。\n总结 Redis作为传统关系型数据库的补充，在某些特定场景确实极大地提升了数据查询效率。下一篇研究一下在Java里如何访问Redis。\n参考 《Redis中文文档》 http://www.redis.cn/documentation.html http://www.redis.cn/commands.html http://blog.csdn.net/hguisu/article/details/8836819\n","permalink":"https://jeremyxu2010.github.io/2016/11/redis%E7%A0%94%E7%A9%B6/","tags":["redis","java"],"title":"redis研究"},{"categories":["java开发"],"contents":"今天工作中需要给tomcat7配置SSL证书，以使用https访问tomcat服务。以前都是自签名，照着网上的文档完成的，这回有一点不同的是https证书已经从GoDaddy买回来了，配置过程中遇到了一点坑，这里记录一下。\ntomcat7配置SSL证书 从GoDaddy买来的证书包括3个文件，test.com.key, test.com.crt, godaddy_intermediate.crt。这里稍微解释一下，这3个文件。\ntest.com.key是私钥文件，文件内容如下：\n-----BEGIN RSA PRIVATE KEY----- ... -----END RSA PRIVATE KEY----- test.com.crt是私钥对应的证书，文件内容如下：\n-----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- godaddy_intermediate.crt是GoDaddy的一些中级证书，内容如下：\n-----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- 在网上查了下，需要先根据这3个文件生成p12文件，命令如下：\nopenssl pkcs12 -export -inkey test.com.key -in test.com.crt -chain -CAfile godaddy_intermediate.crt -out tomcatserver.p12 -name tomcatserver -passout pass:123456 不过我执行这行命令时，报错了：\nError unable to get issuer certificate getting chain. 又到处查阅文档，发现需要将Go_Daddy_Class_2_CA.pem与godaddy_intermediate.crt合并，得到ca_bundle.crt，以保证CA链可以到达根证书颁发节点，到处都是坑啊。\ncat /etc/ssl/certs/Go_Daddy_Class_2_CA.pem godaddy_intermediate.crt \u0026gt; ca_bundle.crt 然后再执行上面的命令，p12文件就生成好了。\nopenssl pkcs12 -export -inkey test.com.key -in test.com.crt -chain -CAfile ca_bundle.crt -out tomcatserver.p12 -name tomcatserver -passout pass:123456 用portecle打开看一下，这个KeyPair的证书详情表明CA链上确实有4个证书。\n再将p12的keystore转化为jks的keystore。\nkeytool -importkeystore -v -srckeystore tomcatserver.p12 -srcstoretype pkcs12 -srcstorepass 123456 -destkeystore tomcatserver.jks -deststoretype jks -deststorepass 123456 这样就得到了jks格式的keystore文件tomcatserver.jks。\n最后在tomcat7的server.xml修改配置。\n\u0026lt;Connector protocol=\u0026#34;org.apache.coyote.http11.Http11NioProtocol\u0026#34; port=\u0026#34;8443\u0026#34; maxThreads=\u0026#34;200\u0026#34; scheme=\u0026#34;https\u0026#34; secure=\u0026#34;true\u0026#34; SSLEnabled=\u0026#34;true\u0026#34; keystoreFile=\u0026#34;/somewhere/tomcatserver.jks\u0026#34; keystorePass=\u0026#34;123456\u0026#34; clientAuth=\u0026#34;false\u0026#34; sslProtocol=\u0026#34;TLS\u0026#34;/\u0026gt; Over!\n申请SSL证书的通用步骤 上一节的步骤有些曲折，仔细研究了下，发现如果按一定步骤从头来申请SSL证书，还是比较简单的。\n 生成私钥文件www.test.com.key  openssl genrsa -out www.test.com.key 2048  生成证书请求文件www.test.com.csr，下次证书过期了，还是同样方法生成一个新的证书请求文件  openssl req -new -sha256 -subj \u0026#34;/C=CN/ST=ProvinceName/L=CityName/O=OrgName/OU=OrgUnitName/CN=www.test.com\u0026#34; -key www.test.com.key -out www.test.com.csr  到SSL证书提供商那里填写在线申请表，并提交www.test.com.csr文件，提交申请 SSL证书提供商证书审核通过后，将得到ca_bundle.crt与www.test.com.crt，于是就有这三个文件：www.test.com.key、www.test.com.crt、ca_bundle.crt 用上述3个文件生成p12文件  openssl pkcs12 -export -inkey www.test.com.key -in www.test.com.crt -chain -CAfile ca_bundle.crt -out tomcatserver.p12 -name tomcatserver -passout pass:123456  将p12文件转换为jks文件  keytool -importkeystore -v -srckeystore tomcatserver.p12 -srcstoretype pkcs12 -srcstorepass 123456 -destkeystore tomcatserver.jks -deststoretype jks -deststorepass 123456 参考 http://www.oschina.net/question/2266279_221175 http://www.fourproc.com/2010/06/23/create-a-ssl-keystore-for-a-tomcat-server-using-openssl-.html https://sg.godaddy.com/zh/help/tomcat-4x5x6x-renew-a-certificate-5355 https://tomcat.apache.org/tomcat-7.0-doc/ssl-howto.html\n","permalink":"https://jeremyxu2010.github.io/2016/11/%E7%BB%99tomcat7%E9%85%8D%E7%BD%AEssl%E8%AF%81%E4%B9%A6/","tags":["java","ssl","tomcat"],"title":"给tomcat7配置SSL证书"},{"categories":["java开发"],"contents":"使用SSM做了好几个项目，今天突然想起来还是建一个脚手架工程，地址在这里，便于以后快速创建这类项目。\nSSM项目脚手架项目 在网上找到一个ssm项目的脚手架工程，我把它clone下来，做了少量修改，做出的修改如下：\n java包都改成personal.jeremyxu包下，也相应地修改了配置文件 springmvc的url-pattern修改为/api/* 拆分了spring配置文件，spring配置文件放置于resources/spring目录下 修改了db.xml里的property-placeholder定义，以允许外部覆盖配置文件  \u0026lt;context:property-placeholder location=\u0026#34;classpath:jdbc.properties,file:///external/jdbc_overwrite.properties\u0026#34; ignore-resource-not-found=\u0026#34;true\u0026#34;/\u0026gt;  修改README.md文件，说明如何覆盖默认的log4j.properties配置文件 修改jdbc.properties文件的注释，说明如何配置读写分离。至于mysql主从复制配置文件可参考这里  MySQL主从读写分离源码实现 上一节基本是拿别人已经搭好的ssm脚手架工程简单改了一下。不过在改动过程中还是加入了自己的一些想法，其中最重要的就是配置MySQL主从读写分离。这一小节简单分析一下这个功能源码层面是如何实现的。\n配置具体步骤 要实现MySQL主从读写分离，首先是配置MySQL服务主从复制，这个比较简单，不再赘述，可参考这里。\n然后再配置jdbc.properties文件。\n# 普通模式 jdbc_driverClass=com.mysql.jdbc.Driver jdbc_driverClass=com.mysql.jdbc.ReplicationDriver # 普通模式 jdbc:mysql://127.0.0.1:3306/test?useUnicode=true jdbc_url=jdbc:mysql:replication://master:3306,slave1:3306,slave2:3306/test?useUnicode=true jdbc_user=accessop jdbc_password=123456 最后在数据库事务管理定义处添加一些AOP advice，当遇到某些只读查询时，设置readonly。\n\u0026lt;!--事务管理--\u0026gt; \u0026lt;tx:advice id=\u0026#34;txAdvice\u0026#34; transaction-manager=\u0026#34;transactionManager\u0026#34;\u0026gt; \u0026lt;tx:attributes\u0026gt; \u0026lt;tx:method name=\u0026#34;select*\u0026#34; read-only=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;tx:method name=\u0026#34;find*\u0026#34; read-only=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;tx:method name=\u0026#34;get*\u0026#34; read-only=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;tx:method name=\u0026#34;*\u0026#34; /\u0026gt; \u0026lt;/tx:attributes\u0026gt; \u0026lt;/tx:advice\u0026gt; \u0026lt;bean id=\u0026#34;transactionManager\u0026#34; class=\u0026#34;org.springframework.jdbc.datasource.DataSourceTransactionManager\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;dataSource\u0026#34; ref=\u0026#34;dataSource\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; 源码实现分析 可以看到与普通模式最大的不同在于jdbc_driverClass，jdbc_url发生变化了。我从com.mysql.jdbc.ReplicationDriver分析起。\nReplicationDriver.java\npublic class ReplicationDriver extends NonRegisteringReplicationDriver implements java.sql.Driver { static { try { java.sql.DriverManager .registerDriver(new NonRegisteringReplicationDriver()); } catch (SQLException E) { throw new RuntimeException(\u0026#34;Can\u0026#39;t register driver!\u0026#34;); } } public ReplicationDriver() throws SQLException { // Required for Class.forName().newInstance() \t} } 这个代码比较简单，其中最重要的部分是在static代码块里注册驱动NonRegisteringReplicationDriver，这个也是一般jdbc驱动的写法。\nNonRegisteringReplicationDriver.java\npublic class NonRegisteringReplicationDriver extends NonRegisteringDriver { public NonRegisteringReplicationDriver() throws SQLException { super(); } public Connection connect(String url, Properties info) throws SQLException { Properties parsedProps = parseURL(url, info); if (parsedProps == null) { return null; } Properties masterProps = (Properties)parsedProps.clone(); Properties slavesProps = (Properties)parsedProps.clone(); // Marker used for further testing later on, also when \t// debugging \tslavesProps.setProperty(\u0026#34;com.mysql.jdbc.ReplicationConnection.isSlave\u0026#34;, \u0026#34;true\u0026#34;); String hostValues = parsedProps.getProperty(HOST_PROPERTY_KEY); if (hostValues != null) { StringTokenizer st = new StringTokenizer(hostValues, \u0026#34;,\u0026#34;); StringBuffer masterHost = new StringBuffer(); StringBuffer slaveHosts = new StringBuffer(); if (st.hasMoreTokens()) { String[] hostPortPair = parseHostPortPair(st.nextToken()); if (hostPortPair[HOST_NAME_INDEX] != null) { masterHost.append(hostPortPair[HOST_NAME_INDEX]); } if (hostPortPair[PORT_NUMBER_INDEX] != null) { masterHost.append(\u0026#34;:\u0026#34;); masterHost.append(hostPortPair[PORT_NUMBER_INDEX]); } } boolean firstSlaveHost = true; while (st.hasMoreTokens()) { String[] hostPortPair = parseHostPortPair(st.nextToken()); if (!firstSlaveHost) { slaveHosts.append(\u0026#34;,\u0026#34;); } else { firstSlaveHost = false; } if (hostPortPair[HOST_NAME_INDEX] != null) { slaveHosts.append(hostPortPair[HOST_NAME_INDEX]); } if (hostPortPair[PORT_NUMBER_INDEX] != null) { slaveHosts.append(\u0026#34;:\u0026#34;); slaveHosts.append(hostPortPair[PORT_NUMBER_INDEX]); } } if (slaveHosts.length() == 0) { throw SQLError.createSQLException( \u0026#34;Must specify at least one slave host to connect to for master/slave replication load-balancing functionality\u0026#34;, SQLError.SQL_STATE_INVALID_CONNECTION_ATTRIBUTE); } masterProps.setProperty(HOST_PROPERTY_KEY, masterHost.toString()); slavesProps.setProperty(HOST_PROPERTY_KEY, slaveHosts.toString()); } return new ReplicationConnection(masterProps, slavesProps); } } NonRegisteringReplicationDriver继承自普通模式驱动NonRegisteringDriver，覆盖了其public Connection connect(String url, Properties info) throws SQLException方法，解析jdbc_url，将其中第一个主机端口组与后面其它主机端口组解析出来，分别拼接为masterHost、slaveHosts。最后以构建好的masterProps、slavesProps构造ReplicationConnection，即一个包含主从连接的抽象概念连接。\nReplicationConnection.java\npublic ReplicationConnection(Properties masterProperties, Properties slaveProperties) throws SQLException { Driver driver = new Driver(); StringBuffer masterUrl = new StringBuffer(\u0026#34;jdbc:mysql://\u0026#34;); StringBuffer slaveUrl = new StringBuffer(\u0026#34;jdbc:mysql://\u0026#34;); String masterHost = masterProperties .getProperty(NonRegisteringDriver.HOST_PROPERTY_KEY); if (masterHost != null) { masterUrl.append(masterHost); } String slaveHost = slaveProperties .getProperty(NonRegisteringDriver.HOST_PROPERTY_KEY); if (slaveHost != null) { slaveUrl.append(slaveHost); } String masterDb = masterProperties .getProperty(NonRegisteringDriver.DBNAME_PROPERTY_KEY); masterUrl.append(\u0026#34;/\u0026#34;); if (masterDb != null) { masterUrl.append(masterDb); } String slaveDb = slaveProperties .getProperty(NonRegisteringDriver.DBNAME_PROPERTY_KEY); slaveUrl.append(\u0026#34;/\u0026#34;); if (slaveDb != null) { slaveUrl.append(slaveDb); } this.masterConnection = (com.mysql.jdbc.Connection) driver.connect( masterUrl.toString(), masterProperties); this.slavesConnection = (com.mysql.jdbc.Connection) driver.connect( slaveUrl.toString(), slaveProperties); this.currentConnection = this.masterConnection; } 上面的代码比较清楚了，就是以正常的连接模式使用masterProperties、slaveProperties构造两个普通的JDBC连接，并且设置当前连接currentConnection为masterConnection。\n如上所述，当数据库事务管理配置的的AOP advice执行时，会调用Connection的setReadOnly方法。我们看一下ReplicationConnection的setReadOnly方法。\npublic synchronized void setReadOnly(boolean readOnly) throws SQLException { if (readOnly) { if (currentConnection != slavesConnection) { switchToSlavesConnection(); } } else { if (currentConnection != masterConnection) { switchToMasterConnection(); } } } private synchronized void switchToMasterConnection() throws SQLException { swapConnections(this.masterConnection, this.slavesConnection); } private synchronized void switchToSlavesConnection() throws SQLException { swapConnections(this.slavesConnection, this.masterConnection); } private synchronized void swapConnections(Connection switchToConnection, Connection switchFromConnection) throws SQLException { String switchFromCatalog = switchFromConnection.getCatalog(); String switchToCatalog = switchToConnection.getCatalog(); if (switchToCatalog != null \u0026amp;\u0026amp; !switchToCatalog.equals(switchFromCatalog)) { switchToConnection.setCatalog(switchFromCatalog); } else if (switchFromCatalog != null) { switchToConnection.setCatalog(switchFromCatalog); } boolean switchToAutoCommit = switchToConnection.getAutoCommit(); boolean switchFromConnectionAutoCommit = switchFromConnection.getAutoCommit(); if (switchFromConnectionAutoCommit != switchToAutoCommit) { switchToConnection.setAutoCommit(switchFromConnectionAutoCommit); } int switchToIsolation = switchToConnection .getTransactionIsolation(); int switchFromIsolation = switchFromConnection.getTransactionIsolation(); if (switchFromIsolation != switchToIsolation) { switchToConnection .setTransactionIsolation(switchFromIsolation); } this.currentConnection = switchToConnection; } 当currentConnection与根据readOnly应该使用的Connection不是同一个时，就会发生currentConnection连接的切换，切换的过程还需要保证Catalog、AutoCommit、TransactionIsolation与切换前一致。至此MySQL的主从读写分离就完成了。\nMySQL的jdbc连接url连接多个MySQL服务分析 从上面的代码来看，当存在多个MySQL slave服务时，这些是由普通连接驱动NonRegisteringDriver完成的。也就是说普通的jdbc_url中主机端口组处也是可以设置多个主机服务的。这个功能以前倒是没用过。这里分析一下它的代码。\n先看看NonRegisteringDriver的connect方法。\npublic java.sql.Connection connect(String url, Properties info) throws SQLException { if (url != null) { if (StringUtils.startsWithIgnoreCase(url, LOADBALANCE_URL_PREFIX)) { return connectLoadBalanced(url, info); } else if (StringUtils.startsWithIgnoreCase(url, REPLICATION_URL_PREFIX)) { return connectReplicationConnection(url, info); } } Properties props = null; if ((props = parseURL(url, info)) == null) { return null; } try { Connection newConn = new com.mysql.jdbc.Connection(host(props), port(props), props, database(props), url); return newConn; } catch (SQLException sqlEx) { // Don\u0026#39;t wrap SQLExceptions, throw \t// them un-changed. \tthrow sqlEx; } catch (Exception ex) { throw SQLError.createSQLException(Messages .getString(\u0026#34;NonRegisteringDriver.17\u0026#34;) //$NON-NLS-1$ \t+ ex.toString() + Messages.getString(\u0026#34;NonRegisteringDriver.18\u0026#34;), //$NON-NLS-1$ \tSQLError.SQL_STATE_UNABLE_TO_CONNECT_TO_DATASOURCE); } } 这里可以看到实际上MySQL的jdbc_url支持三种URL_PREFIX，实现是四种。\nprivate static final String REPLICATION_URL_PREFIX = \u0026#34;jdbc:mysql:replication://\u0026#34;; private static final String URL_PREFIX = \u0026#34;jdbc:mysql://\u0026#34;; private static final String MXJ_URL_PREFIX = \u0026#34;jdbc:mysql:mxj://\u0026#34;; private static final String LOADBALANCE_URL_PREFIX = \u0026#34;jdbc:mysql:loadbalance://\u0026#34;; 我们最常用的是jdbc:mysql://，前面一节我也用到了jdbc:mysql:replication://，jdbc:mysql:loadbalance://可以针对多个MySQL服务采取不同的负载策略，平时也是用得着的。jdbc:mysql:mxj://与jdbc:mysql://很类似，只不过它会使用自定义的SocketFactory com.mysql.management.driverlaunched.ServerLauncherSocketFactory，我没有去阅读它的源码，不过从名称猜测如果使用这个，可以通过JMX管理MySQL连接。\n如果是普通的jdbc:mysql://，则会直接创建Connection。\ncom.mysql.jdbc.Connection#Connection方法。\nConnection(String hostToConnectTo, int portToConnectTo, Properties info, String databaseToConnectTo, String url) throws SQLException { this.charsetToNumBytesMap = new HashMap(); this.connectionCreationTimeMillis = System.currentTimeMillis(); this.pointOfOrigin = new Throwable(); // Stash away for later, used to clone this connection for Statement.cancel \t// and Statement.setQueryTimeout(). \t//  this.origHostToConnectTo = hostToConnectTo; this.origPortToConnectTo = portToConnectTo; this.origDatabaseToConnectTo = databaseToConnectTo; try { Blob.class.getMethod(\u0026#34;truncate\u0026#34;, new Class[] {Long.TYPE}); this.isRunningOnJDK13 = false; } catch (NoSuchMethodException nsme) { this.isRunningOnJDK13 = true; } this.sessionCalendar = new GregorianCalendar(); this.utcCalendar = new GregorianCalendar(); this.utcCalendar.setTimeZone(TimeZone.getTimeZone(\u0026#34;GMT\u0026#34;)); // \t// Normally, this code would be in initializeDriverProperties, \t// but we need to do this as early as possible, so we can start \t// logging to the \u0026#39;correct\u0026#39; place as early as possible...this.log \t// points to \u0026#39;NullLogger\u0026#39; for every connection at startup to avoid \t// NPEs and the overhead of checking for NULL at every logging call. \t// \t// We will reset this to the configured logger during properties \t// initialization. \t// \tthis.log = LogFactory.getLogger(getLogger(), LOGGER_INSTANCE_NAME); // We store this per-connection, due to static synchronization \t// issues in Java\u0026#39;s built-in TimeZone class... \tthis.defaultTimeZone = Util.getDefaultTimeZone(); if (\u0026#34;GMT\u0026#34;.equalsIgnoreCase(this.defaultTimeZone.getID())) { this.isClientTzUTC = true; } else { this.isClientTzUTC = false; } this.openStatements = new HashMap(); this.serverVariables = new HashMap(); this.hostList = new ArrayList(); if (hostToConnectTo == null) { this.host = \u0026#34;localhost\u0026#34;; this.hostList.add(this.host); } else if (hostToConnectTo.indexOf(\u0026#34;,\u0026#34;) != -1) { // multiple hosts separated by commas (failover) \tStringTokenizer hostTokenizer = new StringTokenizer( hostToConnectTo, \u0026#34;,\u0026#34;, false); while (hostTokenizer.hasMoreTokens()) { this.hostList.add(hostTokenizer.nextToken().trim()); } } else { this.host = hostToConnectTo; this.hostList.add(this.host); } this.hostListSize = this.hostList.size(); this.port = portToConnectTo; if (databaseToConnectTo == null) { databaseToConnectTo = \u0026#34;\u0026#34;; } this.database = databaseToConnectTo; this.myURL = url; this.user = info.getProperty(NonRegisteringDriver.USER_PROPERTY_KEY); this.password = info .getProperty(NonRegisteringDriver.PASSWORD_PROPERTY_KEY); if ((this.user == null) || this.user.equals(\u0026#34;\u0026#34;)) { this.user = \u0026#34;\u0026#34;; } if (this.password == null) { this.password = \u0026#34;\u0026#34;; } this.props = info; initializeDriverProperties(info); try { createNewIO(false); this.dbmd = new DatabaseMetaData(this, this.database); } catch (SQLException ex) { cleanup(ex); // don\u0026#39;t clobber SQL exceptions \tthrow ex; } catch (Exception ex) { cleanup(ex); StringBuffer mesg = new StringBuffer(); if (getParanoid()) { mesg.append(\u0026#34;Cannot connect to MySQL server on \u0026#34;); mesg.append(this.host); mesg.append(\u0026#34;:\u0026#34;); mesg.append(this.port); mesg.append(\u0026#34;.\\n\\n\u0026#34;); mesg.append(\u0026#34;Make sure that there is a MySQL server \u0026#34;); mesg.append(\u0026#34;running on the machine/port you are trying \u0026#34;); mesg .append(\u0026#34;to connect to and that the machine this software is \u0026#34; + \u0026#34;running on \u0026#34;); mesg.append(\u0026#34;is able to connect to this host/port \u0026#34; + \u0026#34;(i.e. not firewalled). \u0026#34;); mesg .append(\u0026#34;Also make sure that the server has not been started \u0026#34; + \u0026#34;with the --skip-networking \u0026#34;); mesg.append(\u0026#34;flag.\\n\\n\u0026#34;); } else { mesg.append(\u0026#34;Unable to connect to database.\u0026#34;); } mesg.append(\u0026#34;Underlying exception: \\n\\n\u0026#34;); mesg.append(ex.getClass().getName()); if (!getParanoid()) { mesg.append(Util.stackTraceToString(ex)); } throw SQLError.createSQLException(mesg.toString(), SQLError.SQL_STATE_COMMUNICATION_LINK_FAILURE); } } 这里代码比较多，但整个逻辑是根据参数，构造好hostList、port、database、user、password、props内部变量，最后调用createNewIO(false);建立数据库连接。\ncom.mysql.jdbc.Connection#createNewIO方法最后会根据上述内部变量建立数据库连接。\nif (getRoundRobinLoadBalance()) { hostIndex = getNextRoundRobinHostIndex(getURL(), this.hostList); } for (; hostIndex \u0026lt; this.hostListSize; hostIndex++) { if (hostIndex == 0) { this.hasTriedMasterFlag = true; } try { String newHostPortPair = (String) this.hostList .get(hostIndex); int newPort = 3306; String[] hostPortPair = NonRegisteringDriver .parseHostPortPair(newHostPortPair); String newHost = hostPortPair[NonRegisteringDriver.HOST_NAME_INDEX]; if (newHost == null || newHost.trim().length() == 0) { newHost = \u0026#34;localhost\u0026#34;; } if (hostPortPair[NonRegisteringDriver.PORT_NUMBER_INDEX] != null) { try { newPort = Integer .parseInt(hostPortPair[NonRegisteringDriver.PORT_NUMBER_INDEX]); } catch (NumberFormatException nfe) { throw SQLError.createSQLException( \u0026#34;Illegal connection port value \u0026#39;\u0026#34; + hostPortPair[NonRegisteringDriver.PORT_NUMBER_INDEX] + \u0026#34;\u0026#39;\u0026#34;, SQLError.SQL_STATE_INVALID_CONNECTION_ATTRIBUTE); } } this.io = new MysqlIO(newHost, newPort, mergedProps, getSocketFactoryClassName(), this, getSocketTimeout()); this.io.doHandshake(this.user, this.password, this.database); this.connectionId = this.io.getThreadId(); this.isClosed = false; // save state from old connection \tboolean oldAutoCommit = getAutoCommit(); int oldIsolationLevel = this.isolationLevel; boolean oldReadOnly = isReadOnly(); String oldCatalog = getCatalog(); // Server properties might be different \t// from previous connection, so initialize \t// again... \tinitializePropsFromServer(); if (isForReconnect) { // Restore state from old connection \tsetAutoCommit(oldAutoCommit); if (this.hasIsolationLevels) { setTransactionIsolation(oldIsolationLevel); } setCatalog(oldCatalog); } if (hostIndex != 0) { setFailedOverState(); queriesIssuedFailedOverCopy = 0; } else { this.failedOver = false; queriesIssuedFailedOverCopy = 0; if (this.hostListSize \u0026gt; 1) { setReadOnlyInternal(false); } else { setReadOnlyInternal(oldReadOnly); } } connectionGood = true; break; // low-level connection succeeded \t} catch (Exception EEE) { if (this.io != null) { this.io.forceClose(); } connectionNotEstablishedBecause = EEE; connectionGood = false; if (EEE instanceof SQLException) { SQLException sqlEx = (SQLException)EEE; String sqlState = sqlEx.getSQLState(); // If this isn\u0026#39;t a communications failure, it will probably never succeed, so \t// give up right here and now .... \tif ((sqlState == null) || !sqlState .equals(SQLError.SQL_STATE_COMMUNICATION_LINK_FAILURE)) { throw sqlEx; } } // Check next host, it might be up... \tif (getRoundRobinLoadBalance()) { hostIndex = getNextRoundRobinHostIndex(getURL(), this.hostList) - 1 /* incremented by for loop next time around */; } else if ((this.hostListSize - 1) == hostIndex) { throw new CommunicationsException(this, (this.io != null) ? this.io .getLastPacketSentTimeMs() : 0, EEE); } } } 这样可以看到， 如果设置了RoundRobinLoadBalance，则会根据RoundRobin规则，在多个MySQL服务里选择一个建立连接，否则仅按顺序逐个尝试建立MySQL连接，如果前面一个建立成功，则后面的不再再继续尝试。\n所以这里得到一个经验，如果设置了多个MySQL slave，为了多个slave服务的负载比较均衡，还是应该设置roundRobinLoadBalance参数，因此比较安全且合适的读写分离jdbc_url可能是下面这样的。\njdbc_url=jdbc:mysql:replication://master:3306,slave1:3306,slave2:3306/test?roundRobinLoadBalance=true\u0026amp;allowMasterDownConnections=true\u0026amp;allowSlavesDownConnections=true\u0026amp;readFromMasterNoSlaves=true\u0026amp;useUnicode=true 总结 MySQL的JDBC驱动功能还是挺丰富的，原来没有阅读代码，有很多功能其实并不清楚，这次认真阅读代码，对JDBC的使用有更深刻的认识了。\n","permalink":"https://jeremyxu2010.github.io/2016/11/ssm%E9%A1%B9%E7%9B%AE%E8%84%9A%E6%89%8B%E6%9E%B6/","tags":["java","spring","springmvc","mybatis"],"title":"SSM项目脚手架"},{"categories":["java开发"],"contents":"自从国内的oschina maven仓库镜像停止服务后，一直找不到稳定且速度快的maven仓库镜像，网上搜来搜去，都是说http://mirrors.ibiblio.org/pub/mirrors/maven2/与http://repository.jboss.org/nexus/content/groups/public这些，但我实际试过，很差劲，慢得很。今天偶然在网上发现了一个很稳定且速度快的maven仓库，是阿里云的，看来还是阿里爸爸有米啊。\n配置方法：\n在maven的settings.xml 文件里配置mirrors的子节点，添加如下mirror\n\u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;nexus-aliyun\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;central\u0026lt;/mirrorOf\u0026gt; \u0026lt;name\u0026gt;Nexus aliyun\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://maven.aliyun.com/nexus/content/groups/public\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt; ","permalink":"https://jeremyxu2010.github.io/2016/10/maven%E9%98%BF%E9%87%8C%E4%BA%91%E4%BB%93%E5%BA%93%E9%95%9C%E5%83%8F/","tags":["java","maven"],"title":"maven阿里云仓库镜像"},{"categories":["java开发"],"contents":"今天遇到一个人问我netty3与netty4有什么区别。因为我之前使用netty做过网络程序开发，心里还是有点谱的。很自然地就说到了一些主要区别\n 一些术语的变化，如Upstream变为了Inbound，Downstream变为了Outbound netty3对每个读或写的操作，还会额外创建一个新的ChannelBuffer对象，这带来了很大的GC压力，为了缓解频繁申请回收Buffer时的GC压力，引入了池化的ByteBufs，当然在使用完Buffer后要注意需使用BufUtil.release释放。  那人再问，还有其它区别吗？然后我就说不上来了，惭愧。\n回家查阅资料，终于将这个问题又深入理解了一次，这里记录一下以备忘。\n线程模型的变化 Netty 3.X 版本线程模型 Netty 3.X的I/O操作线程模型比较复杂，它的处理模型包括两部分：\nInbound：主要包括链路建立事件、链路激活事件、读事件、I/O异常事件、链路关闭事件等； Outbound：主要包括写事件、连接事件、监听绑定事件、刷新事件等。 我们首先分析下Inbound操作的线程模型：\n从上图可以看出，Inbound操作的主要处理流程如下：\n I/O线程（Work线程）将消息从TCP缓冲区读取到SocketChannel的接收缓冲区中； 由I/O线程负责生成相应的事件，触发事件向上执行，调度到ChannelPipeline中； I/O线程调度执行ChannelPipeline中Handler链的对应方法，直到业务实现的Last Handler; Last Handler将消息封装成Runnable，放入到业务线程池中执行，I/O线程返回，继续读/写等I/O操作； 业务线程池从任务队列中弹出消息，并发执行业务逻辑。  通过对Netty 3的Inbound操作进行分析我们可以看出，Inbound的Handler都是由Netty的I/O Work线程负责执行。\n下面我们继续分析Outbound操作的线程模型：\n从上图可以看出，Outbound操作的主要处理流程如下：\n业务线程发起Channel Write操作，发送消息；\n Netty将写操作封装成写事件，触发事件向下传播； 写事件被调度到ChannelPipeline中，由业务线程按照Handler Chain串行调用支持Downstream事件的Channel Handler; 执行到系统最后一个ChannelHandler，将编码后的消息Push到发送队列中，业务线程返回； Netty的I/O线程从发送消息队列中取出消息，调用SocketChannel的write方法进行消息发送。  Netty 4.X 版本线程模型 相比于Netty 3.X系列版本，Netty 4.X的I/O操作线程模型比较简答，它的原理图如下所示：\n从上图可以看出，Outbound操作的主要处理流程如下：\n I/O线程NioEventLoop从SocketChannel中读取数据报，将ByteBuf投递到ChannelPipeline，触发ChannelRead事件； I/O线程NioEventLoop调用ChannelHandler链，直到将消息投递到业务线程，然后I/O线程返回，继续后续的读写操作； 业务线程调用ChannelHandlerContext.write(Object msg)方法进行消息发送； 如果是由业务线程发起的写操作，ChannelHandlerInvoker将发送消息封装成Task，放入到I/O线程NioEventLoop的任务队列中，由NioEventLoop在循环中统一调度和执行。放入任务队列之后，业务线程返回； I/O线程NioEventLoop调用ChannelHandler链，进行消息发送，处理Outbound事件，直到将消息放入发送队列，然后唤醒Selector，进而执行写操作。  通过流程分析，我们发现Netty 4修改了线程模型，无论是Inbound还是Outbound操作，统一由I/O线程NioEventLoop调度执行。\n自己的进一步理解 上述这段对比分析摘自这里。说的已经比较清楚了，但我还是加上一些说明：\n netty4里第2步，I/O线程NioEventLoop调用ChannelHandler链，直到将消息投递到业务线程，这里netty并不直接将消息投递到业务线程，主要依赖于程序自行投递，一般方法无非是自行构造Task，将Task投递给自己的业务线程池。当然也可以在添加业务ChannelHandler时指定业务Handler运行所在的业务线程池，如下面的代码。  private final EventExecutorGroup businessGroup = new DefaultEventExecutorGroup(4); private final EventLoopGroup bossGroup = new NioEventLoopGroup(); private final EventLoopGroup workerGroup = new NioEventLoopGroup(); private void start() throws InterruptedException { ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup).channel(NioServerSocketChannel.class) .option(ChannelOption.SO_REUSEADDR, true) .childOption(ChannelOption.TCP_NODELAY, true) .childOption(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT) .handler(new CheckAcceptHandler()) .childHandler(new ChannelInitializer\u0026lt;SocketChannel\u0026gt;() { @Override public void initChannel(SocketChannel ch) throws Exception { ch.pipeline().addLast(\u0026#34;decoder\u0026#34;, new LineBasedFrameDecoder(4096)); ch.pipeline().addLast(\u0026#34;stringDecoder\u0026#34;, new StringDecoder(StandardCharsets.UTF_8)); ch.pipeline().addLast(\u0026#34;lineEncoder\u0026#34;, new LineEncoder(StandardCharsets.UTF_8)); //指定了业务handler运行所在的业务线程池  ch.pipeline().addLast(businessGroup, \u0026#34;logicHandler\u0026#34;, new ServerLogicalHandler()); } }); final int listenPort = 8888; b.bind(listenPort).sync(); }  如果原来的程序逻辑并没有使用单独的业务线程池的话，netty3与netty4在线程模型上就看不到变更了。当然非常不建议这么干，直接使用IO线程处理业务逻辑会极大地影响网络程序的处理性能。 netty3与netty4在线程模型上的变更，看着影响并不大，但其实会造成很多其它的问题，参见这里提到的4个问题，这些问题产生的根本原因均是由于线程模型发生变化造成的。  事件对象从ChannelHandler中消失了 在3.x时代，所有的I/O操作都会创建一个新的ChannelEvent对象，如下面的API\nvoid handleUpstream(ChannelHandlerContext ctx, ChannelEvent e) throws Exception; void handleDownstream(ChannelHandlerContext ctx, ChannelEvent e) throws Exception; 而netty4.x里，为了避免频繁创建与回收ChannelEvent对象所造成的GC压力，上述两个处理所有类型事件的接口被改成了多个接口。\nvoid channelRegistered(ChannelHandlerContext ctx); void channelUnregistered(ChannelHandlerContext ctx); void channelActive(ChannelHandlerContext ctx); void channelInactive(ChannelHandlerContext ctx); void channelRead(ChannelHandlerContext ctx, Object message); void bind(ChannelHandlerContext ctx, SocketAddress localAddress, ChannelPromise promise); void connect( ChannelHandlerContext ctx, SocketAddress remoteAddress, SocketAddress localAddress, ChannelPromise promise); void disconnect(ChannelHandlerContext ctx, ChannelPromise promise); void close(ChannelHandlerContext ctx, ChannelPromise promise); void deregister(ChannelHandlerContext ctx, ChannelPromise promise); void write(ChannelHandlerContext ctx, Object message, ChannelPromise promise); void flush(ChannelHandlerContext ctx); void read(ChannelHandlerContext ctx); ChannelHandlerContext类也被修改来反映上述提到的变化:\n// Before: ctx.sendUpstream(evt); // After: ctx.fireChannelRead(receivedMessage); 如果开发者有特殊需求，确实需要监听自己的事件类型，ChannelHandler也提供了一个处理器方法叫做userEventTriggered()。发送自定义事件类型可参考io.netty.handler.timeout.IdleStateHandler中发送IdleStateEvent的代码。处理自定义事件的代码如下\n@Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception { if (evt instanceof IdleStateEvent) { IdleStateEvent event = (IdleStateEvent) evt; if (event.state().equals(IdleState.READER_IDLE)) { System.out.println(\u0026#34;READER_IDLE\u0026#34;); // 超时关闭channel  ctx.close(); } else if (event.state().equals(IdleState.WRITER_IDLE)) { System.out.println(\u0026#34;WRITER_IDLE\u0026#34;); } else if (event.state().equals(IdleState.ALL_IDLE)) { System.out.println(\u0026#34;ALL_IDLE\u0026#34;); // 发送心跳  ctx.channel().write(\u0026#34;ping\\n\u0026#34;); } } super.userEventTriggered(ctx, evt); } 4.x的netty里Channel的write方法不再自动flush 3.x的netty里Channel的write方法会自动flush, 而netty4.x里不会了，这样程序员可以按照业务逻辑write响应，最后一次flush。但千万要记住最后必须flush了。当然也可以直接用writeAndFlush方法。\n入站流量挂起 3.x有一个由Channel.setReadable(boolean)提供的不是很明显的入站流量挂起机制。它引入了在ChannelHandler之间的复杂交互操作，同时处理器由于不正确实现而很容易互相干扰。 4.x里，新的名为read()的出站操作增加了。如果你使用Channel.config().setAutoRead(false)来关闭默认的auto-read标志，Netty不会读入任何东西，直到你显式地调用read()操作。一旦你发布的read()操作完成，同时通道再次停止读，一个名为channelReadComplete()的入站事件会触发一遍你能够重新发布另外的read()操作。你同样也可以拦截read()操作来执行更多高级的流量控制。\n如下面的代码\n@Override public void channelActive(ChannelHandlerContext ctx) throws Exception { //连接建立后，设置为不自动读入站流量  ctx.channel().config().setAutoRead(false); super.channelActive(ctx); } @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception { //这里可根据条件，决定是否再读一次  ctx.channel().read(); super.channelReadComplete(ctx); } 调度任意的任务到一个I/O线程里运行 当一个Channel被注册到EventLoopGroup时，Channel实际上是注册到由EventLoopGroup管理EventLoop中的一个。在4.x里，EventLoop实现了java.utilconcurrent.ScheduledExecutorService接口。这意味着用户可以在一个用户通道归属的I/O线程里执行或调度一个任意的Runnable或Callable。随着新的娘好定义的线程模型的到来（稍后会介绍），它变得极其容易地编写一个线程安全的处理器。\npublic class MyHandler extends ChannelOutboundHandlerAdapter { ... public void flush(ChannelHandlerContext ctx, ChannelFuture f) { ... ctx.flush(f); // Schedule a write timeout.  ctx.executor().schedule(new MyWriteTimeoutTask(), 30, TimeUnit.SECONDS); ... } } AttributeMap 在4.x里，一个名为AttributeMap的新接口被加入了，它被Channel和ChannelHandlerContext继承。作为替代，ChannelLocal和Channel.attachment被移除。这些属性会在他们关联的Channel被垃圾回收的同时回收。这个确实比原来方便不少。\npublic class MyHandler extends ChannelInboundHandlerAdapter\u0026lt;MyMessage\u0026gt; { private static final AttributeKey\u0026lt;MyState\u0026gt; STATE = new AttributeKey\u0026lt;MyState\u0026gt;(\u0026#34;MyHandler.state\u0026#34;); @Override public void channelRegistered(ChannelHandlerContext ctx) { ctx.attr(STATE).set(new MyState()); ctx.fireChannelRegistered(); } @Override public void messageReceived(ChannelHandlerContext ctx, MyMessage msg) { MyState state = ctx.attr(STATE).get(); } ... } ChannelFuture拆分为ChannelFuture和ChannelPromise 在4.x里，ChannelFuture已经被拆分为ChannelFuture和ChannelPromise了。这不仅仅是让异步操作里的生产者和消费者间的约定更明显，同样也是得在使用从链中返回的ChannelFuture更加安全，因为ChannelFuture的状态是不能改变的。 由于这个编号，一些方法现在都采用ChannelPromise而不是ChannelFuture来改变它的状态。两者的核心区别是ChannelFuture的状态是不可改变的，而ChannelPromise可以。\n如下面的代码\n@Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception { if (timeoutNanos \u0026gt; 0) { promise = promise.unvoid(); scheduleTimeout(ctx, promise); } //下面的方法会改变promise的状态 ctx.write(msg, promise); } 不再有ExecutionHandler 在4.x里，不再有ExecutionHandler，而是提供DefaultEventExecutorGroup，可以在添加业务ChannelHandler时指定业务Handler运行所在的业务线程池，如下面的代码。\nprivate final EventExecutorGroup businessGroup = new DefaultEventExecutorGroup(4); private final EventLoopGroup bossGroup = new NioEventLoopGroup(); private final EventLoopGroup workerGroup = new NioEventLoopGroup(); private void start() throws InterruptedException { ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup).channel(NioServerSocketChannel.class) .option(ChannelOption.SO_REUSEADDR, true) .childOption(ChannelOption.TCP_NODELAY, true) .childOption(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT) .handler(new CheckAcceptHandler()) .childHandler(new ChannelInitializer\u0026lt;SocketChannel\u0026gt;() { @Override public void initChannel(SocketChannel ch) throws Exception { ch.pipeline().addLast(\u0026#34;decoder\u0026#34;, new LineBasedFrameDecoder(4096)); ch.pipeline().addLast(\u0026#34;stringDecoder\u0026#34;, new StringDecoder(StandardCharsets.UTF_8)); ch.pipeline().addLast(\u0026#34;lineEncoder\u0026#34;, new LineEncoder(StandardCharsets.UTF_8)); //指定了业务handler运行所在的业务线程池  ch.pipeline().addLast(businessGroup, \u0026#34;logicHandler\u0026#34;, new ServerLogicalHandler()); } }); final int listenPort = 8888; b.bind(listenPort).sync(); } 灵活的I/O线程分配  在4.x里，能够从一个已存在的jdk套接字上创建一个Channel  java.nio.channels.SocketChannel mySocket = java.nio.channels.SocketChannel.open(); // Perform some blocking operation here. ... // Netty takes over. SocketChannel ch = new NioSocketChannel(mySocket); EventLoopGroup group = ...; group.register(ch);  在4.x里，能够取消注册和重新注册一个Channel从/到一个I/O线程。例如，你能够利用Netty提供的高层次无阻塞I/O的优势来解决复杂的协议，然后取消注册Channel并且切换到阻塞模式来在可能的最大吞吐量下传输一个文件。当然，它能够再次注册已经取消了注册的Channel。  java.nio.channels.FileChannel myFile = ...; java.nio.channels.SocketChannel mySocket = java.nio.channels.SocketChannel.open(); // Perform some blocking operation here. ... // Netty takes over. SocketChannel ch = new NioSocketChannel(mySocket); EventLoopGroup group = ...; group.register(ch); ... // Deregister from Netty. ch.deregister().sync(); // Perform some blocking operation here. mySocket.configureBlocking(true); myFile.transferFrom(mySocket, ...); // Register back again to another event loop group. EventLoopGroup anotherGroup = ...; anotherGroup.register(ch); 简化的关闭 在4.x里，releaseExternalResources()不必再用了。你可以通过调用EventLoopGroup.shutdown()直接地关闭所有打开的连接同时使所有I/O线程停止，就像你使用java.util.concurrent.ExecutorService.shutdown()关闭你的线程池一样。\n参考 http://www.infoq.com/cn/articles/netty-version-upgrade-history-thread-part https://www.oschina.net/translate/netty-4-0-new-and-noteworthy https://www.oschina.net/question/139577_146101 http://netty.io/wiki/new-and-noteworthy-in-4.0.html\n","permalink":"https://jeremyxu2010.github.io/2016/10/netty3%E4%B8%8Enetty4%E7%9A%84%E5%8C%BA%E5%88%AB/","tags":["java","nio","netty"],"title":"netty3与netty4的区别"},{"categories":["数据库开发"],"contents":"今天被人问到InnoDB与MyISAM的区别，突然发现虽然平时做项目时经常时经常用到这两种存储引擎，但却只知道两者在事务支持方面的区别，其它的竟一概不知。\n回到家立即查阅了相关资料，终于搞清楚这两者之间的真正差异，这里记录一下以备忘。\n两者之间的差异  MyISAM类型不支持事务处理等高级处理，而InnoDB类型支持 MyISAM类型的表强调的是性能，其执行数度比 InnoDB类型更快，但是不提供事务支持，而InnoDB提供事务支持以及外键等高级数据库功能 InnoDB不支持FULLTEXT类型的索引，而MyISAM支持 InnoDB 中不保存表的具体行数，也就是说，执行select count() from table时，InnoDB要扫描一遍整个表来计算有多少行，但是MyISAM只要简单的读出保存好的行数即可。注意的是，当count()语句包含 where条件时，两种表的操作是一样的 对于AUTO_INCREMENT类型的字段，InnoDB中必须包含只有该字段的索引，但是在MyISAM表中，可以和其他字段一起建立联合索引 DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的删除，MyISAM里会重新建立表 InnoDB支持行锁，MyISAM不支持。但InnoDB的行锁也不是绝对的，假如在执行一个SQL语句时MySQL不能确定要扫描的范围，InnoDB表同样会锁全表，例如update table set num=1 where name like “%a%” MyISAM的索引和数据是分开的，并且索引是有压缩的，内存使用率就对应提高了不少，能加载更多索引。而Innodb是索引和数据是紧密捆绑的，没有使用压缩从而会造成Innodb比MyISAM体积庞大不小  如何选择  数据量小，不太在乎读写性能，但需要事务、外键支持，可选用InnoDB 数据量大，读多写少，关注读写性能，可选用MyISAM，事务方面可用CAS的方案实现数据操作的原子性 MyISAM表由3个文件构成，可直接将这3个文件拷贝到其它数据库，即完成数据迁移，十分便捷 需要使用全文索引，则选用MyISAM 数据量大，关心数据存储的体积大小，可选用MyISAM  ","permalink":"https://jeremyxu2010.github.io/2016/10/innodb%E4%B8%8Emyisam%E7%9A%84%E5%8C%BA%E5%88%AB/","tags":["mysql","innoDB","MyISAM"],"title":"InnoDB与MyISAM的区别"},{"categories":["工具"],"contents":"之前有一篇博文里写到了如何使用shadowsocks翻墙，可最近感觉这种翻墙方案网络延迟大了很多。最让人难以接受的是每次在Google页面里输入搜索关键字敲回车后，要过很久很久，搜索结果才能显示出来。\n无意间在网上看到kcptun+shadowsocks加速翻墙的方案，一试果然效果很好，网络延迟小了很多，甚至可以看youtube上的1080P视频了。\nkcptun实现原理 Kcptun 是一个非常简单和快速的，基于 KCP 协议的 UDP 隧道，它可以将 TCP 流转换为 KCP+UDP 流。而 KCP 是一个快速可靠协议，能以比 TCP 浪费10%-20%的带宽的代价，换取平均延迟降低 30%-40%，且最大延迟降低三倍的传输效果。\nKcptun 是 KCP 协议的一个简单应用，可以用于任意 TCP 网络程序的传输承载，以提高网络流畅度，降低掉线情况。由于 Kcptun 使用 Go 语言编写，内存占用低（经测试，在64M内存服务器上稳定运行），而且适用于所有平台，甚至 Arm 平台。\nKcptun 工作示意图： 在vps上启动kcptun服务端 ./server_linux_amd64 -l :20086 -t 127.0.0.1:443 -mtu 1400 -sndwnd 2048 -rcvwnd 2048 -mode fast2 --crypt \u0026#34;aes\u0026#34; \u0026amp; 在本机启动kcptun客户端 ./client_darwin_amd64 -l :443 -r $serverip:20086 -mtu 1400 -mode fast2 -dscp 46 --crypt aes 修改shadowsocks-libev的配置文件 将/usr/local/etc/shadowsocks-libev.json文件中server修改为127.0.0.1，然后重启shadowsocks-libev服务。\n","permalink":"https://jeremyxu2010.github.io/2016/10/%E4%BD%BF%E7%94%A8kcptun%E5%8A%A0%E9%80%9F%E7%BF%BB%E5%A2%99/","tags":["fuckgfw","linux"],"title":"使用kcptun加速翻墙"},{"categories":["golang开发"],"contents":"接上一篇博文，这里是我阅读电子书《Network Programming with Go》后，书中一些重点的第二部分。\nHTTP 第8章主要讲到了golang中对HTTP的支持。\n首先是很关键的两个对象Request与Response，及一些常用的发送HTTP请求的方法。\n# Request对象 type Request struct { // Method specifies the HTTP method (GET, POST, PUT, etc.). // For client requests an empty string means GET. Method string // URL specifies either the URI being requested (for server // requests) or the URL to access (for client requests). // // For server requests the URL is parsed from the URI // supplied on the Request-Line as stored in RequestURI. For // most requests, fields other than Path and RawQuery will be // empty. (See RFC 2616, Section 5.1.2) // // For client requests, the URL's Host specifies the server to // connect to, while the Request's Host field optionally // specifies the Host header value to send in the HTTP // request. URL *url.URL // The protocol version for incoming server requests. // // For client requests these fields are ignored. The HTTP // client code always uses either HTTP/1.1 or HTTP/2. // See the docs on Transport for details. Proto string // \u0026quot;HTTP/1.0\u0026quot; ProtoMajor int // 1 ProtoMinor int // 0 // Header contains the request header fields either received // by the server or to be sent by the client. // // If a server received a request with header lines, // //\tHost: example.com //\taccept-encoding: gzip, deflate //\tAccept-Language: en-us //\tfOO: Bar //\tfoo: two // // then // //\tHeader = map[string][]string{ //\t\u0026quot;Accept-Encoding\u0026quot;: {\u0026quot;gzip, deflate\u0026quot;}, //\t\u0026quot;Accept-Language\u0026quot;: {\u0026quot;en-us\u0026quot;}, //\t\u0026quot;Foo\u0026quot;: {\u0026quot;Bar\u0026quot;, \u0026quot;two\u0026quot;}, //\t} // // For incoming requests, the Host header is promoted to the // Request.Host field and removed from the Header map. // // HTTP defines that header names are case-insensitive. The // request parser implements this by using CanonicalHeaderKey, // making the first character and any characters following a // hyphen uppercase and the rest lowercase. // // For client requests, certain headers such as Content-Length // and Connection are automatically written when needed and // values in Header may be ignored. See the documentation // for the Request.Write method. Header Header // Body is the request's body. // // For client requests a nil body means the request has no // body, such as a GET request. The HTTP Client's Transport // is responsible for calling the Close method. // // For server requests the Request Body is always non-nil // but will return EOF immediately when no body is present. // The Server will close the request body. The ServeHTTP // Handler does not need to. Body io.ReadCloser // ContentLength records the length of the associated content. // The value -1 indicates that the length is unknown. // Values \u0026gt;= 0 indicate that the given number of bytes may // be read from Body. // For client requests, a value of 0 means unknown if Body is not nil. ContentLength int64 // TransferEncoding lists the transfer encodings from outermost to // innermost. An empty list denotes the \u0026quot;identity\u0026quot; encoding. // TransferEncoding can usually be ignored; chunked encoding is // automatically added and removed as necessary when sending and // receiving requests. TransferEncoding []string // Close indicates whether to close the connection after // replying to this request (for servers) or after sending this // request and reading its response (for clients). // // For server requests, the HTTP server handles this automatically // and this field is not needed by Handlers. // // For client requests, setting this field prevents re-use of // TCP connections between requests to the same hosts, as if // Transport.DisableKeepAlives were set. Close bool // For server requests Host specifies the host on which the // URL is sought. Per RFC 2616, this is either the value of // the \u0026quot;Host\u0026quot; header or the host name given in the URL itself. // It may be of the form \u0026quot;host:port\u0026quot;. // // For client requests Host optionally overrides the Host // header to send. If empty, the Request.Write method uses // the value of URL.Host. Host string // Form contains the parsed form data, including both the URL // field's query parameters and the POST or PUT form data. // This field is only available after ParseForm is called. // The HTTP client ignores Form and uses Body instead. Form url.Values // PostForm contains the parsed form data from POST, PATCH, // or PUT body parameters. // // This field is only available after ParseForm is called. // The HTTP client ignores PostForm and uses Body instead. PostForm url.Values // MultipartForm is the parsed multipart form, including file uploads. // This field is only available after ParseMultipartForm is called. // The HTTP client ignores MultipartForm and uses Body instead. MultipartForm *multipart.Form // Trailer specifies additional headers that are sent after the request // body. // // For server requests the Trailer map initially contains only the // trailer keys, with nil values. (The client declares which trailers it // will later send.) While the handler is reading from Body, it must // not reference Trailer. After reading from Body returns EOF, Trailer // can be read again and will contain non-nil values, if they were sent // by the client. // // For client requests Trailer must be initialized to a map containing // the trailer keys to later send. The values may be nil or their final // values. The ContentLength must be 0 or -1, to send a chunked request. // After the HTTP request is sent the map values can be updated while // the request body is read. Once the body returns EOF, the caller must // not mutate Trailer. // // Few HTTP clients, servers, or proxies support HTTP trailers. Trailer Header // RemoteAddr allows HTTP servers and other software to record // the network address that sent the request, usually for // logging. This field is not filled in by ReadRequest and // has no defined format. The HTTP server in this package // sets RemoteAddr to an \u0026quot;IP:port\u0026quot; address before invoking a // handler. // This field is ignored by the HTTP client. RemoteAddr string // RequestURI is the unmodified Request-URI of the // Request-Line (RFC 2616, Section 5.1) as sent by the client // to a server. Usually the URL field should be used instead. // It is an error to set this field in an HTTP client request. RequestURI string // TLS allows HTTP servers and other software to record // information about the TLS connection on which the request // was received. This field is not filled in by ReadRequest. // The HTTP server in this package sets the field for // TLS-enabled connections before invoking a handler; // otherwise it leaves the field nil. // This field is ignored by the HTTP client. TLS *tls.ConnectionState // Cancel is an optional channel whose closure indicates that the client // request should be regarded as canceled. Not all implementations of // RoundTripper may support Cancel. // // For server requests, this field is not applicable. // // Deprecated: Use the Context and WithContext methods // instead. If a Request's Cancel field and context are both // set, it is undefined whether Cancel is respected. Cancel \u0026lt;-chan struct{} // Response is the redirect response which caused this request // to be created. This field is only populated during client // redirects. Response *Response // ctx is either the client or server context. It should only // be modified via copying the whole Request using WithContext. // It is unexported to prevent people from using Context wrong // and mutating the contexts held by callers of the same request. ctx context.Context } # Response对象 type Response struct { Status string // e.g. \u0026quot;200 OK\u0026quot; StatusCode int // e.g. 200 Proto string // e.g. \u0026quot;HTTP/1.0\u0026quot; ProtoMajor int // e.g. 1 ProtoMinor int // e.g. 0 // Header maps header keys to values. If the response had multiple // headers with the same key, they may be concatenated, with comma // delimiters. (Section 4.2 of RFC 2616 requires that multiple headers // be semantically equivalent to a comma-delimited sequence.) Values // duplicated by other fields in this struct (e.g., ContentLength) are // omitted from Header. // // Keys in the map are canonicalized (see CanonicalHeaderKey). Header Header // Body represents the response body. // // The http Client and Transport guarantee that Body is always // non-nil, even on responses without a body or responses with // a zero-length body. It is the caller's responsibility to // close Body. The default HTTP client's Transport does not // attempt to reuse HTTP/1.0 or HTTP/1.1 TCP connections // (\u0026quot;keep-alive\u0026quot;) unless the Body is read to completion and is // closed. // // The Body is automatically dechunked if the server replied // with a \u0026quot;chunked\u0026quot; Transfer-Encoding. Body io.ReadCloser // ContentLength records the length of the associated content. The // value -1 indicates that the length is unknown. Unless Request.Method // is \u0026quot;HEAD\u0026quot;, values \u0026gt;= 0 indicate that the given number of bytes may // be read from Body. ContentLength int64 // Contains transfer encodings from outer-most to inner-most. Value is // nil, means that \u0026quot;identity\u0026quot; encoding is used. TransferEncoding []string // Close records whether the header directed that the connection be // closed after reading Body. The value is advice for clients: neither // ReadResponse nor Response.Write ever closes a connection. Close bool // Uncompressed reports whether the response was sent compressed but // was decompressed by the http package. When true, reading from // Body yields the uncompressed content instead of the compressed // content actually set from the server, ContentLength is set to -1, // and the \u0026quot;Content-Length\u0026quot; and \u0026quot;Content-Encoding\u0026quot; fields are deleted // from the responseHeader. To get the original response from // the server, set Transport.DisableCompression to true. Uncompressed bool // Trailer maps trailer keys to values in the same // format as Header. // // The Trailer initially contains only nil values, one for // each key specified in the server's \u0026quot;Trailer\u0026quot; header // value. Those values are not added to Header. // // Trailer must not be accessed concurrently with Read calls // on the Body. // // After Body.Read has returned io.EOF, Trailer will contain // any trailer values sent by the server. Trailer Header // Request is the request that was sent to obtain this Response. // Request's Body is nil (having already been consumed). // This is only populated for Client requests. Request *Request // TLS contains information about the TLS connection on which the // response was received. It is nil for unencrypted responses. // The pointer is shared between responses and should not be // modified. TLS *tls.ConnectionState } # 常用发送HTTP请求方法 func Head(url string) (resp *Response, err error) func Get(url string) (resp *Response, err error) func Post(url string, bodyType string, body io.Reader) (resp *Response, err error) 如果需要对HTTP Request对象进行定制，可使用以下范式。\nrequest, err := http.NewRequest(\u0026quot;GET\u0026quot;, url.String(), nil) checkError(err) request.Header.Add(\u0026quot;Accept-Charset\u0026quot;, \u0026quot;UTF-8;q=1, ISO-8859-1;q=0\u0026quot;) client := \u0026amp;http.Client{} response, err := client.Do(request) ... 发送请求时使用HTTP代理也可以通过定制http.Client对象及HTTP Request对象来完成。\ntransport := \u0026amp;http.Transport{Proxy: http.ProxyURL(proxyURL)} client := \u0026amp;http.Client{Transport: transport} request, err := http.NewRequest(\u0026quot;GET\u0026quot;, url.String(), nil) auth := \u0026quot;jannewmarch:mypassword\u0026quot; basic := \u0026quot;Basic \u0026quot; + base64.StdEncoding.EncodeToString([]byte(auth)) request.Header.Add(\u0026quot;Proxy-Authorization\u0026quot;, basic) client := \u0026amp;http.Client{} response, err := client.Do(request) HTTP服务端主要涉及4个API\n# Handler接口 type Handler interface { ServeHTTP(ResponseWriter, *Request) } # 用于监听并开始伺服，此处handler可定制multiplexer，如handler为nil, 则为DefaultServeMux func ListenAndServe(addr string, handler Handler) error # 某一个url pattern采用`handler`这个Handler接口来处理 func Handle(pattern string, handler Handler) # 某一个url pattern采用`handler`这个请求处理函数来处理 func HandleFunc(pattern string, handler func(ResponseWriter, *Request)) # 将一个请求处理函数转化为实现Handler接口的对象 type HandlerFunc func(ResponseWriter, *Request) // ServeHTTP calls f(w, r). func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) { f(w, r) } 服务端代码的一般范式为\nfileServer := http.FileServer(http.Dir(\u0026quot;/var/www\u0026quot;)) # 使用http.Handle注册实现了Handler接口的对象来处理某个pattern的请求 http.Handle(\u0026quot;/\u0026quot;, fileServer) # 使用http.HandleFunc注册请求处理函数来处理某个pattern的请求 http.HandleFunc(\u0026quot;/cgi-bin/printenv\u0026quot;, printEnv) # 最后调用http.ListenAndServe监听并开始伺服 err := http.ListenAndServe(\u0026quot;:8000\u0026quot;, nil) checkError(err) 模板 动态页面技术免不了需要使用模板，第9章讲了模板相关内容\n使用模板的一般范式为\n# 定义模板字符串 temp := ` .... ` # 使用模板，参考数据形成输出 t := template.New(\u0026quot;Person template\u0026quot;) t, err := t.Parse(templ) if err == nil { buff := bytes.NewBufferString(\u0026quot;\u0026quot;) t.Execute(buff, person) } 模板里主要有以下写法\n{{.Name}} {{range .Emails}} ... {{end}} {{with .Jobs}} {{range .}} ... {{end}} {{end}} {{. | html}} 还可以定义函数，可在模板中使用\nconst templ = `The name is {{.Name}}. {{range .Emails}} An email is \u0026quot;{{. | emailExpand}}\u0026quot; {{end}} ` t := template.New(\u0026quot;Person template\u0026quot;) // add our function t = t.Funcs(template.FuncMap{\u0026quot;emailExpand\u0026quot;: EmailExpander}) t, err := t.Parse(templ) checkError(err) err = t.Execute(os.Stdout, person) checkError(err) 在模板中定义变量\nconst templ = `{{$name := .Name}} {{range .Emails}} Name is {{$name}}, email is {{.}} {{end}} 最后一小节在讲到根据条件进行输出时，讲到两种方法输出序列间的逗号。\n# 使用if判断循环的索引 {{range $index, $elmt := .Emails}} {{if $index}} , \u0026quot;{{$elmt}}\u0026quot; {{else}} \u0026quot;{{$elmt}}\u0026quot; {{end}} {{end}} # 利用定义的函数根据条件输出逗号 tmpl := `{{$comma := sequence \u0026quot;\u0026quot; \u0026quot;, \u0026quot;}} {{range $}} {{$comma.Next}}{{.}} {{end}}` var fmap = template.FuncMap{ \u0026quot;sequence\u0026quot;: sequenceFunc } t, err := template.New(\u0026quot;\u0026quot;).Funcs(fmap).Parse(tmpl) if err != nil { fmt.Printf(\u0026quot;parse error: %v\\n\u0026quot;, err) return } err = t.Execute(os.Stdout, []string{\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;, \u0026quot;e\u0026quot;, \u0026quot;f\u0026quot;}) type generator struct { ss []string i int f func(s []string, i int) string } func (seq *generator) Next() string { s := seq.f(seq.ss, seq.i) seq.i++ return s } func sequenceGen(ss []string, i int) string { if i \u0026gt;= len(ss) { return ss[len(ss)-1] } return ss[i] } func sequenceFunc(ss ...string) (*generator, error) { if len(ss) == 0 { return nil, errors.New(\u0026quot;sequence must have at least one element\u0026quot;) } return \u0026amp;generator{ss, 0, sequenceGen}, nil } 最后利用前面讲的知识，写了一个较完整的HTTP Server。\nXML 第12章讲了golang对XML的处理，这里重点是XML对应结构体中属性的tag值。\ntype Person struct { XMLName Name `xml:\u0026quot;person\u0026quot;` Name Name `xml:\u0026quot;name\u0026quot;` Email []Email `xml:\u0026quot;email\u0026quot;` } type Name struct { Family string `xml:\u0026quot;family\u0026quot;` Personal string `xml:\u0026quot;personal\u0026quot;` } type Email struct { Type string `xml:\u0026quot;type,attr\u0026quot;` Address string `xml:\u0026quot;,chardata\u0026quot;` } str := `\u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;utf-8\u0026quot;?\u0026gt; \u0026lt;person\u0026gt; \u0026lt;name\u0026gt; \u0026lt;family\u0026gt; Newmarch \u0026lt;/family\u0026gt; \u0026lt;personal\u0026gt; Jan \u0026lt;/personal\u0026gt; \u0026lt;/name\u0026gt; \u0026lt;email type=\u0026quot;personal\u0026quot;\u0026gt; jan@newmarch.name \u0026lt;/email\u0026gt; \u0026lt;email type=\u0026quot;work\u0026quot;\u0026gt; j.newmarch@boxhill.edu.au \u0026lt;/email\u0026gt; \u0026lt;/person\u0026gt;` # XML的unmarshal操作 var person Person err := xml.Unmarshal([]byte(str), \u0026amp;person) checkError(err) # XML的marshal操作 bb, err := xml.Marshal(person) checkError(err) str2 := string(bb) RPC 第13章主要讲了在golang里几种RPC编写范式。\n# HTTP RPC Server arith := new(Arith) rpc.Register(arith) rpc.HandleHTTP() err := http.ListenAndServe(\u0026quot;:1234\u0026quot;, nil) if err != nil { fmt.Println(err.Error()) } # HTTP RPC Client client, err := rpc.DialHTTP(\u0026quot;tcp\u0026quot;, \u0026quot;127.0.0.1:1234\u0026quot;) if err != nil { log.Fatal(\u0026quot;dialing:\u0026quot;, err) } // Synchronous call args := Args{17, 8} var reply int err = client.Call(\u0026quot;Arith.Multiply\u0026quot;, args, \u0026amp;reply) # TCP RPC Server arith := new(Arith) rpc.Register(arith) tcpAddr, err := net.ResolveTCPAddr(\u0026quot;tcp\u0026quot;, \u0026quot;:1234\u0026quot;) checkError(err) listener, err := net.ListenTCP(\u0026quot;tcp\u0026quot;, tcpAddr) checkError(err) rpc.Accept(listener) # TCP RPC Client client, err := rpc.Dial(\u0026quot;tcp\u0026quot;, \u0026quot;127.0.0.1:1234\u0026quot;) if err != nil { log.Fatal(\u0026quot;dialing:\u0026quot;, err) } // Synchronous call args := Args{17, 8} var reply int err = client.Call(\u0026quot;Arith.Multiply\u0026quot;, args, \u0026amp;reply) # JSON RPC Server arith := new(Arith) rpc.Register(arith) tcpAddr, err := net.ResolveTCPAddr(\u0026quot;tcp\u0026quot;, \u0026quot;:1234\u0026quot;) checkError(err) listener, err := net.ListenTCP(\u0026quot;tcp\u0026quot;, tcpAddr) checkError(err) for { conn, err := listener.Accept() if err != nil { continue } jsonrpc.ServeConn(conn) } # JSON RPC Client client, err := jsonrpc.Dial(\u0026quot;tcp\u0026quot;, \u0026quot;127.0.0.1:1234\u0026quot;) if err != nil { log.Fatal(\u0026quot;dialing:\u0026quot;, err) } // Synchronous call args := Args{17, 8} var reply int err = client.Call(\u0026quot;Arith.Multiply\u0026quot;, args, \u0026amp;reply) netchan 第14章讲到的netchan包在Go 1.7.1上未找到，可能是早期版本提供的包，这里就不总结了。\nWebSockets 第15章主要讲websocket在golang里的应用，在我这里websocket相关支持在golang.org/x/net/websocket包里。\nServer端范式如下。\n# 这里ws是Conn类型，因此可对其进行读写 func echoServer(ws *Conn) { defer ws.Close() io.Copy(ws, ws) } func main() { http.Handle(\u0026quot;/\u0026quot;, websocket.Handler(echoServer)) err := http.ListenAndServe(\u0026quot;:12345\u0026quot;, nil) checkError(err) } # 使用websocket.Message读写string msgToSend := \u0026quot;Hello\u0026quot; err := websocket.Message.Send(ws, msgToSend) var msgToReceive string err := websocket.Message.Receive(conn, \u0026amp;msgToReceive) # 使用websocket.Message读写byte slice dataToSend := []byte{0, 1, 2} err := websocket.Message.Send(ws, dataToSend) var dataToReceive []byte err := websocket.Message.Receive(conn, \u0026amp;dataToReceive) # 使用websocket.JSON读写json person := Person{Name: \u0026quot;Jan\u0026quot;, Emails: []string{\u0026quot;ja@newmarch.name\u0026quot;, \u0026quot;jan.newmarch@gmail.com\u0026quot;}, } err = websocket.JSON.Send(conn, person) var person Person err := websocket.JSON.Receive(ws, \u0026amp;person) # 创造自定义编解码器 func xmlMarshal(v interface{}) (msg []byte, payloadType byte, err error) { msg, err = xml.Marshal(v) return msg, websocket.TextFrame, nil } func xmlUnmarshal(msg []byte, payloadType byte, v interface{}) (err error) { err = xml.Unmarshal(msg, v) return err } var XMLCodec = websocket.Codec{xmlMarshal, xmlUnmarshal} person := Person{Name: \u0026quot;Jan\u0026quot;, Emails: []string{\u0026quot;ja@newmarch.name\u0026quot;, \u0026quot;jan.newmarch@gmail.com\u0026quot;}, } err = XMLCodec.Send(conn, person) var person Person err := XMLCodec.Receive(ws, \u0026amp;person) 最后一小节演示了HTTP Server使用TLS socket的办法。\nerr := http.ListenAndServeTLS(\u0026quot;:12345\u0026quot;, \u0026quot;jan.newmarch.name.pem\u0026quot;, \u0026quot;private.pem\u0026quot;, nil) 总结 花了点时间将这本电子书《Network Programming with Go》的重点都抓出来了，同时在抓取的过程写了不少小例子进行测试学习，这样学习印象就挺深刻了。以后学习其它东西也可以仿照这个，多记一些笔记。\n","permalink":"https://jeremyxu2010.github.io/2016/10/network-programming-with-go%E9%98%85%E8%AF%BB%E9%87%8D%E7%82%B9%E5%A4%87%E5%BF%98%E4%BA%8C/","tags":["golang","network"],"title":"《Network Programming with Go》阅读重点备忘（二）"},{"categories":["golang开发"],"contents":"最近读了一本老外写的电子书《Network Programming with Go》，感觉写得还可以，在这里将书中一些重点记录一下以备忘。\n架构 跟其它书不同，这本书的第一章主要讲了分布式系统与传统单机系统在架构层面的区别。其中有4节我觉得还挺重要的，设计分布式系统时可以多参考这些方面。\n ###Points of Failure###\n  Distributed applications run in a complex environment. This makes them much more prone to failure than standalone applications on a single computer. The points of failure include\n    The client side of the application could crash The client system may have h/w problems The client's network card could fail Network contention could cause timeouts There may be network address conflicts Network elements such as routers could fail Transmission errors may lose messages The client and server versions may be incompatible The server's network card could fail The server system may have h/w problems The server s/w may crash The server's database may become corrupted   Applications have to be designed with these possible failures in mind. Any action performed by one component must be recoverable if failure occurs in some other part of the system. Techniques such as transactions and continuous error checking need to be employed to avoid errors.\n  ###Acceptance Factors###\n    Reliability Performance Responsiveness Scalability Capacity Security   ###Transparency###\n  The \u0026ldquo;holy grails\u0026rdquo; of distributed systems are to provide the following:\n    access transparency location transparency migration transparency replication transparency concurrency transparency scalability transparency performance transparency failure transparency   ###Eight fallacies of distributed computing###\n  Sun Microsystems was a company that performed much of the early work in distributed systems, and even had a mantra \u0026ldquo;The network is the computer.\u0026rdquo; Based on their experience over many years a number of the scientists at Sun came up with the following list of fallacies commonly assumed:\n    The network is reliable. Latency is zero. Bandwidth is infinite. The network is secure. Topology doesn't change. There is one administrator. Transport cost is zero. The network is homogeneous.   Many of these directly impact on network programming. For example, the design of most remote procedure call systems is based on the premise that the network is reliable so that a remote procedure call will behave in the same way as a local call. The fallacies of zero latency and infinite bandwidth also lead to assumptions about the time duration of an RPC call being the same as a local call, whereas they are magnitudes of order slower.\n  The recognition of these fallacies led Java's RMI (remote method invocation) model to require every RPC call to potentially throw a RemoteException. This forced programmers to at least recognise the possibility of network error and to remind them that they could not expect the same speeds as local calls.\n TCP、UDP与Raw Socket 第三章就讲到golang里面的socket编程了。\n首先是IP地址与端口服务相关概念及关键类型。 type IP []byte # IP类型 ip := net.ParseIP(ipStr) # 由string解析出IP类型 type IPMask []byte # IPMask类型 func IPv4Mask(a, b, c, d byte) IPMask # 得到一个IPMask func (ip IP) DefaultMask() IPMask # 得到一个IP地址默认的IPMask func (ip IP) Mask(mask IPMask) IP # 得到一个IP地址所对应的网络地址 type IPAddr { IP IP } # IPAddr类型 func ResolveIPAddr(net, addr string) (*IPAddr, os.Error) # 由string解析出一个IPAddr， 如net.ResolveIPAddr(\u0026quot;ip4\u0026quot;, \u0026quot;192.168.1.1\u0026quot;) func LookupHost(name string) (cname string, addrs []string, err os.Error) # 根据主机名查找其对应的CName与IP地址 func LookupPort(network, service string) (port int, err os.Error) # 查询一个服务的默认端口， 如port, err := net.LookupPort(\u0026quot;tcp\u0026quot;, \u0026quot;telnet\u0026quot;) type TCPAddr struct { IP IP Port int } # TCPAddr类型 func ResolveTCPAddr(net, addr string) (*TCPAddr, os.Error) # 解析出一个TCPAddr类型， 如addr, err := net.ResolveTCPAddr(\u0026quot;tcp\u0026quot;, \u0026quot;192.168.1.1:23\u0026quot;) 然后TCP相关API # TCPConn实现了Writer与Reader接口 func (c *TCPConn) Write(b []byte) (n int, err os.Error) func (c *TCPConn) Read(b []byte) (n int, err os.Error) # 客户端连接TCP，如conn, err := net.DialTCP(\u0026quot;tcp\u0026quot;, nil, addr) func DialTCP(net string, laddr, raddr *TCPAddr) (c *TCPConn, err os.Error) # 服务端监听TCP，如listener, err := net.ListenTCP(\u0026quot;tcp\u0026quot;, addr) func ListenTCP(net string, laddr *TCPAddr) (l *TCPListener, err os.Error) # 服务端接受了一个连接，如conn, err := listener.Accept() func (l *TCPListener) Accept() (c Conn, err os.Error) TCP服务端的一般范式为\nservice := \u0026quot;:1201\u0026quot; tcpAddr, err := net.ResolveTCPAddr(\u0026quot;tcp\u0026quot;, service) checkError(err) listener, err := net.ListenTCP(\u0026quot;tcp\u0026quot;, tcpAddr) checkError(err) for { conn, err := listener.Accept() if err != nil { continue } // run as a goroutine go handleClient(conn) } UDP相关API # 解析出UDP的地址 func ResolveUDPAddr(net, addr string) (*UDPAddr, os.Error) # 客户端连接UDP， 如conn, err := net.DialUDP(\u0026quot;udp\u0026quot;, nil, addr) func DialUDP(net string, laddr, raddr *UDPAddr) (c *UDPConn, err os.Error) # 服务端监听UDP，如conn, err := net.ListenUDP(\u0026quot;udp\u0026quot;, addr) func ListenUDP(net string, laddr *UDPAddr) (c *UDPConn, err os.Error) # 服务端读写UDP包数据 func (c *UDPConn) ReadFromUDP(b []byte) (n int, addr *UDPAddr, err os.Error func (c *UDPConn) WriteToUDP(b []byte, addr *UDPAddr) (n int, err os.Error) UDP服务端的一般范式为\nservice := \u0026quot;:1200\u0026quot; udpAddr, err := net.ResolveUDPAddr(\u0026quot;udp\u0026quot;, service) checkError(err) conn, err := net.ListenUDP(\u0026quot;udp\u0026quot;, udpAddr) checkError(err) for { handleClient(conn) } Raw Socket相关API # 客户端连接Raw Socket func DialIP(netProto string, laddr, raddr *IPAddr) (*IPConn, error) # 服务端监听Raw Socket func ListenIP(netProto string, laddr *IPAddr) (*IPConn, error) # 客户端或服务端读写Raw Socket func (c *IPConn) ReadFromIP(b []byte) (int, *IPAddr, error) func (c *IPConn) WriteToIP(b []byte, addr *IPAddr) (int, error) 更范型化的接口\n# 客户端连接服务端，如conn, err := net.Dial(\u0026quot;tcp\u0026quot;, nil, addr) 或 conn, err := net.Dial(\u0026quot;udp\u0026quot;, nil, addr) 或 conn, err := net.Dial(\u0026quot;ip\u0026quot;, nil, addr) func Dial(net, laddr, raddr string) (c Conn, err os.Error) # 服务端监听TCP，如listener, err := net.Listen(\u0026quot;tcp\u0026quot;, addr) func Listen(net, laddr string) (l Listener, err os.Error) # 服务端接受TCP连接, 如conn, err := listener.Accept() func (l Listener) Accept() (c Conn, err os.Error) # 服务端监听UDP, 如conn, err := net.ListenPacket(\u0026quot;udp\u0026quot;, addr) func ListenPacket(net, laddr string) (c PacketConn, err os.Error) # 服务端读写UDP包数据 type PacketConn interface { ReadFrom(b []byte) (n int, addr Addr, err error) WriteTo(b []byte, addr Addr) (n int, err error) } 选择接口的原则  The Go net package recommends using these interface types rather than the concrete ones. But by using them, you lose specific methods such as SetKeepAlive or TCPConn and SetReadBuffer of UDPConn, unless you do a type cast. It is your choice.\n 数据序列化 第四章讲的是数据序列化方案\nasn1序列化 # 将对象marshal为字节数组 func Marshal(val interface{}) ([]byte, error) # 从字节数组unmarshal回对象 func Unmarshal(b []byte, val interface{}) (rest []byte, err error) 这个主要用于读写X.509证书。\njson序列化 # 获得一个json encoder encoder := json.NewEncoder(writer) # 将对象序列化 err := encoder.Encode(obj) encoder.Close() # 获得一个json decoder decoder := json.NewDecoder(reader) # 将对象反序列化 err := decoder.Decode(\u0026amp;obj) json序列化方案功能比较完善，语言互操作性好，序列化后结果也可读便于查错。但其基于文本，性能与大小可能没有基于字节流的好。\ngob序列化 # 获得一个gob encoder encoder := gob.NewEncoder(writer) # 将对象序列化 err := encoder.Encode(obj) encoder.Close() # 获得一个gob decoder decoder := gob.NewDecoder(reader) # 将对象反序列化 err := decoder.Decode(\u0026amp;obj) golang语言特有的序列化方案，功能完善，高效，但语言互操作性不好。\n字节数组序列化为string base64序列化方案\n# 获得一个base64 encoder encoder := base64.NewEncoder(base64.StdEncoding, writer) # 将对象序列化 err := encoder.Encode(obj) encoder.Close() # 直接进行base64序列化 str := base64.StdEncoding.EncodeToString(obj) # 获得一个base64 decoder decoder := base64.NewDecoder(base64.StdEncoding, reader) # 将对象反序列化 err := decoder.Decode(\u0026amp;obj) # 直接进行base64反序列化 bb, err := base64.StdEncoding.DecodeString(str) 可以看到golang里各种编码器的API很类似，用会一个，其它就举一反三了，这点很好。\n应用层协议设计 第5章主要讲应用层如何设计。其中讲到的概念其实以前做网络编程都涉及过，只不过在这章归纳总结后，更清晰了。\n应用层协议设计主要有以下方面需要注意：\n 传输层协议选择 是否有回应？如何处理回应丢失，超时。 传输的数据格式，基于文本还是基于字节流。 传输的数据包格式设计 协议的版本控制 应用的状态如何转换 如何控制服务质量 最终目标文件是一个独立的程序还是一个供第三方调用的库  详细的说明可参阅Application-Level Protocols\n上述这些问题并没有一个确定性的答案，需要根据具体场景作决策。\n这里针对两种不同的数据格式，服务端的代码范式如下\n# 基于字节流的数据格式 handleClient(conn) { while (true) { byte b = conn.readByte() switch (b) { case MSG_TYPE_1: ... case MSG_TYPE_2: ... ... } } } # 基于文本的数据格式 handleClient(conn) { while (true) { line = conn.readLine() if (line.startsWith(...) { ... } else if (line.startsWith(...) { ... } } } 字符集与文字编码 第6章主要讲了字符集与文字编码问题，这里有一个概念要理解一下。\n 字符： 某种语言中一个独立的符号，可能是一个字母，一个汉字，一个标点符号等。 字符集： 多个同类型的字符组成的一个集合，比如ASCII字符集、GBK字符集、BIG5字符集、Unicode字符集等。 字符code: 字符在某个字符集中对应的整体值。比如在ASCII字符集中字母'A'的字符code为65。 字符编码：将字符code编码为计算机真正可识别的字节数组，比如ASCII字符编码、GBK字符编码、UTF16字符编码、UTF8字符编码。  golang内部使用UTF-8字符编码对字符串进行编码，UTF-8字符编码是一种针对Unicode的可变长度字符编码。因此在编程中会用到以下方法。\nstr := \u0026quot;百度一下，你就知道\u0026quot; # 得到字符串进行UTF-8编码后最后字节数组的长度 println(\u0026quot;Byte length\u0026quot;, len(str)) # 得到字符串中字符的个数 println(\u0026quot;String length\u0026quot;, len([]rune(str))) 字符集与字符编码的问题很重要，但事实上平时在编程中遇到比较多的可能仅仅是读写中文字符文件，这个记录一下，其它编码的处理也类似。\nfileName := \u0026quot;test.txt\u0026quot; if _, err := os.Stat(fileName); os.IsNotExist(err) { _, err := os.Create(fileName) checkErr(err) } f, err := os.Open(fileName) checkErr(err) writer := transform.NewWriter(f, simplifiedchinese.GB18030.NewEncoder()) io.WriteString(writer, \u0026quot;百度一下，你就知道\u0026quot;) writer.Close() reader := transform.NewReader(f, simplifiedchinese.GB18030.NewDecoder()) bb, err := ioutil.ReadAll(reader) checkErr(err) fmt.Println(string(bb)) f.Close() 安全 第7章主要讲安全，在编程中主要用到的有以下几种技术。\n用于检验数据完整性的hash算法 # 下面的md5的使用，其它sha1, sha256等hash算法的使用方法类似 hash := md5.New() // hash := md5.NewMD5([]byte{27, 23, 13, 55}) bytes := []byte(\u0026quot;hello\\n\u0026quot;) hash.Write(bytes) hashValue := hash.Sum(nil) 对称加密 # 这里演示了blowfish对称加密算法，其它AES，DES使用方法类似 key := []byte(\u0026quot;my key\u0026quot;) cipher, err := blowfish.NewCipher(key) if err != nil { fmt.Println(err.Error()) } # 加密 enc := make([]byte, 255) cipher.Encrypt(enc, []byte(\u0026quot;hello world\u0026quot;)) # 解密 decrypt := make([]byte, 8) cipher.Decrypt(decrypt, enc) fmt.Println(string(decrypt)) 非对称加密 这里仅说明了下如何生成一对RSA公私钥及如何加载RSA公私钥，如何生成证书及如何加载证书，因为在编程中很少自己进行非对称加密，一般用在TLS连接会话里。而使用方法多数仅仅只是在建立连接时配置一下证书。\n# 生成一对公私钥，并保存到文件 func main() { privateKey, err := rsa.GenerateKey(rand.Reader, 2048) checkErr(err) publicKey := \u0026amp;(privateKey.PublicKey) savePEMKey(\u0026quot;pri.key\u0026quot;, privateKey) savePEMPublicKey(\u0026quot;pub.key\u0026quot;, publicKey) } func savePEMPublicKey(path string, key *rsa.PublicKey) { file, err := os.Create(path) checkErr(err) defer file.Close() publicKey, err := ssh.NewPublicKey(key) checkErr(err) file.Write(ssh.MarshalAuthorizedKey(publicKey)) } func savePEMKey(path string, key *rsa.PrivateKey) { file, err := os.Create(path) checkErr(err) defer file.Close() block := \u0026amp;pem.Block{ Type : \u0026quot;RSA PRIVATE KEY\u0026quot;, Bytes : x509.MarshalPKCS1PrivateKey(key), } pem.Encode(file, block) } func checkErr(err error) { if err != nil { fmt.Fprintf(os.Stderr, \u0026quot;error: %v\u0026quot;, err) os.Exit(-1) } } # 从文件中加载公私钥 func main() { privateKey := loadPEMKey(\u0026quot;pri.key\u0026quot;) fmt.Printf(\u0026quot;%v\\n\u0026quot;, *privateKey) publicKey := loadPEMPublicKey(\u0026quot;pub.key\u0026quot;) fmt.Printf(\u0026quot;%v\\n\u0026quot;, *publicKey) } func loadPEMKey(path string) *rsa.PrivateKey { file, err := os.Open(path) checkErr(err) defer file.Close() bb, err := ioutil.ReadAll(file) block, _ := pem.Decode(bb) privateKey, err := x509.ParsePKCS1PrivateKey(block.Bytes) checkErr(err) return privateKey } func loadPEMPublicKey(path string) *rsa.PublicKey{ file, err := os.Open(path) checkErr(err) defer file.Close() bb, err := ioutil.ReadAll(file) checkErr(err) pkey, _, _, _, err:= ssh.ParseAuthorizedKey(bb) checkErr(err) if pkey, ok := pkey.(ssh.CryptoPublicKey); ok { publicKey := pkey.CryptoPublicKey().(*rsa.PublicKey) return publicKey } return nil } func checkErr(err error) { if err != nil { fmt.Fprintf(os.Stderr, \u0026quot;error: %v\u0026quot;, err) os.Exit(-1) } } # 生成证书，并保存证书到文件 func main() { generateCert(\u0026quot;test.cer.pem\u0026quot;) } func generateCert(path string) { random := rand.Reader privateKey := loadPEMKey(\u0026quot;pri.key\u0026quot;) now := time.Now() then := now.Add(60 * 60 * 24 * 365 * 1000 * 1000 * 1000) // one year template := x509.Certificate{ SerialNumber: big.NewInt(1), Subject: pkix.Name{ CommonName: \u0026quot;jan.newmarch.name\u0026quot;, Organization: []string{\u0026quot;Jan Newmarch\u0026quot;}, }, NotBefore: now, NotAfter: then, SubjectKeyId: []byte{1, 2, 3, 4}, KeyUsage: x509.KeyUsageCertSign | x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature, BasicConstraintsValid: true, IsCA: true, DNSNames: []string{\u0026quot;jan.newmarch.name\u0026quot;, \u0026quot;localhost\u0026quot;}, } derBytes, err := x509.CreateCertificate(random, \u0026amp;template, \u0026amp;template, \u0026amp;(privateKey.PublicKey), privateKey) checkErr(err) block := \u0026amp;pem.Block{ Type: \u0026quot;CERTIFICATE\u0026quot;, Bytes: derBytes, } certCerFile, err := os.Create(path) checkErr(err) pem.Encode(certCerFile, block) certCerFile.Close() } func loadPEMKey(path string) *rsa.PrivateKey { file, err := os.Open(path) checkErr(err) defer file.Close() bb, err := ioutil.ReadAll(file) block, _ := pem.Decode(bb) privateKey, _ := x509.ParsePKCS1PrivateKey(block.Bytes) return privateKey } func checkErr(err error) { if err != nil { fmt.Fprintf(os.Stderr, \u0026quot;error: %v\u0026quot;, err) os.Exit(-1) } } # 从文件中加载证书 func main() { cert := loadCert(\u0026quot;test.cer.pem\u0026quot;) fmt.Printf(\u0026quot;%v\\n\u0026quot;, *cert) } func loadCert(path string) *x509.Certificate { certCerFile, err := os.Open(path) checkErr(err) bb, err := ioutil.ReadAll(certCerFile) checkErr(err) certCerFile.Close() block, _ := pem.Decode(bb) // trim the bytes to actual length in call cert, err := x509.ParseCertificate(block.Bytes) checkErr(err) return cert; } func checkErr(err error) { if err != nil { fmt.Fprintf(os.Stderr, \u0026quot;error: %v\u0026quot;, err) os.Exit(-1) } } # TCP服务端启用TLS func main() { startTCPServerWithTLS() } func startTCPServerWithTLS() { cert, err := tls.LoadX509KeyPair(\u0026quot;test.cer.pem\u0026quot;, \u0026quot;pri.key\u0026quot;) checkErr(err) config := tls.Config{Certificates: []tls.Certificate{cert}} now := time.Now() config.Time = func() time.Time { return now } config.Rand = rand.Reader _, err = tls.Listen(\u0026quot;tcp\u0026quot;, \u0026quot;:1200\u0026quot;, \u0026amp;config) checkErr(err) ... } # TCP客户端启用TLS func main() { startTCPClientWithTLS() } func startTCPClientWithTLS() { conn, err := tls.Dial(\u0026quot;tcp\u0026quot;, \u0026quot;127.0.0.1:1200\u0026quot;, nil) checkErr(err) handleClient(conn) } ","permalink":"https://jeremyxu2010.github.io/2016/10/network-programming-with-go%E9%98%85%E8%AF%BB%E9%87%8D%E7%82%B9%E5%A4%87%E5%BF%98%E4%B8%80/","tags":["golang","network"],"title":"《Network Programming with Go》阅读重点备忘（一）"},{"categories":["web开发"],"contents":"这几天产品处在发版阶段，工作比较忙，很久没有更新博客了。不过今天在工作中遇到一个最新版Chrome浏览器的坑，分析解决的过程还比较有意思，在这里记录一下。\n问题描述 现在在做的项目，项目历时很长，之前选用的ReactJS的0.13.3版本，而现在ReactJS已经升级版本至0.15版本了，但旧版本代码一直运行得好好的，所以一直没有动力进行升级。不过今天Chrome自动升级至54版本后，ReactJS开始报错了。如下：\nunhandledRejection.js:23 Potentially unhandled rejection [2] TypeError: Failed to execute \u0026#39;insertBefore\u0026#39; on \u0026#39;Node\u0026#39;: parameter 1 is not of type \u0026#39;Node\u0026#39;. at insertChildAt (webpack:///./~/react/lib/DOMChildrenOperations.js?:34:14)  at Object.processUpdates (webpack:///./~/react/lib/DOMChildrenOperations.js?:106:11)  at Object.dangerouslyProcessChildrenUpdates (webpack:///./~/react/lib/ReactDOMIDOperations.js?:150:27)  at Object.wrapper [as processChildrenUpdates] (webpack:///./~/react/lib/ReactPerf.js?:70:21)  at processQueue (webpack:///./~/react/lib/ReactMultiChild.js?:141:31)  at ReactDOMComponent.updateChildren (webpack:///./~/react/lib/ReactMultiChild.js?:263:13)  at ReactDOMComponent._updateDOMChildren (webpack:///./~/react/lib/ReactDOMComponent.js?:470:12)  at ReactDOMComponent.updateComponent (webpack:///./~/react/lib/ReactDOMComponent.js?:319:10)  at ReactDOMComponent.receiveComponent (webpack:///./~/react/lib/ReactDOMComponent.js?:303:10)  at Object.receiveComponent (webpack:///./~/react/lib/ReactReconciler.js?:97:22) 跟踪了下调用栈，发现问题出在ReactJS操作DOM的代码处\nDOMChildrenOperations.js的105行处\ncase ReactMultiChildUpdateTypes.INSERT_MARKUP: insertChildAt( update.parentNode, renderedMarkup[update.markupIndex], update.toIndex ); break; 这里查看一下update.markupIndex竟然是NaN。继续跟踪ReactMultiChildUpdateTypes.INSERT_MARKUP类型的update是在哪里生成的，于是找到以下代码：\nReactMultiChild.js的40行处\n/** * Queue of markup to be rendered. * * @type {array\u0026lt;string\u0026gt;} * @private */ var markupQueue = []; /** * Enqueues markup to be rendered and inserted at a supplied index. * * @param {string} parentID ID of the parent component. * @param {string} markup Markup that renders into an element. * @param {number} toIndex Destination index. * @private */ function enqueueMarkup(parentID, markup, toIndex) { // NOTE: Null values reduce hidden classes.  updateQueue.push({ parentID: parentID, parentNode: null, type: ReactMultiChildUpdateTypes.INSERT_MARKUP, markupIndex: markupQueue.push(markup) - 1, textContent: null, fromIndex: null, toIndex: toIndex }); } 这里已经标明了生成的update的markupIndex为markupQueue.push(markup) - 1，照理说这肯定不会为NaN的。于是修改代码打印出值看一下。\nfunction enqueueMarkup(parentID, markup, toIndex) { var markupIndex = markupQueue.push(markup) - 1; console.log(markupIndex); // NOTE: Null values reduce hidden classes.  updateQueue.push({ parentID: parentID, parentNode: null, type: ReactMultiChildUpdateTypes.INSERT_MARKUP, markupIndex: markupIndex, textContent: null, fromIndex: null, toIndex: toIndex }); } 发现竟真的为NaN了，看来应该是Chrome的新版本bug了。为了规避问题，简单修改了下代码后，问题解决：\nfunction enqueueMarkup(parentID, markup, toIndex) { var markupIndex = markupQueue.push(markup) - 1; if(isNaN(markupIndex)) { markupIndex = markupQueue.length - 1; } // NOTE: Null values reduce hidden classes.  updateQueue.push({ parentID: parentID, parentNode: null, type: ReactMultiChildUpdateTypes.INSERT_MARKUP, markupIndex: markupIndex, textContent: null, fromIndex: null, toIndex: toIndex }); } 没有改变原来逻辑的本意，仅仅处理了下markupIndex为NaN的情况。\n进一步分析 在Chrome的问题列表上搜索了下，果然找到这个问题。\n总结 ReactJS的源码还挺复杂的，特别是通过虚拟DOM树操作真正DOM那一段。有问题也不要紧，打开Chrome开发者工具，仔细分析还是可以找到问题发生的原因的。\n","permalink":"https://jeremyxu2010.github.io/2016/10/react0.13%E5%9C%A8chrome54%E4%B8%8A%E6%8A%BD%E9%A3%8E%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/","tags":["react","chrome","javascript"],"title":"React0.13在Chrome54上抽风问题总结"},{"categories":["容器编排"],"contents":"上一篇分析了Docker Client的源码运行逻辑，本篇接着分析Docker Daemon的运行逻辑。Docker Daemon的运行逻辑很复杂，大家看着来要有耐心了。\nDocker Daemon的执行 Docker Daemon的入口在cmd/dockerd/docker.go，先看main函数。\nfunc main() { if reexec.Init() { return } // Set terminal emulation based on platform as required. _, stdout, stderr := term.StdStreams() logrus.SetOutput(stderr) cmd := newDaemonCommand() cmd.SetOutput(stdout) if err := cmd.Execute(); err != nil { fmt.Fprintf(stderr, \u0026quot;%s\\n\u0026quot;, err) os.Exit(1) } } 跟Docker Client的main函数，只不过加了个是否执行过初始化方法的检查，看了半天，感觉在这里dockerd并没有注册对应的初始化方法，因此这里reexec.Init()必然返回false，因此这个检查看着很 诡异。\nDaemon命令行不存在子命令，就一个根命令，按上一篇的跟踪方法，很快就跟踪到runDaemon函数。\nfunc runDaemon(opts daemonOptions) error { if opts.version { showVersion() return nil } daemonCli := NewDaemonCli() // On Windows, this may be launching as a service or with an option to // register the service. stop, err := initService(daemonCli) if err != nil { logrus.Fatal(err) } if stop { return nil } err = daemonCli.start(opts) notifyShutdown(err) return err } 与windows系统相关的代码忽略掉，其实就是创建了一个DaemonCli对象，然后调用start方法启动它，因为是个daemon程序，所以这里start方法是一个阻塞的方法，一旦它执行完毕，则进行收尾工作。\n再看daemonCli.start方法，它定义在cmd/dockerd/daemon.go里，前面一大段都是读取解析检验参数，就不多说了，这个有个小地方要注意，daemon程序支持将参数写进json文件里，dockerd启动时会将json配置文件里的选项与命令行的选项进行合并。\n然后是设置合理的umask以避免创建的文件权限不正确，再然后是设置日志选项、生成pid文件。\napi := apiserver.New(serverConfig) 根据serverConfig创建API Server对象。\n\tfor i := 0; i \u0026lt; len(cli.Config.Hosts); i++ { var err error if cli.Config.Hosts[i], err = dopts.ParseHost(cli.Config.TLS, cli.Config.Hosts[i]); err != nil { return fmt.Errorf(\u0026quot;error parsing -H %s : %v\u0026quot;, cli.Config.Hosts[i], err) } protoAddr := cli.Config.Hosts[i] protoAddrParts := strings.SplitN(protoAddr, \u0026quot;://\u0026quot;, 2) if len(protoAddrParts) != 2 { return fmt.Errorf(\u0026quot;bad format %s, expected PROTO://ADDR\u0026quot;, protoAddr) } proto := protoAddrParts[0] addr := protoAddrParts[1] // It's a bad idea to bind to TCP without tlsverify. if proto == \u0026quot;tcp\u0026quot; \u0026amp;\u0026amp; (serverConfig.TLSConfig == nil || serverConfig.TLSConfig.ClientAuth != tls.RequireAndVerifyClientCert) { logrus.Warn(\u0026quot;[!] DON'T BIND ON ANY IP ADDRESS WITHOUT setting -tlsverify IF YOU DON'T KNOW WHAT YOU'RE DOING [!]\u0026quot;) } ls, err := listeners.Init(proto, addr, serverConfig.SocketGroup, serverConfig.TLSConfig) if err != nil { return err } ls = wrapListeners(proto, ls) // If we're binding to a TCP port, make sure that a container doesn't try to use it. if proto == \u0026quot;tcp\u0026quot; { if err := allocateDaemonPort(addr); err != nil { return err } } logrus.Debugf(\u0026quot;Listener created for HTTP on %s (%s)\u0026quot;, proto, addr) api.Accept(addr, ls...) } 因为daemon程序可以根据选项监控多个地址，所以上述代码遍历这些地址，也监听了多个地址。\nregistryService := registry.NewService(cli.Config.ServiceOptions) 从docker的总体架构得知，daemon程序在pull镜像等操作时，需要与registry服务交互，这里即创建了registryService对象，用于与registry服务交互。registryService对象定义在registry目录，这个目录里文件不是太多，逻辑也不是太复杂，暂且略过。总体来说就是实现了下列接口方法，供daemon程序与registry服务交互。\ntype Service interface { Auth(ctx context.Context, authConfig *types.AuthConfig, userAgent string) (status, token string, err error) LookupPullEndpoints(hostname string) (endpoints []APIEndpoint, err error) LookupPushEndpoints(hostname string) (endpoints []APIEndpoint, err error) ResolveRepository(name reference.Named) (*RepositoryInfo, error) ResolveIndex(name string) (*registrytypes.IndexInfo, error) Search(ctx context.Context, term string, limit int, authConfig *types.AuthConfig, userAgent string, headers map[string][]string) (*registrytypes.SearchResults, error) ServiceConfig() *registrytypes.ServiceConfig TLSConfig(hostname string) (*tls.Config, error) } 回到daemon的start方法，接下来构造了与docker-containerd通信的对象containerdRemote。\ncontainerdRemote, err := libcontainerd.New(cli.getLibcontainerdRoot(), cli.getPlatformRemoteOptions()...) if err != nil { return err } docker-containerd是一个控制runC的后台程序，其代码在https://github.com/docker/containerd。与docker-containerd通信的模块源码在libcontainerd目录，这个目录里文件不是太多，简单来说就是提供了下列接口方法，供daemon程序调用以控制管理容器的运行。\n// Client provides access to containerd features. type Client interface { Create(containerID string, checkpoint string, checkpointDir string, spec specs.Spec, options ...CreateOption) error Signal(containerID string, sig int) error SignalProcess(containerID string, processFriendlyName string, sig int) error AddProcess(ctx context.Context, containerID, processFriendlyName string, process Process) error Resize(containerID, processFriendlyName string, width, height int) error Pause(containerID string) error Resume(containerID string) error Restore(containerID string, options ...CreateOption) error Stats(containerID string) (*Stats, error) GetPidsForContainer(containerID string) ([]int, error) Summary(containerID string) ([]Summary, error) UpdateResources(containerID string, resources Resources) error CreateCheckpoint(containerID string, checkpointID string, checkpointDir string, exit bool) error DeleteCheckpoint(containerID string, checkpointID string, checkpointDir string) error ListCheckpoints(containerID string, checkpointDir string) (*Checkpoints, error) } 再回到daemon的start方法，接下来声明监听系统的一些终止信号，如监听到这些信息，就停止DaemonCli。\n\tsignal.Trap(func() { cli.stop() \u0026lt;-stopc // wait for daemonCli.start() to return }) 再接下来，创建Daemon对象，注意创建Daemon对象时传入了registryService，containerdRemote，这个跟docker的总体架构是相符的。\n\td, err := daemon.NewDaemon(cli.Config, registryService, containerdRemote) if err != nil { return fmt.Errorf(\u0026quot;Error starting daemon: %v\u0026quot;, err) } NewDaemon方法完成的功能较多，下一小节再详细描述，这里还是继续看daemon的start方法。\n\tc, err := cluster.New(cluster.Config{ Root: cli.Config.Root, Name: name, Backend: d, NetworkSubnetsProvider: d, DefaultAdvertiseAddr: cli.Config.SwarmDefaultAdvertiseAddr, RuntimeRoot: cli.getSwarmRunRoot(), }) if err != nil { logrus.Fatalf(\u0026quot;Error creating cluster component: %v\u0026quot;, err) } // Restart all autostart containers which has a swarm endpoint // and is not yet running now that we have successfully // initialized the cluster. d.RestartSwarmContainers() V1.12版本的docker还集成了swarm的相关功能，这里将自动启动安装有swarm endpoint的容器。\n\tcli.initMiddlewares(api, serverConfig) 给API Server注册一些中间件，这些中间件主要进行版本兼容性检查、添加CORS跨站点请求相关响应头、对请求进行认证。\nfunc (cli *DaemonCli) initMiddlewares(s *apiserver.Server, cfg *apiserver.Config) { v := cfg.Version vm := middleware.NewVersionMiddleware(v, api.DefaultVersion, api.MinVersion) s.UseMiddleware(vm) if cfg.EnableCors { c := middleware.NewCORSMiddleware(cfg.CorsHeaders) s.UseMiddleware(c) } u := middleware.NewUserAgentMiddleware(v) s.UseMiddleware(u) cli.authzMiddleware = authorization.NewMiddleware(cli.Config.AuthorizationPlugins) s.UseMiddleware(cli.authzMiddleware) } 再接下来初始化API Server的路由。\n\tinitRouter(api, d, c) ... func initRouter(s *apiserver.Server, d *daemon.Daemon, c *cluster.Cluster) { decoder := runconfig.ContainerDecoder{} routers := []router.Router{} // we need to add the checkpoint router before the container router or the DELETE gets masked routers = addExperimentalRouters(routers, d, decoder) routers = append(routers, []router.Router{ container.NewRouter(d, decoder), image.NewRouter(d, decoder), systemrouter.NewRouter(d, c), volume.NewRouter(d), build.NewRouter(dockerfile.NewBuildManager(d)), swarmrouter.NewRouter(c), }...) if d.NetworkControllerEnabled() { routers = append(routers, network.NewRouter(d, c)) } s.InitRouter(utils.IsDebugEnabled(), routers...) } 可能看到API Server处理的请求主要包括容器、镜像、系统、卷、编译、swarm、网络这几个方法。构建路由时均将Daemon对象传入了，也就是说某条路由对应的handler将会调用Daemon对象的相关方法进行业务操作，最后向Docker Client输出回应。API Server这些路由对应的Handler在api/server/router目录下都可以找到，每个handler逻辑都很简单，就不详细描述了。\n再回到daemon的start方法，下面的方法当监听到系统信号SIGHUP，就会重新加载daemon程序的配置。\ncli.setupConfigReloadTrap() 再然后创建goroutine使API Server开始对外提供服务，并在该goroutine里等待API Server停止。API Server默认也不会停止，除非主动停止它或出现什么错误，所以这个goroutine是阻塞的。另外API Server很有可能是监听多个地址的，所以serveAPI方法使用了多个goroutine以调用多个HTTPServer的Serve方法，并同样阻塞住。\n\tserveAPIWait := make(chan error) go api.Wait(serveAPIWait) func (s *Server) Wait(waitChan chan error) { if err := s.serveAPI(); err != nil { logrus.Errorf(\u0026quot;ServeAPI error: %v\u0026quot;, err) waitChan \u0026lt;- err return } waitChan \u0026lt;- nil } // serveAPI loops through all initialized servers and spawns goroutine // with Server method for each. It sets createMux() as Handler also. func (s *Server) serveAPI() error { var chErrors = make(chan error, len(s.servers)) for _, srv := range s.servers { srv.srv.Handler = s.routerSwapper go func(srv *HTTPServer) { var err error logrus.Infof(\u0026quot;API listen on %s\u0026quot;, srv.l.Addr()) if err = srv.Serve(); err != nil \u0026amp;\u0026amp; strings.Contains(err.Error(), \u0026quot;use of closed network connection\u0026quot;) { err = nil } chErrors \u0026lt;- err }(srv) } for i := 0; i \u0026lt; len(s.servers); i++ { err := \u0026lt;-chErrors if err != nil { return err } } return nil } 然后调用操作系统的systemd服务，docker的daemon进程已成功启动。\n\t// after the daemon is done setting up we can notify systemd api notifySystem() \t// Daemon is fully initialized and handling API traffic // Wait for serve API to complete errAPI := \u0026lt;-serveAPIWait c.Cleanup() shutdownDaemon(d, 15) containerdRemote.Cleanup() if errAPI != nil { return fmt.Errorf(\u0026quot;Shutting down due to ServeAPI error: %v\u0026quot;, errAPI) } return nil 最后由于上面调用serveAPI方法的goroutine阻塞住了，所以errAPI := \u0026lt;-serveAPIWait这行代码就会使main goroutine阻塞住了，这样daemon进程就不会退出。一旦阻塞解除了，也就意味着daemon进程需要退出了，这时做一些清理工作。\n以上就是Docker Daemon的整体执行逻辑了。\nDocker Daemon的创建 上面一小节里，有一个方法daemon.NewDaemon(cli.Config, registryService, containerdRemote)简单地跳过了，但其实这个方法是相当重要的，这里将这个方法详细说明一下。\n\tsetDefaultMtu(config) // Ensure that we have a correct root key limit for launching containers. if err := ModifyRootKeyLimit(); err != nil { logrus.Warnf(\u0026quot;unable to modify root key limit, number of containers could be limitied by this quota: %v\u0026quot;, err) } // Ensure we have compatible and valid configuration options if err := verifyDaemonSettings(config); err != nil { return nil, err } // Do we have a disabled network? config.DisableBridge = isBridgeNetworkDisabled(config) // Verify the platform is supported as a daemon if !platformSupported { return nil, errSystemNotSupported } // Validate platform-specific requirements if err := checkSystem(); err != nil { return nil, err } // set up SIGUSR1 handler on Unix-like systems, or a Win32 global event // on Windows to dump Go routine stacks setupDumpStackTrap(config.Root) 首先就是一大段选项、环境的检测调整，代码注释得很清楚，就不多说了。\n\tuidMaps, gidMaps, err := setupRemappedRoot(config) if err != nil { return nil, err } rootUID, rootGID, err := idtools.GetRootUIDGID(uidMaps, gidMaps) if err != nil { return nil, err } docker支持用户空间重映射特性，这里就是在解析与之相关的userns-remap参数。docker用户空间重映射特性其实就是将容器内的用户映射为宿主机上的普通用户，这个主要是为了加强容器安全的。\n\t// get the canonical path to the Docker root directory var realRoot string if _, err := os.Stat(config.Root); err != nil \u0026amp;\u0026amp; os.IsNotExist(err) { realRoot = config.Root } else { realRoot, err = fileutils.ReadSymlinkedDirectory(config.Root) if err != nil { return nil, fmt.Errorf(\u0026quot;Unable to get the full path to root (%s): %s\u0026quot;, config.Root, err) } } if err := setupDaemonRoot(config, realRoot, rootUID, rootGID); err != nil { return nil, err } if err := setupDaemonProcess(config); err != nil { return nil, err } // set up the tmpDir to use a canonical path tmp, err := tempDir(config.Root, rootUID, rootGID) if err != nil { return nil, fmt.Errorf(\u0026quot;Unable to get the TempDir under %s: %s\u0026quot;, config.Root, err) } realTmp, err := fileutils.ReadSymlinkedDirectory(tmp) if err != nil { return nil, fmt.Errorf(\u0026quot;Unable to get the full path to the TempDir (%s): %s\u0026quot;, tmp, err) } os.Setenv(\u0026quot;TMPDIR\u0026quot;, realTmp) 接下来对存储目录进行必要的权限调整、对daemon进程的oom_score_adj参数进行必要的调整（减小daemon进程被OS杀掉的可能性）、创建临时目录。\n\td := \u0026amp;Daemon{configStore: config} // Ensure the daemon is properly shutdown if there is a failure during // initialization defer func() { if err != nil { if err := d.Shutdown(); err != nil { logrus.Error(err) } } }() // Set the default isolation mode (only applicable on Windows) if err := d.setDefaultIsolation(); err != nil { return nil, fmt.Errorf(\u0026quot;error setting default isolation mode: %v\u0026quot;, err) } logrus.Debugf(\u0026quot;Using default logging driver %s\u0026quot;, config.LogConfig.Type) if err := configureMaxThreads(config); err != nil { logrus.Warnf(\u0026quot;Failed to configure golang's threads limit: %v\u0026quot;, err) } installDefaultAppArmorProfile() 上述代码完成以下几个功能：\n 创建Daemon对象 如NewDaemon方法有任何异常，则退出方法时关闭Daemon 调整进程的最大线程数限制 安装AppArmor相关的配置  \tdaemonRepo := filepath.Join(config.Root, \u0026quot;containers\u0026quot;) if err := idtools.MkdirAllAs(daemonRepo, 0700, rootUID, rootGID); err != nil \u0026amp;\u0026amp; !os.IsExist(err) { return nil, err } driverName := os.Getenv(\u0026quot;DOCKER_DRIVER\u0026quot;) if driverName == \u0026quot;\u0026quot; { driverName = config.GraphDriver } d.pluginStore = pluginstore.NewStore(config.Root) d.layerStore, err = layer.NewStoreFromOptions(layer.StoreOptions{ StorePath: config.Root, MetadataStorePathTemplate: filepath.Join(config.Root, \u0026quot;image\u0026quot;, \u0026quot;%s\u0026quot;, \u0026quot;layerdb\u0026quot;), GraphDriver: driverName, GraphDriverOptions: config.GraphOptions, UIDMaps: uidMaps, GIDMaps: gidMaps, PluginGetter: d.pluginStore, }) if err != nil { return nil, err } graphDriver := d.layerStore.DriverName() imageRoot := filepath.Join(config.Root, \u0026quot;image\u0026quot;, graphDriver) // Configure and validate the kernels security support if err := configureKernelSecuritySupport(config, graphDriver); err != nil { return nil, err } logrus.Debugf(\u0026quot;Max Concurrent Downloads: %d\u0026quot;, *config.MaxConcurrentDownloads) d.downloadManager = xfer.NewLayerDownloadManager(d.layerStore, *config.MaxConcurrentDownloads) logrus.Debugf(\u0026quot;Max Concurrent Uploads: %d\u0026quot;, *config.MaxConcurrentUploads) d.uploadManager = xfer.NewLayerUploadManager(*config.MaxConcurrentUploads) ifs, err := image.NewFSStoreBackend(filepath.Join(imageRoot, \u0026quot;imagedb\u0026quot;)) if err != nil { return nil, err } d.imageStore, err = image.NewImageStore(ifs, d.layerStore) if err != nil { return nil, err } // Configure the volumes driver volStore, err := d.configureVolumes(rootUID, rootGID) if err != nil { return nil, err } trustKey, err := api.LoadOrCreateTrustKey(config.TrustKeyPath) if err != nil { return nil, err } trustDir := filepath.Join(config.Root, \u0026quot;trust\u0026quot;) if err := system.MkdirAll(trustDir, 0700); err != nil { return nil, err } distributionMetadataStore, err := dmetadata.NewFSMetadataStore(filepath.Join(imageRoot, \u0026quot;distribution\u0026quot;)) if err != nil { return nil, err } eventsService := events.New() referenceStore, err := reference.NewReferenceStore(filepath.Join(imageRoot, \u0026quot;repositories.json\u0026quot;)) if err != nil { return nil, fmt.Errorf(\u0026quot;Couldn't create Tag store repositories: %s\u0026quot;, err) } 再就是创建初始化了一堆与镜像存储相关的目录及Store，有以下几个：\n /var/lib/docker/containers 这个目录是用来记录的是容器相关的信息，每运行一个容器，就在这个目录下面生成一个容器Id对应的子目录。 /var/lib/docker/image/${graphDriverName}/layerdb 这个目录是用来记录layer元数据的 /var/lib/docker/image/${graphDriverName}/imagedb 这个目录是用来记录镜像元数据的 /var/lib/docker/image/${graphDriverName}/distribution 这个目录用来记录layer元数据与镜像元数据之间的关联关系 /var/lib/docker/image/${graphDriverName}/repositories.json 这个目录是用来记录镜像仓库元数据的 /var/lib/docker/trust 这个目录用来放一些证书文件 /var/lib/docker/volumes 这个目录是用来记录卷元数据的  \tmigrationStart := time.Now() if err := v1.Migrate(config.Root, graphDriver, d.layerStore, d.imageStore, referenceStore, distributionMetadataStore); err != nil { logrus.Errorf(\u0026quot;Graph migration failed: %q. Your old graph data was found to be too inconsistent for upgrading to content-addressable storage. Some of the old data was probably not upgraded. We recommend starting over with a clean storage directory if possible.\u0026quot;, err) } logrus.Infof(\u0026quot;Graph migration to content-addressability took %.2f seconds\u0026quot;, time.Since(migrationStart).Seconds()) 接下来是一个迁移旧版Graph数据的逻辑。\n\t// Discovery is only enabled when the daemon is launched with an address to advertise. When // initialized, the daemon is registered and we can store the discovery backend as its read-only if err := d.initDiscovery(config); err != nil { return nil, err } 如果配置了在集群中向外发布的访问地址，则需要初始化集群节点的服务发现Agent。一般来说就是定时向KV库报告自身的状态及公布访问地址，代码如下：\n// initDiscovery initializes the nodes discovery subsystem by connecting to the specified backend // and starts a registration loop to advertise the current node under the specified address. func initDiscovery(backendAddress, advertiseAddress string, clusterOpts map[string]string) (discoveryReloader, error) { heartbeat, backend, err := parseDiscoveryOptions(backendAddress, clusterOpts) if err != nil { return nil, err } reloader := \u0026amp;daemonDiscoveryReloader{ backend: backend, ticker: time.NewTicker(heartbeat), term: make(chan bool), readyCh: make(chan struct{}), } // We call Register() on the discovery backend in a loop for the whole lifetime of the daemon, // but we never actually Watch() for nodes appearing and disappearing for the moment. go reloader.advertiseHeartbeat(advertiseAddress) return reloader, nil } // advertiseHeartbeat registers the current node against the discovery backend using the specified // address. The function never returns, as registration against the backend comes with a TTL and // requires regular heartbeats. func (d *daemonDiscoveryReloader) advertiseHeartbeat(address string) { var ready bool if err := d.initHeartbeat(address); err == nil { ready = true close(d.readyCh) } for { select { case \u0026lt;-d.ticker.C: if err := d.backend.Register(address); err != nil { logrus.Warnf(\u0026quot;Registering as %q in discovery failed: %v\u0026quot;, address, err) } else { if !ready { close(d.readyCh) ready = true } } case \u0026lt;-d.term: return } } } // initHeartbeat is used to do the first heartbeat. It uses a tight loop until // either the timeout period is reached or the heartbeat is successful and returns. func (d *daemonDiscoveryReloader) initHeartbeat(address string) error { // Setup a short ticker until the first heartbeat has succeeded t := time.NewTicker(500 * time.Millisecond) defer t.Stop() // timeout makes sure that after a period of time we stop being so aggressive trying to reach the discovery service timeout := time.After(60 * time.Second) for { select { case \u0026lt;-timeout: return errors.New(\u0026quot;timeout waiting for initial discovery\u0026quot;) case \u0026lt;-d.term: return errors.New(\u0026quot;terminated\u0026quot;) case \u0026lt;-t.C: if err := d.backend.Register(address); err == nil { return nil } } } } 再然后就是给Daemon对象的一系列属性赋上值。\n\td.ID = trustKey.PublicKey().KeyID() d.repository = daemonRepo d.containers = container.NewMemoryStore() d.execCommands = exec.NewStore() d.referenceStore = referenceStore d.distributionMetadataStore = distributionMetadataStore d.trustKey = trustKey d.idIndex = truncindex.NewTruncIndex([]string{}) d.statsCollector = d.newStatsCollector(1 * time.Second) d.defaultLogConfig = containertypes.LogConfig{ Type: config.LogConfig.Type, Config: config.LogConfig.Config, } d.RegistryService = registryService d.EventsService = eventsService d.volumes = volStore d.root = config.Root d.uidMaps = uidMaps d.gidMaps = gidMaps d.seccompEnabled = sysInfo.Seccomp d.nameIndex = registrar.NewRegistrar() d.linkIndex = newLinkIndex() d.containerdRemote = containerdRemote go d.execCommandGC() d.containerd, err = containerdRemote.Client(d) if err != nil { return nil, err } 首先确保插件系统初始化完毕，然后根据/var/lib/docker/containers目录里容器目录还原部分容器、初始化容器依赖的网络环境，初始化容器之间的link关系等。\n\t// Plugin system initialization should happen before restore. Dont change order. if err := pluginInit(d, config, containerdRemote); err != nil { return nil, err } if err := d.restore(); err != nil { return nil, err } func (daemon *Daemon) restore() error { var ( debug = utils.IsDebugEnabled() currentDriver = daemon.GraphDriverName() containers = make(map[string]*container.Container) ) if !debug { logrus.Info(\u0026quot;Loading containers: start.\u0026quot;) } dir, err := ioutil.ReadDir(daemon.repository) if err != nil { return err } containerCount := 0 for _, v := range dir { id := v.Name() container, err := daemon.load(id) if !debug \u0026amp;\u0026amp; logrus.GetLevel() == logrus.InfoLevel { fmt.Print(\u0026quot;.\u0026quot;) containerCount++ } if err != nil { logrus.Errorf(\u0026quot;Failed to load container %v: %v\u0026quot;, id, err) continue } // Ignore the container if it does not support the current driver being used by the graph if (container.Driver == \u0026quot;\u0026quot; \u0026amp;\u0026amp; currentDriver == \u0026quot;aufs\u0026quot;) || container.Driver == currentDriver { rwlayer, err := daemon.layerStore.GetRWLayer(container.ID) if err != nil { logrus.Errorf(\u0026quot;Failed to load container mount %v: %v\u0026quot;, id, err) continue } container.RWLayer = rwlayer logrus.Debugf(\u0026quot;Loaded container %v\u0026quot;, container.ID) containers[container.ID] = container } else { logrus.Debugf(\u0026quot;Cannot load container %s because it was created with another graph driver.\u0026quot;, container.ID) } } var migrateLegacyLinks bool removeContainers := make(map[string]*container.Container) restartContainers := make(map[*container.Container]chan struct{}) activeSandboxes := make(map[string]interface{}) for _, c := range containers { if err := daemon.registerName(c); err != nil { logrus.Errorf(\u0026quot;Failed to register container %s: %s\u0026quot;, c.ID, err) continue } if err := daemon.Register(c); err != nil { logrus.Errorf(\u0026quot;Failed to register container %s: %s\u0026quot;, c.ID, err) continue } // verify that all volumes valid and have been migrated from the pre-1.7 layout if err := daemon.verifyVolumesInfo(c); err != nil { // don't skip the container due to error logrus.Errorf(\u0026quot;Failed to verify volumes for container '%s': %v\u0026quot;, c.ID, err) } // The LogConfig.Type is empty if the container was created before docker 1.12 with default log driver. // We should rewrite it to use the daemon defaults. // Fixes https://github.com/docker/docker/issues/22536 if c.HostConfig.LogConfig.Type == \u0026quot;\u0026quot; { if err := daemon.mergeAndVerifyLogConfig(\u0026amp;c.HostConfig.LogConfig); err != nil { logrus.Errorf(\u0026quot;Failed to verify log config for container %s: %q\u0026quot;, c.ID, err) continue } } } var wg sync.WaitGroup var mapLock sync.Mutex for _, c := range containers { wg.Add(1) go func(c *container.Container) { defer wg.Done() if err := backportMountSpec(c); err != nil { logrus.Errorf(\u0026quot;Failed to migrate old mounts to use new spec format\u0026quot;) } rm := c.RestartManager(false) if c.IsRunning() || c.IsPaused() { if err := daemon.containerd.Restore(c.ID, libcontainerd.WithRestartManager(rm)); err != nil { logrus.Errorf(\u0026quot;Failed to restore %s with containerd: %s\u0026quot;, c.ID, err) return } if !c.HostConfig.NetworkMode.IsContainer() \u0026amp;\u0026amp; c.IsRunning() { options, err := daemon.buildSandboxOptions(c) if err != nil { logrus.Warnf(\u0026quot;Failed build sandbox option to restore container %s: %v\u0026quot;, c.ID, err) } mapLock.Lock() activeSandboxes[c.NetworkSettings.SandboxID] = options mapLock.Unlock() } } // fixme: only if not running // get list of containers we need to restart if !c.IsRunning() \u0026amp;\u0026amp; !c.IsPaused() { // Do not autostart containers which // has endpoints in a swarm scope // network yet since the cluster is // not initialized yet. We will start // it after the cluster is // initialized. if daemon.configStore.AutoRestart \u0026amp;\u0026amp; c.ShouldRestart() \u0026amp;\u0026amp; !c.NetworkSettings.HasSwarmEndpoint { mapLock.Lock() restartContainers[c] = make(chan struct{}) mapLock.Unlock() } else if c.HostConfig != nil \u0026amp;\u0026amp; c.HostConfig.AutoRemove { mapLock.Lock() removeContainers[c.ID] = c mapLock.Unlock() } } if c.RemovalInProgress { // We probably crashed in the middle of a removal, reset // the flag. // // We DO NOT remove the container here as we do not // know if the user had requested for either the // associated volumes, network links or both to also // be removed. So we put the container in the \u0026quot;dead\u0026quot; // state and leave further processing up to them. logrus.Debugf(\u0026quot;Resetting RemovalInProgress flag from %v\u0026quot;, c.ID) c.ResetRemovalInProgress() c.SetDead() c.ToDisk() } // if c.hostConfig.Links is nil (not just empty), then it is using the old sqlite links and needs to be migrated if c.HostConfig != nil \u0026amp;\u0026amp; c.HostConfig.Links == nil { migrateLegacyLinks = true } }(c) } wg.Wait() daemon.netController, err = daemon.initNetworkController(daemon.configStore, activeSandboxes) if err != nil { return fmt.Errorf(\u0026quot;Error initializing network controller: %v\u0026quot;, err) } // migrate any legacy links from sqlite linkdbFile := filepath.Join(daemon.root, \u0026quot;linkgraph.db\u0026quot;) var legacyLinkDB *graphdb.Database if migrateLegacyLinks { legacyLinkDB, err = graphdb.NewSqliteConn(linkdbFile) if err != nil { return fmt.Errorf(\u0026quot;error connecting to legacy link graph DB %s, container links may be lost: %v\u0026quot;, linkdbFile, err) } defer legacyLinkDB.Close() } // Now that all the containers are registered, register the links for _, c := range containers { if migrateLegacyLinks { if err := daemon.migrateLegacySqliteLinks(legacyLinkDB, c); err != nil { return err } } if err := daemon.registerLinks(c, c.HostConfig); err != nil { logrus.Errorf(\u0026quot;failed to register link for container %s: %v\u0026quot;, c.ID, err) } } group := sync.WaitGroup{} for c, notifier := range restartContainers { group.Add(1) go func(c *container.Container, chNotify chan struct{}) { defer group.Done() logrus.Debugf(\u0026quot;Starting container %s\u0026quot;, c.ID) // ignore errors here as this is a best effort to wait for children to be // running before we try to start the container children := daemon.children(c) timeout := time.After(5 * time.Second) for _, child := range children { if notifier, exists := restartContainers[child]; exists { select { case \u0026lt;-notifier: case \u0026lt;-timeout: } } } // Make sure networks are available before starting daemon.waitForNetworks(c) if err := daemon.containerStart(c, \u0026quot;\u0026quot;); err != nil { logrus.Errorf(\u0026quot;Failed to start container %s: %s\u0026quot;, c.ID, err) } close(chNotify) }(c, notifier) } group.Wait() removeGroup := sync.WaitGroup{} for id := range removeContainers { removeGroup.Add(1) go func(cid string) { if err := daemon.ContainerRm(cid, \u0026amp;types.ContainerRmConfig{ForceRemove: true, RemoveVolume: true}); err != nil { logrus.Errorf(\u0026quot;Failed to remove container %s: %s\u0026quot;, cid, err) } removeGroup.Done() }(id) } removeGroup.Wait() // any containers that were started above would already have had this done, // however we need to now prepare the mountpoints for the rest of the containers as well. // This shouldn't cause any issue running on the containers that already had this run. // This must be run after any containers with a restart policy so that containerized plugins // can have a chance to be running before we try to initialize them. for _, c := range containers { // if the container has restart policy, do not // prepare the mountpoints since it has been done on restarting. // This is to speed up the daemon start when a restart container // has a volume and the volume dirver is not available. if _, ok := restartContainers[c]; ok { continue } else if _, ok := removeContainers[c.ID]; ok { // container is automatically removed, skip it. continue } group.Add(1) go func(c *container.Container) { defer group.Done() if err := daemon.prepareMountPoints(c); err != nil { logrus.Error(err) } }(c) } group.Wait() if !debug { if logrus.GetLevel() == logrus.InfoLevel \u0026amp;\u0026amp; containerCount \u0026gt; 0 { fmt.Println() } logrus.Info(\u0026quot;Loading containers: done.\u0026quot;) } return nil } 至此Daemon对象就创建成功了。\n总结 Docker Daemon的运行逻辑属于Docker的核心，它相关的组件很多，代码理起来很复杂，但如果仔细看还是能看明白它的条理的。\n另外在看docker源码的过程中发现docker中有三块还是比较有意思的，这三块分别是：容器的创建与启动过程、镜像的存储过程、容器网络的创建过程。后面抽空将这三部分也写个文档分析一下。\n参考 http://www.kancloud.cn/infoq/docker-source-code-analysis/80527 http://www.kancloud.cn/infoq/docker-source-code-analysis/80528 http://www.kancloud.cn/infoq/docker-source-code-analysis/80529 http://www.aboutyun.com/thread-16811-1-1.html http://pipul.org/2016/03/how-docker-image-stored-on-aufs-filesystem/ http://yihongwei.com/2015/10/docker-volume-plugin/\n","permalink":"https://jeremyxu2010.github.io/2016/10/docker%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-daemon%E5%88%9B%E5%BB%BA%E5%8F%8A%E5%90%AF%E5%8A%A8/","tags":["docker","golang"],"title":"docker源码分析-Daemon创建及启动"},{"categories":["容器编排"],"contents":"一直在研究docker，最近被人问到docker到底是怎么工作的却不是太清楚，在网上偶然看到一本讲docker源码的电子书，花了整晚看了下，终于对docker的实现细节比较清楚了。但这本电子书讲的是1.2版本时的docker源码，跟最新的1.12版本相比差别还是挺大的，在这本书里讲到的源码与最新源码已经对应不上了。因此我计划写一份针对1.12版本的docker源码分析。\ndocker的总体架构 这部分基本没有太大的变化，我觉得可以直接参照1.2版本的总体架构，就不重复分析了。见这里。\nClient创建与命令执行 在1.12版本里，最终编译生成的二进制拆分成了两个：docker和dockerd。从名字就很容易猜得出一个是客户端，一个是daemon端。这里我们先分析客户端。\ndocker客户端的入口文件在cmd/docker/docker.go。\nmain函数的头两句\nstdin, stdout, stderr := term.StdStreams() logrus.SetOutput(stderr) 取得了终端的标准输入、标准输出、标准错误，并将日志输出至标准错误。\ndockerCli := command.NewDockerCli(stdin, stdout, stderr) 然后创建DockerCli对象，DockerCli对象在cli/cli.go里声明。\ncmd := newDockerCommand(dockerCli) 然后创建DockerCommand对象，这个是github.com/spf13/cobra库里所提及的所有命令的根命令。\n\tif err := cmd.Execute(); err != nil { if sterr, ok := err.(cli.StatusError); ok { if sterr.Status != \u0026quot;\u0026quot; { fmt.Fprintln(stderr, sterr.Status) } // StatusError should only be used for errors, and all errors should // have a non-zero exit status, so never exit with 0 if sterr.StatusCode == 0 { os.Exit(1) } os.Exit(sterr.StatusCode) } fmt.Fprintln(stderr, err) os.Exit(1) } 最后执行命令，如果有错误则打印到标准输出里，然后退出。\n看了下main函数，大家肯定知道关键代码肯定在cmd := newDockerCommand(dockerCli)这里。再来看newDockerCommand函数。\nopts := cliflags.NewClientOptions() var flags *pflag.FlagSet 这里首先创建了一个ClientOptions对象，一个*pflag.FlagSet对象\n\tcmd := \u0026amp;cobra.Command{ Use: \u0026quot;docker [OPTIONS] COMMAND [arg...]\u0026quot;, Short: \u0026quot;A self-sufficient runtime for containers.\u0026quot;, SilenceUsage: true, SilenceErrors: true, TraverseChildren: true, Args: noArgs, RunE: func(cmd *cobra.Command, args []string) error { if opts.Version { showVersion() return nil } fmt.Fprintf(dockerCli.Err(), \u0026quot;\\n\u0026quot;+cmd.UsageString()) return nil }, PersistentPreRunE: func(cmd *cobra.Command, args []string) error { // flags must be the top-level command flags, not cmd.Flags() opts.Common.SetDefaultOptions(flags) dockerPreRun(opts) return dockerCli.Initialize(opts) }, } cli.SetupRootCommand(cmd) 然后构造了一个github.com/spf13/cobra库里所提及的根命令，当用户执行docker命令，并且不匹配其它子命令时，则这个根命令将得到执行，也即打印docker命令的用法。再使用cli.SetupRootCommand(cmd)初始化根命令。这个方法在cli/cobra.go里声明。\n这里要提一下github.com/spf13/cobra库的工作原理。github.com/spf13/cobra库将一个命令行工具的所有命令抽象为一个层次结构，最上层为根命令，每个命令又可以定义它的子命令。每个命令在定义时可设置它的描述性文字，支持的选项、用法描述、命令的执行逻辑、相关模板等。用户执行命令行时，会根据命令行参数自动查找对应的命令，然后就可以运行该命令的执行逻辑了。详细用法可参阅github.com/spf13/cobra库的文档\n\tflags = cmd.Flags() flags.BoolVarP(\u0026amp;opts.Version, \u0026quot;version\u0026quot;, \u0026quot;v\u0026quot;, false, \u0026quot;Print version information and quit\u0026quot;) flags.StringVar(\u0026amp;opts.ConfigDir, \u0026quot;config\u0026quot;, cliconfig.ConfigDir(), \u0026quot;Location of client config files\u0026quot;) opts.Common.InstallFlags(flags) 这些是一些命令行参数的定义。\n\tcmd.SetOutput(dockerCli.Out()) 设置命令的输出为DockerCli的输出。\ncmd.AddCommand(newDaemonCommand()) 将DaemonCommand添加为根命令的子命令，这样docker daemon命令即可启动docker daemon。代码里也说到这个特性以后会移除的，所以这个命令的Hidden被设置为了true，即显示命令用法时，并不会显示它。newDaemonCommand函数定义在cmd/docker/daemon_unix.go里。\ncommands.AddCommands(cmd, dockerCli) 将其它子命令添加至根命令，commands.AddCommands函数定义在cli/command/commands/commands.go里。\nfunc AddCommands(cmd *cobra.Command, dockerCli *command.DockerCli) { cmd.AddCommand( node.NewNodeCommand(dockerCli), service.NewServiceCommand(dockerCli), stack.NewStackCommand(dockerCli), stack.NewTopLevelDeployCommand(dockerCli), swarm.NewSwarmCommand(dockerCli), container.NewContainerCommand(dockerCli), image.NewImageCommand(dockerCli), system.NewSystemCommand(dockerCli), container.NewRunCommand(dockerCli), image.NewBuildCommand(dockerCli), network.NewNetworkCommand(dockerCli), hide(system.NewEventsCommand(dockerCli)), registry.NewLoginCommand(dockerCli), registry.NewLogoutCommand(dockerCli), registry.NewSearchCommand(dockerCli), system.NewVersionCommand(dockerCli), volume.NewVolumeCommand(dockerCli), hide(system.NewInfoCommand(dockerCli)), hide(container.NewAttachCommand(dockerCli)), ... hide(system.NewInspectCommand(dockerCli)), checkpoint.NewCheckpointCommand(dockerCli), plugin.NewPluginCommand(dockerCli), ) } 可以看到这里定义了很多子命令，并添加为根命令的子命令，每个子命令构建时都将DockerCli对象传入了。同样为了保证兼容性的，对其它不少子命令用的hide函数对原有命令进行了处理，将其Hidden属性设置为了true。\n\treturn cmd 添加好子命令后，newDockerCommand函数就返回这个根命令退出了。\nClient命令行示例 这里我拿一个非常简单的子命令示例，来说明Docker客户端是如何运行的。\n比如执行docker system info命令，根据子命令定义，首先找到了system.NewSystemCommand函数，它是在cli/command/system/cmd.go里定义的。\nfunc NewSystemCommand(dockerCli *command.DockerCli) *cobra.Command { cmd := \u0026amp;cobra.Command{ Use: \u0026quot;system\u0026quot;, Short: \u0026quot;Manage Docker\u0026quot;, Args: cli.NoArgs, Run: func(cmd *cobra.Command, args []string) { fmt.Fprintf(dockerCli.Err(), \u0026quot;\\n\u0026quot;+cmd.UsageString()) }, } cmd.AddCommand( NewEventsCommand(dockerCli), NewInfoCommand(dockerCli), NewDiskUsageCommand(dockerCli), NewPruneCommand(dockerCli), ) return cmd } 又由于子命令info，所以找到NewInfoCommand函数，这是在cli/command/system/info.go里定义的。\n// NewInfoCommand creates a new cobra.Command for `docker info` func NewInfoCommand(dockerCli *command.DockerCli) *cobra.Command { var opts infoOptions cmd := \u0026amp;cobra.Command{ Use: \u0026quot;info [OPTIONS]\u0026quot;, Short: \u0026quot;Display system-wide information\u0026quot;, Args: cli.NoArgs, RunE: func(cmd *cobra.Command, args []string) error { return runInfo(dockerCli, \u0026amp;opts) }, } flags := cmd.Flags() flags.StringVarP(\u0026amp;opts.format, \u0026quot;format\u0026quot;, \u0026quot;f\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;Format the output using the given go template\u0026quot;) return cmd } func runInfo(dockerCli *command.DockerCli, opts *infoOptions) error { ctx := context.Background() info, err := dockerCli.Client().Info(ctx) if err != nil { return err } if opts.format == \u0026quot;\u0026quot; { return prettyPrintInfo(dockerCli, info) } return formatInfo(dockerCli, info, opts.format) } 找到了匹配的子命令后，当命令等到执行时，该命令的RunE属性就会得到调用，即会调用runInfo函数，这个函数会调用dockerCli.Client().Info函数，并将输出结果格式化并写到DockerCli的输出。\nInfo(ctx context.Context) (types.Info, error)是一个接口，定义在client/interface.go里，其实现定义在client/info.go里。\nfunc (cli *Client) Info(ctx context.Context) (types.Info, error) { var info types.Info serverResp, err := cli.get(ctx, \u0026quot;/info\u0026quot;, url.Values{}, nil) if err != nil { return info, err } defer ensureReaderClosed(serverResp) if err := json.NewDecoder(serverResp.body).Decode(\u0026amp;info); err != nil { return info, fmt.Errorf(\u0026quot;Error reading remote info: %v\u0026quot;, err) } return info, nil } 上述代码就比较简单了，就是向docker daemon里的api服务发送了一个get请求，并将响应结果用json解码，最终返回info。\n再看看cli.get函数，这个定义在client/request.go，说白了就是发送了一个HTTP请求，不解释。\n// getWithContext sends an http request to the docker API using the method GET with a specific go context. func (cli *Client) get(ctx context.Context, path string, query url.Values, headers map[string][]string) (serverResponse, error) { return cli.sendRequest(ctx, \u0026quot;GET\u0026quot;, path, query, nil, headers) } func (cli *Client) sendRequest(ctx context.Context, method, path string, query url.Values, obj interface{}, headers map[string][]string) (serverResponse, error) { var body io.Reader if obj != nil { var err error body, err = encodeData(obj) if err != nil { return serverResponse{}, err } if headers == nil { headers = make(map[string][]string) } headers[\u0026quot;Content-Type\u0026quot;] = []string{\u0026quot;application/json\u0026quot;} } return cli.sendClientRequest(ctx, method, path, query, body, headers) } func (cli *Client) sendClientRequest(ctx context.Context, method, path string, query url.Values, body io.Reader, headers map[string][]string) (serverResponse, error) { serverResp := serverResponse{ body: nil, statusCode: -1, } expectedPayload := (method == \u0026quot;POST\u0026quot; || method == \u0026quot;PUT\u0026quot;) if expectedPayload \u0026amp;\u0026amp; body == nil { body = bytes.NewReader([]byte{}) } req, err := cli.newRequest(method, path, query, body, headers) if err != nil { return serverResp, err } if cli.proto == \u0026quot;unix\u0026quot; || cli.proto == \u0026quot;npipe\u0026quot; { // For local communications, it doesn't matter what the host is. We just // need a valid and meaningful host name. (See #189) req.Host = \u0026quot;docker\u0026quot; } scheme, err := resolveScheme(cli.client.Transport) if err != nil { return serverResp, err } req.URL.Host = cli.addr req.URL.Scheme = scheme if expectedPayload \u0026amp;\u0026amp; req.Header.Get(\u0026quot;Content-Type\u0026quot;) == \u0026quot;\u0026quot; { req.Header.Set(\u0026quot;Content-Type\u0026quot;, \u0026quot;text/plain\u0026quot;) } resp, err := ctxhttp.Do(ctx, cli.client, req) if err != nil { if scheme == \u0026quot;https\u0026quot; \u0026amp;\u0026amp; strings.Contains(err.Error(), \u0026quot;malformed HTTP response\u0026quot;) { return serverResp, fmt.Errorf(\u0026quot;%v.\\n* Are you trying to connect to a TLS-enabled daemon without TLS?\u0026quot;, err) } if scheme == \u0026quot;https\u0026quot; \u0026amp;\u0026amp; strings.Contains(err.Error(), \u0026quot;bad certificate\u0026quot;) { return serverResp, fmt.Errorf(\u0026quot;The server probably has client authentication (--tlsverify) enabled. Please check your TLS client certification settings: %v\u0026quot;, err) } // Don't decorate context sentinel errors; users may be comparing to // them directly. switch err { case context.Canceled, context.DeadlineExceeded: return serverResp, err } if err, ok := err.(net.Error); ok { if err.Timeout() { return serverResp, ErrorConnectionFailed(cli.host) } if !err.Temporary() { if strings.Contains(err.Error(), \u0026quot;connection refused\u0026quot;) || strings.Contains(err.Error(), \u0026quot;dial unix\u0026quot;) { return serverResp, ErrorConnectionFailed(cli.host) } } } return serverResp, errors.Wrap(err, \u0026quot;error during connect\u0026quot;) } if resp != nil { serverResp.statusCode = resp.StatusCode } if serverResp.statusCode \u0026lt; 200 || serverResp.statusCode \u0026gt;= 400 { body, err := ioutil.ReadAll(resp.Body) if err != nil { return serverResp, err } if len(body) == 0 { return serverResp, fmt.Errorf(\u0026quot;Error: request returned %s for API route and version %s, check if the server supports the requested API version\u0026quot;, http.StatusText(serverResp.statusCode), req.URL) } var errorMessage string if (cli.version == \u0026quot;\u0026quot; || versions.GreaterThan(cli.version, \u0026quot;1.23\u0026quot;)) \u0026amp;\u0026amp; resp.Header.Get(\u0026quot;Content-Type\u0026quot;) == \u0026quot;application/json\u0026quot; { var errorResponse types.ErrorResponse if err := json.Unmarshal(body, \u0026amp;errorResponse); err != nil { return serverResp, fmt.Errorf(\u0026quot;Error reading JSON: %v\u0026quot;, err) } errorMessage = errorResponse.Message } else { errorMessage = string(body) } return serverResp, fmt.Errorf(\u0026quot;Error response from daemon: %s\u0026quot;, strings.TrimSpace(errorMessage)) } serverResp.body = resp.Body serverResp.header = resp.Header return serverResp, nil } func (cli *Client) newRequest(method, path string, query url.Values, body io.Reader, headers map[string][]string) (*http.Request, error) { apiPath := cli.getAPIPath(path, query) req, err := http.NewRequest(method, apiPath, body) if err != nil { return nil, err } // Add CLI Config's HTTP Headers BEFORE we set the Docker headers // then the user can't change OUR headers for k, v := range cli.customHTTPHeaders { req.Header.Set(k, v) } if headers != nil { for k, v := range headers { req.Header[k] = v } } return req, nil } 总结 Docker Client创建与命令执行整体逻辑也是比较清楚的。就是定义了一堆命令，然后根据命令行参数，找到cli/command目录下对应的命令执行，而执行逻辑又一般被转至client目录下对应的代码，这里一般都是拼凑一些HTTP请求的URL、参数等，然后使用client/request.go定义的方法向Docker API Server发送请求得到响应，再对响应进行解码得到对象，命令再对得到的对象进行分析处理，最终打印必要的输出。上面我仅分析了docker system info的执行过程，其它命令也很类似。\n参考 http://www.kancloud.cn/infoq/docker-source-code-analysis/80526 https://github.com/spf13/cobra https://github.com/spf13/pflag\n其它 docker的源码看起来倒不是很复杂，但代码的执行逻辑经常跳来跳去，看得比较累，建议还是将它的代码导入IDE，这样跳转比较方便。导入IDE的步骤如下：\nmkdir -p ~/dev/docker/src/github.com/docker cd ~/dev/docker/src/github.com/docker git clone https://github.com/docker/docker.git 然后在IDEA里新建一个Go的Module，名称为docker, 路径选择~/dev/docker。\n最后在IDEA里设置该项目的GOPATH，如下图。\n有时看不清楚代码的执行逻辑，打个断点调试一下也是不错的办法，可以在IDEA里建一个运行项目，如下图。\n","permalink":"https://jeremyxu2010.github.io/2016/10/docker%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-client%E5%88%9B%E5%BB%BA%E4%B8%8E%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C/","tags":["docker","golang"],"title":"docker源码分析-Client创建与命令执行"},{"categories":["golang开发"],"contents":"最近花了一个星期的时间看完了《Go语言程序设计》这本书，这本书不愧是大师的作品，写得很好。看过之后对golang语言的理解更深刻了。下面将书中提到的一些关键语言范式记录下来以备忘。\n常见范式  普通for循环  var s, sep string for i := 0; i \u0026lt; len(os.Args); i++ { s += sep + os.Args[i] sep = \u0026quot; \u0026quot; } fmt.Println(s)  for配合range  var s, sep string for _, arg := range os.Args[:] { s += sep + arg sep = \u0026quot; \u0026quot; } fmt.Println(s)  逐行处理输入  input := bufio.NewScanner(os.Stdin) for input.Scan() { fmt.Println(input.Text()) }  普通方法调用错误处理  f, err := os.Open(filename) //延迟关闭文件 defer f.Close() if err != nil { fmt.Fprintf(os.Stderr, \u0026quot;dup2: %v\\n\u0026quot;, err) return nil, err } // 正常地使用f  元组赋值  x, y = y, x a[i], a[j] = a[j], a[i]  定义命名类型  type Celsius float64 // 摄氏温度 type Fahrenheit float64 // 华氏温度  定义常量  const ( AbsoluteZeroC Celsius = -273.15 // 绝对零度 FreezingC Celsius = 0 // 结冰点温度 BoilingC Celsius = 100 // 沸水温度 )  定义命名类型的方法  func (c Celsius) String() string { return fmt.Sprintf(\u0026quot;%g°C\u0026quot;, c) }  定义包的初始化函数  func init() { // 进行必要的初始化动作 }  iota常量生成器  const ( _ = 1 \u0026lt;\u0026lt; (10 * iota) KiB // 1024 MiB // 1048576 GiB // 1073741824 TiB // 1099511627776 (exceeds 1 \u0026lt;\u0026lt; 32) PiB // 1125899906842624 EiB // 1152921504606846976 ZiB // 1180591620717411303424 (exceeds 1 \u0026lt;\u0026lt; 64) YiB // 1208925819614629174706176 )  遍历数组  var a [3]int for i, v := range a { fmt.Printf(\u0026quot;%d %d\\n\u0026quot;, i, v) }  定义长度根据初始值来确定的数组  q := [...]int{1, 2, 3} fmt.Printf(\u0026quot;%T\\n\u0026quot;, q) // \u0026quot;[3]int\u0026quot;  创建slice  var a []int // 值为nil的slice类型 a = make([]int) 或\na := make([]int) 或\na := []int{} 或\na := []int{1, 2}  数组转换为slice  q := [...]int{1, 2, 3} a := q[:]  slice的append操作  var runes []rune for _, r := range \u0026quot;Hello, 世界\u0026quot; { runes = append(runes, r) } fmt.Printf(\u0026quot;%q\\n\u0026quot;, runes) // \u0026quot;['H' 'e' 'l' 'l' 'o' ',' ' ' '世' '界']\u0026quot;  创建map  var m map[string]int //值为nil的map类型 m = make(map[string]int) 或\nm := make(map[string]int) 或\nm := map[string]int{} 或\nm := map[string]int{ \u0026quot;jerry\u0026quot;: 1 \u0026quot;tom\u0026quot;: 2, }  对map按序遍历  m := map[string]int{ \u0026quot;jerry\u0026quot;: 1 \u0026quot;tom\u0026quot;: 2, } var keys := make([]string, 0, len(m) for key := range m { keys = append(keys, key) } sort.Strings(keys) for _, key := range keys { fmt.Printf(\u0026quot;%s\\t%d\\n\u0026quot;, key, m[key]) }  用map模拟set  seen := make(map[string]struct{}) input := bufio.NewScanner(os.Stdin) for input.Scan() { line := input.Text() if !seen[line] { seen[line] = struct{}{} } } var lines := make([]string, 0, len(seen) for line := range seen { fmt.Println(line) }  定义结构体  type Employee struct { ID int Name string Address string DoB time.Time Position string Salary int ManagerID int }  创建结构变量  var dilbert Employee //结构体变量的成员均初始化为零值 或\ndilbert := Employee{ ID 1 Name \u0026quot;jerry\u0026quot; Address \u0026quot;blablabla\u0026quot; DoB time.Date(2016, time.April, 11, 0, 0, 0, 0, time.Local) Position \u0026quot;PD\u0026quot; Salary 13500 ManagerID 1 }  创建函数值  var f func(int)int //值为nil的函数值类型 func square(n int) int { return n * n } f = square 或\nfunc square(n int) int { return n * n } f := square  使用匿名函数  strings.Map(func(r rune) rune { return r + 1 }, \u0026quot;HAL-9000\u0026quot;)  递归使用匿名函数(闭包)  var order []string seen := make(map[string]bool) var visitAll func(items []string) visitAll = func(items []string) { for _, item := range items { if !seen[item] { seen[item] = true visitAll(m[item]) order = append(order, item) } } } var keys []string for key := range m { keys = append(keys, key) } sort.Strings(keys) visitAll(keys)  避免匿名函数使用循环变量快照  var rmdirs []func() for _, d := range tempDirs() { dir := d // NOTE: necessary! os.MkdirAll(dir, 0755) // creates parent directories too rmdirs = append(rmdirs, func() { os.RemoveAll(dir) }) } // ...do some work… for _, rmdir := range rmdirs { rmdir() // clean up } 或\nvar rmdirs []func() for _, d := range tempDirs() { os.MkdirAll(d, 0755) // creates parent directories too rmdirs = append(rmdirs, func(d *os.File){ return func(){ os.RemoveAll(d) } }(d)) } // ...do some work… for _, rmdir := range rmdirs { rmdir() // clean up }  函数使用可变参数  func sum(vals...int) int { total := 0 for _, val := range vals { total += val } return total }  defer配合使用互斥锁  var mu sync.Mutex var m = make(map[string]int) func lookup(key string) int { mu.Lock() defer mu.Unlock() return m[key] }  defer跟踪进入函数与退出函数  func bigSlowOperation() { defer trace(\u0026quot;bigSlowOperation\u0026quot;)() // don't forget the extra parentheses // ...lots of work… time.Sleep(10 * time.Second) // simulate slow operation by sleeping } func trace(msg string) func() { start := time.Now() log.Printf(\u0026quot;enter %s\u0026quot;, msg) return func() { log.Printf(\u0026quot;exit %s (%s)\u0026quot;, msg,time.Since(start)) } }  主动panic异常  func MustCompile(expr string) *Regexp { re, err := Compile(expr) if err != nil { panic(err) } return re }  处理自定义panic异常  // soleTitle returns the text of the first non-empty title element // in doc, and an error if there was not exactly one. func soleTitle(doc *html.Node) (title string, err error) { type bailout struct{} defer func() { switch p := recover(); p { case nil: // no panic case bailout{}: // \u0026quot;expected\u0026quot; panic err = fmt.Errorf(\u0026quot;multiple title elements\u0026quot;) default: panic(p) // unexpected panic; carry on panicking } }() // Bail out of recursion if we find more than one nonempty title. forEachNode(doc, func(n *html.Node) { if n.Type == html.ElementNode \u0026amp;\u0026amp; n.Data == \u0026quot;title\u0026quot; \u0026amp;\u0026amp; n.FirstChild != nil { if title != \u0026quot;\u0026quot; { panic(bailout{}) // multiple titleelements } title = n.FirstChild.Data } }, nil) if title == \u0026quot;\u0026quot; { return \u0026quot;\u0026quot;, fmt.Errorf(\u0026quot;no title element\u0026quot;) } return title, nil }  定义结构体的方法  type Point struct{ X, Y float64 } func (p Point) Distance(q Point) float64 { return math.Hypot(q.X-p.X, q.Y-p.Y) } func (p *Point) ScaleBy(factor float64) { p.X *= factor p.Y *= factor }  使用方法值  p := Point{1, 2} distanceFromP := p.Distance // method value var origin Point // {0, 0} fmt.Println(distanceFromP(origin)) // \u0026quot;2.23606797749979\u0026quot;, sqrt(5) scaleP := p.ScaleBy // method value scaleP(2) // p becomes (2, 4)  使用方法表达式  type Point struct{ X, Y float64 } func (p Point) Add(q Point) Point { return Point{p.X + q.X, p.Y + q.Y} } var op func(p, q Point) Point op = Point.Add var p Point var offset Point{3.0, 4.0} q := op(p, offset)  定义接口类型  // Writer is the interface that wraps the basic Write method. type Writer interface { // Write writes len(p) bytes from p to the underlying data stream. // It returns the number of bytes written from p (0 \u0026lt;= n \u0026lt;= len(p)) // and any error encountered that caused the write to stop early. // Write must return a non-nil error if it returns n \u0026lt; len(p). // Write must not modify the slice data, even temporarily. // // Implementations must not retain p. Write(p []byte) (n int, err error) }  具体类型转换为接口类型  var w io.Writer w = os.Stdout // OK: *os.File has Write method w = new(bytes.Buffer) // OK: *bytes.Buffer has Write method w = time.Second // compile error: time.Duration lacks Write method  接口类型转换为具体类型  var w io.Writer w = os.Stdout f := w.(*os.File) // success: f == os.Stdout c := w.(*bytes.Buffer) // panic: interface holds *os.File, not *bytes.Buffer 或\nvar w io.Writer w = os.Stdout if f, ok := w.(*os.File); ok { // ...use f... }  通过类型断言询问行为  // writeString writes s to w. // If w has a WriteString method, it is invoked instead of w.Write. func writeString(w io.Writer, s string) (n int, err error) { type stringWriter interface { WriteString(string) (n int, err error) } if sw, ok := w.(stringWriter); ok { return sw.WriteString(s) // avoid a copy } return w.Write([]byte(s)) // allocate temporary copy }  运用类型开关  func sqlQuote(x interface{}) string { switch x := x.(type) { case nil: return \u0026quot;NULL\u0026quot; case int, uint: return fmt.Sprintf(\u0026quot;%d\u0026quot;, x) // x has type interface{} here. case bool: if x { return \u0026quot;TRUE\u0026quot; } return \u0026quot;FALSE\u0026quot; case string: return sqlQuoteString(x) // (not shown) default: panic(fmt.Sprintf(\u0026quot;unexpected type %T: %v\u0026quot;, x, x)) } }  等待其它goroutine  done := make(chan struct{}) go func() { // do something done \u0026lt;- struct{}{} // signal the main goroutine }() // do something \u0026lt;-done // wait for background goroutine to finish  串联Channels  naturals := make(chan int) squares := make(chan int) // Counter go func() { for x := 0; x \u0026lt; 100; x++ { naturals \u0026lt;- x } close(naturals) }() // Squarer go func() { for x := range naturals { squares \u0026lt;- x * x } close(squares) }() // Printer (in main goroutine) for x := range squares { fmt.Println(x) }  单方向Channel  func counter(out chan\u0026lt;- int) { for x := 0; x \u0026lt; 100; x++ { out \u0026lt;- x } close(out) } func squarer(out chan\u0026lt;- int, in \u0026lt;-chan int) { for v := range in { out \u0026lt;- v * v } close(out) } func printer(in \u0026lt;-chan int) { for v := range in { fmt.Println(v) } } func main() { naturals := make(chan int) squares := make(chan int) go counter(naturals) go squarer(squares, naturals) printer(squares) }  使用带缓存的Channel  func mirroredQuery() string { responses := make(chan string, 3) go func() { responses \u0026lt;- request(\u0026quot;asia.gopl.io\u0026quot;) }() go func() { responses \u0026lt;- request(\u0026quot;europe.gopl.io\u0026quot;) }() go func() { responses \u0026lt;- request(\u0026quot;americas.gopl.io\u0026quot;) }() return \u0026lt;-responses // return the quickest response }  等待多个goroutines结束  // makeThumbnails6 makes thumbnails for each file received from the channel. // It returns the number of bytes occupied by the files it creates. func makeThumbnails6(filenames \u0026lt;-chan string) int64 { sizes := make(chan int64) var wg sync.WaitGroup // number of working goroutines for f := range filenames { wg.Add(1) // worker go func(f string) { defer wg.Done() thumb, err := thumbnail.ImageFile(f) if err != nil { log.Println(err) return } info, _ := os.Stat(thumb) // OK to ignore error sizes \u0026lt;- info.Size() }(f) } // closer go func() { wg.Wait() close(sizes) }() var total int64 for size := range sizes { total += size } return total }  中断某个goroutine  abort := make(chan struct{}) go func() { os.Stdin.Read(make([]byte, 1)) // read a single byte abort \u0026lt;- struct{}{} }() //!+ fmt.Println(\u0026quot;Commencing countdown. Press return to abort.\u0026quot;) tick := time.Tick(1 * time.Second) for countdown := 10; countdown \u0026gt; 0; countdown-- { fmt.Println(countdown) select { case \u0026lt;-tick: // Do nothing. case \u0026lt;-abort: fmt.Println(\u0026quot;Launch aborted!\u0026quot;) return } }  Channel实现信号量  // sema is a counting semaphore for limiting concurrency in dirents. var sema = make(chan struct{}, 20) // dirents returns the entries of directory dir. func dirents(dir string) []os.FileInfo { sema \u0026lt;- struct{}{} // acquire token defer func() { \u0026lt;-sema }() // release token entries, err := ioutil.ReadDir(dir) if err != nil { fmt.Fprintf(os.Stderr, \u0026quot;du1: %v\\n\u0026quot;, err) return nil } return entries }  中断多个goroutines  var done = make(chan struct{}) func cancelled() bool { select { case \u0026lt;-done: return true default: return false } } // Cancel traversal when input is detected. go func() { os.Stdin.Read(make([]byte, 1)) // read a single byte close(done) //close掉的Channel，其它goroutines如试图从该Channel接收，则可立即接收到 }() var sema = make(chan struct{}, 20) func dirents(dir string) []os.FileInfo { select { case sema \u0026lt;- struct{}{}: // acquire token case \u0026lt;-done: return nil // cancelled } defer func() { \u0026lt;-sema }() // release token // ...read directory... }  使用互斥锁  var ( mu sync.Mutex // guards balance balance int ) func Deposit(amount int) { mu.Lock() defer mu.Unlock() balance = balance + amount } func Balance() int { mu.Lock() defer mu.Unlock() b := balance return b }  使用读写锁  var ( mu sync.RWMutex // guards balance balance int ) func Deposit(amount int) { mu.Lock() defer mu.Unlock() balance = balance + amount } func Balance() int { mu.RLock() defer mu.RUnlock() return balance }  懒加载数据  var loadIconsOnce sync.Once var icons map[string]image.Image // Concurrency-safe. func Icon(name string) image.Image { loadIconsOnce.Do(loadIcons) return icons[name] }  表格驱动的测试  func TestIsPalindrome(t *testing.T) { var tests = []struct { input string want bool }{ {\u0026quot;\u0026quot;, true}, {\u0026quot;a\u0026quot;, true}, {\u0026quot;aa\u0026quot;, true}, {\u0026quot;ab\u0026quot;, false}, {\u0026quot;kayak\u0026quot;, true}, {\u0026quot;detartrated\u0026quot;, true}, {\u0026quot;A man, a plan, a canal: Panama\u0026quot;, true}, {\u0026quot;Evil I did dwell; lewd did I live.\u0026quot;, true}, {\u0026quot;Able was I ere I saw Elba\u0026quot;, true}, {\u0026quot;été\u0026quot;, true}, {\u0026quot;Et se resservir, ivresse reste.\u0026quot;, true}, {\u0026quot;palindrome\u0026quot;, false}, // non-palindrome {\u0026quot;desserts\u0026quot;, false}, // semi-palindrome } for _, test := range tests { if got := IsPalindrome(test.input); got != test.want { t.Errorf(\u0026quot;IsPalindrome(%q) = %v\u0026quot;, test.input, got) } } }  使用命令行参数  var n = flag.Bool(\u0026quot;n\u0026quot;, false, \u0026quot;omit trailing newline\u0026quot;) var sep = flag.String(\u0026quot;s\u0026quot;, \u0026quot; \u0026quot;, \u0026quot;separator\u0026quot;) func main() { flag.Parse() fmt.Print(strings.Join(flag.Args(), *sep)) if !*n { fmt.Println() } } 其它 另外在看这本书的过程中也对一些其它概念有了进一步认识。\n Unicode与UTF8的关系   在很久以前，世界还是比较简单的，起码计算机世界就只有一个ASCII字符集：美国信息交换标准代码。ASCII，更准确地说是美国的ASCII，使用7bit来表示128个字符：包含英文字母的大小写、数字、各种标点符号和设置控制符。对于早期的计算机程序来说，这些就足够了，但是这也导致了世界上很多其他地区的用户无法直接使用自己的符号系统。随着互联网的发展，混合多种语言的数据变得很常见（译注：比如本身的英文原文或中文翻译都包含了ASCII、中文、日文等多种语言字符）。如何有效处理这些包含了各种语言的丰富多样的文本数据呢？\n  答案就是使用Unicode（ http://unicode.org ），它收集了这个世界上所有的符号系统，包括重音符号和其它变音符号，制表符和回车符，还有很多神秘的符号，每个符号都分配一个唯一的Unicode码点，Unicode码点对应Go语言中的rune整数类型（译注：rune是int32等价类型）。\n  在第八版本的Unicode标准收集了超过120,000个字符，涵盖超过100多种语言。这些在计算机程序和数据中是如何体现的呢？通用的表示一个Unicode码点的数据类型是int32，也就是Go语言中rune对应的类型；它的同义词rune符文正是这个意思。\n  我们可以将一个符文序列表示为一个int32序列。这种编码方式叫UTF-32或UCS-4，每个Unicode码点都使用同样的大小32bit来表示。这种方式比较简单统一，但是它会浪费很多存储空间，因为大数据计算机可读的文本是ASCII字符，本来每个ASCII字符只需要8bit或1字节就能表示。而且即使是常用的字符也远少于65,536个，也就是说用16bit编码方式就能表达常用字符。但是，还有其它更好的编码方法吗？\n  UTF8是一个将Unicode码点编码为字节序列的变长编码。UTF8编码由Go语言之父Ken Thompson和Rob Pike共同发明的，现在已经是Unicode的标准。UTF8编码使用1到4个字节来表示每个Unicode码点，ASCII部分字符只使用1个字节，常用字符部分使用2或3个字节表示。每个符号编码后第一个字节的高端bit位用于表示总共有多少编码个字节。如果第一个字节的高端bit为0，则表示对应7bit的ASCII字符，ASCII字符每个字符依然是一个字节，和传统的ASCII编码兼容。如果第一个字节的高端bit是110，则说明需要2个字节；后续的每个高端bit都以10开头。更大的Unicode码点也是采用类似的策略处理。\n  0xxxxxxx runes 0-127 (ASCII) 110xxxxx 10xxxxxx 128-2047 (values \u0026lt;128 unused) 1110xxxx 10xxxxxx 10xxxxxx 2048-65535 (values \u0026lt;2048 unused) 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx 65536-0x10ffff (other values unused)\n  变长的编码无法直接通过索引来访问第n个字符，但是UTF8编码获得了很多额外的优点。首先UTF8编码比较紧凑，完全兼容ASCII码，并且可以自动同步：它可以通过向前回朔最多2个字节就能确定当前字符编码的开始字节的位置。它也是一个前缀编码，所以当从左向右解码时不会有任何歧义也并不需要向前查看（译注：像GBK之类的编码，如果不知道起点位置则可能会出现歧义）。没有任何字符的编码是其它字符编码的子串，或是其它编码序列的字串，因此搜索一个字符时只要搜索它的字节编码序列即可，不用担心前后的上下文会对搜索结果产生干扰。同时UTF8编码的顺序和Unicode码点的顺序一致，因此可以直接排序UTF8编码序列。同时因为没有嵌入的NUL(0)字节，可以很好地兼容那些使用NUL作为字符串结尾的编程语言。\n 总结来说，Unicode用一个int32序列表示了每个符文(rune)。而UTF8则将Unicode码点编码为字节序列，按照UTF8编码的规则，它编码出的长度是不固定的，从1个Byte至4个Byte。\n Goroutines和线程的区别   每一个OS线程都有一个固定大小的内存块(一般会是2MB)来做栈，这个栈会用来存储当前正在被调用或挂起(指在调用其它函数时)的函数的内部变量。这个固定大小的栈同时很大又很小。因为2MB的栈对于一个小小的goroutine来说是很大的内存浪费，比如对于我们用到的，一个只是用来WaitGroup之后关闭channel的goroutine来说。而对于go程序来说，同时创建成百上千个gorutine是非常普遍的，如果每一个goroutine都需要这么大的栈的话，那这么多的goroutine就不太可能了。除去大小的问题之外，固定大小的栈对于更复杂或者更深层次的递归函数调用来说显然是不够的。修改固定的大小可以提升空间的利用率允许创建更多的线程，并且可以允许更深的递归调用，不过这两者是没法同时兼备的。相反，一个goroutine会以一个很小的栈开始其生命周期，一般只需要2KB。一个goroutine的栈，和操作系统线程一样，会保存其活跃或挂起的函数调用的本地变量，但是和OS线程不太一样的是一个goroutine的栈大小并不是固定的；栈的大小会根据需要动态地伸缩。而goroutine的栈的最大值有1GB，比传统的固定大小的线程栈要大得多，尽管一般情况下，大多goroutine都不需要这么大的栈。\n  OS线程会被操作系统内核调度。每几毫秒，一个硬件计时器会中断处理器，这会调用一个叫作scheduler的内核函数。这个函数会挂起当前执行的线程并保存内存中它的寄存器内容，检查线程列表并决定下一次哪个线程可以被运行，并从内存中恢复该线程的寄存器信息，然后恢复执行该线程的现场并开始执行线程。因为操作系统线程是被内核所调度，所以从一个线程向另一个“移动”需要完整的上下文切换，也就是说，保存一个用户线程的状态到内存，恢复另一个线程的到寄存器，然后更新调度器的数据结构。这几步操作很慢，因为其局部性很差需要几次内存访问，并且会增加运行的cpu周期。Go的运行时包含了其自己的调度器，这个调度器使用了一些技术手段，比如m:n调度，因为其会在n个操作系统线程上多工(调度)m个goroutine。Go调度器的工作和内核的调度是相似的，但是这个调度器只关注单独的Go程序中的goroutine。和操作系统的线程调度不同的是，Go调度器并不是用一个硬件定时器而是被Go语言\u0026quot;建筑\u0026quot;本身进行调度的。因为因为这种调度方式不需要进入内核的上下文，所以重新调度一个goroutine比调度一个线程代价要低得多。\n  在大多数支持多线程的操作系统和程序语言中，当前的线程都有一个独特的身份(id)，并且这个身份信息可以以一个普通值的形式被被很容易地获取到，典型的可以是一个integer或者指针值。这种情况下我们做一个抽象化的thread-local storage(线程本地存储，多线程编程中不希望其它线程访问的内容)就很容易，只需要以线程的id作为key的一个map就可以解决问题，每一个线程以其id就能从中获取到值，且和其它线程互不冲突。goroutine没有可以被程序员获取到的身份(id)的概念。这一点是设计上故意而为之，由于thread-local storage总是会被滥用。比如说，一个web server是用一种支持tls的语言实现的，而非常普遍的是很多函数会去寻找HTTP请求的信息，这代表它们就是去其存储层(这个存储层有可能是tls)查找的。这就像是那些过分依赖全局变量的程序一样，会导致一种非健康的“距离外行为”，在这种行为下，一个函数的行为可能不是由其自己内部的变量所决定，而是由其所运行在的线程所决定。因此，如果线程本身的身份会改变——比如一些worker线程之类的——那么函数的行为就会变得神秘莫测。Go鼓励更为简单的模式，这种模式下参数对函数的影响都是显式的。这样不仅使程序变得更易读，而且会让我们自由地向一些给定的函数分配子任务时不用担心其身份信息影响行为。\n  golang中CSP并发编程模型   CSP模型是上个世纪七十年代提出的，用于描述两个独立的并发实体通过共享的通讯 channel(管道)进行通信的并发模型。 CSP中channel是第一类对象，它不关注发送消息的实体，而关注与发送消息时使用的channel。\n  Golang 就是借用CSP模型的一些概念为之实现并发进行理论支持，其实从实际上出发，go语言并没有，完全实现了CSP模型的所有理论，仅仅是借用了 process和channel这两个概念。process是在go语言上的表现就是 goroutine 是实际并发执行的实体，每个实体之间是通过channel通讯来实现数据共享。\n  Golang中使用 CSP中 channel 这个概念。channel 是被单独创建并且可以在进程之间传递，它的通信模式类似于 boss-worker 模式的，一个实体通过将消息发送到channel 中，然后又监听这个 channel 的实体处理，两个实体之间是匿名的，这个就实现实体中间的解耦，其中 channel 是同步的一个消息被发送到 channel 中，最终是一定要被另外的实体消费掉的，在实现原理上其实是一个阻塞的消息队列。\n 参考 Go语言程序设计 http://www.jianshu.com/p/36e246c6153d\n","permalink":"https://jeremyxu2010.github.io/2016/10/golang%E8%AF%AD%E8%A8%80%E5%B8%B8%E8%A7%81%E8%8C%83%E5%BC%8F/","tags":["golang"],"title":"golang语言常见范式"},{"categories":["golang开发"],"contents":"之前项目中使用go写过不少命令行工具，这些命令行工具主要进行网络扫描任务。很可惜当时并没有留下任何记录，今天突然想起这个事，觉得还是有必要将学习golang的过程记录下来，以便以后复习。\n安装golang环境 mac下安装golang很简单\nbrew install golang 安装gotour及goimports 一列周边的工具命令还是有必要安装一下的，比如gotour，goimports。由于使用get命令默认会将命令安装到GOPATH的bin目录下，这些全局命令还是单独放在一个全局GOPATH里比较好。\nmkdir ~/dev/go_global export GOPATH=$HOME/dev/go_global go get golang.org/x/tour/gotour go get golang.org/x/tools/cmd/goimports # 设置全局的GOPATH，设置gotour、goimports的别名 echo \u0026#34;GO_GLOBAL_PATH=$HOME/dev/go_global GOPATH=$GO_GLOBAL_PATHalias gotour=$GO_GLOBAL_PATH/bin/gotour alias goimports=$GO_GLOBAL_PATH/bin/goimports \u0026#34; \u0026gt;\u0026gt; ~/.zshrc # 刷新zsh的配置缓存 src   gotour: 我主要是用来做一下小实验，在终端里直接输入gotour，这时就打开一个浏览器，在里面可以输入代码，点Run按钮就可以直接运行看结果，类似其他很多语言提供的REPL即时运行的工具。\n  goimports: 很多IDE会用这个命令对go的源代码格式化。\n  标准库文档 标准库中文文档地址：https://studygolang.com/pkgdoc\n设置项目GOPATH 另外我希望每进入一个不同的go项目，都能将这个项目的目录加入到GOPATH里，在网上找了一下，找到一个好办法\n# 重写cd命令，cd进入目录时，向上查找.gopath文件，如查找到，则设置.gopath文件所在目录为GOPATH echo \u0026#39; cd () { builtin cd \u0026#34;$@\u0026#34; cdir=$PWD while [ \u0026#34;$cdir\u0026#34; != \u0026#34;/\u0026#34; ]; do if [ -e \u0026#34;$cdir/.gopath\u0026#34; ]; then export GOPATH=$cdir break fi cdir=$(dirname \u0026#34;$cdir\u0026#34;) done } \u0026#39; \u0026gt;\u0026gt; ~/.zshrc # 刷新zsh的配置缓存 src 这样以后只须在go项目根目录下创建一个.gopath目录就可以了。\nIntelliJ IDEA设置 IDEA要开发go程序，需要安装Go语言支持，如下。\n然后就可以导入工程了。这里导入一下gopl.io这个工程，Go语言圣经这本书是引用这个工程的源码，把这个工程里源代码导入IDEA，以后查看代码会很方便。\nmkdir ~/dev/gobook export GOPATH=$HOME/dev/gobook go get gopl.io touch ~/dev/gobook/.gopath 最后在IDEA里新建一个名为gobook的Go项目，项目目录就指定为$HOME/dev/gobook，走完向导，这个Go项目就创建好了。\n总结 因为以前用过golang，简单看了下golang的入门教程还是大概知道怎么写go语言的代码了，接下来要再把书温习一翻，再多动手写点代码，争取早日将以前学过的go语言技巧都找回来。\n","permalink":"https://jeremyxu2010.github.io/2016/09/golang%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/","tags":["golang","idea"],"title":"golang开发环境搭建"},{"categories":["web开发"],"contents":"这两天移动互联网领域发生了一件大事，微信发布了小程序，然后网上突然涌出了诸多教程，貌似大家对微信小程序的功能与想象空间兴奋不已，我免不了俗，搭建个简易的开发环境玩一玩。\n搭建开发环境 按官方教程，是得先获得内测资格，取得微信小程序appid后，才可进行小程序开发的。很不幸，我没获取内测资格，但随便一搜，就找到网上的破解教程，我就学习玩一玩，就直接破解吧。\n首先下载0.7版的微信web开发工具\n百度: https://pan.baidu.com/s/1pLxqFzH （密码: bwt9）\n安装完毕后，打开开发工具用手机微信扫描登录进去，然后就可以关闭微信web开发工具，删除刚安装的微信web开发工具。你没有听错，就是删除，0.7版本的作用仅仅是用于登录，呵呵。\n再下载0.9版的微信web开发工具\n百度: https://pan.baidu.com/s/1pLTKIqJ （密码: iswg）\n再完成安装。\n完成后需替换3个文件：\n 使用https://raw.githubusercontent.com/gavinkwoe/weapp-ide-crack/master/createstep.js这个文件替换开发工具里/Resources/app.nw/app/dist/components/create/createstep.js。 使用https://raw.githubusercontent.com/gavinkwoe/weapp-ide-crack/master/projectStores.js这个文件替换开发工具里/Resources/app.nw/app/dist/stroes/projectStores.js。 使用https://raw.githubusercontent.com/gavinkwoe/weapp-ide-crack/master/asdebug.js这个文件替换开发工具里/Resources/app.nw/app/dist/weapp/appservice/asdebug.js。  之后打开开发工具，然后点添加项目创建一个新的小程序项目\n然后填写项目细节，我没有内测帐号，就随便填appid, 项目名称也随便填，本地开发目录选个空目录就好了。\n然后项目就打开了。\n不一会儿提示有更新，微信web开发工具升级了，提示重启，那么就重启吧。重启后要重替换一下asdebug.js文件，这样开发环境就算OK了。\n分析程序结构 先看一下程序结构\n其中app.js是用来注册一个小程序的，主要用来响应应用生命周期事件，还可以保存应用的全局状态，详细文档可参考http://wxopen.notedown.cn/framework/app-service/app.html。\napp.json是应用的全局配置文件，主要用来配置应用所包含的页面、设置应用全局样式、多tab应用的配置、各种网络请求的超时时间、是否开启调试等，详细文档可参考http://wxopen.notedown.cn/framework/config.html。\napp.wxss是应用的全局样式文件，样式文件的规范可参考http://wxopen.notedown.cn/framework/view/wxss.html。\npages目录下每个目录都是一个独立的页面，每个页面的结构都很类似。\n页面目录中的*.js定义了该页面的处理逻辑，可以定义初始化数据、响应页面的生命周期事件、定义视图层要用的事件处理函数。详细文档可参考http://wxopen.notedown.cn/framework/app-service/page.html。在这里可以调用微信提供的各种API，API参见http://wxopen.notedown.cn/api/，看样子还是挺丰富的。\n页面目录中的*.wxml定义了该页面的视图层，用于将逻辑层的数据展现，同时将组件的某些用户操作绑定至事件处理函数。详细文档参见http://wxopen.notedown.cn/framework/view/wxml/。可用的组件文档参见http://wxopen.notedown.cn/component/。\n页面目录中的*.wxss定义了该页面用到的样式。\n简单分析示例代码 app.js\n//app.js App({ onLaunch: function () { //调用API从本地缓存中获取数据  var logs = wx.getStorageSync(\u0026#39;logs\u0026#39;) || [] logs.unshift(Date.now()) wx.setStorageSync(\u0026#39;logs\u0026#39;, logs) }, getUserInfo:function(cb){ var that = this; if(this.globalData.userInfo){ typeof cb == \u0026#34;function\u0026#34; \u0026amp;\u0026amp; cb(this.globalData.userInfo) }else{ //调用登录接口  wx.login({ success: function () { wx.getUserInfo({ success: function (res) { that.globalData.userInfo = res.userInfo; typeof cb == \u0026#34;function\u0026#34; \u0026amp;\u0026amp; cb(that.globalData.userInfo) } }) } }); } }, globalData:{ userInfo:null } }) 这个代码比较简单。\n 响应应用的启动事件，在其中调用微信API读取本地存储中的logs值，往里插入一条当前时间后，再存回本地存储的logs值 提供一个getUserInfo方法，便于其它页面快捷地获取用户信息 为了减少调用微信API的次数，将获取到的用户信息保存在全局变量中  index.js\n//index.js //获取应用实例 var app = getApp() Page({ data: { motto: \u0026#39;Hello World\u0026#39;, userInfo: {} }, //事件处理函数  bindViewTap: function() { wx.navigateTo({ url: \u0026#39;../logs/logs\u0026#39; }) }, onLoad: function () { console.log(\u0026#39;onLoad\u0026#39;) var that = this //调用应用实例的方法获取全局数据  app.getUserInfo(function(userInfo){ //更新数据  that.setData({ userInfo:userInfo }) that.update() }) } }) 上述代码也很简单。\n 定义页面所依赖的初始数据data 定义了一个事件处理函数bindViewTap 响应页面的加载事件，在其中获取用户信息，获取到用户信息后更新数据data，最后强制刷新视图  index.wxml\n\u0026lt;!--index.wxml--\u0026gt; \u0026lt;view class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;view class=\u0026#34;userinfo\u0026#34; bindtap=\u0026#34;bindViewTap\u0026#34;\u0026gt; \u0026lt;image class=\u0026#34;userinfo-avatar\u0026#34; src=\u0026#34;{{userInfo.avatarUrl}}\u0026#34; background-size=\u0026#34;cover\u0026#34;\u0026gt;\u0026lt;/image\u0026gt; \u0026lt;text class=\u0026#34;userinfo-nickname\u0026#34;\u0026gt;{{userInfo.nickName}}\u0026lt;/text\u0026gt; \u0026lt;/view\u0026gt; \u0026lt;view class=\u0026#34;usermotto\u0026#34;\u0026gt; \u0026lt;text class=\u0026#34;user-motto\u0026#34;\u0026gt;{{motto}}\u0026lt;/text\u0026gt; \u0026lt;/view\u0026gt; \u0026lt;/view\u0026gt; 视图层就更简单了，就是绑定逻辑层数据，然后使用bindtap=\u0026quot;bindViewTap\u0026quot;绑定了一个逻辑层的事件处理函数。\nlogs目录下的代码与上面也很类似，就不具体解释了。唯一要说明的是其中引入自已写的其它js，这个地方是使用的CommonJS的引入语法。\nvar util = require(\u0026#39;../../utils/util.js\u0026#39;) 评价 整合看整个小程序的源码架构还是比较简单的，特别是每个页面使用了View-ViewModel的结构，以前使用过ReactJS，对这个还是挺熟悉的，用起来很自然。\n小程序提供的组件还是比较丰富的，而且试用了下，反应很迅速，猜测是类似ReactNative的实现方案。\n微信给小程序提供的API也还算丰富，剩下就看利用这些API都构想出什么创意了。\n小遗憾 估计真实开发很少人会直接用开发工具里的编辑器的，反正我是习惯用sublime的。但每次在sublime里改完代码都需要回到开发工具里按Cmd+B重新编辑后才可以查看效果，习惯了webpack，gulp那种保存后即编译刷新的开发方式，这样还真不习惯。\n参考 http://blog.csdn.net/xiehuimx/article/details/52629657 https://github.com/gavinkwoe/weapp-ide-crack http://wxopen.notedown.cn/framework/structure.html\n","permalink":"https://jeremyxu2010.github.io/2016/09/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E4%B8%8A%E6%89%8B/","tags":["weixin","reactnative","mvvm"],"title":"微信小程序上手"},{"categories":["web开发"],"contents":"今天在工作中遇到一个关于promise有趣的小问题，这里分享一下分析的过程。\n原始版本 //这个方法模拟从服务端加载数据 var loadData = function(){ return fetch(\u0026#39;/\u0026#39;).then(function(data){ return data.statusText }); }; loadData().then(function(data){ console.log(data); }); 上面这一小段方法本也没什么错，但考虑如果使用数据的地方比较多，每个地方都向服务端加载数据，这样会不会加重服务端压力？\n来个简单的缓存 你一定会说来个简单的缓存吧，如下所示：\n//定义一个变量充当缓存 var cache = null; //下面的方法使用了cache var loadData = function(){ if(cache === null) { return fetch(\u0026#39;/\u0026#39;).then(function(data){ cache = data.statusText; return cache; }); } else { return Promise.resolve(cache); } }; //再定义了一个重新加载数据的方法 var reloadData = function(){ cache = null; return loadData(); }; loadData().then(function(data){ console.log(data); }); 一眼看过去，好像没有什么问题。\n但经过仔细推敲代码，发现还是存在问题的。当调用两次loadData()方法，而在调用第二次方法时，cache还为null，因此最终还是fetch了两次。\n判断一下promise的状态 你一定会说要判断一下promise的状态，好吧，这样试一下。\nvar loadPromise = null; var loadData = function(){ //在加载数据时，如发现loadPromise为null，才重新加载  if(loadPromise === null) { loadPromise = fetch(\u0026#39;/\u0026#39;).then(function(data){ return data.statusText; }); } //否则返回已经存在的promise对象  return loadPromise; }; var reloadData = function(){ loadPromise = null; return loadData(); }; loadData().then(function(data){ console.log(data); }); 可以看到上述代码连cache变量都没使用了。这里是将loadPromise的resolved值当成缓存来用了。\n为啥可以这么干？参见这里https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Promise\n Promise 对象是一个返回值的代理，这个返回值在promise对象创建时未必已知。它允许你为异步操作的成功或失败指定处理方法。 这使得异步方法可以像同步方法那样返回值：异步方法会返回一个包含了原返回值的 promise 对象来替代原返回值。\n  Promise对象有以下几种状态:\n   pending: 初始状态, 既不是 fulfilled 也不是 rejected. fulfilled: 成功的操作. rejected: 失败的操作.    pending状态的promise对象既可转换为带着一个成功值的fulfilled 状态，也可变为带着一个失败信息的 rejected 状态。当状态发生转换时，promise.then绑定的方法（函数句柄）就会被调用。(当绑定方法时，如果 promise对象已经处于 fulfilled 或 rejected 状态，那么相应的方法将会被立刻调用， 所以在异步操作的完成情况和它的绑定方法之间不存在竞争条件。)\n 你估计会认为这次看上去OK了吧？\n很遗憾还是存在问题。。。\n试想一下，如果在加载数据时偶尔出现异常，loadPromise最终变为一个rejected状态的promise对象。即使以后故障解决了，这时调用loadData()还是只能拿到一个rejected状态的promise对象。\n判断一下rejected状态 这次我们判断一下rejected状态。很可惜，原生的Promise并没有提供同步API直接获取某个promise对象的状态，所以这里采取一个变通的办法。\nvar loadPromise = null; //定义一个变量用来保存Promise是否处于rejected状态 var loadRejected = false; var loadData = function(){ //在加载数据时，如发现loadPromise为null或promise为rejected状态，才重新加载  if(loadPromise === null || loadRejected) { //一旦准备加载数据，则重置rejected状态  loadRejected = false; loadPromise = fetch(\u0026#39;/\u0026#39;).then(function(data){ return data.statusText; }).then(undefined, function(){ //如加载过程出现异常，则记录rejected状态  loadRejected = true; }); } return loadPromise; }; var reloadData = function(){ loadPromise = null; return loadData(); }; loadData().then(function(data){ console.log(data); }); 仔细检查了好几遍，暂时没有发现其它问题。如有高手发现问题请通知我。\n总结 HTML5中的Promise确实是个好特性，但用起来真的有很小心，不然很容易出问题。\n参考 https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Promise\n","permalink":"https://jeremyxu2010.github.io/2016/09/%E5%88%A9%E7%94%A8promise%E5%AE%9E%E7%8E%B0%E7%AE%80%E5%8D%95%E7%9A%84%E5%89%8D%E7%AB%AFcache/","tags":["html5","promise"],"title":"利用promise实现简单的前端cache"},{"categories":["云计算"],"contents":"以前写到一篇文章制作CentOS6基础镜像，今天在工作中突然要临时创建很多虚拟机，于是结合那篇文章得到的基础镜像，写了个简单的脚本快速创建KVM虚拟机。\n快速创建一个虚拟机的脚本 这里假设创建的基础镜像为centos6.7-sys.img，而且是qcow2格式的。\ncreate_vm.sh\n#!/bin/bash  domain_name=$1 ip_fetch_method=$2 static_ip=$3 static_netmask=$4 static_gateway=$5 static_dns1=$6 static_dns2=$7 base_img_path=/vmdata/base/centos6.7-sys.img vm_img_dir=/vmdata #创建虚拟机的磁盘镜像文件 qemu-img create -f qcow2 -b $base_img_path $vm_img_dir/$domain_name.img #将磁盘镜像文件挂载至宿主机目录，便于修改内部文件 guestmount -i -w -a $vm_img_dir/$domain_name.img /mnt #设置主机名 echo \u0026#34;NETWORKING=yes HOSTNAME=$domain_name\u0026#34; \u0026gt; /mnt/etc/sysconfig/network #如采用静态IP，则设置IP地址相关信息 if [ $ip_fetch_method == \u0026#34;static\u0026#34; ] ; then echo \u0026#34;DEVICE=eth0 TYPE=Ethernet ONBOOT=yes NM_CONTROLLED=no BOOTPROTO=static IPADDR=$static_ipNETMASK=$static_netmaskGATEWAY=$static_gatewayDNS1=$static_dns1DNS2=$static_dns2\u0026#34; \u0026gt; /mnt/etc/sysconfig/network-scripts/ifcfg-eth0 fi #取消挂载 umount /mnt #创建并启动虚拟机 virt-install --import --name=$domain_name --vcpus=2 --ram 2048 --boot hd --disk path=$vm_img_dir/$domain_name.img,format=qcow2,bus=virtio --network bridge=br0,model=virtio --autostart --graphics vnc,keymap=en-us --noautoconsole 脚本的逻辑比较简单，注释写得很清楚。需要注意跟以前不太一样的地方有两点：\n 这次是使用guestmount命令将磁盘文件先挂载至临时目录，这样修改磁盘文件里的内容就很方便了，个人感觉这个比以前用的virt-copy-in命令方案还是简单一些的。guestmount命令的用法可直接man guestmount查看。 这次是使用virt-install命令创建并启动虚拟机。这种方式相当快速，也是一般推荐的快速命令行创建虚拟机的办法。virt-install命令的参数相当多，虚拟机配置的方方面面都有参数，这里只使用了一些必须的，再详细的参数说明可直接man virt-install查看。  快速创建N个虚拟机 再写一个脚本，根据业务需要，调用上述脚本快速创建虚拟机。\n#!/bin/bash  vm_name_prefix=\u0026#39;test\u0026#39; #循环创建20个虚拟机 for((i=1; i\u0026lt;=20; i++)) do create_vm.sh $vm_name_prefix$i static 10.10.10.$i 255.255.255.0 10.10.10.254 202.96.134.133 8.8.8.8 done 其它 /usr/sbin/virt-install本身就是用python编写的，使用了libvirt库API的python绑定，如果想了解如何使用libvirt库API，个人觉得这个源码还是可以读一读的。\n","permalink":"https://jeremyxu2010.github.io/2016/09/%E5%BF%AB%E9%80%9F%E5%88%9B%E5%BB%BAkvm%E8%99%9A%E6%8B%9F%E6%9C%BA/","tags":["kvm","qemu","libvirt","libguestfs"],"title":"快速创建KVM虚拟机"},{"categories":["devops"],"contents":"昨天阅读一篇讲tcpdump使用技巧的文章，里面简单提到了TCP握手挥手的流程，还附了一片图片。\n虽然TCP握手挥手大学里学到，但很惭愧竟忘得差不多了。今天抽了点时间将这些细节重新复习一下。\n在复习的过程中看到别人写的一篇文章，感觉别人讲得挺详细的，这里先直接将别人的文章转载过来。\n TCP是什么？ 具体的关于TCP是什么，我不打算详细的说了；当你看到这篇文章时，我想你也知道TCP的概念了，想要更深入的了解TCP的工作，我们就继续。它只是一个超级麻烦的协议，而它又是互联网的基础，也是每个程序员必备的基本功。首先来看看OSI的七层模型：\n我们需要知道TCP工作在网络OSI的七层模型中的第四层——Transport层，IP在第三层——Network层，ARP在第二层——Data Link层；在第二层上的数据，我们把它叫Frame，在第三层上的数据叫Packet，第四层的数据叫Segment。 同时，我们需要简单的知道，数据从应用层发下来，会在每一层都会加上头部信息，进行封装，然后再发送到数据接收端。这个基本的流程你需要知道，就是每个数据都会经过数据的封装和解封装的过程。 在OSI七层模型中，每一层的作用和对应的协议如下：\nTCP是一个协议，那这个协议是如何定义的，它的数据格式是什么样子的呢？要进行更深层次的剖析，就需要了解，甚至是熟记TCP协议中每个字段的含义。哦，来吧。\n 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Source Port | Destination Port | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Sequence Number | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Acknowledgment Number | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Data | |C|E|U|A|P|R|S|F| | | Offset| Res. |W|C|R|C|S|S|Y|I| Window | | | |R|E|G|K|H|T|N|N| | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Checksum | Urgent Pointer | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Options | Padding | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | data | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ 上面就是TCP协议头部的格式，由于它太重要了，是理解其它内容的基础，下面就将每个字段的信息都详细的说明一下。\n  Source Port和Destination Port:分别占用16位，表示源端口号和目的端口号；用于区别主机中的不同进程，而IP地址是用来区分不同的主机的，源端口号和目的端口号配合上IP首部中的源IP地址和目的IP地址就能唯一的确定一个TCP连接；\n  Sequence Number:用来标识从TCP发端向TCP收端发送的数据字节流，它表示在这个报文段中的的第一个数据字节在数据流中的序号；主要用来解决网络报乱序的问题；\n  Acknowledgment Number:32位确认序列号包含发送确认的一端所期望收到的下一个序号，因此，确认序号应当是上次已成功收到数据字节序号加1。不过，只有当标志位中的ACK标志（下面介绍）为1时该确认序列号的字段才有效。主要用来解决不丢包的问题；\n  Offset:给出首部中32 bit字的数目，需要这个值是因为任选字段的长度是可变的。这个字段占4bit（最多能表示15个32bit的的字，即4*15=60个字节的首部长度），因此TCP最多有60字节的首部。然而，没有任选字段，正常的长度是20字节；\n  TCP Flags:TCP首部中有6个标志比特，它们中的多个可同时被设置为1，主要是用于操控TCP的状态机的，依次为URG，ACK，PSH，RST，SYN，FIN。每个标志位的意思如下：\n  URG：此标志表示TCP包的紧急指针域（后面马上就要说到）有效，用来保证TCP连接不被中断，并且督促中间层设备要尽快处理这些数据；\n  ACK：此标志表示应答域有效，就是说前面所说的TCP应答号将会包含在TCP数据包中；有两个取值：0和1，为1的时候表示应答域有效，反之为0；\n  PSH：这个标志位表示Push操作。所谓Push操作就是指在数据包到达接收端以后，立即传送给应用程序，而不是在缓冲区中排队；\n  RST：这个标志表示连接复位请求。用来复位那些产生错误的连接，也被用来拒绝错误和非法的数据包；\n  SYN：表示同步序号，用来建立连接。SYN标志位和ACK标志位搭配使用，当连接请求的时候，SYN=1，ACK=0；连接被响应的时候，SYN=1，ACK=1；这个标志的数据包经常被用来进行端口扫描。扫描者发送一个只有SYN的数据包，如果对方主机响应了一个数据包回来 ，就表明这台主机存在这个端口；但是由于这种扫描方式只是进行TCP三次握手的第一次握手，因此这种扫描的成功表示被扫描的机器不很安全，一台安全的主机将会强制要求一个连接严格的进行TCP的三次握手；\n  FIN： 表示发送端已经达到数据末尾，也就是说双方的数据传送完成，没有数据可以传送了，发送FIN标志位的TCP数据包后，连接将被断开。这个标志的数据包也经常被用于进行端口扫描。\n  Window:窗口大小，也就是有名的滑动窗口，用来进行流量控制；这是一个复杂的问题，这篇博文中并不会进行总结的；\n  好了，基本知识都已经准备好了，开始下一段的征程吧。\n三次握手又是什么？ TCP是面向连接的，无论哪一方向另一方发送数据之前，都必须先在双方之间建立一条连接。其实，网络上的传输是没有连接的，包括TCP也是一样的。而TCP所谓的“连接”，其实只不过是在通讯的双方维护一个“连接状态”，让它看上去好像有连接一样。所以，TCP的状态变换是非常重要的。在TCP/IP协议中，TCP协议提供可靠的连接服务，连接是通过三次握手进行初始化的。三次握手的目的是同步连接双方的序列号和确认号并交换 TCP窗口大小信息。这就是面试中经常会被问到的TCP三次握手。只是了解TCP三次握手的概念，对你获得一份工作是没有任何帮助的，你需要去了解TCP三次握手中的一些细节。先来看图说话。\n多么清晰的一张图，当然了，也不是我画的，我也只是引用过来说明问题了。\n 第一次握手：建立连接。客户端发送连接请求报文段，将SYN位置为1，Sequence Number为x；然后，客户端进入SYN_SEND状态，等待服务器的确认； 第二次握手：服务器收到SYN报文段。服务器收到客户端的SYN报文段，需要对这个SYN报文段进行确认，设置Acknowledgment Number为x+1(Sequence Number+1)；同时，自己自己还要发送SYN请求信息，将SYN位置为1，Sequence Number为y；服务器端将上述所有信息放到一个报文段（即SYN+ACK报文段）中，一并发送给客户端，此时服务器进入SYN_RECV状态； 第三次握手：客户端收到服务器的SYN+ACK报文段。然后将Acknowledgment Number设置为y+1，向服务器发送ACK报文段，这个报文段发送完毕以后，客户端和服务器端都进入ESTABLISHED状态，完成TCP三次握手。  完成了三次握手，客户端和服务器端就可以开始传送数据。以上就是TCP三次握手的总体介绍。\n那四次分手呢？ 当客户端和服务器通过三次握手建立了TCP连接以后，当数据传送完毕，肯定是要断开TCP连接的啊。那对于TCP的断开连接，这里就有了神秘的“四次分手”。\n 第一次分手：主机1（可以使客户端，也可以是服务器端），设置Sequence Number和Acknowledgment Number，向主机2发送一个FIN报文段；此时，主机1进入FIN_WAIT_1状态；这表示主机1没有数据要发送给主机2了； 第二次分手：主机2收到了主机1发送的FIN报文段，向主机1回一个ACK报文段，Acknowledgment Number为Sequence Number加1；主机1进入FIN_WAIT_2状态；主机2告诉主机1，我“同意”你的关闭请求； 第三次分手：主机2向主机1发送FIN报文段，请求关闭连接，同时主机2进入LAST_ACK状态； 第四次分手：主机1收到主机2发送的FIN报文段，向主机2发送ACK报文段，然后主机1进入TIME_WAIT状态；主机2收到主机1的ACK报文段以后，就关闭连接；此时，主机1等待2MSL后依然没有收到回复，则证明Server端已正常关闭，那好，主机1也可以关闭连接了。  至此，TCP的四次分手就这么愉快的完成了。当你看到这里，你的脑子里会有很多的疑问，很多的不懂，感觉很凌乱；没事，我们继续总结。\n为什么要三次握手 既然总结了TCP的三次握手，那为什么非要三次呢？怎么觉得两次就可以完成了。那TCP为什么非要进行三次连接呢？在谢希仁的《计算机网络》中是这样说的：\n 为了防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误。\n 在书中同时举了一个例子，如下：\n “已失效的连接请求报文段”的产生在这样一种情况下：client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。”\n 这就很明白了，防止了服务器端的一直等待而浪费资源。\n为什么要四次分手 那四次分手又是为何呢？TCP协议是一种面向连接的、可靠的、基于字节流的运输层通信协议。TCP是全双工模式，这就意味着，当主机1发出FIN报文段时，只是表示主机1已经没有数据要发送了，主机1告诉主机2，它的数据已经全部发送完毕了；但是，这个时候主机1还是可以接受来自主机2的数据；当主机2返回ACK报文段时，表示它已经知道主机1没有数据发送了，但是主机2还是可以发送数据到主机1的；当主机2也发送了FIN报文段时，这个时候就表示主机2也没有数据要发送了，就会告诉主机1，我也没有数据要发送了，之后彼此就会愉快的中断这次TCP连接。如果要正确的理解四次分手的原理，就需要了解四次分手过程中的状态变化。\n  FIN_WAIT_1: 这个状态要好好解释一下，其实FIN_WAIT_1和FIN_WAIT_2状态的真正含义都是表示等待对方的FIN报文。而这两种状态的区别是：FIN_WAIT_1状态实际上是当SOCKET在ESTABLISHED状态时，它想主动关闭连接，向对方发送了FIN报文，此时该SOCKET即进入到FIN_WAIT_1状态。而当对方回应ACK报文后，则进入到FIN_WAIT_2状态，当然在实际的正常情况下，无论对方何种情况下，都应该马上回应ACK报文，所以FIN_WAIT_1状态一般是比较难见到的，而FIN_WAIT_2状态还有时常常可以用netstat看到。（主动方）\n  FIN_WAIT_2：上面已经详细解释了这种状态，实际上FIN_WAIT_2状态下的SOCKET，表示半连接，也即有一方要求close连接，但另外还告诉对方，我暂时还有点数据需要传送给你(ACK信息)，稍后再关闭连接。（主动方）\n  CLOSE_WAIT：这种状态的含义其实是表示在等待关闭。怎么理解呢？当对方close一个SOCKET后发送FIN报文给自己，你系统毫无疑问地会回应一个ACK报文给对方，此时则进入到CLOSE_WAIT状态。接下来呢，实际上你真正需要考虑的事情是察看你是否还有数据发送给对方，如果没有的话，那么你也就可以 close这个SOCKET，发送FIN报文给对方，也即关闭连接。所以你在CLOSE_WAIT状态下，需要完成的事情是等待你去关闭连接。（被动方）\n  LAST_ACK: 这个状态还是比较容易好理解的，它是被动关闭一方在发送FIN报文后，最后等待对方的ACK报文。当收到ACK报文后，也即可以进入到CLOSED可用状态了。（被动方）\n  TIME_WAIT: 表示收到了对方的FIN报文，并发送出了ACK报文，就等2MSL后即可回到CLOSED可用状态了。如果FINWAIT1状态下，收到了对方同时带FIN标志和ACK标志的报文时，可以直接进入到TIME_WAIT状态，而无须经过FIN_WAIT_2状态。（主动方）\n  CLOSED: 表示连接中断。\n   下面记录一下这次翻阅文档，看到的一些以前不太清楚的TCP细节。\nSYN Flood攻击 试想一下，如果server端接到了clien发的SYN后回了SYN-ACK后client掉线了，server端没有收到client回来的ACK，那么，这个连接处于一个中间状态，即没成功，也没失败。于是，server端如果在一定时间内没有收到的TCP会重发SYN-ACK。在Linux下，默认重试次数为5次，重试的间隔时间从1s开始每次都翻售，5次的重试时间间隔为1s, 2s, 4s, 8s, 16s，总共31s，第5次发出后还要等32s都知道第5次也超时了，所以，总共需要 1s + 2s + 4s+ 8s+ 16s + 32s = 2^6 -1 = 63s，TCP才会把断开这个连接。\n一些恶意的人就为此制造了SYN Flood攻击——给服务器发了一个SYN后，就下线了，于是服务器需要默认等63s才会断开连接，这样，攻击者就可以把服务器的syn连接的队列耗尽，让正常的连接请求不能处理。于是，Linux下给了一个叫tcp_syncookies的参数来应对这个事——当SYN队列满了后，TCP会通过源地址端口、目标地址端口和时间戳打造出一个特别的Sequence Number发回去（又叫cookie），如果是攻击者则不会有响应，如果是正常连接，则会把这个 SYN Cookie发回来，然后服务端可以通过cookie建连接（即使你不在SYN队列中）。请注意，请先千万别用tcp_syncookies来处理正常的大负载的连接的情况。因为，synccookies是妥协版的TCP协议，并不严谨。对于正常的请求，你应该调整三个TCP参数可供你选择，第一个是：tcp_synack_retries 可以用他来减少重试次数；第二个是：tcp_max_syn_backlog，可以增大SYN连接数；第三个是：tcp_abort_on_overflow 处理不过来干脆就直接拒绝连接了。\nISN的初始化 ISN是不能hard code的，不然会出问题的——比如：如果连接建好后始终用1来做ISN，如果client发了30个segment过去，但是网络断了，于是 client重连，又用了1做ISN，但是之前连接的那些包到了，于是就被当成了新连接的包，此时，client的Sequence Number 可能是3，而Server端认为client端的这个号是30了。全乱了。RFC793中说，ISN会和一个假的时钟绑在一起，这个时钟会在每4微秒对ISN做加一操作，直到超过2^32，又从0开始。这样，一个ISN的周期大约是4.55个小时。因为，我们假设我们的TCP Segment在网络上的存活时间不会超过Maximum Segment Lifetime（缩写为MSL），所以，只要MSL的值小于4.55小时，那么，我们就不会重用到ISN。\nMSL 和 TIME_WAIT 通过上面的ISN的描述，相信你也知道MSL是怎么来的了。我们注意到，在TCP的状态图中，从TIME_WAIT状态到CLOSED状态，有一个超时设置，这个超时设置是 2*MSL（RFC793定义了MSL为2分钟，Linux设置成了30s）为什么要这有TIME_WAIT？为什么不直接给转成CLOSED状态呢？主要有两个原因：1）TIME_WAIT确保有足够的时间让对端收到了ACK，如果被动关闭的那方没有收到Ack，就会触发被动端重发Fin，一来一去正好2个MSL，2）有足够的时间让这个连接不会跟后面的连接混在一起（你要知道，有些自做主张的路由器会缓存IP数据包，如果连接被重用了，那么这些延迟收到的包就有可能会跟新连接混在一起）。\nTIME_WAIT数量太多 从上面的描述我们可以知道，TIME_WAIT是个很重要的状态，但是如果在大并发的短链接下，TIME_WAIT 就会太多，这也会消耗很多系统资源。只要搜一下，你就会发现，十有八九的处理方式都是教你设置两个参数，一个叫tcp_tw_reuse，另一个叫tcp_tw_recycle的参数，这两个参数默认值都是被关闭的，后者recyle比前者resue更为激进，resue要温柔一些。另外，如果使用tcp_tw_reuse，必需设置tcp_timestamps=1，否则无效。这里，你一定要注意，打开这两个参数会有比较大的坑——可能会让TCP连接出一些诡异的问题（因为如上述一样，如果不等待超时重用连接的话，新的连接可能会建不上。)\n  关于tcp_tw_reuse。官方文档上说tcp_tw_reuse 加上tcp_timestamps（又叫PAWS, for Protection Against Wrapped Sequence Numbers）可以保证协议的角度上的安全，但是你需要tcp_timestamps在两边都被打开（你可以读一下tcp_twsk_unique的源码 ）。我个人估计还是有一些场景会有问题。\n  关于tcp_tw_recycle。如果是tcp_tw_recycle被打开了话，会假设对端开启了tcp_timestamps，然后会去比较时间戳，如果时间戳变大了，就可以重用。但是，如果对端是一个NAT网络的话（如：一个公司只用一个IP出公网）或是对端的IP被另一台重用了，这个事就复杂了。建链接的SYN可能就被直接丢掉了（你可能会看到connection time out的错误）（如果你想观摩一下Linux的内核代码，请参看源码 tcp_timewait_state_process）。\n  关于tcp_max_tw_buckets。这个是控制并发的TIME_WAIT的数量，默认值是180000，如果超限，那么，系统会把多的给destory掉，然后在日志里打一个警告（如：time wait bucket table overflow），官网文档说这个参数是用来对抗DDoS攻击的。也说的默认值180000并不小。这个还是需要根据实际情况考虑。\n  使用tcp_tw_reuse和tcp_tw_recycle来解决TIME_WAIT的问题是非常非常危险的，因为这两个参数违反了TCP协议。\n其实，TIME_WAIT表示的是你主动断连接，所以，这就是所谓的“不作死不会死”。试想，如果让对端断连接，那么这个破问题就是对方的了，呵呵。另外，如果你的服务器是于HTTP服务器，那么设置一个HTTP的KeepAlive有多重要（浏览器会重用一个TCP连接来处理多个HTTP请求），然后让客户端去断链接（你要小心，浏览器可能会非常贪婪，他们不到万不得已不会主动断连接）。\nTCP滑动窗口 我们都知道，TCP必需要解决的可靠传输以及包乱序（reordering）的问题，所以，TCP必需要知道网络实际的数据处理带宽或是数据处理速度，这样才不会引起网络拥塞，导致丢包。\n所以，TCP引入了一些技术和设计来做网络流控，Sliding Window是其中一个技术。 前面我们说过，TCP头里有一个字段叫Window，又叫Advertised-Window，这个字段是接收端告诉发送端自己还有多少缓冲区可以接收数据。于是发送端就可以根据这个接收端的处理能力来发送数据，而不会导致接收端处理不过来。\nTCP的拥塞处理-慢热启动算法 TCP通过Sliding Window来做流控（Flow Control），但是TCP觉得这还不够。如果网络上的延时突然增加，那么，TCP对这个事做出的应对只有重传数据，但是，重传会导致网络的负担更重，于是会导致更大的延迟以及更多的丢包，于是，这个情况就会进入恶性循环被不断地放大。试想一下，如果一个网络内有成千上万的TCP连接都这么行事，那么马上就会形成“网络风暴”，TCP这个协议就会拖垮整个网络。这是一个灾难。\n对此TCP的设计理念是：TCP不是一个自私的协议，当拥塞发生的时候，要做自我牺牲。就像交通阻塞一样，每个车都应该把路让出来，而不要再去抢路了。\n慢启动的意思是，刚刚加入网络的连接，一点一点地提速，不要一上来就像那些特权车一样霸道地把路占满。新同学上高速还是要慢一点，不要把已经在高速上的秩序给搞乱了。\n参考 http://www.jellythink.com/archives/705\nhttp://coolshell.cn/articles/11564.html\nhttp://coolshell.cn/articles/11609.html\n","permalink":"https://jeremyxu2010.github.io/2016/09/tcp%E7%BB%86%E8%8A%82%E5%88%86%E6%9E%90/","tags":["network","tcp"],"title":"TCP细节分析"},{"categories":["devops"],"contents":"很早就用过Wireshark进行抓包分析，但当时写过滤表达式很是一知半解，多半是从网上抄来的，根本没理解过滤表达式的含义。今天有幸看到一篇tcpdump入门使用技巧，看了下感觉挺好，终于知道到底怎么写过滤表达式了，这里转载过来备忘一下。\n以下内容转载自这里http://linuxwiki.github.io/NetTools/tcpdump.html\n 一般情况下，非HTTP协议的网络分析，在服务器端用tcpdump比较多，在客户端用wireshark比较多，两个抓包软件的语法是一样的。\n基本语法  过滤主机  抓取所有经过eth1，目的或源地址是192.168.1.1的网络数据\ntcpdump -i eth1 host 192.168.1.1 指定源地址\ntcpdump -i eth1 src host 192.168.1.1 指定目的地址\ntcpdump -i eth1 dst host 192.168.1.1  过滤端口  抓取所有经过eth1，目的或源端口是25的网络数据\ntcpdump -i eth1 port 25 指定源端口\ntcpdump -i eth1 src port 25 指定目的端口\ntcpdump -i eth1 dst port 25  网络过滤  tcpdump -i eth1 net 192.168 tcpdump -i eth1 src net 192.168 tcpdump -i eth1 dst net 192.168  协议过滤  tcpdump -i eth1 arp tcpdump -i eth1 ip tcpdump -i eth1 tcp tcpdump -i eth1 udp tcpdump -i eth1 icmp  常用表达式  非 : ! or \u0026quot;not\u0026quot; (去掉双引号) 且 : \u0026amp;\u0026amp; or \u0026quot;and\u0026quot; 或 : || or \u0026quot;or\u0026quot; 抓取所有经过eth1，目的地址是192.168.1.254或192.168.1.200端口是80的TCP数据\ntcpdump -i eth1 \u0026#39;((tcp) and (port 80) and ((dst host 192.168.1.254) or (dst host 192.168.1.200)))\u0026#39; 抓取所有经过eth1，目标MAC地址是00:01:02:03:04:05的ICMP数据\ntcpdump -i eth1 \u0026#39;((icmp) and ((ether dst host 00:01:02:03:04:05)))\u0026#39; 抓取所有经过eth1，目的网络是192.168，但目的主机不是192.168.1.200的TCP数据\ntcpdump -i eth1 \u0026#39;((tcp) and ((dst net 192.168) and (not dst host 192.168.1.200)))\u0026#39; 高级包头过滤 首先了解如何从包头过滤信息\nproto[x:y] : 过滤从x字节开始的y字节数。比如ip[2:2]过滤出3、4字节（第一字节从0开始排） proto[x:y] \u0026amp; z = 0 : proto[x:y]和z的与操作为0 proto[x:y] \u0026amp; z !=0 : proto[x:y]和z的与操作不为0 proto[x:y] \u0026amp; z = z : proto[x:y]和z的与操作为z proto[x:y] = z : proto[x:y]等于z 操作符 : \u0026gt;, \u0026lt;, \u0026gt;=, \u0026lt;=, =, !=\n IP头   0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Version| IHL |Type of Service| Total Length | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Identification |Flags| Fragment Offset | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Time to Live | Protocol | Header Checksum | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Source Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Destination Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Options | Padding | \u0026lt;-- optional +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | DATA ... | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ 本文只针对IPv4。\n IP选项设置了吗？  “一般”的IP头是20字节，但IP头有选项设置，不能直接从偏移21字节处读取数据。IP头有个长度字段可以知道头长度是否大于20字节。\n +-+-+-+-+-+-+-+-+ |Version| IHL | +-+-+-+-+-+-+-+-+ 通常第一个字节的二进制值是：01000101，分成两个部分：\n0100 = 4 表示IP版本 0101 = 5 表示IP头32 bit的块数，5 x 32 bits = 160 bits or 20 bytes\n如果第一字节第二部分的值大于5，那么表示头有IP选项。\n下面介绍两种过滤方法（第一种方法比较操蛋，可忽略）：\n 比较第一字节的值是否大于01000101，这可以判断IPv4带IP选项的数据和IPv6的数据。  01000101十进制等于69，计算方法如下（小提示：用计算器更方便）\n0 : 0 \\ 1 : 2^6 = 64 \\ 第一部分 (IP版本) 0 : 0 / 0 : 0 / - 0 : 0 \\ 1 : 2^2 = 4 \\ 第二部分 (头长度) 0 : 0 / 1 : 2^0 = 1 / 64 + 4 + 1 = 69 如果设置了IP选项，那么第一自己是01000110（十进制70），过滤规则：\ntcpdump -i eth1 \u0026#39;ip[0] \u0026gt; 69\u0026#39; IPv6的数据也会匹配，看看第二种方法。\n 位操作  0100 0101 : 第一字节的二进制 0000 1111 : 与操作 \u0026lt;========= 0000 0101 : 结果 正确的过滤方法\ntcpdump -i eth1 \u0026#39;ip[0] \u0026amp; 15 \u0026gt; 5\u0026#39; 或者\ntcpdump -i eth1 \u0026#39;ip[0] \u0026amp; 0x0f \u0026gt; 5\u0026#39;  分片标记  当发送端的MTU大于到目的路径链路上的MTU时就会被分片，这段话有点拗口，权威的请参考《TCP/IP详解》。唉，32借我的书没还，只能凑合写，大家记得看书啊。\n分片信息在IP头的第七和第八字节：\n +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Flags| Fragment Offset | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Bit 0: 保留，必须是0 Bit 1: (DF) 0 = 可能分片, 1 = 不分片 Bit 2: (MF) 0 = 最后的分片, 1 = 还有分片\nFragment Offset字段只有在分片的时候才使用。\n要抓带DF位标记的不分片的包，第七字节的值应该是：\n01000000 = 64\ntcpdump -i eth1 \u0026#39;ip[6] = 64\u0026#39;  抓分片包  匹配MF，分片包\ntcpdump -i eth1 \u0026#39;ip[6] = 32\u0026#39; 最后分片包的开始3位是0，但是有Fragment Offset字段。\n匹配分片和最后分片\ntcpdump -i eth1 \u0026#39;((ip[6:2] \u0026gt; 0) and (not ip[6] = 64))\u0026#39; 测试分片可以用下面的命令：\nping -M want -s 3000 192.168.1.1  匹配小TTL  TTL字段在第九字节，并且正好是完整的一个字节，TTL最大值是255，二进制为11111111。\n可以用下面的命令验证一下：\n$ ping -M want -s 3000 -t 256 192.168.1.200 ping: ttl 256 out of range  +-+-+-+-+-+-+-+-+ | Time to Live | +-+-+-+-+-+-+-+-+ 在网关可以用下面的命令看看网络中谁在使用traceroute\ntcpdump -i eth1 \u0026#39;ip[8] \u0026lt; 5\u0026#39;  抓大于X字节的包  大于600字节\ntcpdump -i eth1 \u0026#39;ip[2:2] \u0026gt; 600\u0026#39;  更多的IP过滤  首先还是需要知道TCP基本结构，再次推荐《TCP/IP详解》，卷一就够看的了，避免走火入魔。\nTCP头\n 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Source Port | Destination Port | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Sequence Number | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Acknowledgment Number | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Data | |C|E|U|A|P|R|S|F| | | Offset| Res. |W|C|R|C|S|S|Y|I| Window | | | |R|E|G|K|H|T|N|N| | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Checksum | Urgent Pointer | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Options | Padding | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | data | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ 抓取源端口大于1024的TCP数据包\ntcpdump -i eth1 \u0026#39;tcp[0:2] \u0026gt; 1024\u0026#39; 匹配TCP数据包的特殊标记 TCP标记定义在TCP头的第十四个字节\n +-+-+-+-+-+-+-+-+ |C|E|U|A|P|R|S|F| |W|C|R|C|S|S|Y|I| |R|E|G|K|H|T|N|N| +-+-+-+-+-+-+-+-+ 重复一下TCP三次握手，两个主机是如何勾搭的：\n源发送SYN 目标回答SYN, ACK 源发送ACK 没女朋友的童鞋要学习一下：\n MM，你的手有空吗？\u0026ndash; 有空，你呢？~~ 我也有空 _  失败的loser是酱紫的：\n MM，这是你掉的板砖吗？(SYN) ￣▽￣ 不是，找拍啊？(RST-ACK) ˋ﹏ˊ  只抓SYN包，第十四字节是二进制的00000010，也就是十进制的2\ntcpdump -i eth1 \u0026#39;tcp[13] = 2\u0026#39; 抓SYN, ACK （00010010 or 18）\ntcpdump -i eth1 \u0026#39;tcp[13] = 18\u0026#39; 抓SYN或者SYN-ACK\ntcpdump -i eth1 \u0026#39;tcp[13] \u0026amp; 2 = 2\u0026#39; 用到了位操作，就是不管ACK位是啥。\n抓PSH-ACK\ntcpdump -i eth1 \u0026#39;tcp[13] = 24\u0026#39; 抓所有包含FIN标记的包（FIN通常和ACK一起，表示幽会完了，回见）\ntcpdump -i eth1 \u0026#39;tcp[13] \u0026amp; 1 = 1\u0026#39; 抓RST（勾搭没成功，伟大的greatwall对她认为有敏感信息的连接发RST包，典型的棒打鸳鸯）\ntcpdump -i eth1 \u0026#39;tcp[13] \u0026amp; 4 = 4\u0026#39; 下图详细描述了TCP各种状态的标记，方便分析。\n 大叔注  tcpdump考虑了一些数字恐惧症者的需求，提供了部分常用的字段偏移名字：\nicmptype (ICMP类型字段) icmpcode (ICMP符号字段) tcpflags (TCP标记字段)\nICMP类型值有：\nicmp-echoreply, icmp-unreach, icmp-sourcequench, icmp-redirect, icmp-echo, icmp-routeradvert, icmp-routersolicit, icmp-timxceed, icmp-paramprob, icmp-tstamp, icmp-tstampreply, icmp-ireq, icmp-ireqreply, icmp-maskreq, icmp-maskreply\nTCP标记值：\ntcp-fin, tcp-syn, tcp-rst, tcp-push, tcp-push, tcp-ack, tcp-urg\n这样上面按照TCP标记位抓包的就可以写直观的表达式了：\n只抓SYN包\ntcpdump -i eth1 \u0026#39;tcp[tcpflags] = tcp-syn\u0026#39; 抓SYN, ACK\ntcpdump -i eth1 \u0026#39;tcp[tcpflags] \u0026amp; tcp-syn != 0 and tcp[tcpflags] \u0026amp; tcp-ack != 0\u0026#39;  抓SMTP数据  tcpdump -i eth1 \u0026#39;((port 25) and (tcp[(tcp[12]\u0026gt;\u0026gt;2):4] = 0x4d41494c))\u0026#39; 抓取数据区开始为\u0026quot;MAIL\u0026quot;的包，\u0026ldquo;MAIL\u0026quot;的十六进制为0x4d41494c。\n 抓HTTP GET数据  tcpdump -i eth1 \u0026#39;tcp[(tcp[12]\u0026gt;\u0026gt;2):4] = 0x47455420\u0026#39; \u0026ldquo;GET \u0026ldquo;的十六进制是47455420\n 抓SSH返回  tcpdump -i eth1 \u0026#39;tcp[(tcp[12]\u0026gt;\u0026gt;2):4] = 0x5353482D\u0026#39; \u0026ldquo;SSH-\u0026ldquo;的十六进制是0x5353482D\ntcpdump -i eth1 \u0026#39;(tcp[(tcp[12]\u0026gt;\u0026gt;2):4] = 0x5353482D) and (tcp[((tcp[12]\u0026gt;\u0026gt;2)+4):2] = 0x312E)\u0026#39; 抓老版本的SSH返回信息，如\u0026quot;SSH-1.99..\u0026rdquo;\n大叔注 如果是为了查看数据内容，建议用tcpdump -s 0 -w filename把数据包都保存下来，然后用wireshark的Follow TCP Stream/Follow UDP Stream来查看整个会话的内容。\n-s 0是抓取完整数据包，否则默认只抓68字节。\n另外，用tcpflow也可以方便的获取TCP会话内容，支持tcpdump的各种表达式。\n UDP头   0 7 8 15 16 23 24 31 +--------+--------+--------+--------+ | Source | Destination | | Port | Port | +--------+--------+--------+--------+ | | | | Length | Checksum | +--------+--------+--------+--------+ | | | DATA ... | +-----------------------------------+ 抓DNS请求数据\ntcpdump -i eth1 udp dst port 53  其他  -c参数对于运维人员来说也比较常用，因为流量比较大的服务器，靠人工CTRL+C还是抓的太多，甚至导致服务器宕机，于是可以用-c参数指定抓多少个包。\ntime tcpdump -nn -i eth0 \u0026#39;tcp[tcpflags] = tcp-syn\u0026#39; -c 10000 \u0026gt; /dev/null 上面的命令计算抓10000个SYN包花费多少时间，可以判断访问量大概是多少。\n","permalink":"https://jeremyxu2010.github.io/2016/09/tcpdump%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%E8%BD%AC%E8%BD%BD/","tags":["network","tcpdump"],"title":"tcpdump使用技巧（转载）"},{"categories":["容器编排"],"contents":"起因 今天看到一个做docker开发工程师写的如何实现docker网络隔离的方案，总的来说就是找到docker容器对应的主机虚拟网卡，然后使用wondershaper或traffic control对虚拟网卡进行流量控制。这个方案还是比较简单的，不过看了下他给出的如何找容器对应的主机虚拟网卡的步骤，觉得还是过于麻烦，而且还依赖于nsenter与ethtool命令，这个感觉不太好，就想着要进行一下这个过程。\n改进 因为以前看到pipework的源码，对如何操作容器网络还是比较了解的，于是写了个简单脚本完成上述任务\n#首先得到容器进程的pid CON_PID=$(docker inspect \u0026#39;--format={{ .State.Pid }}\u0026#39; test) #首先得到容器的命名空间目录 CON_NET_SANDBOX=$(docker inspect \u0026#39;--format={{ .NetworkSettings.SandboxKey }}\u0026#39; test) #在netns目录下创建至容器网络名字空间的链接，方便下面在docker主机上执行ip netns命令对容器的网络名字空间进行操作 rm -f /var/run/netns/$CON_PID mkdir -p /var/run/netns ln -s $CON_NET_SANDBOX /var/run/netns/$CON_PID #获取主机虚拟网卡ID VETH_ID=$(ip netns exec $CON_PID ip link show eth0|head -n 1|awk -F: \u0026#39;{print $1}\u0026#39;) #获取主机虚拟网卡名称 VETH_NAME=$(ip link|grep \u0026#34;if${VETH_ID}:\u0026#34;|awk \u0026#39;{print $2}\u0026#39;|awk -F@ \u0026#39;{print $1}\u0026#39;) #最后删除在netns目录下创建的链接 rm -f /var/run/netns/$CON_PID 可以看到上述方案比原方案的优点在于仅使用了ip命令，比较简单，可惜原作者的博客没有开放评论权限，我也没法将这个改进办法告诉他。\n","permalink":"https://jeremyxu2010.github.io/2016/09/%E8%8E%B7%E5%8F%96docker%E5%AE%B9%E5%99%A8%E7%9A%84%E4%B8%BB%E6%9C%BA%E8%99%9A%E6%8B%9F%E7%BD%91%E5%8D%A1/","tags":["docker","netns"],"title":"获取docker容器的主机虚拟网卡"},{"categories":["devops"],"contents":"概念 现在云计算大行其道，以kvm和docker为代表，极大地利用了机器的硬件资源，模拟了操作系统，而在海量虚拟机场景下，传统的硬件交换机越来越难以满足需求了。为了更加高效地利用网络，SDN应运而生。而SDN如何落地很大程度上取决于如何用软件交换机替代传统的交换机。\n从名称来看，openvswitch就是一个用软件实现的虚拟交换机。一个物理交换机基本支持flows, VLANs, trunking, QoS, port aggregation, firewalling, 还有一些具备3层交换的功能，而虚拟环境kvm或者docker下的网络层就贫乏多了，没什么像样的东西。ovs就恰恰补充了这方面的功能。Open vSwitch 支持flows，VLANS, trunking和port aggregation，跟其他主流交换机基本一样。\n名词解释 Open vSwitch中许多网络上的概念与平时接触到的不同，这里介绍一下Open vSwitch中用到的一些名词。\n Packet （数据包） 网络转发的最小数据单元，每个包都来自某个端口，最终会被发往一个或多个目标端口，转发数据包的过程就是网络的唯一功能。 Bridge （网桥） Open vSwitch中的网桥对应物理交换机，其功能是根据一定流规则，把从端口收到的数据包转发到另一个或多个端口。    Normal Port: 用户可以把操作系统中的网卡绑定到Open vSwitch上，Open vSwitch会生成一个普通端口处理这块网卡进出的数据包。    Internal Port: 当设置端口类型为internal，Open vSwitch会创建一快虚拟网卡，此端口收到的所有数据包都会交给这块网卡，网卡发出的包会通过这个端口交给Open vSwitch。当Open vSwitch创建一个新网桥时，默认会创建一个与网桥同名的Internal Port，同时也创建一个与Port同名的Interface。三位一体，所以操作系统里就多了一块网卡，但是状态是down的。 Patch Port: 当机器中有多个Open vSwitch网桥时，可以使用Patch Port把两个网桥连起来。Patch Port总是成对出现，分别连接在两个网桥上，在两个网桥之间交换数据。Patch Port是机房术语，特指用于切换网线连接的接线卡。此卡上面网口成对出现，当需要把两台设备连接起来时，只需要把两台设备接入同一对网口即可。 Tunnel Port: 隧道端口是一种虚拟端口，支持使用gre或vxlan等隧道技术与位于网络上其他位置的远程端口通讯。 Interface （iface/接口） 接口是Open vSwitch与外部交换数据包的组件。一个接口就是操作系统的一块网卡，这块网卡可能是Open vSwitch生成的虚拟网卡，也可能是物理网卡挂载在Open vSwitch上，也可能是操作系统的虚拟网卡（TUN/TAP）挂载在Open vSwitch上。 Flow （流） 流定义了端口之间数据包的交换规则。每条流分为匹配和动作两部分，匹配部分选择哪些数据包需要可以通过这条流处理，动作决定这些匹配到的数据包如何转发。流描述了一个网桥上，端口到端口的转发规则。比如我可以定义这样一条流：当数据包来自端口A，则发往端口B, 来自端口A就是匹配部分，发往端口B就是动作部分。流的定义可能非常复杂，比如：当数据包来自端口A，并且其源MAC是aa:aa:aa:aa:aa:aa，并且其拥有vlan tag为a，并且其源IP是a.a.a.a，并且其协议是TCP，其TCP源端口号为a，则修改其源IP为b.b.b.b，发往端口B Datapath 由于流可能非常复杂，对每个进来的数据包都去尝试匹配所有流，效率会非常低，所以有了datapath这个东西。Datapath是流的一个缓存，会把流的执行结果保存起来，当下次遇到匹配到同一条流的数据包，直接通过datapath处理。考虑到转发效率，datapath完全是在内核态实现的，并且默认的超时时间非常短，大概只有3秒左右。  实操 安装OVS ovs的概念还比较复杂，还是实际操作一下印象比较深刻，我这里在CentOS7系统上操作一下。\n一向懒得编译安装，这里就直接使用别人编译好的rpm包安装。\nyum install -y http://mirror.beyondhosting.net/OpenVSwitch/openvswitch-2.3.1-1.el7.x86_64.rpm modprobe openvswitch systemctl enable openvswitch \u0026amp;\u0026amp; systemctl start openvswitch #检查一下是否安装成功 ovs-vsctl show 然后创建一个OVS的网桥，可以命令行操作如下：\novs-vsctl add-br ovsbr0 ovs-vsctl add-port ovsbr0 enp3s1 ifconfig enp3s1 0.0.0.0 ifconfig ovsbr0 188.188.100.54/24 route del default route add default gw 188.188.100.1 dev ovsbr0 不过由于我这里是远程操作，还是采取修改配置文件的方式：\n修改/etc/sysconfig/network-scripts/ifcfg-ovsbr0\nTYPE=\u0026#34;OVSBridge\u0026#34; DEVICETYPE=\u0026#34;ovs\u0026#34; BOOTPROTO=\u0026#34;static\u0026#34; DEFROUTE=\u0026#34;yes\u0026#34; IPV4_FAILURE_FATAL=\u0026#34;no\u0026#34; IPV6INIT=\u0026#34;yes\u0026#34; IPV6_AUTOCONF=\u0026#34;yes\u0026#34; IPV6_DEFROUTE=\u0026#34;yes\u0026#34; IPV6_FAILURE_FATAL=\u0026#34;no\u0026#34; NAME=\u0026#34;ovsbr0\u0026#34; DEVICE=\u0026#34;ovsbr0\u0026#34; ONBOOT=\u0026#34;yes\u0026#34; IPADDR=\u0026#34;188.188.100.54\u0026#34; PREFIX=\u0026#34;24\u0026#34; GATEWAY=\u0026#34;188.188.100.1\u0026#34; DNS1=\u0026#34;202.96.134.133\u0026#34; IPV6_PEERDNS=\u0026#34;yes\u0026#34; IPV6_PEERROUTES=\u0026#34;yes\u0026#34; IPV6_PRIVACY=\u0026#34;no\u0026#34; 修改/etc/sysconfig/network-scripts/ifcfg-enp3s1\nOVS_BRIDGE=\u0026#34;ovsbr0\u0026#34; TYPE=\u0026#34;OVSPort\u0026#34; DEVICETYPE=\u0026#34;ovs\u0026#34; BOOTPROTO=\u0026#34;none\u0026#34; DEFROUTE=\u0026#34;yes\u0026#34; IPV4_FAILURE_FATAL=\u0026#34;no\u0026#34; IPV6INIT=\u0026#34;yes\u0026#34; IPV6_AUTOCONF=\u0026#34;yes\u0026#34; IPV6_DEFROUTE=\u0026#34;yes\u0026#34; IPV6_FAILURE_FATAL=\u0026#34;no\u0026#34; NAME=\u0026#34;enp3s1\u0026#34; DEVICE=\u0026#34;enp3s1\u0026#34; ONBOOT=\u0026#34;yes\u0026#34; PREFIX=\u0026#34;24\u0026#34; IPV6_PEERDNS=\u0026#34;yes\u0026#34; IPV6_PEERROUTES=\u0026#34;yes\u0026#34; IPV6_PRIVACY=\u0026#34;no\u0026#34; 最后重启一下网络\nsystemctl restart network #检查一下OVS网桥 ovs-vsctl show 上述步骤跟Linux Bridge网桥的创建过程很相似。\n在KVM里代替Linux Bridge网桥使用 在KVM里想用ovs网桥步骤也与Linux Bridge网桥很类似，如下：\n编辑ovsbr0.xml\n\u0026lt;network\u0026gt; \u0026lt;name\u0026gt;ovsbr0\u0026lt;/name\u0026gt; \u0026lt;forward mode=\u0026#39;bridge\u0026#39;/\u0026gt; \u0026lt;bridge name=\u0026#39;ovsbr0\u0026#39;/\u0026gt; \u0026lt;virtualport type=\u0026#39;openvswitch\u0026#39;/\u0026gt; \u0026lt;/network\u0026gt; 使用libvirt创建一个网络\nvirsh net-define ovsbr0.xml virsh net-start ovsbr0 virsh net-autostart ovsbr0 最后在安装kvm虚拟机时使用ovsbr0\nvirt-install \\  -n nagios \\  -r 4096 \\  --disk path=/export/kvm/nagios.qcow2,format=qcow2,size=60 \\  --vcpus 4 \\  --noautoconsole \\  --cdrom=/export/kvm/iso/FAN-2.4-x86_64.iso \\  --os-type=linux \\  --network network:ovsbr0 \\  --vnc --vnclisten=0.0.0.0 --vncport=5901 或使用virsh edit vm1.xml修改kvm虚拟机的定义\n\u0026lt;interface type=\u0026#39;bridge\u0026#39;\u0026gt; \u0026lt;source bridge=\u0026#39;ovsbr0\u0026#39;/\u0026gt; \u0026lt;virtualport type=\u0026#39;openvswitch\u0026#39;\u0026gt; \u0026lt;/interface\u0026gt; 还可以将正在运行的KVM虚拟机的vnet网络接口强制接到ovs上\n#可使用virsh dumpxml $vmname|grep vnet得到某个KVM虚拟机在宿主机上对应的网络接口 ovs-vsctl add-port ovsbr0 vnet0 设置VLAN 如果只是上面的用法，那跟Linux Bridge并没有太大的任何区别，ovs还可以支持VLAN\n首先给ovsbr0增加两个端口vlan10，vlan20，并给它们vlan tag ID\novs-vsctl add-port ovsbr0 vlan10 tag=10 -- set interface vlan10 type=internal ifconfig vlan10 192.168.10.1 netmask 255.255.255.0 ovs-vsctl add-port ovsbr0 vlan20 tag=20 -- set interface vlan20 type=internal ifconfig vlan20 192.168.20.1 netmask 255.255.255.0 然后在两个KVM虚拟机里执行命令\n在kvm1里执行\nip link add link eth0 name eth0.10 type vlan id 10 ifconfig eth0.10 192.168.10.33 netmask 255.255.255.0 broadcast 192.168.10.255 up route add default gw 192.168.10.1 dev eth0.10 在kvm2里执行\nip link add link eth0 name eth0.20 type vlan id 20 ifconfig eth0.20 192.168.20.33 netmask 255.255.255.0 broadcast 192.168.20.255 up route add default gw 192.168.20.1 dev eth0.20 在kvm3里执行\nip link add link eth0 name eth0.10 type vlan id 10 ifconfig eth0.20 192.168.10.34 netmask 255.255.255.0 broadcast 192.168.10.255 up route add default gw 192.168.10.1 dev eth0.10 这样kvm1与kvm3就在同一个vlan里，而kvm2在另一个vlan里，使用vlan隔离了KVM虚拟机。\n还可以使用在定义libvirt网络时使用portgroup，这样在guest os里就不用专门设置网络接口的vlan tag ID了。\n编辑ovsbr0.xml\n\u0026lt;network\u0026gt; \u0026lt;name\u0026gt;ovsbr0\u0026lt;/name\u0026gt; \u0026lt;forward mode=\u0026#39;bridge\u0026#39;/\u0026gt; \u0026lt;bridge name=\u0026#39;ovsbr0\u0026#39;/\u0026gt; \u0026lt;virtualport type=\u0026#39;openvswitch\u0026#39;/\u0026gt; \u0026lt;portgroup name=\u0026#39;ovsbr0\u0026#39; default=\u0026#39;yes\u0026#39;\u0026gt; \u0026lt;/portgroup\u0026gt; \u0026lt;portgroup name=\u0026#39;vlan10\u0026#39;\u0026gt; \u0026lt;vlan\u0026gt; \u0026lt;tag id=\u0026#39;10\u0026#39;/\u0026gt; \u0026lt;/vlan\u0026gt; \u0026lt;/portgroup\u0026gt; \u0026lt;portgroup name=\u0026#39;vlan20\u0026#39;\u0026gt; \u0026lt;vlan\u0026gt; \u0026lt;tag id=\u0026#39;20\u0026#39;/\u0026gt; \u0026lt;/vlan\u0026gt; \u0026lt;/portgroup\u0026gt; \u0026lt;/network\u0026gt; KVM虚拟机中的网络配置\n\u0026lt;interface type=\u0026#39;bridge\u0026#39;\u0026gt; \u0026lt;source bridge=\u0026#39;ovsbr0\u0026#39; portgroup=\u0026#39;vlan10\u0026#39;/\u0026gt; \u0026lt;virtualport type=\u0026#39;openvswitch\u0026#39;\u0026gt; \u0026lt;/interface\u0026gt; OVS的VLAN Trunk配置 带 VLAN 的交换机的端口分为两类：\n Access port：这些端口被打上了 VLAN Tag。离开交换机的 Access port 进入计算机的以太帧中没有 VLAN Tag，这意味着连接到 access ports 的机器不会觉察到 VLAN 的存在。离开计算机进入这些端口的数据帧被打上了 VLAN Tag。 Trunk port： 有多个交换机时，组A中的部分机器连接到 switch 1，另一部分机器连接到 switch 2。要使得这些机器能够相互访问，你需要连接两台交换机。 要避免使用一根电缆连接每个 VLAN 的两个端口，我们可以在每个交换机上配置一个 VLAN trunk port。Trunk port 发出和收到的数据包都带有 VLAN header，该 header 表明了该数据包属于那个 VLAN。因此，只需要分别连接两个交换机的一个 trunk port 就可以转发所有的数据包了。通常来讲，只使用 trunk port 连接两个交换机，而不是用来连接机器和交换机，因为机器不想看到它们收到的数据包带有 VLAN Header。  所以假如一台物理机上的交换机上有vlan10与vlan20, 另一台物理机上的交换机上也有vlan10与vlan20，如果想让两台物理的vlan10内部可互相访问，则要用到OVS的VLAN Trunk端口。\n具体配置可参考这里，不过我感觉这个例子里使用有些奇怪，一般使用trunk port的目的是为了连接两个交换机，使得两个交换机上的相同Tag的VLAN可互相访问。\nOVS链路聚合 OVS也支持链路聚合，见这里，不过据说性能不是太好。所以还是建议参照这里创建Linux的Bonding，再将bonding出来的网口接入ovs的网桥。\nVLAN的限制 来看看VLAN的定义：\n LAN 表示 Local Area Network，本地局域网，通常使用 Hub 和 Switch 来连接LAN 中的计算机。一般来说，当你将两台计算机连入同一个 Hub 或者 Switch 时，它们就在同一个 LAN 中。同样地，你连接两个 Switch 的话，它们也在一个 LAN 中。一个 LAN 表示一个广播域，它的意思是，LAN 中的所有成员都会收到 LAN 中一个成员发出的广播包。可见，LAN 的边界在路由器或者类似的3层设备。\n  VLAN 表示 Virutal LAN。一个带有 VLAN 功能的switch 能够同时处于多个 LAN 中。最简单地说，VLAN 是一种将一个交换机分成多个交换机的一种方法。比方说，你有两组机器，group A 和 B，你想配置成组 A 中的机器可以相互访问，B 中的机器也可以相互访问，但是A组中的机器不能访问B组中的机器。你可以使用两个交换机，两个组分别接到一个交换机。如果你只有一个交换机，你可以使用 VLAN 达到同样的效果。你在交换机上分配配置连接组A和B的机器的端口为 VLAN access ports。这个交换机就会只在同一个 VLAN 的端口之间转发包。\n  IEEE 802.1Q 标准定义了 VLAN Header 的格式。它在普通以太网帧结构的 SA （src addr）之后加入了 4bytes 的 VLAN Tag/Header 数据，其中包括 12-bits 的 VLAN ID。VLAN ID 最大值为4096，但是有效值范围是 1 - 4094。\n 可以看到VLAN ID的bit位只有12位，因此一个网络架构中最多只可能设置4094个VLAN。试想一下在云厂商的环境，VPC的数量可能远远大于4094，因此简单的VLAN并不能解决云厂商对虚拟子网的要求。于是又出现了VXLAN，这个比较复杂，后面研究后再开贴说明。\n总结 最后附一个Open vSwitch的ovs-vsctl常用命令\nOVS确实还挺复杂的，要理解它的概念很考验网络知识基础，通过对它的学习对网络知识的了解有进一步加深。\n参考 http://blog.chinaunix.net/uid-25518484-id-5707513.html http://www.rendoumi.com/openvswitch/ http://www.rendoumi.com/open-vswitch-gong-zuo-yuan-li/ http://www.rendoumi.com/openvswitchzai-centos-6-6xia-de-an-zhuang-2/ http://www.rendoumi.com/ovsxia-ru-he-she-zhi-linuxde-wang-qia/ https://github.com/openvswitch/ovs/blob/master/INSTALL.md http://www.rendoumi.com/open-vswitchshe-zhi-vlande-ce-shi/ http://www.rendoumi.com/open-vswitchyu-kvmshi-yong-vlanjin-jie/ http://www.rendoumi.com/open-vswitchxia-zuo-duan-kou-bang-ding-bonding/ http://www.cnblogs.com/qmfsun/p/3810905.html http://geek.csdn.net/news/detail/68291 http://www.rendoumi.com/open-vswitchde-ovs-vsctlming-ling-xiang-jie/\n","permalink":"https://jeremyxu2010.github.io/2016/09/%E7%A0%94%E7%A9%B6open-vswitch/","tags":["Open vSwitch","sdn"],"title":"研究Open vSwitch"},{"categories":["web开发"],"contents":"Web页面优化中有一条很重要的规则说应在不影响代码可阅读性的前提下尽量减少请求数。以前一直以为过多的请求数会导致要建立大量连接，所以影响页面加载速度。但今天看到阮一峰的一篇文章，发现真相原来不是这样的。\n持久连接的概念 HTTP/1.0 版的主要缺点是，每个TCP连接只能发送一个请求。发送数据完毕，连接就关闭，如果还要请求其他资源，就必须再新建一个连接。 TCP连接的新建成本很高，因为需要客户端和服务器三次握手，并且开始时发送速率较慢（slow start）。所以，HTTP 1.0版本的性能比较差。随着网页加载的外部资源越来越多，这个问题就愈发突出了。 为了解决这个问题，HTTP/1.1引入了持久连接（persistent connection），即TCP连接默认不关闭，可以被多个请求复用，不用声明Connection: keep-alive。\n客户端和服务器发现对方一段时间没有活动，就可以主动关闭连接。不过，规范的做法是，客户端在最后一个请求时，发送Connection: close，明确要求服务器关闭TCP连接。\nConnection: close 目前，对于同一个域名，大多数浏览器允许同时建立6个持久连接。\n产生疑问 从上面的概念展开来想，HTTP/1.1中的持久连接仅仅是复用连接而已，但在HTTP协议层面并没有给每个请求添加编号，如果在一条TCP连接上同时发送多个请求，当响应返回时，并没有办法确定某个响应是对应哪个请求的。所以猜想在一条TCP连接上，所有的数据通信是按次序进行的。\n这一猜想果然得到印证：\n 虽然1.1版允许复用TCP连接，但是同一个TCP连接里面，所有的数据通信是按次序进行的。服务器只有处理完一个回应，才会进行下一个回应。要是前面的回应特别慢，后面就会有许多请求排队等着。这称为\u0026quot;队头堵塞\u0026rdquo;（Head-of-line blocking）。\n  为了避免这个问题，只有两种方法：一是减少请求数，二是同时多开持久连接。这导致了很多的网页优化技巧，比如合并脚本和样式表、将图片嵌入CSS代码、域名分片（domain sharding）等等。\n 也就是说对于同一个域名，假设浏览器允许同时建立6个持久连接。通过ajax请求向服务器发送6个请求，如果这6个请求业务处理都比较慢，则此时再发起第7个ajax请求，这个请求将被阻塞住。所以说页面的异步请求问题仅靠AJAX是无法完全解决，当多个AJAX请求均阻塞TCP连接时，这个时候再怎么发送AJAX请求也达不到异步请求响应的需求。\n想象一下，当一个页面被加载时，会同时向服务端发起多个请求，有的在加载js、有的在加载css、有的在加载图片，一旦某个资源加载过慢，它就会阻塞在这条TCP连接上其它的请求，最终导致整个页面加载时间过长。这个才是连接数过多页面加载慢的真正原因。\nHTTP/2中的改进 HTTP/2中引入了“多工”与“数据流”的概念来对上述缺陷进行改进，如下：\n 多工   HTTP/2 复用TCP连接，在一个连接里，客户端和浏览器都可以同时发送多个请求或回应，而且不用按照顺序一一对应，这样就避免了\u0026quot;队头堵塞\u0026rdquo;。 举例来说，在一个TCP连接里面，服务器同时收到了A请求和B请求，于是先回应A请求，结果发现处理过程非常耗时，于是就发送A请求已经处理好的部分， 接着回应B请求，完成后，再发送A请求剩下的部分。 这样双向的、实时的通信，就叫做多工（Multiplexing）。\n  数据流   因为 HTTP/2 的数据包是不按顺序发送的，同一个连接里面连续的数据包，可能属于不同的回应。因此，必须要对数据包做标记，指出它属于哪个回应。 HTTP/2 将每个请求或回应的所有数据包，称为一个数据流（stream）。每个数据流都有一个独一无二的编号。数据包发送的时候，都必须标记数据流ID，用来区分它属于哪个数据流。另外还规定，客户端发出的数据流，ID一律为奇数，服务器发出的，ID为偶数。 数据流发送到一半的时候，客户端和服务器都可以发送信号（RST_STREAM帧），取消这个数据流。1.1版取消数据流的唯一方法，就是关闭TCP连接。这就是说，HTTP/2 可以取消某一次请求，同时保证TCP连接还打开着，可以被其他请求使用。 客户端还可以指定数据流的优先级。优先级越高，服务器就会越早回应。\n 基于WebSocket的Web请求机制 看到HTTP/2中“数据流”的实现方案，突然想到我之前实现的一套基于WebSocket的Web请求机制好像也是这么完成的。下面贴一段核心的实现代码：\nwebIO.js：\nvar socketio_client = require(\u0026#39;socket.io-client\u0026#39;); window.io = socketio_client; // IE8 need io  var AppConstant = require(\u0026#39;../constants/AppConstant.js\u0026#39;); window.WEB_SOCKET_SWF_LOCATION = AppConstant.PORTAL_CONTEXT_PATH + \u0026#39;/flash/WebSocketMain.swf\u0026#39;; var EventEmitter = require(\u0026#39;events\u0026#39;).EventEmitter; var inherits = require(\u0026#39;inherits\u0026#39;); var when = require(\u0026#39;when\u0026#39;); var Logger = require(\u0026#39;./Logger.js\u0026#39;); var AjaxAPI = require(\u0026#39;./AjaxAPI.js\u0026#39;); var websocketHostName = window.location.hostname; var WebIO = function(opts){ EventEmitter.call(this); opts = opts || {}; this.opts = opts; this.opts.reconnectionAttempts = this.opts.reconnectionAttempts || 5; this.opts.reconnectionDelay = this.opts.reconnectionDelay || 2000; this.opts.connectTimeout = this.opts.connectTimeout || (this.opts.reconnectionAttempts * this.opts.reconnectionDelay); this.opts.disconnectTimeout = this.opts.disconnectTimeout || 5000; this.connected = false; }; inherits(WebIO, EventEmitter); var connectPromise = null; WebIO.prototype.connect = function(opts){ var that = this; if(connectPromise === null || connectPromise.inspect().state !== \u0026#39;pending\u0026#39;){ opts = opts || {}; connectPromise = when.promise(function(resolve, reject) { if (!that.connected) { var connectCb = function () { resolve(); }; AjaxAPI.request(AppConstant.PORTAL_CONTEXT_PATH + \u0026#39;/api/getSocketIOAccessInfo\u0026#39;, { data: { t : new Date().getTime() }, dataType: \u0026#39;json\u0026#39;, method: \u0026#39;post\u0026#39;, cache: false }).then(function(data){ let socketIOPort; if(window.location.protocol == \u0026#39;https:\u0026#39;){ socketIOPort = data.socketIOHttpsAccessPort; } else if(window.location.protocol == \u0026#39;http:\u0026#39;){ socketIOPort = data.socketIOAccessPort; } let websocketHostName = window.location.hostname; if(data.socketIOAccessHostName){ websocketHostName = data.socketIOAccessHostName; } that.socket = socketio_client.connect(window.location.protocol + \u0026#39;//\u0026#39; + websocketHostName + \u0026#39;:\u0026#39; + socketIOPort, { reconnect: true, \u0026#39;max reconnection attempts\u0026#39;: that.opts.reconnectionAttempts, \u0026#39;reconnection delay\u0026#39;: that.opts.reconnectionDelay, transports: [\u0026#39;websocket\u0026#39;, \u0026#39;flashsocket\u0026#39;, \u0026#39;xhr-polling\u0026#39;], resource: \u0026#39;portal_socketio\u0026#39;, \u0026#39;force new connection\u0026#39;: true }); that.socket.on(\u0026#39;connect\u0026#39;, function(){ that.connected = true; that.emit(\u0026#39;open\u0026#39;); }); that.socket.on(\u0026#39;disconnect\u0026#39;, function(){ if(that.connected) { try { that.socket.disconnect(); } catch (e) {} that.connected = false; } that.emit(\u0026#39;close\u0026#39;); }); that.socket.on(\u0026#39;connect_failed\u0026#39;, function(){ reject(new Error(\u0026#39;connect failed\u0026#39;)); }); that.socket.on(\u0026#39;data\u0026#39;, function(data){ that.emit(\u0026#39;msg\u0026#39;, data); }); that.socket.once(\u0026#39;connect\u0026#39;, connectCb); }, function(){ reject(new Error(\u0026#39;connect failed\u0026#39;)); }); } else { resolve(); } }); connectPromise = connectPromise.timeout(that.opts.connectTimeout, \u0026#39;connect timeout\u0026#39;).then(undefined, function(e){ that.disconnect(); Logger.error(e); }); } return connectPromise; }; WebIO.prototype.disconnect = function(opts){ var that = this; opts = opts || {}; var promise = when.promise(function(resolve, reject) { if(that.connected){ var disconnectCb = function(){ resolve(); }; that.socket.once(\u0026#39;disconnect\u0026#39;, disconnectCb); try { that.socket.disconnect(); } catch (e){ reject(e); } } else { resolve(); } }); return promise.timeout(that.opts.disconnectTimeout, \u0026#39;disconnect timeout\u0026#39;).then(undefined, function(e){ Logger.error(e); }); }; WebIO.prototype.sendData = function(opts){ var that = this; opts = opts || {}; opts.msg = opts.msg || \u0026#39;\u0026#39;; return when.resolve().then(function(){ if(that.connected){ that.socket.emit(\u0026#39;data\u0026#39;, opts.msg); } else { throw new Error(\u0026#39;not connected\u0026#39;); } }).then(undefined, function(e){ Logger.error(e); }); }; WebIO.prototype.isConnected = function(){ return this.connected; }; module.exports = new WebIO(); webAPI.js：\nvar webIO = require(\u0026#39;./webIO.js\u0026#39;); var when = require(\u0026#39;when\u0026#39;); var Logger = require(\u0026#39;./Logger.js\u0026#39;); var msgIdCounter = 0; var REQ_TIMEOUT = 1000 * 60 * 5; var reqCb = {}; var nextMsgId = function(){ msgIdCounter = (msgIdCounter + 1) % (Number.MAX_VALUE - 1); return msgIdCounter; }; var MSG_TYPE = { REQ_MSG : 0, RES_MSG : 1, ... }; var webAPI = {}; webAPI.request = function(opts){ opts = opts || {}; if(!opts.path){ throw new Error(\u0026#39;path must not be empty\u0026#39;); } opts.data = opts.data || {}; var reqId = nextMsgId(); window.isDebug \u0026amp;\u0026amp; Logger.debug(\u0026#34;webAPI request\u0026#34;, reqId, opts); var promise = when.promise(function(resolve, reject) { webAPI.connect().then(function(){ var sendReqMsg = function(){ var msg = { type: MSG_TYPE.REQ_MSG, reqId: reqId, path: opts.path, body: opts.data }; return webIO.sendData({ msg: JSON.stringify(msg) }); }; reqCb[reqId] = {\u0026#39;resovle\u0026#39;: resolve, \u0026#39;reject\u0026#39;: reject}; sendReqMsg().then(undefined, reject); }, reject); }); promise = promise.timeout(REQ_TIMEOUT, \u0026#39;request timeout\u0026#39;).then(undefined, function(e){ if (reqCb[reqId]) { delete reqCb[reqId]; } Logger.error(\u0026#39;webAPI.request :\u0026#39;, e); throw e; }); return promise; }; webAPI.connect = function(opts){ opts = opts || {}; if(connectPromise === null || connectPromise.inspect().state !== \u0026#39;pending\u0026#39;){ connectPromise = when.promise(function(resolve, reject) { webIO.connect().then(function(){ if(connected) { resolve(); } else { reject(); } }, reject); }); } return connectPromise; }; ... webIO.on(\u0026#39;msg\u0026#39;, function(data){ var msg = JSON.parse(data); window.isDebug \u0026amp;\u0026amp; Logger.debug(\u0026#34;webAPI response\u0026#34;, msg); var msgType = msg.type; if(msgType === MSG_TYPE.RES_MSG){ var reqId = msg.reqId; var resp = msg.body; if(reqCb[reqId]){ if(resp.success) { reqCb[reqId].resovle(resp); } else { var e = new Error(resp.msg); e.msg = resp.msg; reqCb[reqId].reject(e); } delete reqCb[reqId]; } } ... }); ... module.exports = webAPI; 上述代码中webIO.js比较复杂，因为封装了与WebSocket连接的相关细节，但只需要知道webIO利用socketio-client连接WebSocket服务端，并将其常用方法进行了封闭，均返回Promise对象就好了。webAPI.js就比较简单了，这里的request方法与HTTP/2的“数据流”实现一致，也是给每个请求加上一个编号，当响应回来时，根据这个编号找到对应的回调方法执行回调。\n总结 看阮一峰的这篇文章终于扭转了我之前对HTTP异步请求的误解，看来还是应该多看书多思考。另外发现只要是认真思考出来的思路也不会太差。\n参考 http://www.ruanyifeng.com/blog/2016/08/http.html https://developer.mozilla.org/en-US/docs/Web/HTTP/Connection_management_in_HTTP_1.x\n","permalink":"https://jeremyxu2010.github.io/2016/09/%E9%87%8D%E6%96%B0%E7%90%86%E8%A7%A3http%E4%B8%AD%E7%9A%84%E6%8C%81%E4%B9%85%E8%BF%9E%E6%8E%A5/","tags":["HTTP"," javascript"],"title":"重新理解HTTP中的“持久连接”"},{"categories":["java开发"],"contents":"在java开发中经常会遇到下面的代码：\nGraph get(Key key) { Graph result = get( key ); if( null == result ) { result = createNewGraph( key ); put( key, result ); } return result; } 即根据某个Key值，到缓存里查找是否有对应的值，如没有则创建，并把创建的结果保存在缓存里，供下次使用。\n上述代码表面上看没有什么问题。但仔细分析一下就会开始多线程访问时，会根据一个Key导致创建了多个Graph。加个锁可以立即解决问题，但今天发现Google提供了更优的方案Google Guava Cache。\n使用Google Guava Cache上面的代码可以改写为：\nprivate static LoadingCache\u0026lt;Key, Graph\u0026gt; cache = CacheBuilder.newBuilder().build(new CacheLoader\u0026lt;Key, Graph\u0026gt;() { @Override public Graph load(Key key) throws Exception { return createNewGraph( key ); } private Graph createNewGraph(Key key) { return new Graph(); } }); public static void main(String[] args) { try { cache.get(new Key(1)); } catch (ExecutionException e) { e.printStackTrace(); } } 这种方案，cache使用起来更方便了。\n注意，get方法有一个重载方法，可以在get时定义load方式，如下代码：\nfinal Key key = new Key(1); cache.get(key, new Callable\u0026lt;Graph\u0026gt;(){ public Graph call() throws Exception { return createNewGraph(key); } private Graph createNewGraph(Key key) { return new Graph(); } }); 如果Google Guava Cache仅仅只是完成这个功能，那就很一般了。关键是CacheBuilder有很多选项可以来定制Cache的行为，如下：\n 大小的设置：CacheBuilder.maximumSize(long) CacheBuilder.weigher(Weigher) CacheBuilder.maxumumWeigher(long) 时间：expireAfterAccess(long, TimeUnit) expireAfterWrite(long, TimeUnit) 引用：CacheBuilder.weakKeys() CacheBuilder.weakValues() CacheBuilder.softValues() 明确的删除：invalidate(key) invalidateAll(keys) invalidateAll() 删除监听器：CacheBuilder.removalListener(RemovalListener)  ","permalink":"https://jeremyxu2010.github.io/2016/09/java%E4%B8%AD%E7%94%A8%E5%A5%BDcache/","tags":["java","guava","cache"],"title":"java中用好cache"},{"categories":["容器编排"],"contents":"很早以前就听说过pipework，据说面对一些复杂的网络配置场景，docker自带的网络模式就有些力不从心了，很多人都在用pipework。今天终于能够抽出时间研究一下它。\ndocker默认支持的网络模式 除了overlay网络外，docker默认支持4种网络模式，如下：\n host模式，使用\u0026ndash;net=host指定，容器和宿主机共用一个Network Namespace。 container模式，使用\u0026ndash;net=container:NAME_or_ID指定，容器和已经存在的一个容器共享一个Network Namespace。 none模式，使用\u0026ndash;net=none指定，容器拥有自己的Network Namespace，但是，并不为Docker容器进行任何网络配置。 bridge模式，使用\u0026ndash;net=bridge指定，默认设置，为容器分配Network Namespace、设置IP等，并将容器连接到一个虚拟网桥上，默认是docker0。  祭出pipework docker默认支持的网络模式，再配合docker的-p选项，一般场景都是可以满足需求，但有时我们就是想把某个固定IP设置到某个容器上，这时pipework就派上用场了。\n安装pipework很简单，直接把pipework的源代码clone下来，将pipework这个脚本拷贝至PATH路径即可。\ngit clone https://github.com/jpetazzo/pipework.git cp pipework/pipework /usr/local/bin/ chmod +x /usr/local/bin/pipework 这里做个试验\n首先创建一个none网络模式的容器\ndocker run -ti --rm --name test --net none apline 进入容器后，执行ifconfig查看一下，容器里并没有一个网络接口。\n再在docker主机上执行pipework命令，当然前提是先在docker主机上创建一个linux bridge网桥br1。\npipework br1 -i eth0 test 188.188.100.33/24@188.188.100.1 此时再在上面那个容器里执行ifconfig查看一下，就可以发现容器里已经有一个网络接口eth0了，并且设置好了IP地址、网关地址等。\n这时在docker主机上执行brctl show br1就可以看到多了一个网络接口桥接到了br1。\n看一看pipework的官方文档，其中还有一些高级用法，比如：定制主机上对应的veth peer网络接口名称、直接连接本地的物理网络接口、服务等待网络接口就绪、配置网络接口为DHCP模式、定制网络接口的MAC地址、指定容器至某个VLAN、控制容器内部的路由规则、支持Open vSwitch。\n研究pipework的原理 pipework是用脚本实现的，因此很方便研究其实现原理。\n首先创建一个none网络模式的容器\ndocker run -ti --rm --name test --net none apline 再在docker主机上执行pipework命令，这里我使用bash -x以显示bash的执行过程。\nbash -x /usr/local/bin/pipework br1 -i eth0 test 188.188.100.33/24@188.188.100.1 从输出观察，可以看到以下关键过程：\n#首先得到容器进程的pid DOCKERPID=$(docker inspect \u0026#39;--format={{ .State.Pid }}\u0026#39; test) #在netns目录下创建至容器网络名字空间的目录链接，方便下面在docker主机上执行ip netns命令对容器的网络名字空间进行操作 rm -f /var/run/netns/31076 ln -s /proc/31076/ns/net /var/run/netns/31076 #获取网桥网络接口上的MTU值 MTU=$(ip link show br1|awk \u0026#39;{print $5}\u0026#39;) #创建一对veth peer链接 ip link add name veth0pl31076 mtu 1500 type veth peer name veth0pg31076 mtu 1500 #将veth peer对的一端桥接至br1 ip link set veth0pl31076 master br1 ip link set veth0pl31076 up #将veth peer对的另一端放入容器的网络名字空间，并重命名为eth0 ip link set veth0pg31076 netns 31076 ip netns exec 31076 ip link set veth0pg31076 name eth0 #给容器里的eth0网络接口设置IP地址 ip netns exec 31076 ip addr add 188.188.100.33/24 brd 188.188.100.255 dev eth0 #给容器设置默认网关地址 ip netns exec 31076 ip route delete default ip netns exec 31076 ip link set eth0 up ip netns exec 31076 ip route replace default via 188.188.100.1 #在容器里向邻居广播arp消息 ip netns exec 31076 arping -c 1 -A -I eth0 188.188.100.33 #删除之前创建的目录链接 rm -f /var/run/netns/31076 解释得已经很清楚了。pipework并没有提供反向操作的脚本，了解了其实现原理，我这里顺手写一个\nGUESTNAME=test CONTAINER_IFNAME=eth0 #首先得到容器进程的pid DOCKERPID=$(docker inspect \u0026#39;--format={{ .State.Pid }}\u0026#39; $GUESTNAME) #在netns目录下创建至容器网络名字空间的目录链接，方便下面在docker主机上执行ip netns命令对容器的网络名字空间进行操作 ln -s /proc/$DOCKERPID/ns/net /var/run/netns/$DOCKERPID #停用容器中$CONTAINER_IFNAME的网络接口 ip netns exec $DOCKERPID ip link set $CONTAINER_IFNAME down #删除容器网络名字空间中$CONTAINER_IFNAME的网络接口，同时其对应的veth peer链接也将销毁 ip netns exec $DOCKERPID ip link delete $CONTAINER_IFNAME #删除之前创建的目录链接 rm -f /var/run/netns/$DOCKERPID 总结 pipework实现原理还是比较简单的，就是利用了veth peer链接对及独立的网络名字空间，这个跟docker的bridge桥接模式是一样的。在本篇里用到的是linux bridge网桥，linux bridge网桥使用起来比较方便，但面对细粒度的网络隔离也不太行，下一步计划研究一下Open vSwitch。\n","permalink":"https://jeremyxu2010.github.io/2016/09/%E7%A0%94%E7%A9%B6pipework/","tags":["docker","pipework"],"title":"研究pipework"},{"categories":["容器编排"],"contents":"docker搭建起集群后，跨主机的容器相互之间通信就要另想方案了。幸好docker1.9之后内置了跨节点通信技术Overlay网络，这里将使用方法简单示例一下以备忘。\n下面的操作还是在上周搭建的docker集群中进行，集群的搭建见这里。\n解决docker集群遗留问题 上周搭建的docker集群还有一个小问题\n 每次所有docker主机再启动后，docker主机内部通信的网络接口地址有很大可能发生变化，这个会造成docker集群无法达到健康状态。搜索了下，找到一个简单办法将virtualbox创建的docker主机ip固定下来。  #ssh登入一台docker主机 docker-machine ssh node1 #创建bootsync.sh文件，里面杀死dhcp客户端进程，并静态设置docker主机ip地址，注意不同的docker主机要设置不同的IP地址 sudo echo \u0026#34;if [ -f /var/run/udhcpc.eth1.pid ] ; then kill $(more /var/run/udhcpc.eth1.pid)fi ifconfig eth1 192.168.99.104 netmask 255.255.255.0 broadcast 192.168.99.255 up\u0026#34; \u0026gt; /var/lib/boot2docker/bootsync.sh #设置bootsync.sh文件可执行权限，docker主机启动时会执行该文件 chmod +x /var/lib/boot2docker/bootsync.sh 集群中所有docker主机重启一次。\n etcd服务的3个节点不能随docker主机的启动而启动。搜索了下，找一个简单办法在docker daemon启动后自动启动3个etcd容器。  sudo echo \u0026#34; sleep 5 /usr/local/bin/docker start etcd1 etcd2 etcd3\u0026#34; \u0026gt; /var/lib/boot2docker/bootlocal.sh #设置bootlocal.sh文件可执行权限，docker daemon启动后会执行该文件 chmod +x /var/lib/boot2docker/bootlocal.sh 创建overlay网络并使用它 连入docker集群\neval $(docker-machine env --swarm node1) 创建名称为ovr0的overlay网络并验证ovr0网络的信息\ndocker network create --driver=overlay ovr0 docker network inspect ovr0 创建两个容器试验一下\ndocker run -ti --rm --name alpine1 --net ovr0 alpine /bin/sh docker run -ti --rm --name alpine2 --net ovr0 alpine /bin/sh docker ps ... CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 137b0fa48ad9 alpine \u0026#34;/bin/sh\u0026#34; 5 seconds ago Up 4 seconds node4/alpine2 67ed2ad7d8aa alpine \u0026#34;/bin/sh\u0026#34; 9 seconds ago Up 8 seconds node3/alpine1 ... 上面可以看到这两个容器是创建在node3, node4两个docker主机上的。\n再验证一下ovr0网络的信息\ndocker network inspect ovr0 ... [ { \u0026#34;Name\u0026#34;: \u0026#34;ovr0\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;f86789f2d9575830b57aae8385bcd02e76b342b466c677c8aa02c2557e3eacb3\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;global\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;overlay\u0026#34;, \u0026#34;EnableIPv6\u0026#34;: false, \u0026#34;IPAM\u0026#34;: { \u0026#34;Driver\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Options\u0026#34;: {}, \u0026#34;Config\u0026#34;: [ { \u0026#34;Subnet\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;10.0.0.1/24\u0026#34; } ] }, \u0026#34;Internal\u0026#34;: false, \u0026#34;Containers\u0026#34;: { \u0026#34;137b0fa48ad9df084afda45fd88f3312d481dafade0cc758f177761babd5f262\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;alpine2\u0026#34;, \u0026#34;EndpointID\u0026#34;: \u0026#34;860c6930714e41ebbe03315525c8c07f67119f1ececf05f2d15c3c0da4d28d62\u0026#34;, \u0026#34;MacAddress\u0026#34;: \u0026#34;02:42:0a:00:00:03\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;10.0.0.3/24\u0026#34;, \u0026#34;IPv6Address\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;67ed2ad7d8aaa1447d8e4d97782e917e544e18be283dee5c471425087d2d6639\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;alpine1\u0026#34;, \u0026#34;EndpointID\u0026#34;: \u0026#34;aa3bdff0cd7a6dc81e9df89241768b7ff21ef6b3010c96f291bd1a927e82a233\u0026#34;, \u0026#34;MacAddress\u0026#34;: \u0026#34;02:42:0a:00:00:02\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;10.0.0.2/24\u0026#34;, \u0026#34;IPv6Address\u0026#34;: \u0026#34;\u0026#34; } }, \u0026#34;Options\u0026#34;: {}, \u0026#34;Labels\u0026#34;: {} } ] ... 可以看到两个容器均接入到了ovr0这网络，ip分别是10.0.0.2与10.0.0.3。\n同时在两个容器里ping对方的主机名，均可正常ping通。\nping alpine2 PING alpine2 (10.0.0.3): 56 data bytes 64 bytes from 10.0.0.3: seq=0 ttl=64 time=0.686 ms 64 bytes from 10.0.0.3: seq=1 ttl=64 time=0.677 ms 同时再检查docker集群上所有的网络，发现node3、node4两个节点上多出了两个网络node3/docker_gwbridge、node4/docker_gwbridge。初步估计是overlay网络底层实现时依赖的桥接网络。\ndocker network ls NETWORK ID NAME DRIVER 72afcd321dca node1/bridge bridge 109efe6a8422 node1/host host 93162425725b node1/none null 0225062d4555 node2/bridge bridge 9889390b8909 node2/host host cf303662cde0 node2/none null 34f56fd67dcc node3/bridge bridge 0d91c9626d8f node3/docker_gwbridge bridge 52fa10224530 node3/host host 8f3212e00813 node3/none null c9f525cd4380 node4/bridge bridge 787ce948a501 node4/docker_gwbridge bridge 853b5214d03a node4/host host 1e1e88ef6e50 node4/none null f86789f2d957 ovr0 overlay 另外如果想某个容器断开某网络，可执行下面的命令\ndocker network disconnect ovr0 alpine1 disconnect之后，容器中与这个网络相关的网络接口立马就消失了，同时在同一网络的其它的主机即不可ping通该容器主机名。\n如果又想将某个容器连接某网络，可执行下面的命令\ndocker network connect ovr0 alpine1 connect之后，容器中将会出现与这个网络相关的网络接口，同时在同一网络的其它的主机即可ping通该容器主机名。\n总结 docker的overlay网络使用起来还是比较方便的，但如果要给容器配上固定的外部访问IP还是有点麻烦，后面准备研究一下pipework的用法。\n参考 http://www.alauda.cn/2016/01/18/docker-1-9-network/ https://github.com/docker/machine/issues/1709 https://github.com/boot2docker/boot2docker/blob/master/rootfs/rootfs/bootscript.sh\n","permalink":"https://jeremyxu2010.github.io/2016/08/%E5%B0%9D%E8%AF%95docker%E7%9A%84overlay%E7%BD%91%E7%BB%9C/","tags":["docker","sdn"],"title":"尝试docker的overlay网络"},{"categories":["容器编排"],"contents":"docker最近一年可真是火，不过刚好看到下面这篇文章，觉得还是很有道理的。转载过来研读并思考一下，转载自这里。\n Docker的兴起和Hadoop何其相似 2015年说是Docker之年不为过，Docker热度高涨，IT从业人员要是说自己不知道Docker都不好意说自己是做IT的。2016年开始容器管理、集群调度成为热点，K8s开始成为热点。但这一幕和2013年的Hadoop大数据何其相似，当年你要说自己不知道大数据，或是知道大数据不知道Hadoop，那必然招来鄙视的眼光。\n云计算喊了这么久，从来没有像Docker这么火过，究其原因不外乎两条：\n1、开发者能够用Docker，开发者要一个开发环境，总会涉及到种种资源，比如数据库，比如消息中间件，去装这些东西不是开发人员的技能，是运维人员的技能。而用Docker去Pull一个mySQL镜像,或是Tomcat镜像，或是RabbitMQ镜像，简易轻松，几乎是零运维。做好了应用代码，打一个Docker镜像给测试或是运维人员，避免了从前打个程序包给测试或是运维人员，测试或运维人员要部署、配置应用，还得反反复复来麻烦开发人员，现在好了，丢个Docker镜像过去，让运维人员跑镜像就可以，配置在镜像里基本都做好了。\n这正好满足了DevOps的要求，所以DevOps也一下热起来了。开发者是一个巨大的市场，是海量的个体，通过类似于病毒式的传销，Docker一下在开发者中热起来了。\n2、镜像仓库和开源，谁都可以用，Docker镜像库非常丰富，谁做好一个镜像都可以往公有仓库推送，开发人员需要一个环境的时候，可以到Docker镜像仓库去查，有海量的选择，减少了大量无谓的环境安装工作。而通过开源，又开始大规模传播。\n我们再来回顾看看2010-2013年，大数据的名词火遍大江南北，各行各业都在谈大数据，但是落到技术上就是Hadoop，还记得2012年的时候，和Hadoop没啥毛关系的VMWare也赶紧的做了一个虚机上部署Hadoop的serengeti，谁家产品要是和Hadoop不沾点边，不好意思说自己是IT公司。Hadoop当年的热度绝对不亚于2014-2015的Docker。而且时间上有一定的连续性，2014年开始，Hadoop热度达到顶点，开始逐渐降温，标志事件就是Intel投资Cloudera。而Docker是从2014年开始热度升高的。\n再看Hadoop为何在2010年前后开始热起来，之前的大数据都是数据仓库，是昂贵的企业级数据分析并行数据库，而Hadoop是廉价的大数据处理模式，通过开源和X86廉价硬件，使得Hadoop可以大规模使用，而互联网时代产生的海量数据虽然垃圾居多，但是沙里淘金，也能淘出点价值，Hadoop正好迎合了这两个需求，虽然Hadoop的无论是功能还是性能远比MPP数据库差，但做简单的数据存储、数据查询、简单数据统计分析还是可以胜任的，事实上，到目前为止，大多数的Hadoop应用也就是数据存储、数据查询和简单的数据统计分析、ETL的业务处理。\nDocker和Hadoop的热起来的原因不同，但是现象是差不多，开源和使用者群体大是共同要素。\nHadoop从狂热走向了理性 Hadoop最热的时候，几乎就是要replace所有数据库，连Oracle也面临了前所未有的冲击，甚至Hadoop成了去IOE的Oracle的使命之一。在狂热的那个阶段，客户怎么也得做一两个大数据项目，否则会被同行瞧不起，各IT厂商也必须推出大数据产品，否则可能成为IT过时的典范，这不IBM成立了专门的大数据部门，打造了一个以Hadoop为核心的庞大的大数据解决方案。Intel虽然是做芯片的，但是大数据必须掺和，成立大数据部门，做Intel Hadoop 。连数据库的老大Oracle也憋不住了，做了个大数据一体机。\n任何曾经狂热的新技术都会走向理性，Hadoop也不例外，只不过，这个进程还比较快。随着大数据的大跃进，随着Hadoop的应用越来越多，大家发现在被夸大的场景应用大数据效果并不好，只在特定场景有效，Hadoop进入理性发展阶段，比如一开始Hadoop据取代MPP数据库，取代数据仓库，取代Oracle，完美支持SQL等等均基本成为泡影。这其实本来是一个常识，任何技术都有其应用场景，夸大应用场景，任意扩展应用场景只会伤害这个技术的发展。\n这和目前无限夸大Docker的应用场景有异曲同工之妙，比如Docker向下取代虚拟化，Docker向上取代PaaS之类，几乎成了云计算的唯一技术，这种论调一直充斥各种Meetup/论坛。虽然技术从夸大到理性需要时间，但是理性不会总是迟到。\nHadoop技术在发展，大数据的相关技术也在发展，Hadoop一直被诟病的处理速度慢，慢慢的被Spark/Storm等解决，特别在流数据处理领域。\n所以，时至今日，人们对Hadoop的态度趋于理性，它只适合在特定场景使用，可是，当初那些在Hadoop不太适用的场景使用了Hadoop的客户交了学费的事情估计没人再提了。Docker估计也是一样的，总有在夸大的场景中交学费的客户，可是只是客户没眼光吗？和无限夸大某种技术的布道师无关么？\n再反观大数据和Docker在全球的发展，在美国，无论是Hadoop和Docker并没有像国内这么狂热过。Hadoop技术来源于Google，成型于Yahoo(DougCutting)，而炒作却是在国内。同样，Docker也在走这么个流程，在美国没有这么多的Docker创业公司，主要就是Docker，然后各大厂商支持，创业公司和创投公司都知道，没有自己的技术或是技术受制于人的公司不值得投资，既然Docker一家独大，再去Docker分一杯羹会容易吗？\n而国内二三十家的Docker创业公司，没有一家能对Docker/K8s源码有让人醒目的贡献(反倒是华为在K8s上有些贡献)，但是都在市场上拼嗓门，不是比谁的技术有潜力最有市场，而是比谁最能布道谁嗓门大，谁做的市场活动多，某Docker创业公司据说80%的资金用在市场宣传、Meetup上，而且不是个别现象，是普遍现象。反应了某些Docker创业者的浮躁心态。\nHadoop生态圈的演进 Hadoop兴起和生态圈紧密相关，Hadoop的生态圈的公司主要是两大类:第一类是Hadoop的各个发行版公司，如Cloudera、HortonWorks、MapR、Intel、IBM等，第二类基于Hadoop做各行业的大数据项目实施或大数据应用和工具，如Tableau、Markerto、新炬、环星等\n随着大数据的热度提升，Hadoop生态圈的两大类公司蓬勃发展，但是市场有限，市场还没有成熟，竞争就很激烈，特别是第二类做项目实施的企业，那就只能靠烧钱。\n问题是如果是消费者市场，通过烧钱先把市场占领，然后再通过其他手段收费盈利，比如淘宝通过向卖家收费盈利，滴滴打车之类的未来可以通过让司机花的米抢好单、大单可以实现盈利，而Hadoop是企业级市场，通过亏钱树立案例标杆，然后复制，这条路走的并不顺利，因为复制案例的时候会碰到竞争，一开始就低价做烂的市场，客户不愿花钱，总有低价者抢市场，复制案例往往变成低价竞争。低价竞争很难把项目实施好，基本是个多输的模式，客户并没有得到自己预期的大数据项目价值，或是打折的价值，实施厂商没赚钱，留不住人，招不到好的人才。\n这类生态圈公司的发展趋势是最终会留下少数几家公司，规模做大，其他的公司会被淘汰，现在还没有走出各个集成商苦苦支撑的时代。\n对于第一类做发行版的Hadoop厂商，在烧钱进入后期阶段，日子也开始不好过，因为项目实施厂商的大多选择不用发行版，只用开源，发行版和相应的付费支持很难卖出量，进而难以盈利。资本市场成了继续烧钱的救命稻草，Hortonworks第一个上市， 16元的发行价，大数据概念蜜月期一过，要开始考察业绩的时候，股价步入漫漫熊途，到现在只有9元左右。CloudEra虽然没有上市，但是已经融资12亿美元，目前在Hadoop发行版，CloudEra和Hortonworks占据了最大的市场，特别是CloudEra的市场更大，即使CloudEra有巨大的Hadoop市场和技术优势，CloudEra到现在也不敢上市。\n现实很残酷，CloudEra的盈利并不让人满意，现在上市，资本市场不会给出个好股价，但是CloudEra的困境是如果迟迟不上市，大数据热点成为非热点，资本市场那就考察的是盈利能力—市盈率，不是市梦率了，而CloudEra的盈利能力在目前状态能让资本市场满意吗？虽然Hortonworks上市的时候赶上了市梦率。\n技术的演进同样在影响发行版的Hadoop厂商，Hadoop从1.0到2.0，技术有较大的改进，Yarn取代Map-Reduce，原来众多的发行版面临着自己对Hadoop 1.0的定制如何合并到Hadoop 2.0去的问题，定制的越多，合并的难度越大；定制的越少，和开源没啥区别，体现不出价值，这是发行版面对的两难问题。\nHadoop 1.0到2.0的升级成为一个重要的转折点\u0026mdash; Hadoop从1.0到2.0直接导致Intel的发行版出局，Intel的Hadoop部门裁撤销，Intel废弃自己的Hadoop转而直接投资CloudEra。因为Intel对Hadoop 1.0做了很多的定制、优化，这些定制优化本来一直是Intel宣称的竞争技术优势，现在1.0到2.0,立马优势变劣势，定制越多合并到新版本越难合并，而Hadoop不是Intel的主业，所以Intel权衡利弊，及时止损，放弃了自家的Hadoop，选择投资CloudEra。对Intel Hadoop客户而言，要吸取的教训是买产品一定要买卖家的核心产品，即使是大卖家，其边缘产品很容易被抛弃，受伤的是客户。这个道理其实是个大概率道理，可是吃这种亏的客户不会绝迹。\nHadoop发行版厂商面临一样的趋势潮流——留下不超过3家Hadoop发行商，其他的都会被淘汰。\n再看大数据的应用和生态圈的公司，云端营销服务公司Marketo,13元的IPO价格，趁着大数据的东风，很快就飞到了45，随后熊途漫漫，在2016年2月份跌破发行价。和HortonWorks相比，Marketo是大数据的应用，是能通过大数据直接产生营收的，而且业绩也确实比Hortonworks要好，但是回天无力，持续亏损，随后被私募公司Vista Equity Partners收购。Tableau其实和Hadoop关系不大，当初接Hadoop东风股价飙起，现在也是熊途漫漫。\nDocker的生态圈 历史不会简单的重复，但是有惊人的相似!\nDocker的生态圈和Hadoop的生态圈类似。\nDocker的生态圈也分为两大类，第一类就是Mesosphere、Google这类做Docker的企业运行集群管理，类似于Hadoop的发行版的厂商。第二类是做Docker的项目实施或是做Docker开发者公有云，类似于Hadoop的项目实施厂商。\nDocker的流行开始于开发者，也是在开发者中传播，真正进入企业级生产系统的很少，由于Docker天生就是从开发者起家的，缺乏进入企业的基因，Docker的设计就不是运行于企业级环境下。\n可是从开发者身上很难赚钱，这已经成为共识了，如果想从开发者身上赚钱，那开发者都跑路了。Docker也意识到这点，所以Docker在2016开年就提出，要进入企业级—“Ready for Production”，但是理想是理想，理想和现实之间需要跨越巨大的鸿沟。\nDocker进入企业级的需求，造就了第一类的生态公司，主要就是Mesosphere、Google和Redhat三家，Mesos本来就是部署、集群管理，之前部署Hadoop大数据、批处理、ETL之类的，随着Docker东风吹来，马上支持部署、管理Docker集群，再加一个Marathon管理长周期任务,就可以实现部署应用的CaaS，虽然离PaaS还有很大距离，缺乏很多PaaS功能。\n首先要深刻理解PaaS。\nPaaS的P是Application Platform，是应用平台As a Service，是着眼于应用和应用平台。\n很多人往往把PaaS和CaaS混淆，Container As A Service是容器即服务，只管提供容器和容器管理，并不管容器里面跑的是应用还是数据库或是数据应用，所以CaaS要弄出个编排，而PaaS并无编排一说。如果只是提供容器，和IaaS其实并没有太大的区别，只不过把应用从虚机转移到容器里来。\nPaaS的设计原理和方法论是要实现应用的零运维，通过平台本身来监控应用，而不是传统的思维方式，传统的运维是要针对不同的应用去不同的监控、不同的调度、不同的故障恢复，所以运维成了救火。\nPaaS通过平台本身来监控应用、监控容器、监控虚机、监控物理机，应用不用去管监控的事情，无论是应用故障、容器故障、虚机故障还是物理机故障，统统故障自动恢复，应用实现一键部署，资源实现弹性伸缩。运维的三大任务：应用和系统部署、升级，故障恢复，根据业务的资源分配，这三大任务在PaaS全自动化。\n当然，要达到这个目标，你的应用要符合十二要素，要向云原生应用靠近。退一步讲，即使是传统应用，不做改造，搬到PaaS下虽然不能100%达到上述的零运维，但是也可以达到相当程度的运维自动化。\nPaaS和CaaS的另外一个根本区别是，PaaS区别对待应用和服务，应用运行在容器中，实现零运维。服务就是比如数据库、消息中间件、大数据、缓存等，并不适合运行于容器中，PaaS把这些服务部署在虚机中，服务的弹性伸缩要求并不强，不像应用弹性伸缩的要求比较强，谁会去把一个mySQL或是Oracle数据库的集群在运行中弹性扩展一下？\n服务没必要放在容器中，服务更多的是需要备份、调优等操作系统相关的运维，而且往往会涉及到操作系统内核的调优，而应用是往往操作系统无关了，所以放在容器中。在容器中做操作系统内核参数调优是有风险的。通过区分应用和服务，并且把应用放在容器中，服务放在虚机中，自然的消除了编排的需求。容器是个革新性的技术，但是不是任何场合都适用，作用企业应用，应当在不同的场景选择不同的技术，而不是一个技术包揽全部。\nBorg是谷歌公司很早以前就在使用的内部容器管理系统，随着Docker的兴起，把Borg的精华部分抽取出来，支持Docker，弄出了个Kubernetes，但是Kubernetes出生于复杂的Borg系统，框架就比较大，而且复杂，而Docker进入企业，总是从小到大的过程，企业和互联网公司不一样，互联网公司可能经过几年的积累，已经有成千上万个容器需要管理，而且运维人员就是公司的主要资源。对于企业公司而言，把Docker弄到生产环境，都是尝试性的，一开始就弄一个超复杂的系统，哪个企业都吃不消，所以Kubernetes进入企业之路并不顺利。\n特别是国内，在2015年大多第二类公司Docker项目集成公司都选择Mesos，毕竟Mesos简单易上手，一般客户也要不了Kubernetes那么复杂的功能来管理一个初始的小集群，所以在2015年国内鲜有采用Kubernetes的企业客户，当然2016年形势逆转，K8s成为热点了，因为大家发现Mesos不是正宗的Docker集群管理，K8s从一开始就定位到容器集群管理，虽然技术复杂了一点，但是2016的Docker生态圈创业公司很多是海归，更从技术根源上认可K8s对Docker的集群管理，加上这些创业公司不遗余力的宣传，K8s在2016年逆袭了，成为最热的Docker集群管理软件，这其实也说明了技术最终能够被发现。\n而Redhat就直接用kubernetes加一些自己的功能来做PaaS云解决方案。Redhat自己宣传也是Docker+kubernetes=Openshift，那Redhat的Openshift其实自己可以掌控的东西很少，把几个不是自己主导的开源的产品组合成一个OpenShift，和国内的山寨思想没什么太大区别，也体现不出自己的价值，既然是山寨思想，那山寨的害处马上就体现出来了，一旦正宗的产品推出来，山寨之路就艰难了，而且OpenShift又是Redhat的边缘产品，记住前面的法则：买产品一定要买公司的核心产品。\nDocker第二类生态公司，做企业Docker项目实施的，在国内众多，包括：DaoCloud、数人云、CSphere、云雀、迅达云、高伟达、宇信、飞致云、时速云等，其中有些一开始尝试做开发者公有云PaaS，但都证明不能盈利，这其实已经在新浪、盛大公有云证明是不成功的，有多少开发者或是中小软件公司愿意花钱在公有云上开发呢？这个市场还太小，不足以养活开发者公有云。\nDocker公司的战略野心受生态圈狙击 Docker本来是做PaaS的公司，原来称为DotCloud, 其提供了类似IDC的服务，为客户提供PaaS服务，包括Web、Application、Transaction、Database等服务。但PaaS云运营并不成功，DotCloud痛定思痛，技术转型到做容器，而且一开始就开源，吸引大量的开发者使用。\n随着Docker在开发者中越来越流行，2013年10月，DotCloud干脆换名为Docker公司，2014年8月 Docker 宣布把平台即服务的业务「dotCloud」出售给位于德国柏林的平台即服务提供商「cloudControl」，Docker开始专心致志做Docker。\n于此同时，Docker也开始融资准备把公司做大，适应Docker的发展势头。Docker从2013年开始，经过ABCD四轮融资，累计超过1.5亿美元的融资，Docker融资这么多，那么一定要上市IPO，才能给投资方以回报。而上市是需要业绩的，既然开发者市场是几乎不可能赚钱盈利，只能转向企业级市场，一旦启动进入企业级市场的进程，就必然会挤压Docker生态圈的第二类厂商——做Docker集群管理的厂商们。\nDocker进入企业环境，第一个就是要运行Docker镜像，而且不是一个两个镜像，要运行一个集群，这样Docker集群的部署、管理、调度就成为Docker进入企业级第一需求。\n如下图是Docker的发展历程，下面是Docker容器的进展，上面是CaaS(Container As a Servie)解决方案的并购和进展。有心人很容易发现，从2014年底开始，Docker密密麻麻的收购，全都投入在CaaS，无论是收购还是从产品到解决方案，可见Docker在CaaS上了大赌注。\nDocker进入企业级市场有比较宏大的目标，2014年10月，Docker收购持续集成服务商 Koality，Docker把Koality在企业市场方面的成熟经验引入到 Docker Hub企业版本中，瞄准的是企业市场。同月，Docker收购了总部位于伦敦的Orchard Laboratories，进入复杂应用编排功能的企业市场。2014年底推出Docker Machine、Swarm、Compose,　2015年对这三个产品持续升级。\n2015年3月，Docker收购SDN公司SocketPlane,解决Docker集群的网络问题。同月Docker收购了用于Docker管理的开源图形用户界面工具Kitematic，自动化了Docker安装和配置过程。2015年 10年，Docker收购Tutum，补充Docker Hub，补充对Docker运行时的支持。进入2016年，Docker再次动作频频，2016年1月，Docker收购Unikernel Systems,进入OS领域，把Docker容器带入最简OS内核。如下图，打造适合运行容器的最简OS, 整个容器所占的资源进一步减少，从而让机器跑的更快，把容器的价值发挥到极限，至于这是否适合企业应用还需要验证。\n向下，Docker侵入OS领域，向上，Docker挤占CaaS市场空间。\n2016年3月，Docker收购Conductant，入主Aurora，根据如下Docker的规划，Aurora直接和kubernetes以及Marathon竞争，特别是和Mesos的架构完全对应， Docker Aurora+Swarm直接和Marathon+Mesos竞争。使得Docker Swarm从小规模集群管理，扩展到大规模Docker集群管理。\n在这个Docker提供架构图，可以清楚的看到没有了Mesos,而是Docker Swarm+Aurora直接取代Mesos+Marathon。同时，看看下面来自Docker的博客\n“There are manycommercial distributions of Mesos, but none of them incorporate Aurora. Webelieve that is a wasted opportunity. We plan on incorporating the best ideasfrom Aurora into Docker Swarm, and are exploring integrating Aurora as anoptional component of the official Docker stack.”\n翻译过来：\n“虽然Mesos拥有多款商业发行版，但其中没有任何一款受到Aurora的启发。我们认为这实在是一种巨大的浪费。我们计划将Aurora中的各类卓越思维成果引入Docker Swarm，并正在尝试将Aurora作为Docker正式堆栈的可选组件之一。”\nDocker已经在抱怨有太多的Mesos商业发行版，搭了Docker便车，已经在赚Docker进入企业级市场的钱，而Docker自己的产品居然还没开始赚钱。\n是可忍孰不可忍！\n2016年2月，Docker公布了其DDC(Docker DataCenter)的架构图和报价，如下图，蓝色部分是Docker的CaaS解决方案，青色部分是还需要第三代的产品或开源产品来补充形成完整的解决方案，青色部分所占比例还不小，可见Docker的CaaS上要走的路还很长，Docker的这个CaaS有不少模块是前面收购来的。既然是商业发行版，DDC也保留了部分模块不开源，走的甚至比CloudEra的发行版更远。Docker也很快给了个并不便宜公共订阅报价，难道是Docker在盈利上有急切的需求？\nDocker生态圈的演进 Docker在2013-2014年专注于把容器做好，没来得及顾得上企业级市场，Mesosphere和Google瞄上了这个市场，同时Redhat也把自己的PaaS推倒重来，准备用Docker+kubernetes。2014年底，Docker已经准备进入企业级市场，推出Docker Machine,Swarm和Compose。\nDocker作为Docker的宗主，着眼于Docker市场环境最有利润的Docker生产环境集群管理是很自然而然的，我们再来分析这个市场三只早起的鸟儿：Google、Mesosphere、Redhat。\n一旦Docker进入企业级CaaS市场，Google第一个就感受到了这个压力。\nGoogle无疑是最有技术敏锐性和市场敏锐性的，早早的看到了Docker企业级市场的企图心，所以Google是第一个支持Docker的竞争对手\u0026mdash;-CoreOS的Rocket容器，2014年四月份谷歌风险投资公司牵头对CoreOS进行了1200万美元的投资，目标明确\u0026mdash;对准docker。Google不再是Docker+ kubernetes,而是容器抽象+ kubernetes。\nGoogle对容器层进行了抽象，使得kubernetes即能支持Rocket,也能支持Docker,而Rocket和Docker有很大的不同，kubernetes对此进行了折中，不再对所以的Docker的功能支持，只支持kubernetes抽象出的容器功能，如果Docker自己的功能不在kubernetes抽象的容器功能之中，kubernetes选择不支持。最典型的是libnetwork/CNM，kubernetes认为这是Docker的特定功能，不予支持，Google自己搞了一个CNI。所以kubernetes和Docker走在分道扬镳的路上，距离越来越远。\n除了支持CoreOS，Google更是联合容器业界相关的厂商,组成OCI(Open Container Initiative)。业界对Docker在容器领域一家把控早有怨言，所以OCI一成立，就得到热烈响应。和普通的联盟或标准化组织不一样，OCI成立之初就定下目标—容器标准化，包括容器引擎的标准化实现—RunC，定个标准化规范容易给各方钻空子，但是做一个标准化的实现，就可以在相当程度上实现真正的容器统一。Docker眼看OCI实在太热烈，不得不折中考虑，加入OCI，实现RunC。\n但是总是心有不爽，虽然RunC发展很快Docker从1.11开始就采用了RunC的引擎，但是这不就开始和Google也业界大佬开撕了，过程很简单，Kubernetes的KelseyHightower说不要Docker引擎就可以跑Docker镜像，Docker CTO Solomon Hykes马上说，不用Docker引擎， 10%的运行会有问题，然后就扯到OCI，Docker说OCI是个伪标准，立马得到无数的砖头。\nDocker也加入了OCI，对RunC的贡献也不小，现在出尔反尔，现在看到Docker可能会受OCI/RunC的牵制影响，立马不管脸面了，利益第一。\n但是技术潮流是无法一家控制的，OCI/RunC作为业界各大厂商制约Docker的标准迟早会越发展越好，容器并不是什么可以垄断性的技术，或者说容器本身的技术含量并没有高到其他厂商做不好，只不过Docker在合适的时间点点燃了一个干柴烈火的市场。\n关于开撕的细节，大家可以看：\nhttp://mp.weixin.qq.com/s?\\_\\_biz=MzI3OTEzNjI1OQ==\u0026amp;mid=2651492692\u0026amp;idx=1\u0026amp;sn=e24efbcc6dcc5ce50773c505a13ccab9\u0026amp;scene=1\u0026amp;srcid=0801BkQ10pDA18gQEy39nObK\\#wechat\\_redirect\nGoogle对Docker容器的制约，不止体现在OCI容器层面，这不，前两天Google宣布和Mirantis的合作，K8s直接支持OpenStack，意味着K8s除了可以管理容器，还会延伸到管理虚机集群，在这个架构下，Google弱化Docker容器的的意图很明显。\n由于Google早早的对容器进行抽象，可以预见，即使脱离Docker生态圈，kubernetes依然有其市场，而且主要是大型容器和虚机部署的市场。\n再说Mesosphere。2016年上半年迟钝的Mesosphere终于意识到Docker的野心和意图，开始尝试脱离Docker，在新的Mesos Containerizer中支持脱离Docker Daemon建立容器，为下一步支持Rocket/RunC做准备。\nMesosphere相当于而言是比较不敏感的，一直跟着Docker跑，即使在Docker要做Swarm时，而且Docker已经做了Swarm仍然不敏感，终于Docker已经明确的对Mesosphere通过发行版赚钱表示了明显的不满，可以理解，Docker自己花这么多资源做出一个Docker和相应生态，还没开始赚钱，搭车的先赚钱了，换谁谁也不乐意。2016年2月Docker发布DDC(Docker DataCenter)和报价,已经非常明确了Docker要进入企业级市场。\n2016年上半年迟钝的Mesosphere终于意识到Docker的野心和意图，开始尝试脱离Docker，在新的Mesos Containerizer中支持脱离Docker Daemon建立容器，为下一步支持Rocket/RunC做准备。\nDocker进入企业级市场，第一个碾压的就是Mesosphere。一方面Docker通过收购Conductant获得Aurora，未来必定会合并到DDC中，DDC完全可以覆盖Mesosphere的DCOS在Docker集群管理上的功能，而且Docker还有一招，未来只提供Swarm的API，封闭或是大幅改变Docker API，那么Mesos就只能调Swarm的API，Swarm本来就和Mesos有很大的重叠，如果Mesos再通过Swarm去管理Docker集群，那Mesos的价值可能小于功能重叠带来的复杂性。当然Docker是否会祭出这个大招让我们拭目以待，虽然在Docker的新的架构图已经有此规划。\n在这种生态环境下，用户的选择是最难的，已经选择了Docker+Mesos的用户，必然会面临未来继续走Docker+Mesos的路，还是壮士断腕，切换到正宗的Docker DDC，但DDC还不成熟，目前阶段还不适合选择。\n而Docker+Mesos是注定未来很难升级，越往后越边缘化。\n再看Redhat，2014年的时候看到Docker火起来，Redhat见异思迁，马上抛弃自己的OpenShift V2的整体架构，包括抛弃原有应用容器Gear，全面采用Docker替代Gear，同时用kubernetes替代原有的容器管理。如此革命式的改造，完全不考虑前后的兼容性。\n这导致已经部署在OpenShift V2的应用迁移到OpenShift V3非常困难，虽然提供了一个迁移工具，但是Redhat自己都不敢用，到目前为止，Redhat的OpenShift公有云还是OpenShift V2，在给客户推销OpenShift V3的时候自己不从V2升级到V3，一定是难度极大，否则拼了命也要升级，”自己的狗粮自己得吃”。\nRedhat在OpenShift V3技术架构的选择显得鲁莽，核心技术是Docker和kubernetes,而Redhat对这两个技术都没有掌控，一旦Docker的发展或是kubernetes的发展和自己的战略目标不一致，可能还得推倒重来，又是一次”见异思迁”，目前就实际的遇到了这个问题，Docker要发展自己的DDC，和kubernetes竞争,而kubernetes已经对容器进行了抽象，不再支持Docker的特定功能，Docker和kubernetes已经处于分道扬镳的阶段，未来只会越走越远，而Redhat“只能眼睁睁的看着你却无能为力”，即无法说服Docker，也无法影响Google。\n另外，既然Docker要进入企业级容器集群管理市场，那OpenShift就必然和Docker存在竞争，Docker因为绝对的掌控了Docker容器，在竞争中有天然的优势，这种优势随着Docker的升级和DDC的升级会与日俱增。到底是选择正宗(奥迪)还是山寨(奥拓)，客户也容易陷入困惑。\n开源技术也需要商业的成功 开源不等于免费，开源是一种商业模式，一个开源组织和开源项目要想生存下去，最重要的基础就是普遍被使用，不然很快就会被竞争者替代。\n一个软件被普遍被使用之后，还需要因此衍生出相关服务，团队可以通过这些服务获得比较好的收入，商业模式就成型了，没有商业的支持的开源是很难成为一个成熟、商业可用的技术。就拿Linux来说，Redhat和Suse Linux作为比较成功的Linux商业化，通过发行版和技术支持获得了商业成功，反过来推动了Linux的发展。Linux持续有开源路线和商业路线，商业客户需要商业的版本和支持，有Redhat和Suse等提供。开源的Linux如CentOS,CoreOS等。\n反面的例子就是OpenSSL,去年暴露出“心脏出血“漏洞，大家发现就一个人在维护在OpenSSL，也没人捐钱，Theo de Raadt\u0026ndash; OpenBSD项目的创始人说OpenSSL的代码”令人作呕“，主要原因就是没有商业的支持。这不最近又爆出新型高危漏洞。\n我们再看看Docker的生态圈，无论是Docker，还是Mesosphere，还是Google，都还没有在Docker开源生态圈获得商业成功。而开源技术终将走向商业，包括Docker，必然面临企业市场挑战，微软奋斗了几十年其企业市场跟甲骨文SAP比起来仍然望其项背，这需要积累。\n面对二三十家Docker创业公司，投资人是需要这些创业公司能商业成功的，而Docker本身技术没有成熟，特别在Docker集群管理、资源调度等生产应用方面。Docker生产使用都成熟，要Docker商业成功，不会是一个短期的过程。而docker和mesos这两家核心生态圈的公司到今天为止离盈利还非常远，那国内这些二三十家外围生态圈的创业公司短时期内商业成功几乎不可能。\n对于想在Docker上尝鲜的企业来说，要认清开源不等于免费。\nDocker生态圈的推论 Docker进入企业级市场，有优势，也有劣势，优势是挟Docker的大量开发者，劣势是没有做过企业级市场，开发者市场和企业级市场的做法完全不同，微软从消费者用户拓展到企业用户化了十多年的时间，在企业市场并没有取得和消费者市场一样的成功。\n做消费者市场，只要把产品做好，而做企业级客户，要一个一个去谈，每个客户的需求都不一样，需要一只庞大的销售、定制、支持队伍。Docker公司到目前为止也就100多人，做企业级市场没有几千人的销售、支持队伍是很难打开全球市场的。100人到数千人，管理模式、业务模式都需要几次转型。而Docker公司目前也只是提供DDC的订阅License和支付服务，并不提供面对面的销售和定制服务\nDocker公司进入企业级市场在技术上有最大的优势，撬夺Mesosphere、Google、Redhat的CaaS市场还是有相当的技术优势。通过对Docker API的控制、升级，可以完全影响上面所有对Docker容器集群管理的软件，也许3-5年，Docker的企业级产品会有相当的成功，但不会在2016年。但也许3-5年后，CaaS(Container As a Service)会有新的技术演进。\n2016年，作为Docker集群部署管理的生态圈公司：Google kubernetes、Redhat OpenShift、Mesos，面临Docker DDC的不平等技术竞争，会承载巨大的压力，他们会联合起来反制Docker公司。他们的应对就是釜底抽薪，弱化Docker容器，尽快让RunC成熟，在一定程度上取代Docker引擎。另外，直接就是支持虚机，不再受制于Docker，而是直接在企业级市场全面竞争。\n和Hadoop生态圈的第一类公司类似，2016-2017年，可能有Docker集群管理的公司会逐步退出这个市场。\n事实上，无论是Google还是Mesos,都已经走在和Docker分道扬镳的路上。如果我们和大数据对比一下，Docker有点像CloudEra，技术领先； Redhat像Hortonworks，先上市再说，Redhat是先把产品上市，Hortonworks是先资本上市。Google有点像Intel投资Hadoop，不属于主业，在副业上也投资。Mesos有点像MapR，总是不在核心圈子里，越来越式微。\n2016年．作为Docker CaaS私有云项目实施公司，包括：Rancher、才云、数人云、CSphere、云雀云、Hyper、DaoCloud、有容云、好雨云、轻元科技、迅达云、飞致云、时速云、精灵云、领科云等。和Hadoop的第二类项目实施的生态圈公司走过的历程类似，2016-2017年，各大Docker CaaS项目实施厂商是陷入低价血战的时代。同时，技术方向的选择和标杆客户案例非常关键，如果技术方向选择不对，所选择的Docker集群管理软件被边缘化，那么技术的积累价值会大幅打折，客户的标杆也可能会成为反例。最典型的就是选择Mesos的技术路线的，目前已见颓势。也有的抱Docker大腿不放，选择纯Docker的技术线路，容器集群管理也用Docker Swarm，Docker Swarm有可能会一直很难成熟，特别是和K8s相比,存在巨大的技术风险。\n2016年，有些企业级客户开始选择Docker做CaaS，但是客户面临最大的问题是战略性的问题，到底选择哪个Docker集群管理软件，在Docker纷繁复杂的生态圈里做出正确的选择并不容易，考验客户的技术眼光，选择了一个短命的产品以后再纠正并不容易。\n其实，Hadoop的客户走过这样的困境，我想起上海某政府客户，在2012年选择Intel的Hadoop实施信息共享项目，成为Intel的全球案例，2013年10月上线，2014年Intel放弃自己的Hadoop，裁撤了几乎所有的Hadoop团队，这时数据和系统都已经上线半年多了，对于一个已经上线提供服务的Hadoop，再去换一个Hadoop，难度可想而知，数据迁移和应用迁移不一样，难度高出许多。如果不换Hadoop,永远停留在Intel的Hadoop 1.0上又失去了采用开源软件的意义，采用开源软件很重要一点是能随开源的成长而成长。面临这种尴尬的时候再次提醒我们产品和技术选择的重要性。推导到Docker企业应用，早期尝鲜的企业客户把Docker集群管理调度部署到生产环境，会不会碰到这种尴尬呢？\n目前国内的Docker创业公司超过20家，都想进入企业市场，导致异乎激烈的竞争。而过于激烈的竞争，带来一个畸形的模式————大家主要把钱和资源化在吸引眼球上，而不是把主要力量放在把产品做好（国内的OpenSatck公司何尝不是如此！）。\n有个银行客户，准备测试一下Docker，居然超过十家Docker创业公司主动要去测试，据说预算只有几十万。\n20多家Docker创业的小公司，少的十几个人，多的几十人，上百的还很罕见，毕竟投资人的钱烧起来很快，搞个上百人的，一年的工资支出可能就几千万。20多家小公司，怎么让客户知道你？这是Docker创业公司的面临的第一个困境，解决办法就是搞市场活动，据说有的公司居然80%的资金都花在市场活动上，常用在酒店给客户讲方案的市场活动不凑效，那就搞技术Fans的Meetup，大家去看看今年的Docker/K8s/Mesos的Meetup多如牛毛，每周都有。Meetup就一定有效吗？对企业级市场来说，并不完全有效，参加Meetup的都是工程师，不是企业项目的决策人，工程师想用，企业决策人还没看清暂时不用是普遍现象。\n一方面，把投资人的钱花在各种市场活动上，另外一方面，Docker/K8s/Mesos作为开源技术，这些创业公司对Docker/K8s/Mesos的代码贡献很小，仅仅有一两家对代码有微量贡献，大多数对Docker开源代码是零贡献，对于没有对开源有多少贡献，希望从开源项目赚钱，这多少有点投机取巧，而且，Docker/Google都还没有从自己主导的开源项目赚钱，搭便车的先赚钱，商业上合理吗？\n中国目前的容器市场能支撑的了20多家Docker创业公司吗？而这些创业公司绝大多数拿的是投资人的钱，投资人的钱也不是风刮来的，天使轮投资可以只要个Idea,但到A轮/B轮，怎么也得看点数据，你是拿了几个单，还是有多少营收，有多少利润，Docker创业公司面对的是B2B市场，不是B2C市场，B2C市场可以烧钱拉用户，只要用户量在持续增长，可以扩大亏损继续烧。对于B2B市场，是要建立标杆案例项目再复制，标杆可以不赚钱，复制项目总得赚钱，而目前的残酷现实是标杆项目大家打破头，没赚钱。你想复制的时候，20 多家的竞争对手还想着不要钱做自己的标杆，所以标杆项目的复制为盈利项目几乎不可能。\n在目前这个市场形势下，投资人再往下投多少会更谨慎一些，那在市场上花钱如流水的Docker创业公司，一旦钱花的差不多，并没有达到预期数字，投资人在投钱上再谨慎起来，一些Docker创业公司的死掉只是时间的问题，也许年底就可以看到倒掉的Docker创业公司。\n给准备Docker尝鲜的客户的建议 目前有些企业已经在采用Docker和相关技术，据观察，有以下几类企业：\n  互联网公司，比较早期就开始关注Docker技术，在互联网应用中采用Docker容器，对应用的一致性要求不高，能接受数据的最终一致性。有的仅仅是容器，自己做管理，有的采用Mesos来管理集群，也有采用K8s来管理Docker容器的。这类客户应当占了目前Docker用户的95%以上。\n  为了混Docker圈子的重IT型公司，数量少，频繁出现在各种Docker市场活动中介绍成功经验。 这些企业有个特点，喜欢在众多的Docker市场活动上介绍使用Docker的成功经验。作为一个企业，使用新技术可能可以造就新的竞争力，但是在各种Docker的市场活动中介绍成功经验好像和公司的核心竞争力并不完全一致，这些公司的核心竞争力肯定不是是用Docker而带来的。\n  传统企业在技术创新中采用Docker，取代了很好的效果。\n  这种企业不多，企业的IT领导人有很强的技术驾驭能力，能够吸纳新技术。通过试点采用Docker成功以后，逐步推广，取代了比较好的效果。\n分析这些Docker的使用者，很容易发现在真正企业级环境使用不多，在美国也如此。主要还是互联网公司在使用。\n对于企业客户而言，要采用Docker，一般是两种方式：要么选择Docker相关公司来实施，要不然自己基于Docker定制，这种方式工作量太大，需要巨大的团队，对企业来说不合适。如果选择Docker相关公告，目前选择这些Docker创业公司是有巨大风险的，一方面他能生存多久很难说，考虑客户的选择眼光。\n如果做Docker项目选择这些公司的产品来实施，对于技术力量稍微强一点的Docker创业公司，他会自己做一些包装定制，把自己的产品提供给客户，但是风险在于，对于定制过的产品，Docker/K8s/Mesos/Swarm的后续发展非常快，这些开源版本一升级，你就得跟着升级，定制了以后升级并不容易，往往会要去改动代码或是配置。作为客户如果采用开源的产品，而不跟着升级，那就失去了采用开源的非常关键的一个价值，开源的发展一开始是不成熟的，如果不能跟着开源逐渐成熟，那选择早期的开源不升级往往是很难达到效果。\n也有小的创业公司，对Docker几乎不做定制，甚至界面都不改，就是把环境部署起来，那对企业级应用来说，过于简陋，对使用者要求很高，很大程度上是达不到应有效果的。\n对于企业级客户准备采用Docker,鉴于目前Docker在企业级的生产环境应用规模很小，成熟度有待提高，而且对于企业级应用很难在Mesos/K8s/Swarm之间做选择，一旦技术路线选择错误，后面调头重来成本非常高。而且这么多小的Docker创业公司，能不能生存下去，长期提供技术支持，也是需要考虑的问题。\n企业要综合考察IaaS/CaaS/PaaS，选择相对成熟的技术，相比而言IaaS和PaaS均比较成熟，基于Docker的CaaS还没有定型，还在成熟的过程中。要结合企业的实际需求，对于潮流技术，先小规模验证，体验取得的实际效果，分析存在的隐患，再做综合决策不迟。\n ","permalink":"https://jeremyxu2010.github.io/2016/08/docker%E7%94%9F%E6%80%81%E4%BC%9A%E9%87%8D%E8%B9%88hadoop%E7%9A%84%E8%A6%86%E8%BE%99%E5%90%97%E8%BD%AC/","tags":["docker","hadoop"],"title":"Docker生态会重蹈Hadoop的覆辙吗？(转)"},{"categories":["容器编排"],"contents":"以前尝试使用consul搭建了docker集群，当时对底层为什么要执行那些命令不是太理解，直到昨天研究了etcd集群之后，终于对docker集群搭建时的一些命令有了新的认识。今天尝试配合etcd搭建docker集群，以这里记录以备忘。\n创建docker swarm集群mange节点 还是使用上一篇文章里搭建好的etcd集群服务，其地址为http://192.168.99.100:2381,http://192.168.99.100:2383,http://192.168.99.100:2385\ndocker-machine create -d virtualbox --engine-registry-mirror=https://xxx.mirror.aliyuncs.com --swarm --swarm-master --swarm-opt=\u0026#34;replication\u0026#34; --swarm-discovery=\u0026#34;etcd://192.168.99.100:2381\u0026#34; --engine-opt=\u0026#34;cluster-store=etcd://192.168.99.100:2381\u0026#34; --engine-opt=\u0026#34;cluster-advertise=eth1:2376\u0026#34; node1 这里解释一下这条命令：\n --swarm指定了创建docker主机时开启swarm集群功能 --swarm-master指定了创建docker主机后要在docker主机里运行一个swarm manage的docker容器 --swarm-opt=\u0026quot;replication\u0026quot; 启用swarm manage节点之间的复制功能 --swarm-discovery=\u0026quot;etcd://192.168.99.100:2381\u0026quot;指定swarm集群所使用的发现服务地址 --engine-opt=\u0026quot;cluster-store=etcd://192.168.99.100:2381\u0026quot; docker引擎所使用的KV存储服务地址 --engine-opt=\u0026quot;cluster-advertise=eth1:2376\u0026quot; swarm集群里该节点向外公布的服务地址。这里为什么是eth1，刚开始我也觉得很奇怪，后来我使用docker-machine ssh node1登入docker主机，再执行ifconfig才发现使用docker-machine创建的docker主机一般eth1为外部可访问的网络接口。  为了避免swarm集群manage节点的单点故障，这里再创建一个manage节点\ndocker-machine create -d virtualbox --engine-registry-mirror=https://xxx.mirror.aliyuncs.com --swarm --swarm-master --swarm-opt=\u0026#34;replication\u0026#34; --swarm-discovery=\u0026#34;etcd://192.168.99.100:2381\u0026#34; --engine-opt=\u0026#34;cluster-store=etcd://192.168.99.100:2381\u0026#34; --engine-opt=\u0026#34;cluster-advertise=eth1:2376\u0026#34; node2 创建docker swarm集群普通节点 docker-machine create -d virtualbox --engine-registry-mirror=https://xxx.mirror.aliyuncs.com --swarm --swarm-discovery=\u0026#34;etcd://192.168.99.100:2381\u0026#34; --engine-opt=\u0026#34;cluster-store=etcd://192.168.99.100:2381\u0026#34; --engine-opt=\u0026#34;cluster-advertise=eth1:2376\u0026#34; node3 docker-machine create -d virtualbox --engine-registry-mirror=https://xxx.mirror.aliyuncs.com --swarm --swarm-discovery=\u0026#34;etcd://192.168.99.100:2381\u0026#34; --engine-opt=\u0026#34;cluster-store=etcd://192.168.99.100:2381\u0026#34; --engine-opt=\u0026#34;cluster-advertise=eth1:2376\u0026#34; node4 这里创建了两个docker swarm集群普通节点\n检查swarm集群的状况 docker集群创建好了，用docker客户端连上去查看一下集群的状况。\neval $(docker-machine env --swarm node2) docker info Containers: 6 Running: 6 Paused: 0 Stopped: 0 Images: 4 Server Version: swarm/1.2.5 Role: replica Primary: 192.168.99.104:3376 Strategy: spread Filters: health, port, containerslots, dependency, affinity, constraint Nodes: 4 node1: 192.168.99.104:2376 └ ID: A77F:BLVI:ZVRM:7RIL:XHGX:5K4F:OO3I:MPTE:JVJU:IBK2:NIPM:LDRJ └ Status: Healthy └ Containers: 2 (2 Running, 0 Paused, 0 Stopped) └ Reserved CPUs: 0 / 1 └ Reserved Memory: 0 B / 1.021 GiB └ Labels: kernelversion=4.4.17-boot2docker, operatingsystem=Boot2Docker 1.12.1 (TCL 7.2); HEAD : ef7d0b4 - Thu Aug 18 21:18:06 UTC 2016, provider=virtualbox, storagedriver=aufs └ UpdatedAt: 2016-08-24T15:33:34Z └ ServerVersion: 1.12.1 node2: 192.168.99.105:2376 └ ID: EGYZ:TXBG:S3AR:W35S:Q3Y5:24GH:NEWN:ZAQP:R7QN:W3KR:VWGQ:YFRH └ Status: Healthy └ Containers: 2 (2 Running, 0 Paused, 0 Stopped) └ Reserved CPUs: 0 / 1 └ Reserved Memory: 0 B / 1.021 GiB └ Labels: kernelversion=4.4.17-boot2docker, operatingsystem=Boot2Docker 1.12.1 (TCL 7.2); HEAD : ef7d0b4 - Thu Aug 18 21:18:06 UTC 2016, provider=virtualbox, storagedriver=aufs └ UpdatedAt: 2016-08-24T15:33:07Z └ ServerVersion: 1.12.1 node3: 192.168.99.106:2376 └ ID: OJAQ:GLMP:2LKC:6PSU:JYT6:DOGM:O5LA:IAS3:EZYG:G5NV:ON3F:76TF └ Status: Healthy └ Containers: 1 (1 Running, 0 Paused, 0 Stopped) └ Reserved CPUs: 0 / 1 └ Reserved Memory: 0 B / 1.021 GiB └ Labels: kernelversion=4.4.17-boot2docker, operatingsystem=Boot2Docker 1.12.1 (TCL 7.2); HEAD : ef7d0b4 - Thu Aug 18 21:18:06 UTC 2016, provider=virtualbox, storagedriver=aufs └ UpdatedAt: 2016-08-24T15:33:00Z └ ServerVersion: 1.12.1 node4: 192.168.99.107:2376 └ ID: W6EL:DUI6:LWEV:T55B:YJ2O:GBIQ:S3CA:GAKD:OMC7:YBVE:NKE4:TT4U └ Status: Healthy └ Containers: 1 (1 Running, 0 Paused, 0 Stopped) └ Reserved CPUs: 0 / 1 └ Reserved Memory: 0 B / 1.021 GiB └ Labels: kernelversion=4.4.17-boot2docker, operatingsystem=Boot2Docker 1.12.1 (TCL 7.2); HEAD : ef7d0b4 - Thu Aug 18 21:18:06 UTC 2016, provider=virtualbox, storagedriver=aufs └ UpdatedAt: 2016-08-24T15:33:23Z └ ServerVersion: 1.12.1 Plugins: Volume: Network: Kernel Version: 4.4.17-boot2docker Operating System: linux Architecture: amd64 CPUs: 4 Total Memory: 4.085 GiB Name: 7d436f0f312b Docker Root Dir: Debug mode (client): false Debug mode (server): false WARNING: No kernel memory limit support Over，一个四节点的docker集群就这么简单的搭建好了。\n下一步计划 参照http://www.alauda.cn/2016/01/18/docker-1-9-network/研究一下容器网络模型（Container Network Model，简称CNM），同时研究一下实际场景中如何使用pipework来灵活地定制容器的网络。\n","permalink":"https://jeremyxu2010.github.io/2016/08/%E9%87%8D%E6%90%ADdocker%E9%9B%86%E7%BE%A4/","tags":["docker","etcd"],"title":"重搭docker集群"},{"categories":["devops"],"contents":"今天到一个朋友的创业公司进行技术交流，交流过程中，朋友提到他在阿里云上买的linux服务器上ssh服务经常被人暴力破解。我感觉很奇怪，一般来说ssh服务经过简单设置是很安全的，怎么可能会出现这种情况呢。进一步交流才知道他们购买linux服务器后，连一些基本的安全措施都没做。原来并不是所有人都知道放在公网上的服务器是要进行简单的安全加固的。下面把我这些年使用linux时对ssh服务的安全加固步骤写下来，以便其它人参考。（以下的命令脚本基于CentOS6，其它发行版类似）\n使用普通用户密钥文件登录 直接使用root用户登录服务器当然是不安全的，建议创建一个普通用户用于ssh远程登录。命令如下：\nuseradd -m cloudop 在另一台PC上生成登录密钥。命令如下：\nssh-keygen -N \u0026#39;\u0026#39; -f ~/.ssh/cloudkey_rsa 然后将上述命令生成的~/.ssh/cloudkey_rsa.pub文件拷贝到服务器，并执行以下命令配置好密钥登录：\nmkdir -p /home/cloudop/.ssh cat /tmp/cloudkey_rsa.pub \u0026gt;\u0026gt; /home/cloudop/.ssh/authorized_keys chown -R cloudop:cloudop /home/cloudop/.ssh \u0026amp;\u0026amp; chmod 700 /home/cloudop/.ssh \u0026amp;\u0026amp; chmod 600 /home/cloudop/.ssh/authorized_keys 然后在另一台PC使用以下命令登录服务器：\nssh -i ~/.ssh/cloudkey_rsa cloudop@${server_ip} 如果可成功登录，则说明密钥登录配置成功，登录成功后可执行su - root命令切换至root权限进行操作，当然你得知道root密码。\n最后修改ssh服务的配置文件禁用root用户登录、仅开放密钥验证方式禁用其它验证方式\nsed -i -e \u0026#39;/^PermitRootLogin.*$/d\u0026#39; /etc/ssh/sshd_config echo \u0026#34;PermitRootLogin no\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config sed -i -e \u0026#39;/^PasswordAuthentication.*$/d\u0026#39; /etc/ssh/sshd_config echo \u0026#34;PasswordAuthentication no\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config sed -i -e \u0026#39;/^ChallengeResponseAuthentication.*$/d\u0026#39; /etc/ssh/sshd_config echo \u0026#34;ChallengeResponseAuthentication no\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config sed -i -e \u0026#39;/^GSSAPIAuthentication.*$/d\u0026#39; /etc/ssh/sshd_config echo \u0026#34;GSSAPIAuthentication no\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config service sshd restart 修改ssh服务端口，升起防火墙 有很多攻击程序会扫描公网上服务器的22端口，一旦发现22端口就开始调用程序暴力破解。安全起见，建议修改ssh服务器的端口，命令如下：\nsed -i -e \u0026#39;/^Port.*$/d\u0026#39; /etc/ssh/sshd_config echo \u0026#34;Port 1221\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config service sshd restart 同时修改防火墙设置，以允许外部访问修改后的端口，命令如下：\nservice iptables start iptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 1221 -j ACCEPT iptables-save \u0026gt; /etc/sysconfig/iptables service iptables restart 这里特意说明一下，放在公网上的服务器一定要升起防火墙，并只开必要的端口。我一般只开放80、443、1221三个端口。\n当然以后访问服务器时得使用以下命令：\nssh -i ~/.ssh/cloudkey_rsa -p 1221 cloudop@${server_ip} scp拷贝文件时得使用以下命令：\nscp -i ~/.ssh/cloudkey_rsa -P 1221 cloudop@${server_ip}:/tmp/xxxx.txt ./ 防止用户暴力破解ssh服务 尽管已经只允许使用密钥登录，而且ssh服务的端口也修改了，可还是难以避免攻击者暴力破解，我一般使用denyhosts这个小软件防范。\n这个小软件的原理是分析/var/log/secure（redhat，Fedora Core）等日志文件，当发现同一IP在进行多次SSH密码尝试时就会记录IP到/etc/hosts.deny文件，从而达到自动屏蔽该IP的目的。\n安装配置过程如下：\n# 下载DenyHosts并解压 wget http://heanet.dl.sourceforge.net/project/denyhosts/denyhosts/2.6/DenyHosts-2.6.tar.gz tar zxvf DenyHosts-2.6.tar.gz cd DenyHosts-2.6 # 安装前清空以前的日志并重启一下rsyslog echo \u0026#34;\u0026#34; \u0026gt; /var/log/secure \u0026amp;\u0026amp; service rsyslog restart # 因为DenyHosts是基于python的，所以要已安装python，大部分Linux发行版一般都有。默认是安装到/usr/share/denyhosts/目录的,进入相应的目录修改配置文件 python setup.py install cd /usr/share/denyhosts/ cp denyhosts.cfg-dist denyhosts.cfg cp daemon-control-dist daemon-control \u0026amp;\u0026amp; chown root daemon-control \u0026amp;\u0026amp; chmod 700 daemon-control # 如果要使DenyHosts每次重起后自动启动还需做如下设置 ln -sf /usr/share/denyhosts/daemon-control /etc/init.d/denyhosts \u0026amp;\u0026amp; chkconfig --add denyhosts \u0026amp;\u0026amp; chkconfig --level 2345 denyhosts on # 启动denyhosts服务 service denyhosts start /usr/share/denyhosts/denyhosts.cfg为denyhosts服务的配置文件，默认的设置已经可以适合centos系统环境，这里对这个配置文件里一些关键配置项作一个说明，大家可以按需修改。\nSECURE_LOG = /var/log/secure #sshd日志文件，它是根据这个文件来判断的，不同的操作系统，文件名稍有不同。 HOSTS_DENY = /etc/hosts.deny #控制用户登陆的文件 PURGE_DENY = 5m DAEMON_PURGE = 5m #过多久后清除已经禁止的IP，如5m（5分钟）、5h（5小时）、5d（5天）、5w（5周）、1y（一年） BLOCK_SERVICE = sshd #禁止的服务名，可以只限制不允许访问ssh服务，也可以选择ALL DENY_THRESHOLD_INVALID = 5 #允许无效用户失败的次数 DENY_THRESHOLD_VALID = 10 #允许普通用户登陆失败的次数 DENY_THRESHOLD_ROOT = 5 #允许root登陆失败的次数 HOSTNAME_LOOKUP=NO #是否做域名反解 DAEMON_LOG = /var/log/denyhosts 如果你有一些确定的IP不希望被denyhosts屏蔽，可以执行以下命令将这些IP加入到白名单：\necho \u0026#34;你的IP\u0026#34; \u0026gt;\u0026gt; /usr/share/denyhosts/allowed-hosts # 重启denyhosts服务 service denyhosts restart 如有IP被误封，可以执行下面的命令解封：\nwget -O /usr/share/denyhosts/denyhosts_removeip.sh http://soft.vpser.net/security/denyhosts/denyhosts_removeip.sh bash /usr/share/denyhosts/denyhosts_removeip.sh 要解封的IP 其它 服务器的root密码一般只在由普通用户切换至root用户时用到，设置得太复杂，操作不方便，太复杂形同虚设。\n所以我一般在团队内部约定一个root密码的规则，这个规则仅团队内部人员知道，比如设置为'R00T@主机名@公司名\u0026rsquo;。\n另外服务器的登录密钥文件一定要妥善保存与分发。\n总结 做完以上几步，攻击者基本上就很难暴力攻击你的ssh服务了。\n","permalink":"https://jeremyxu2010.github.io/2016/08/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8Assh%E6%9C%8D%E5%8A%A1%E5%AE%89%E5%85%A8%E5%8A%A0%E5%9B%BA/","tags":["linux","denyhosts","ssh"],"title":"云服务器上ssh服务安全加固"},{"categories":["云计算"],"contents":"很早之前就听说过etcd，只记得是一个跟zookeeper很类似的东西，可以用来实现分布式锁。但一直没有关心这个东西到底是如何部署的，部署时怎么保证高可用，除了分布式锁外是否还有其它趣的功能。今天下班回家研究了下这个东东，很有收获，这里记录一下。\n部署 创建一个docker主机 由于我本机并没安装etcd，于是想就直接在docker里玩etcd好了，所以先创建一个docker主机。\ndocker-machine create --driver virtualbox --engine-registry-mirror=https://xxxxx.mirror.aliyuncs.com etcd-servers #执行下面的命令拿到这个docker主机的ip，假设是192.168.99.100 docker-machine ip etcd-servers #设置好使用docker命令依赖的环境变量 eval $(docker-machine env etcd-servers) etcd发现创建etcd集群 为了保证etcd服务的高可用性，我决定还是创建一个etcd服务集群。官方文档里讲到了三种方式：静态、etcd发现、DNS发现。见这里。我感觉静态方式必须全都预先规划好才行，而DNS发现依赖于可控的域名解析服务，这些都挺不灵活的。所以最终还是选择了etcd发现的方式创建etcd集群。\n首先创建一个单节点的etcd服务\ndocker run --rm -p 2380:2380 -p 2379:2379 --name etcd0 quay.io/coreos/etcd etcd --name etcd0 --initial-advertise-peer-urls http://192.168.99.100:2380 --listen-peer-urls http://0.0.0.0:2380 --advertise-client-urls http://192.168.99.100:2379 --listen-client-urls http://0.0.0.0:2379 --initial-cluster etcd0=http://192.168.99.100:2380 生成一个唯一的UUID\n#假设生成的UUID为C7167DFB-6031-443B-9ED5-01DC3815F720 uuidgen 在单节点的etcd服务上创建集群服务发现的目录，并配置成当3个节点加入集群，整个集群开始启动。\ncurl -X PUT http://192.168.99.100:2379/v2/keys/discovery/C7167DFB-6031-443B-9ED5-01DC3815F720/_config/size -d value=3 分别启动三个etcd服务节点，并加入集群，当然这里只是试验，正常部署场景为了保证高可用，避免单点故障，集群节点需要部署在多台机器上。\ndocker run --restart=always -d -p 2382:2382 -p 2381:2381 --name etcd1 quay.io/coreos/etcd etcd --name etcd1 --initial-advertise-peer-urls http://192.168.99.100:2382 \\  --listen-peer-urls http://0.0.0.0:2382 \\  --listen-client-urls http://0.0.0.0:2381 \\  --advertise-client-urls http://192.168.99.100:2381 \\  --discovery http://192.168.99.100:2379/v2/keys/discovery/C7167DFB-6031-443B-9ED5-01DC3815F720 docker run --restart=always -d -p 2384:2384 -p 2383:2383 --name etcd2 quay.io/coreos/etcd etcd --name etcd2 --initial-advertise-peer-urls http://192.168.99.100:2384 \\  --listen-peer-urls http://0.0.0.0:2384 \\  --listen-client-urls http://0.0.0.0:2383 \\  --advertise-client-urls http://192.168.99.100:2383 \\  --discovery http://192.168.99.100:2379/v2/keys/discovery/C7167DFB-6031-443B-9ED5-01DC3815F720 docker run --restart=always -d -p 2386:2386 -p 2385:2385 --name etcd3 quay.io/coreos/etcd etcd --name etcd3 --initial-advertise-peer-urls http://192.168.99.100:2386 \\  --listen-peer-urls http://0.0.0.0:2386 \\  --listen-client-urls http://0.0.0.0:2385 \\  --advertise-client-urls http://192.168.99.100:2385 \\  --discovery http://192.168.99.100:2379/v2/keys/discovery/C7167DFB-6031-443B-9ED5-01DC3815F720 稍等一小会儿，然后就可以检查集群的运行状态了。\ndocker run --rm --name etcdctl quay.io/coreos/etcd etcdctl --endpoints http://192.168.99.100:2381,http://192.168.99.100:2383,http://192.168.99.100:2385 member list #上述命令的输出一般为下面这样，表示其中有一个节点已被选举为Leader了 #294e0faabf651c4: name=etcd1 peerURLs=http://192.168.99.100:2382 clientURLs=http://192.168.99.100:2381 isLeader=false #74afde18c4466428: name=etcd2 peerURLs=http://192.168.99.100:2384 clientURLs=http://192.168.99.100:2383 isLeader=false #f979555993d104ee: name=etcd3 peerURLs=http://192.168.99.100:2386 clientURLs=http://192.168.99.100:2385 isLeader=true docker run --rm --name etcdctl quay.io/coreos/etcd etcdctl --endpoints http://192.168.99.100:2381,http://192.168.99.100:2383,http://192.168.99.100:2385 cluster-health #上述命令的输出一般为下面这样，表示集群是健康的 #member 294e0faabf651c4 is healthy: got healthy result from http://192.168.99.100:2381 #member 74afde18c4466428 is healthy: got healthy result from http://192.168.99.100:2383 #member f979555993d104ee is healthy: got healthy result from http://192.168.99.100:2385 #cluster is healthy 然后就可以把先前创建的单节点etcd服务停止了\ndocker stop etcd0 docker rm etcd0 后续可参考官方文档对集群作进一步调整，如增删成员节点，见官方文档\n使用etcd集群 etcd通过HTTP API对外提供服务，同时etcd还提供了一个etcdctl的命令，使用也很简单，就里就不重复描述了。可参考这里。\npython版本的etcd客户端在这里，使用方法也很简单，项目首页文档写得很清楚。\n搭建docker swarm集群如要使用etcd作为外部配置存储服务，可在docker daemon的启动参数里加入–cluster-store=etcd://${etcd服务所在主机IP}:2379/store -–cluster-advertise=${docker daemon服务所在主机外部可访问IP}:2376\n总结  etcd相对于consul来看，提供了更多的访问方式，特别是HTTP API服务，对于调试问题来说很方便。之前搭建docker swarm集群使用的是consul，后面找个机会尝试一下使用etcd。 翻阅etcd文档后，发现etcd提供的功能比我数年前见到的zookeeper功能丰富了不少，很强大。以后项目中如果遇到分布式锁、监听变更事件、监测主机alive场景，可以考虑使用它。  参考 http://soft.dog/2016/02/16/etcd-cluster/#section-7 http://www.alauda.cn/2016/01/18/docker-1-9-network/ https://github.com/jplana/python-etcd https://coreos.com/etcd/docs/latest/docker_guide.html https://github.com/coreos/etcd/blob/master/Documentation/dev-internal/discovery_protocol.md https://github.com/coreos/etcd/blob/master/Documentation/op-guide/runtime-configuration.md https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md#static https://coreos.com/etcd/docs/latest/configuration.html#member-flags https://skyao.gitbooks.io/leaning-etcd3/content/ http://dockone.io/article/801\n","permalink":"https://jeremyxu2010.github.io/2016/08/%E5%AD%A6%E4%B9%A0etcd/","tags":["etcd","python","docker"],"title":"学习etcd"},{"categories":["devops"],"contents":"本时工作中经常需要向阿里云环境部署新的版本，部署完毕之后需要到阿里云管理控制台刷新页面的CDN缓存。这个过程中部署部分我现在是使用bash脚本完成的，很方便。但刷新页面CDN缓存一直是手工操作的，每次都要登录进入阿里云管理控制台，很是麻烦。今天突然想到是否可以调用阿里云API完成这个动作了，查一查还真查到了，链接在这里。下面就想办法调用一下这个API。正好最近在学python，而且阿里云API也有python的SDK，就拿到使使。\n编译python 由于SLES11SP2系统本身所带的python版本比较低，而阿里云依赖的python版本至少要2.7。时为了不影响系统自带的python，这个手工编译python。\nzypper install -y -t pattern Basis-Devel zypper install -y libbz2-devel readline-devel ncurses-devel libopenssl-devel libxslt-devel wget https://www.python.org/ftp/python/2.7.12/Python-2.7.12.tgz tar zxvf Python-2.7.12.tgz cd Python-2.7.12 ./configure --prefix=/opt/python2.7 make \u0026amp;\u0026amp; make installl 创建virtualenv环境 为了不在全局安装第三方python模块，这里使用virtaulenv构建出一个虚拟环境\nwget https://pypi.python.org/packages/8b/2c/c0d3e47709d0458816167002e1aa3d64d03bdeb2a9d57c5bd18448fd24cd/virtualenv-15.0.3.tar.gz#md5=a5a061ad8a37d973d27eb197d05d99bf tar zxvf virtualenv-15.0.3.tar.gz cd virtualenv-15.0.3 /opt/python2.7/bin/python setup.py install /opt/python2.7/bin/virtualenv /opt/refresh_cdn_cache source /opt/refresh_cdn_cache/bin/activate pip install aliyun-python-sdk-cdn deactivate 编写调用阿里云API的脚本 将调用api的python脚本放到这个目录\n/opt/refresh_cdn_cache/refresh_cdn_cache.py\n#!/usr/bin/env python from aliyunsdkcore import client Client=client.AcsClient(\u0026#39;${AccessKey}\u0026#39;,\u0026#39;${AccessSecret}\u0026#39;,\u0026#39;cn-hangzhou\u0026#39;) from aliyunsdkcdn.request.v20141111 import RefreshObjectCachesRequest request = RefreshObjectCachesRequest.RefreshObjectCachesRequest() request.set_accept_format(\u0026#39;json\u0026#39;) request.set_ObjectPath(\u0026#39;https://yun.cloudbility.com/\\nhttp://yun.cloudbility.com/\u0026#39;) request.set_ObjectType(\u0026#34;Directory\u0026#34;) result=Client.do_action(request) print result 改造原来的部署脚本 最后在原来的bash部署脚本最后添加一小段脚本如下：\n...... #部署完毕之后，稍等一会儿，然后调用python脚本完成CDN页面缓存的刷新 sleep 15 source /opt/refresh_cdn_cache/bin/activate python /opt/refresh_cdn_cache/refresh_cdn_cache.py deactivate 总结 python配合virtualenv、pip等工具搭建一个独立不受干扰的环境确实很方便。另外这种第三方的API还是使用python这种脚本语言去调用更方便，调试起来还很方便。\n","permalink":"https://jeremyxu2010.github.io/2016/08/%E4%BD%BF%E7%94%A8%E9%98%BF%E9%87%8C%E4%BA%91api%E5%88%B7%E6%96%B0cdn%E7%BC%93%E5%AD%98/","tags":["python","virtualenv","阿里云API"],"title":"使用阿里云API刷新CDN缓存"},{"categories":["python开发"],"contents":"编写软件最基础莫过于算法了。今天在翻阅python的学习资料时，看到了别人用python实现的8大排序算法。很惭愧作为一个9年工作经验的程序员，现在还记得的排序只剩下冒泡排序、快速排序等寥寥几个了。于是花了数个小时将这些排序算法又仔细揣度了一番，同时再一次感叹python语言的精练。\n八大排序算法  插入排序  插入排序的基本操作就是将一个数据插入到已经排好序的有序数据中，从而得到一个新的、个数加一的有序数据，算法适用于少量数据的排序。时间复杂度最好的情况为O(n),最坏的情况是O(n^2) 。是稳定的排序方法。\n插入算法把要排序的数组分成两部分：第一部分包含了这个数组的所有元素，但将最后一个元素除外（让数组多一个空间才有插入的位置），而第二部分就只包含这一个元素（即待插入元素）。在第一部分排序完成后，再将这个最后元素插入到已排好序的第一部分中。\n代码实现：\ndef insert_sort(lists): # 插入排序 count = len(lists) for i in range(1, count): key = lists[i] j = i - 1 while j \u0026gt;= 0: if lists[j] \u0026gt; key: lists[j + 1] = lists[j] lists[j] = key j -= 1 return lists 上述算法逻辑是假设队列中第一个元素处于第一部分，然后逐个将2至n个元素插入到第一部分。\n 希尔排序  希尔排序(Shell Sort)是插入排序的一种。也称缩小增量排序，是直接插入排序算法的一种更高效的改进版本。希尔排序是非稳定排序算法。该方法因DL．Shell于1959年提出而得名。\n希尔排序是把记录按下标的一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止。时间复杂度跟步长序列有很大关系，步长序列为n/2^i时时间复杂度最坏的情况是O(n^2) 。是不稳定的排序方法。\n代码实现：\ndef shell_sort(lists): # 希尔排序 count = len(lists) step = 2 group = count / step while group \u0026gt; 0: for i in range(0, group): j = i + group while j \u0026lt; count: k = j - group key = lists[j] while k \u0026gt;= 0: if lists[k] \u0026gt; key: lists[k + group] = lists[k] lists[k] = key k -= group j += group group /= step return lists 希尔排序中如果group变量为1，则就与插入排序是一样的逻辑了，所以它是插入排序的一种改进。把group变量从count/2递减至1是为了在前面n-1次迭代时将记录变得基本有序，以避免插入排序时过多地交换元素位置\n 冒泡排序  它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。时间复杂度最好的情况为O(n),最坏的情况是O(n^2) 。是稳定的排序方法。\n代码实现：\ndef bubble_sort(lists): # 冒泡排序 count = len(lists) for i in range(0, count): for j in range(i + 1, count): if lists[i] \u0026gt; lists[j]: temp = lists[j] lists[j] = lists[i] lists[i] = temp return lists 最简单的排序算法了，不解释\n 快速排序  通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。时间复杂度最好的情况为O(nlogn),最坏的情况是O(n^2) 。是不稳定的排序方法。\n代码实现：\ndef quick_sort(lists, left, right): # 快速排序 if left \u0026gt;= right: return lists key = lists[left] low = left high = right while left \u0026lt; right: while left \u0026lt; right and lists[right] \u0026gt;= key: right -= 1 lists[left] = lists[right] while left \u0026lt; right and lists[left] \u0026lt;= key: left += 1 lists[right] = lists[left] lists[right] = key quick_sort(lists, low, left - 1) quick_sort(lists, left + 1, high) return lists 大学里最常考的排序算法了，不解释\n 直接选择排序  基本思想：第1趟，在待排序记录r[1] ~ r[n]中选出最小的记录，将它与r[1]交换；第2趟，在待排序记录r[2] ~ r[n]中选出最小的记录，将它与r[2]交换；以此类推，第i趟在待排序记录r[i] ~ r[n]中选出最小的记录，将它与r[i]交换，使有序序列不断增长直到全部排序完毕。时间复杂度是O(n^2) 。是稳定的排序方法。\n算法实现：\ndef select_sort(lists): # 选择排序 count = len(lists) for i in range(0, count): min = i for j in range(i + 1, count): if lists[min] \u0026gt; lists[j]: min = j temp = lists[min] lists[min] = lists[i] lists[i] = temp return lists 逻辑很简单，不解释\n 堆排序  堆排序(Heapsort)是指利用堆积树（堆）这种数据结构所设计的一种排序算法，它是选择排序的一种。可以利用数组的特点快速定位指定索引的元素。堆分为大根堆和小根堆，是完全二叉树。大根堆的要求是每个节点的值都不大于其父节点的值，即A[PARENT[i]] \u0026gt;= A[i]。在数组的非降序排序中，需要使用的就是大根堆，因为根据大根堆的要求可知，最大的值一定在堆顶。时间复杂度为O(nlogn) 。是不稳定的排序方法。\n代码实现：\n# 调整堆 def adjust_heap(lists, i, size): lchild = 2 * i + 1 rchild = 2 * i + 2 max = i if i \u0026lt; size / 2: if lchild \u0026lt; size and lists[lchild] \u0026gt; lists[max]: max = lchild if rchild \u0026lt; size and lists[rchild] \u0026gt; lists[max]: max = rchild if max != i: lists[max], lists[i] = lists[i], lists[max] adjust_heap(lists, max, size) # 创建堆 def build_heap(lists, size): for i in range(0, (size/2))[::-1]: adjust_heap(lists, i, size) # 堆排序 def heap_sort(lists): size = len(lists) build_heap(lists, size) for i in range(0, size)[::-1]: lists[0], lists[i] = lists[i], lists[0] adjust_heap(lists, 0, i) 上述代码首先将lists调整为大根堆，其中lists[0]即为这个堆里最大的值。然后依次取掉堆的顶层元素，放置到队列中合适的位置，然后再调整堆为大根堆。\n 归并排序  归并排序是建立在归并操作上的一种有效的排序算法,该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为二路归并。\n归并过程为：比较a[i]和a[j]的大小，若a[i]≤a[j]，则将第一个有序表中的元素a[i]复制到r[k]中，并令i和k分别加上1；否则将第二个有序表中的元素a[j]复制到r[k]中，并令j和k分别加上1，如此循环下去，直到其中一个有序表取完，然后再将另一个有序表中剩余的元素复制到r中从下标k到下标t的单元。归并排序的算法我们通常用递归实现，先把待排序区间[s,t]以中点二分，接着把左边子区间排序，再把右边子区间排序，最后把左区间和右区间用一次归并操作合并成有序的区间[s,t]。时间复杂度为O(nlogn) 。是稳定的排序方法。\n代码实现：\ndef merge(left, right): i, j = 0, 0 result = [] while i \u0026lt; len(left) and j \u0026lt; len(right): if left[i] \u0026lt;= right[j]: result.append(left[i]) i += 1 else: result.append(right[j]) j += 1 result += left[i:] result += right[j:] return result def merge_sort(lists): # 归并排序 if len(lists) \u0026lt;= 1: return lists num = len(lists) / 2 left = merge_sort(lists[:num]) right = merge_sort(lists[num:]) return merge(left, right) 每次都将待排序的队列分为left、right两部分，使用递归方法使这两部分有序之后，再使用merge方法将这两部分合并起来\n 基数排序  基数排序（radix sort）属于“分配式排序”（distribution sort），又称“桶子法”（bucket sort）或bin sort，顾名思义，它是透过键值的部份资讯，将要排序的元素分配至某些“桶”中，藉以达到排序的作用，在某些时候，基数排序法的效率高于其它的稳定性排序法。时间复杂度是O(k*n)，其中k=logB(N)，N为待排数据类型全集数字的个数。举个例子如果待排数据类型为4个byte位的int，基本B设置为16，则k=log16(2^32)等于8。是稳定的排序方法。\n代码实现：\nimport math def radix_sort(lists, radix=16): k = int(math.ceil(math.log(max(lists), radix))) bucket = [[] for i in range(radix)] for i in range(1, k+1): for j in lists: bucket[j/(radix**(i-1)) % (radix**i)].append(j) del lists[:] for z in bucket: lists += z del z[:] return lists 考虑到基数排序基本不进行比较操作，仅仅进行位操作就可以把元素分配至桶中。所以基数排序一般要快过基于比较的排序，比如快速排序。\n另外排序算法里经常会谈到算法的稳定性，这里特意说明一下：\n 假定在待排序的记录序列中，存在多个具有相同的关键字的记录，若经过排序，这些记录的相对次序保持不变，即在原序列中，ri=rj，且ri在rj之前，而在排序后的序列中，ri仍在rj之前，则称这种排序算法是稳定的；否则称为不稳定的。\n 总结 如果希望时间复杂度最小，不关心是否稳定，应该选择堆排序或归并排序。两者时间复杂度在最好或最坏情况下都是O(nlogn)，但归并排序由于使用了递归，占用的内存较大，所以还是应该选择堆排序。python里也有heapq模块可用。\n如果待排序的元素是整数，并待排序的元素个数较大，也可以选择基数排序。\n如果很关心稳定性，可选择冒泡排序、选择排序、直接插入排序、归并排序。但考虑到占用内存问题，应该选择性能相对较好一点的直接插入排序。\n","permalink":"https://jeremyxu2010.github.io/2016/08/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95python%E5%AE%9E%E7%8E%B0/","tags":["python","排序算法"],"title":"排序算法python实现"},{"categories":["python开发"],"contents":"作为一个有多年Java开发经验的老程序员，最近却被python精练的语法迷住了。同时发现python语言在云计算、系统运维领域确实用得很多，感觉有必要把python这门语言学一学。以下记录了学习过程中的关键点以备忘。\n语言关键点  数字特殊计算符号  print 4**3 print 17//3.0  复数支持  print 3+4j print (3+4j).real print (3+4j).imag print abs(3+4j)  字符串  print \u0026#39;abcdef\u0026#39; print r\u0026#39;abc\\ndef\u0026#39; print \u0026#39;\u0026#39;\u0026#39;abcdefghi\u0026#39;\u0026#39;\u0026#39; print \u0026#39;abc\\ def\\ ghi\u0026#39; \u0026#39;abc\u0026#39;+\u0026#39;def\u0026#39; \u0026#39;abc\u0026#39;*3  unicode与str之间相互转换  print u\u0026#39;中国\u0026#39; unicodestr=u\u0026#39;中国\u0026#39; utf8str=unicodestr.encode(\u0026#39;utf-8\u0026#39;) unicodestr2=utf8str.decode(\u0026#39;utf-8\u0026#39;)  list与slice  list1=[1, 2, 3, 4] print list1[3] print list1[1:] print list1[:3] print list1[::2]  流程控制  if a\u0026lt;b: print a elif a==b: print a else: print b list1=[1, 2, 3, 4] for num in list1: print num for idx in range(len(list1)): print idx for (idx, num) in enumerate(list1): print idx, num for num in list1: if num==3: break for num in list1: if num==3: continue print num while running: print \u0026#39;alive\u0026#39; if 3==3: pass  函数相关  def fun1(a, b): print a+b def fun2(a, b=4): print a+b def cheeseshop(kind, *arguments, **keywords): pass cheeseshop(\u0026#34;Limburger\u0026#34;, \u0026#34;It\u0026#39;s very runny, sir.\u0026#34;, \u0026#34;It\u0026#39;s really very, VERY runny, sir.\u0026#34;, shopkeeper=\u0026#39;Michael Palin\u0026#39;, client=\u0026#34;John Cleese\u0026#34;, sketch=\u0026#34;Cheese Shop Sketch\u0026#34;) pairs = [(1, \u0026#39;one\u0026#39;), (2, \u0026#39;two\u0026#39;), (3, \u0026#39;three\u0026#39;), (4, \u0026#39;four\u0026#39;)] pairs.sort(key=lambda pair: pair[1]) def fun3(a, b) \u0026#39;\u0026#39;\u0026#39;Calculate a+bComment here\u0026#39;\u0026#39;\u0026#39; pass  list作为堆栈  stack=[3, 4, 5] stack.append(6) print stack.pop()  队列  from collections import deque queue = deque([3, 4, 5]) queue.append(6) queue.popleft()  filter, map, reduce  filter(lambda x: x%3==0 or x%5==0, range(1, 100)) map(lambda x: x*2, range(1, 10)) reduce(lambda x, y: x+y, range(1, 10))  列表推导  [num*2 for num in range(1, 10) if num%4!=0]  删除变量、删除list子项  a=3 b=a del a list1=[1,2,3,4,5] del list1[3] del list1[:]  tuple元组  t = 12345, 54321, \u0026#39;hello!\u0026#39; a, b=b,a  set集合  fruit = set([\u0026#39;apple\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;apple\u0026#39;, \u0026#39;pear\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;banana\u0026#39;]) print \u0026#39;orange\u0026#39; in fruit a={\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;} a = set(\u0026#39;abracadabra\u0026#39;) b = set(\u0026#39;alacazam\u0026#39;) print a-b print a|b print a\u0026amp;b print a^b  set集合推导式  print {x for x in \u0026#39;abracadabra\u0026#39; if x not in \u0026#39;abc\u0026#39;}  dict字典  tel = {\u0026#39;jack\u0026#39;: 4098, \u0026#39;sape\u0026#39;: 4139} dict([(\u0026#39;sape\u0026#39;, 4139), (\u0026#39;guido\u0026#39;, 4127), (\u0026#39;jack\u0026#39;, 4098)]) dict(sape=4139, guido=4127, jack=4098)  dict字典推导式  {x: x**2 for x in (2, 4, 6)}  模块  print __name__ import sys print sys.path print sys.argv  格式化输出  print \u0026#39;%d+ %d= %d\u0026#39; % (3, 4, 5) //不建议使用这种旧的语法 print \u0026#39;{} + {} = {}\u0026#39;.format(3, 4, 5) print \u0026#39;{0} + {1} = {2}\u0026#39;.format(3, 4, 5) print \u0026#39;{num1} + {num2} = {num3}\u0026#39;.format(num1=3, num2=4, num3=5) print \u0026#39;{num1:.2f} + {num2:.2f} = {num3:.2f}\u0026#39;.format(num1=3, num2=4, num3=5) print \u0026#39;{num1:4d} + {num2:4d} = {num3:4d}\u0026#39;.format(num1=3, num2=4, num3=5) from string import Template print Template(\u0026#39;$who likes $what\u0026#39;).substitute(who=\u0026#39;tim\u0026#39;, what=\u0026#39;kung pao\u0026#39;) tel = {\u0026#39;jack\u0026#39;: 4098, \u0026#39;sape\u0026#39;: 4139} print json.dumps(tel, indent=2)  文件读写  f=open(\u0026#39;/somewhere/filename\u0026#39;, \u0026#39;w\u0026#39;) f.read() for line in f: print line, f.close()  json处理  import json f=open(\u0026#39;/somewhere/filename\u0026#39;, \u0026#39;w\u0026#39;) tel = {\u0026#39;jack\u0026#39;: 4098, \u0026#39;sape\u0026#39;: 4139} json.dumps(tel, f) f.close() f=open(\u0026#39;/somewhere/filename\u0026#39;, \u0026#39;r\u0026#39;) x=json.load(f)  异常  try: pass except ValueError as e: pass finally: pass def fun1(): raise ValueError(\u0026#39;something\u0026#39;) class MyError(Exception): def __init__(self, value): self.value = value def __str__(self): return repr(self.value)  with语法  with open(\u0026#39;/somewhere/filename\u0026#39;, r) as f: for line in f: print line,  类的定义  class MyClass(BaseClass): pubPro=None def __init__(self, arg1): BaseClass.__init__(self) self.arg1 = def pubFun1(self, arg1, arg2): pass def __priFun1(self, arg1): pass cls=MyClass(\u0026#39;hello\u0026#39;) print type(cls) print cls.__class__  生成器  def reverse(data): for index in range(len(data)-1, -1, -1): yield data[index] for char in reverse(\u0026#39;golf\u0026#39;): print char  标准库  import os print os.getcwd() os.chdir(\u0026#39;/somewhere\u0026#39;) os.system(\u0026#39;ping -c 4 127.0.0.1\u0026#39;) import shutil shutil.copyfile(\u0026#39;/somewhere/filename\u0026#39;, \u0026#39;/anotherwhere/filename\u0026#39;) import glob print glob.glob(\u0026#39;*.py\u0026#39;) import sys sys.exit(0) sys.stderr.write(\u0026#39;err msg\u0026#39;) sys.stdout.write(\u0026#39;output msg\u0026#39;) import re print re.findall(r\u0026#39;([a-z]+)\u0026#39;, \u0026#39;which foot or hand fell fastest\u0026#39;) import math math.cos(math.pi / 4.0) math.log(1024, 2) import random random.choise([\u0026#39;apple\u0026#39;, \u0026#39;pear\u0026#39;, \u0026#39;banana\u0026#39;]) random.sample(xrange(100), 10) random.random() random.randrange(6) import urllib2 f=urllib2.urlopen(\u0026#39;http://www.baidu.com\u0026#39;) for line in f: print line, f.close() import smtplib smtpConn = smtplib.SMTP(\u0026#39;stmp.qq.com\u0026#39;) smtpConn.login(\u0026#39;username\u0026#39;, \u0026#39;password\u0026#39;) smtpConn.sendmail(\u0026#39;fromuser@qq.com\u0026#39;, \u0026#39;touser@qq.com\u0026#39;,\\ \u0026#39;\u0026#39;\u0026#39;some long text\u0026#39;\u0026#39;\u0026#39;) smtpConn.quit() from datetime import date now = date.today() time1 = date(2016, 8, 15) print now.strftime(\u0026#34;%m-%d-%y. %d%b %Y is a %A on the %dday of %B.\u0026#34;) range1 = now - time1 print range1.days import zlib s = b\u0026#39;witch which has which witches wrist watch\u0026#39; t = zlib.compress(s) zlib.decompress(t) import locale locale.setlocale(locale.LC_ALL, locale=\u0026#39;zh_CN.UTF-8\u0026#39;) locale.getlocale() locale.getdefaultlocale() import logging logging.debug(\u0026#39;Debugging information\u0026#39;) logging.info(\u0026#39;Informational message\u0026#39;) logging.warning(\u0026#39;Warning:config file %snot found\u0026#39;, \u0026#39;server.conf\u0026#39;) logging.error(\u0026#39;Error occurred\u0026#39;) logging.critical(\u0026#39;Critical error -- shutting down\u0026#39;) 练手 使用编程语言完成一个简单的任务是学习某个语言最快捷的办法，于是我想了一个简单任务：通过抓取页面自动从ishadowsocks.com上得到一个shadowsocks服务器的连接信息，并更新本机shadowsocks-libev服务的配置文件，再自动重启shadowsocks-libev服务。如果再配合cron定时执行脚本，基本可以做到免费的翻墙方案。源代码如下：\n#!/usr/bin/env python # coding: utf-8 import urllib import re from lxml import etree import json import os # 请求www.ishadowsocks.org服务器, 获取shadowsocks服务器信息 content = \u0026#39;\u0026#39; f = None try: f = urllib.urlopen(\u0026#39;http://www.ishadowsocks.org/\u0026#39;) content = f.read() finally: if f is not None: f.close() tree = etree.HTML(content) nodes = tree.xpath(\u0026#39;//*[@id=\u0026#34;free\u0026#34;]/div/div[2]/div[3]/h4\u0026#39;) addr = \u0026#39;\u0026#39; port = 0 pwd = \u0026#39;\u0026#39; method = \u0026#39;\u0026#39; for node in nodes: txt = node.text if type(txt) is unicode: utf8str = txt.encode(\u0026#39;utf-8\u0026#39;) ret = re.findall(r\u0026#39;^.*服务器地址:(.*)$\u0026#39;, utf8str) if len(ret) \u0026gt; 0: addr = ret[0] ret = re.findall(r\u0026#39;^端口:(.*)$\u0026#39;, utf8str) if len(ret) \u0026gt; 0: port = int(ret[0]) ret = re.findall(r\u0026#39;^.*密码:(.*)$\u0026#39;, utf8str) if len(ret) \u0026gt; 0: pwd = ret[0] ret = re.findall(r\u0026#39;^加密方式:(.*)$\u0026#39;, utf8str) if len(ret) \u0026gt; 0: method = ret[0].lower() # 生成shadowsocks-libev配置文件 config = { \u0026#34;server\u0026#34;: addr, \u0026#34;server_port\u0026#34;: port, \u0026#34;local_port\u0026#34;: 1080, \u0026#34;password\u0026#34;: pwd, \u0026#34;timeout\u0026#34;: 600, \u0026#34;method\u0026#34;: method } configstr = json.dumps(config, indent=2) with open(\u0026#39;/tmp/shadowsocks-libev.json\u0026#39;, \u0026#39;w\u0026#39;) as f: f.write(configstr) f.write(\u0026#39;\\n\u0026#39;) # 重启shadowsocks-libev服务 os.system(\u0026#39;launchctl unload ~/Library/LaunchAgents/homebrew.mxcl.shadowsocks-libev.plist\u0026#39;) os.system(\u0026#39;launchctl load ~/Library/LaunchAgents/homebrew.mxcl.shadowsocks-libev.plist\u0026#39;) 下一步计划 在kindle上买了本python cookbook，下一步计划把这本书先看完。\n","permalink":"https://jeremyxu2010.github.io/2016/08/python%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%E5%A4%87%E5%BF%98/","tags":["python","shadowsocks"],"title":"python语言学习备忘"},{"categories":["云计算"],"contents":"ceph作为新一代的PB级高可靠性分布存储系统已经流行很长时间了，在研究了glusterfs之后，一直想找个机会研究一下它，这周终于抽出来时间了。\n概念 相对于其它分布式存储系统，它开创性地用统一的系统提供了对象、块、和文件存储功能，它可靠性高、管理简便、并且是自由软件。Ceph提供了一个可无限伸缩的Ceph存储集群，它基于RADOSA Scalable, Reliable Storage Service for Petabyte-scale Storage Clusters\n从上面Ceph的架构图可以看到，底层都是RADOS，通过不同的Client（RADOSGW、RBD、CEPHFS）让应用上层识别为一个对象存储系统、块设备、Posix兼容的文件，另外LIBRADOS也使得各种开发语言都可以操作RADOS。\nCeph存储集群包含两种类型的守护进程: Ceph监视器、Ceph OSD守护进程。\nCeph监视器维护着集群运行图的主副本。一个监视器集群确保了当某个监视器失效时的高可用性。存储集群客户端向Ceph监视器索取集群运行图的最新副本。\nCeph OSD守护进程检查自身状态、以及其它OSD的状态，并报告给监视器们。同时Ceph OSD守护进程负责将数据存储为扁平的对象。\nCeph客户端和OSD守护进程都用CRUSH算法来计算对象的位置信息，而不是依赖于一个中心化的查询表。与以往方法相比，CRUSH的数据管理机制更好，它很干脆地把工作分配给集群内的所有客户端和OSD来处理，因此具有极大的伸缩性。CRUSH用智能数据复制确保弹性，更能适应超大规模存储。\nCeph存储系统有存储池的概念，它是存储对象的逻辑分区。每个存储池里都有很多归置组PG（Placement Group），CRUSH算法动态地将PG映射到实际的OSD。\nCeph客户端要进行IO读写操作流程如下:\n Ceph客户端负责把展现给用户的数据格式（一块设备映像、 REST 风格对象、 CephFS 文件系统目录）转换为可存储于 Ceph 存储集群的对象 Ceph客户端先连接到某个 Ceph 监视器、获得最新的集群运行图副本 Ceph客户端根据对象的ID及存储池的ID计算得出目标PG的ID Ceph客户端得到目标PG的主OSD地址，连接该OSD进行读写操作  从上面的流程可以看出与glusterfs相比，存储池与OSD之间有PG这么一个中间层。这个中间层使客户端与OSD之间松耦合了，从客户端的角色来看，它只知道对象被存储在某个PG里了，至于对象最终要存储在哪个OSD里它是不感知的。这样当新的OSD上线时，Ceph可以更方便地进行重均衡。\n随着OSD数量的增加，一个存储池PG的数量设置将非常重要，它显著地影响着集群的行为、以及出错时的数据持久性（即灾难性事件导致数据丢失的概率）。官方还提供了一个工具pgcalc。\n另外CRUSH算法还支持根据数据存储位置来确定如何存储和检索对象。所以部署一个大规模数据集群的时候，应该好好设计自己的CRUSH图，因为它可以帮助管理 Ceph 集群、提升性能、和保证数据安全性。调整Ceph的CRUSH布局的方法见这里\n实操 我是在5台CentOS6上进行Ceph在安装的，部署节点的拓扑结构如下: 预检 所有节点更换软件源\n#使用阿里云的centos6软件源镜像 mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo #使用阿里云的epel软件源镜像 rpm --import http://mirrors.aliyun.com/epel/RPM-GPG-KEY-EPEL-6 wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-6.repo #使用阿里云的ceph软件源镜像 rpm --import https://download.ceph.com/keys/release.asc echo \u0026#34;[ceph] name=Ceph packages for $basearchbaseurl=http://mirrors.aliyun.com/ceph/rpm-el6/hammer/$basearchenabled=1 priority=2 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc [ceph-noarch] name=Ceph noarch packages baseurl=http://mirrors.aliyun.com/ceph/rpm-el6/hammer/noarch enabled=1 priority=2 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc [ceph-source] name=Ceph source packages baseurl=http://mirrors.aliyun.com/ceph/rpm-el6/hammer/SRPMS enabled=0 priority=2 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc \u0026#34; \u0026gt; /etc/yum.repos.d/Ceph.repo 所有节点设置NTP时间同步，可参考之前的`私有云数据中心NTP服务搭建` 所有节点上创建ceph操作用户 ```bash useradd -d /home/cephop -m cephop passwd cephop #确保cephop用户有sudo权限 同步hosts文件，确保各节点都可以正确解析节点名称\n确保admin-node可无密码SSH登录其它节点\n#在admin-node节点执行以下操作 su - cephop ssh-keygen ssh-copy-id cephop@node1 ssh-copy-id cephop@node2 ssh-copy-id cephop@node3 ssh-copy-id cephop@ceph-client echo \u0026#34; Host node1 Hostname node1 User cephop Host node2 Hostname node2 User cephop Host node3 Hostname node3 User cephop Host ceph-client Hostname ceph-client User cephop \u0026#34; \u0026gt; ~/.ssh/config 禁用ssh的requiretty特性\n设置网络接口开机自启动\n关闭防火墙\n关闭selinux\n管理节点安装ceph-deploy\nsudo yum install -y ceph-deploy 存储集群安装 mkdir my-cluster cd my-cluster ceph-deploy new node1 echo \u0026#34;osd pool default size = 2\u0026#34; \u0026gt;\u0026gt; ceph.conf ceph-deploy install admin-node node1 node2 node3 ceph-client ceph-deploy mon create-initial #在node2, node3上创建上创建osd目录 ssh node2 sudo mkdir /var/local/osd0 exit ssh node3 sudo mkdir /var/local/osd1 exit ceph-deploy osd prepare node2:/var/local/osd0 node3:/var/local/osd1 ceph-deploy osd activate node2:/var/local/osd0 node3:/var/local/osd1 ceph-deploy admin admin-node node1 node2 node3 sudo chmod +r /etc/ceph/ceph.client.admin.keyring ceph health 块设备客户端安装 在管理节点执行下面的命令\nceph-deploy install ceph-client ceph-deploy admin ceph-client CentOS6需升级内核才能有brd内核模块，所以在ceph-client上执行下面的命令\nrpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org rpm -ivh http://www.elrepo.org/elrepo-release-6-6.el6.elrepo.noarch.rpm yum --enablerepo=elrepo-kernel install -y kernel-lt #修改/boot/grub/menu.lst确保启动后进入新的内核 reboot #创建块设备映像 rbd create rbd/foo --size 4096 #将块设备映像映射为块设备 sudo rbd map rbd/foo --name client.admin #格式化块设备 sudo mkfs.ext4 -m0 /dev/rbd1 #后面就可以使用这个块设备了，比如将/dev/rbd1挂载至目录等操作 其它Ceph客户端安装方法类似\n总结  Ceph概念比较多，内部实现细节有很多精彩的地方，使用前务必要把它的体系结构这一章通读一遍。 Ceph推荐的网络方案是区分了公共网与集群网的，这点比glusterfs进步不少。见这里。 Ceph里可以自定义CRUSH图，这个比glusterfs只能以地理位置分布进行复制的方案还是灵活不少。  参考 http://docs.ceph.org.cn/architecture/ http://docs.ceph.org.cn/start/ http://blog.dnsbed.com/archives/1714\n","permalink":"https://jeremyxu2010.github.io/2016/08/%E5%88%9D%E8%AF%86ceph/","tags":["ceph","cloud","storage"],"title":"初识ceph"},{"categories":["云计算"],"contents":"搭建私有云时需要制作一些操作系统的基础镜像，这里也有一些持巧，在这里记录下来以备忘。\n安装CentOS6操作系统 这里没有太多好说的，我是从这里下载最小安装ISO进行安装的，安装的硬盘大小为20G。安装时大部分选项都是默认的，只有分区采用了自定义分区方案，200M的boot分区，其它全部作为根分区。如果需要交换分区，以后可以使用文件分区，使用文件分区的操作方法如下：\ndd if=/dev/zero of=/swapfile bs=1G count=2 chmod 600 /swapfile mkswap -f /swapfile swapon /swapfile echo \u0026#34;/swapfile none swap defaults 0 0\u0026#34; \u0026gt;\u0026gt; /etc/fstab mount -a 操作系统的一些基础设置 安装好操作系统后，使用root帐户登入系统做一些基础设置\n#关闭selinux setenforce 0 sed -i -e \u0026#39;s/^SELINUX=.*$/SELINUX=disabled/\u0026#39; /etc/sysconfig/selinux #由于公司没有搭建centos的私有源，这里我都换用aliyun的centos源 rpm --import http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-6 mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo rpm --import http://mirrors.aliyun.com/epel/RPM-GPG-KEY-EPEL-6 wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-6.repo #安装平时经常用到的vim wget等软件 yum install -y vim wget #配置ntp服务，指向私有云中的时间服务器 yum install -y ntp echo \u0026#34;#在与上级时间服务器联系时所花费的时间，记录在driftfile参数后面的文件 driftfile /var/lib/ntp/drift #默认关闭所有的 NTP 联机服务 restrict default ignore restrict -6 default ignore #如从loopback网口请求，则允许NTP的所有操作 restrict 127.0.0.1 restrict -6 ::1 #使用指定的时间服务器 server ${inner_ntp_server_ip}#允许指定的时间服务器查询本时间服务器的信息 restrict ${inner_ntp_server_ip}nomodify notrap nopeer noquery #其它认证信息 includefile /etc/ntp/crypto/pw keys /etc/ntp/keys \u0026#34; \u0026gt; /etc/ntp.conf service ntpd start chkconfig ntpd on #禁用sshd服务的UseDNS、GSSAPIAuthentication两项特性 sed -i -e \u0026#39;s/^#UseDNS.*$/UseDNS no/\u0026#39; /etc/ssh/sshd_config sed -e \u0026#39;s/^GSSAPIAuthentication.*$/GSSAPIAuthentication no/\u0026#39; /etc/ssh/sshd_config #升级系统 yum update -y \u0026amp;\u0026amp; yum clean all #更改系统网络接口配置文件，设置该网络接口随系统启动而开启 sed -i -e \u0026#39;/^HWADDR=/d\u0026#39; -e \u0026#39;/^UUID=/d\u0026#39; /etc/sysconfig/network-scripts/ifcfg-eth0 sed -i -e \u0026#39;s/^ONBOOT.*$/ONBOOT=yes/\u0026#39; /etc/sysconfig/network-scripts/ifcfg-eth0 sed -i -e \u0026#39;s/^NM_CONTROLLED.*$/NM_CONTROLLED=no/\u0026#39; /etc/sysconfig/network-scripts/ifcfg-eth0 #删除已存在的网络接口udev规则定义 sed -i -e \u0026#39;/PCI device/d\u0026#39; -e \u0026#39;/^SUBSYSTEM/d\u0026#39; /etc/udev/rules.d/70-persistent-net.rules #关闭系统 halt 最近精简一下生成的镜像文件\nqemu-img convert -f qcow2 -O qcow2 centos6.img centos6_c.img mv centos6_c.img centos6.img 对镜像文件预处理 使用虚拟机平台基于上述centos6基础镜像文件创建虚拟机后，在虚拟机启动前需对镜像文件进行预处理，我这里写个脚本处理这件事\n执行脚本前需安装libguestfs-tools\nyum install -y libguestfs-tools #以后可能还要修改windows镜像文件，顺便把libguestfs-winsupport也安装一下 yum install -y libguestfs-winsupport preprocess_img.sh脚本内容：\n#!/bin/bash domain_name=$1 ip_fetch_method=$2 static_ip=$3 static_netmask=$4 static_gateway=$5 static_dns1=$6 static_dns2=$7 TMP_CONFIG_DIR=\u0026quot;$(mktemp -d /tmp/$$_config_XXXX)\u0026quot; trap \u0026quot;[ -d \u0026quot;$TMP_CONFIG_DIR\u0026quot; ] \u0026amp;\u0026amp; rm -rf $TMP_CONFIG_DIR\u0026quot; HUP INT QUIT TERM EXIT echo \u0026quot; NETWORKING=yes HOSTNAME=$domain_name \u0026quot; \u0026gt; $TMP_CONFIG_DIR/network virt-copy-in -d $domain_name $TMP_CONFIG_DIR/network /etc/sysconfig if [ $ip_fetch_method == \u0026quot;static\u0026quot; ] ; then echo \u0026quot; DEVICE=eth0 TYPE=Ethernet ONBOOT=yes NM_CONTROLLED=no BOOTPROTO=static IPADDR=$static_ip NETMASK=$static_netmask GATEWAY=$static_gateway DNS1=$static_dns1 DNS2=$static_dns2 \u0026quot; \u0026gt; $TMP_CONFIG_DIR/ifcfg-eth0 virt-copy-in -d $domain_name $TMP_CONFIG_DIR/ifcfg-eth0 /etc/sysconfig/network-scripts fi 执行脚本\n# 设置主机名为test，且eth0网络接口动态获取IP地址 ./preprocess_img.sh test dhcp # 设置主机名为test，且eth0网络接口配置静态IP地址 ./preprocess_img.sh test static 188.188.100.137 255.255.255.0 188.188.100.1 202.96.134.133 202.96.128.86 镜像文件预处理完毕后，再启动虚拟机，可以看到虚拟机的主机名及IP地址均已设置OK\n待改进的地方  目前的preprocess_img.sh脚本还比较原始，只能处理centos6操作系统，接下来会对这个脚本进行加强，以支持其它linux操作系统及windows系统 要是能扩展虚拟化管理平台WebVirtMgr，能在首次启动时执行指定的脚本对镜像进行预处理就好了。可惜我对python不是太熟。  参考  深度实践KVM/第16章 虚拟机镜像制作、配置与测试/16.2 Linux镜像制作方法 http://www.361way.com/kvm-libguestfs-tools/3175.html  ","permalink":"https://jeremyxu2010.github.io/2016/08/%E5%88%B6%E4%BD%9Ccentos6%E5%9F%BA%E7%A1%80%E9%95%9C%E5%83%8F/","tags":["kvm","linux"],"title":"制作CentOS6基础镜像"},{"categories":["云计算"],"contents":"最近一段时间一直在试用各种KVM虚拟化管理平台，主要试用了ovirt、openstack、WebVirtMgr。最后发现针对我目前的工作场景(不超过10台物理机)，WebVirtMgr是最适合的场景，这里将WebVirtMgr的安装部署简单写下来备忘。\n安装 安装WebVirtMgr 我是在CentOS6上进行安装，官方给出的文档还是比较详细的，照做就可以了。\n# 启用epel的源，我一般是使用阿里云的centos源及epel源，见`http://mirrors.aliyun.com/help/centos`， `http://mirrors.aliyun.com/help/epel` # 这里跟官方文档有一点点不一样，不要安装epel源里的supervisor，那个太老了，另外多安装了novnc包，这个后面通过网页连接虚拟机的控制台要用到 yum -y install git python-pip libvirt-python libxml2-python python-websockify nginx novnc # 安装较新版本的supervisor wget -O python-supervisor-3.0-3.noarch.rpm https://packagecloud.io/haf/oss/packages/el/6/python-supervisor-3.0-3.noarch.rpm/download yum localinstall -y python-supervisor-3.0-3.noarch.rpm mkdir -p /srv/www/ cd /srv/www/ git clone git://github.com/retspen/webvirtmgr.git cd webvirtmgr pip install -r requirements.txt # 这里会提示让创建一个登录用户，按照提示创建就可以了，以后可以执行`./manage.py createsuperuser`再创建其它登录用户 ./manage.py syncdb ./manage.py collectstatic #将该目录的拥有者修改为nginx用户 chown -R nginx:nginx /var/www/webvirtmgr #增加webvirtmgr的nginx配置 echo \u0026#34;server { listen 80 default_server; server_name _; #access_log /var/log/nginx/webvirtmgr_access_log; location /static/ { root /srv/www/webvirtmgr/webvirtmgr; # or /srv instead of /var expires max; } location / { proxy_pass http://127.0.0.1:8000; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-for $proxy_add_x_forwarded_for; proxy_set_header Host $host:$server_port; proxy_set_header X-Forwarded-Proto $scheme; proxy_connect_timeout 600; proxy_read_timeout 600; proxy_send_timeout 600; client_max_body_size 1024M; # Set higher depending on your needs } } \u0026#34;\u0026gt; /etc/nginx/conf.d/webvirtmgr.conf # 注释掉nginx原有的默认主机配置 sed -i -e \u0026#34;s/^/#/g\u0026#34; /etc/nginx/conf.d/default.conf # 增加webvirtmgr的supervisor配置 echo \u0026#34; command=/usr/bin/python /srv/www/webvirtmgr/manage.py run_gunicorn -c /srv/www/webvirtmgr/conf/gunicorn.conf.py directory=/srv/www/webvirtmgr autostart=true autorestart=true logfile=/var/log/supervisor/webvirtmgr.log log_stderr=true user=nginx [program:webvirtmgr-console] command=/usr/bin/python /srv/www/webvirtmgr/console/webvirtmgr-console directory=/srv/www/webvirtmgr autostart=true autorestart=true stdout_logfile=/var/log/supervisor/webvirtmgr-console.log redirect_stderr=true user=nginx \u0026#34; \u0026gt; /etc/supervisor/conf.d/webvirtmgr.conf # 设置nginx、supervisord开机自启动并启动它们 chkconfig nginx on service nginx start chkconfig supervisord on service supervisord start 安装被管物理机 被管物理机的操作系统同样是CentOS6，安装过程也是很简单。\ncurl http://retspen.github.io/libvirt-bootstrap.sh | sh service messagebus restart service libvirtd restart 将被管物理机接入WebVirtMgr 我这里是使用ssh接入的方式\n先在被管物理机上创建普通可执行libvirt相关命令的用户\nadduser webvirtmgr passwd webvirtmgr usermod -G qemu,kvm -a webvirtmgr echo \u0026#34;[Remote libvirt SSH access] Identity=unix-user:webvirtmgr Action=org.libvirt.unix.manage ResultAny=yes ResultInactive=yes ResultActive=yes \u0026#34; \u0026gt; /etc/polkit-1/localauthority/50-local.d/50-libvirt-remote-access.pkla 再在安装WebVirtMgr所在主机上做好密钥登录\n#切换至nginx用户 su - nginx -s /bin/bash #生成nginx用户默认的ssh登录密钥文件 ssh-keygen touch ~/.ssh/config \u0026amp;\u0026amp; echo -e \u0026#34;StrictHostKeyChecking=no\\nUserKnownHostsFile=/dev/null\u0026#34; \u0026gt;\u0026gt; ~/.ssh/config chmod 0600 ~/.ssh/config #设置nginx使用webvirtmgr无密码登录至被管物理机 ssh-copy-id -P .ssh/id_rsa webvirtmgr@${libvirt_host_ip} #从nginx用户跳出 exit 使用 访问http://${webvirtmgr_host_ip}/servers/，首先会要求登录，用之前安装时创建的帐户登入系统。然后添加一个连接，将被管物理机接入WebVirtMgr，如下图所示，这里IP输入被管物理机IP，用户名输入webvirtmgr就可以了。\n然后进入这台物理机的界面，在左侧可以看到几大功能，分别是：虚机实例管理、存储池管理、网络池管理、网络接口管理、密钥管理、物理机概览，如图。\n网络接口管理 原来我都是手工创建桥接网络接口配置的，虽然不难，但确实比较麻烦。在这里发现可以在界面上完成，如图。\n后来调查了下，发现原来libvirt自带这个功能的，命令如下\niface-bridge \u0026lt;interface\u0026gt; \u0026lt;bridge\u0026gt; [--no-stp] [--delay \u0026lt;number\u0026gt;] [--no-start] 网络池管理 这里可以管理libvirt里的网络池，其实就是libvirt网络管理功能的UI展现，这里贴一下libvirt网络管理相关的命令\n net-autostart 自动开始网络   net-create 从一个 XML 文件创建一个网络 net-define 从一个 XML 文件定义(但不开始)一个网络 net-destroy destroy (stop) a network net-dumpxml XML 中的网络信息 net-edit 为网络编辑 XML 配置 net-info network information net-list 列出网络 net-name 把一个网络UUID 转换为网络名 net-start 开始一个(以前定义的)不活跃的网络 net-undefine 取消定义一个非活跃的网络 net-update update parts of an existing network's configuration net-uuid 把一个网络名转换为网络UUID  存储池管理 这里可以进行存储池的管理，支持的卷类型有五种：目录类型卷、LVM类型卷、Ceph类型卷、NETFS类型卷、iso镜像卷。\niso镜像卷一般是用来存放ISO镜像的。\n在单机上，存放虚拟机镜像文件一般是使用目录类型卷，有的虚拟机为了有比较好的硬盘IO性能会使用LVM类型卷。\n如果想将虚拟机镜像放到分布式存储或共享存储中，就会用到Ceph类型卷、NETFS类型卷。\n虚机实例管理 这里可以根据向导创建虚拟机，也可以根据预先创建好的模板镜像创建虚拟机、也可以根据预先创建好的配置模板创建虚拟机，还可以根据xml文件内容创建虚拟机。\n对虚拟机的管理，功能基本是覆盖virt-manager的功能，常用的功能都可以界面操作了，有些没有的功能也可以手工修改虚拟机xml配置文件实现，如图。\n待改进的地方  创建虚拟机后，默认的主机名、IP地址还得在虚机控制台设置，太麻烦，下一步尝试使用gusetfs的命令行工具，编写一个脚本对虚拟机的镜像文件进行预处理以解决这个问题。 KVM集群中虚拟机要做到实时迁移，必须配合集中存储，而且需要在每个物理机上将其配置为存储池。目前我所了解的廉价、可扩展性好的集中存储方案有Ceph与Glusterfs，下一步需要对比这两种方案，以找出最合适的方案 KVM集群中的虚拟机如果全部采用静态设置IP地址，管理查看虚机的IP地址将很麻烦；如果全部采用动态获取IP地址，则需要在网络内部安装dhcp服务器，然后可在dhcp服务器上管理查看虚机的IP地址。很显然后一种方案更合理一点，但这个还需要验证 webvirtmgr并不是像openstack一样的虚拟化一站式解决方案，它的工作原理其中就是通过一个web页面，将多台物理机接入进来，然后通过libvirt分别管理每个物理机上的计算资源。要创建虚拟机时，才是管理员自行找到一个合适的物理，然后在上面创建虚拟机。那么在KVM集群环境，存储已经通过Ceph或Glusterfs方案解决了，当要创建某个配置的虚拟机时，最好能有一个调度器，依据CPU、内存的需求，帮助管理员从众多物理机中选取一个合适的物理机。简单处理，也许可以写一个脚本，根据CPU、内存的需求自动得出一个物理机选取推荐列表，以供管理员参考 webvirtmgr所部署的主机需考虑高可用方案。简单处理可以将其做成docker镜像，一旦发现该服务故障了，可以快速地在其它地方启动起来  总结 webvirtmgr不是完美的，但目前在我来看，它是最适合我这种小型私有云建设的，后面我将在研发按这个方案推行下去。\n参考 https://github.com/retspen/webvirtmgr/wiki/Install-WebVirtMgr https://github.com/retspen/webvirtmgr/wiki/Setup-Host-Server https://github.com/retspen/webvirtmgr/wiki/Setup-SSH-Authorization\n","permalink":"https://jeremyxu2010.github.io/2016/08/%E8%AF%95%E7%94%A8webvirtmgr/","tags":["linux","kvm"],"title":"试用WebVirtMgr"},{"categories":["数据库开发"],"contents":"问题描述 mysql数据库有auto_increment这样一个特性，一般是用来设置Integer类型主键自增长。比如下面的代码：\n-- 刚创建表，该表没有AUTO_INCREMENT值 create table test( id int(11) primary key not null auto_increment, field1 varchar(40) not null default \u0026#39;\u0026#39; ) engine=InnoDB; show create table test\\G; ... Create Table: CREATE TABLE `test` ( `id` int(11) NOT NULL AUTO_INCREMENT, `field1` varchar(40) NOT NULL DEFAULT \u0026#39;\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 ... -- 插入两条数据后，可以看到该表的AUTO_INCREMENT变为3了 insert into test(field1) values(\u0026#39;test1\u0026#39;); insert into test(field1) values(\u0026#39;test2\u0026#39;); show create table test\\G; ... Create Table: CREATE TABLE `test` ( `id` int(11) NOT NULL AUTO_INCREMENT, `field1` varchar(40) NOT NULL DEFAULT \u0026#39;\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8mb4 ... -- 删除一条数据后，该表的AUTO_INCREMENT还是3 delete from test where field1=\u0026#39;test2\u0026#39;; show create table test\\G; ... Create Table: CREATE TABLE `test` ( `id` int(11) NOT NULL AUTO_INCREMENT, `field1` varchar(40) NOT NULL DEFAULT \u0026#39;\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8mb4 ... -- 再插入一条数据后，该表的AUTO_INCREMENT变为4 insert into test(field1) values(\u0026#39;test2\u0026#39;); show create table test\\G; ... Create Table: CREATE TABLE `test` ( `id` int(11) NOT NULL AUTO_INCREMENT, `field1` varchar(40) NOT NULL DEFAULT \u0026#39;\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb4 ... -- 删除一条数据后，该表的AUTO_INCREMENT还是4 delete from test where field1=\u0026#39;test2\u0026#39;; show create table test\\G; ... Create Table: CREATE TABLE `test` ( `id` int(11) NOT NULL AUTO_INCREMENT, `field1` varchar(40) NOT NULL DEFAULT \u0026#39;\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb4 ... -- /etc/init.d/mysql restart 重启后，该表又没有AUTO_INCREMENT值了 show create table test\\G; ... Create Table: CREATE TABLE `test` ( `id` int(11) NOT NULL AUTO_INCREMENT, `field1` varchar(40) NOT NULL DEFAULT \u0026#39;\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 ... -- 再插入一条数据后，这里本来预期该表的AUTO_INCREMENT应该是5的，但实际上却又变为3了 insert into test(field1) values(\u0026#39;test2\u0026#39;); show create table test\\G; ... Create Table: CREATE TABLE `test` ( `id` int(11) NOT NULL AUTO_INCREMENT, `field1` varchar(40) NOT NULL DEFAULT \u0026#39;\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8mb4 ... mysql的上述行为说明在mysql运行过程中InnoDB存储引擎的表，其AUTO_INCREMENT值会随着插入操作持续增长的，但mysql重启之后，AUTO_INCREMENT值并没有持久保存下来，重启后再插入数据，mysql会以表中最大的id+1作为当前的AUTO_INCREMENT值，新插入的数据的ID就变为这个了。\n在mysql的bug跟踪系统里，上述问题已经被很多人反映了，如链接1、链接2\nmysql上述行为本身也没有太大的问题，但如果业务系统将这种自增ID当成业务ID就存在问题了。比如在业务系统里创建了一个工单A，该工单对应的自增ID为1002，后来由于业务操作，删除了ID为1002的工单记录，然后系统维护时重启了mysql，后面业务系统里又创建了一个工单B，该工单对应的自增ID就有可能也为1002，然后再以1002为查询条件，就会查到两个不同工单对应的日志。\n当然本质上应避免用mysql的这种自增ID作为业务ID，而且应该使用自定义的业务ID生成器。\n很不幸，我们目前做的项目，在设计之初并没有考虑到这个问题，因此大量使用这种自增ID作为业务ID。\n另类解决方案 要从根源上解决这个问题，当然是使用自定义的业务ID来代替mysql的这种自增ID，但项目涉及的表非常多，基于这些表的数据访问方法也相当多，为了避免大规模修改业务代码，只能想办法规避这个问题。查阅mysql的问题跟踪系统，也没找到合理的解决方案。最后在一个讲触发器的帖子影响下想到一种另类解决方案，代码如下：\n#!/bin/bash  MYSQL_HOST=127.0.0.1 MYSQL_USER=root MYSQL_PWD=mysqlpwd MYSQL_DBNAME=mysqldb AUTOINCR_INDEXES_TABLE_NAME=autoincr_indexes AUTOINCR_INDEXES_TABLE_NAME_COLUMN_NAME=table_name AUTOINCR_INDEXES_INDEX_VALUE_COLUMN_NAME=index_value PROCEDURE_NAME=restore_table_indexes #需保证mysql用户对此文件可读 MYSQL_INIT_FILE=/var/call_procedure.sql # 1. 创建记录数据库里每个表的auto_increment值的表$AUTOINCR_INDEXES_TABLE_NAME mysql --batch -u$MYSQL_USER -p$MYSQL_PWD -h$MYSQL_HOST $MYSQL_DBNAME -e \u0026#34;DROP TABLE IF EXISTS $AUTOINCR_INDEXES_TABLE_NAME; CREATE TABLE $AUTOINCR_INDEXES_TABLE_NAME($AUTOINCR_INDEXES_TABLE_NAME_COLUMN_NAMEvarchar(40) PRIMARY KEY NOT NULL, $AUTOINCR_INDEXES_INDEX_VALUE_COLUMN_NAMEint(11) NOT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8;\u0026#34; # 2. 列出数据库里每个表的表名 TABLES=`mysql --batch -u$MYSQL_USER -p$MYSQL_PWD -h$MYSQL_HOST mysql -e \u0026#34;SELECT t.table_name FROM INFORMATION_SCHEMA.TABLES t WHERE t.table_schema = \u0026#39;$MYSQL_DBNAME\u0026#39;\u0026#34; | sed -n \u0026#39;1!p\u0026#39;` # 3. 针对有自增ID的表，为每个表创建一个自动更新$AUTOINCR_INDEXES_TABLE_NAME表中对应记录的触发器 TMP_CREATE_TRIGGER_FILE=\u0026#34;$(mktemp /tmp/$$_create_trigger_XXXX.sql)\u0026#34; trap \u0026#34;[ -f \u0026#34;$TMP_CREATE_TRIGGER_FILE\u0026#34;] \u0026amp;\u0026amp; rm -f $TMP_CREATE_TRIGGER_FILE\u0026#34; HUP INT QUIT TERM EXIT for T in ${TABLES[@]} ; do autoIncrIndexValue=`mysql --batch -u$MYSQL_USER -p$MYSQL_PWD -h$MYSQL_HOST mysql -e \u0026#34;SELECT AUTO_INCREMENT FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = \u0026#39;$MYSQL_DBNAME\u0026#39; AND TABLE_NAME = \u0026#39;$T\u0026#39;;\u0026#34; | sed -n \u0026#39;1!p\u0026#39; | awk \u0026#39;{print $1}\u0026#39;` if [[ $autoIncrIndexValue != \u0026#34;NULL\u0026#34; ]]; then #创建插入之后的触发器 echo \u0026#34;DELIMITER \\$\\$ drop trigger /*! IF EXISTS */ ${T}_autoincr_saver \\$\\$ create trigger ${T}_autoincr_saver after insert on $Tfor each row begin DECLARE x integer; SET @x = (SELECT AUTO_INCREMENT FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = \u0026#39;$MYSQL_DBNAME\u0026#39; AND TABLE_NAME = \u0026#39;$T\u0026#39;); INSERT INTO $AUTOINCR_INDEXES_TABLE_NAMEVALUES (\u0026#39;$T\u0026#39;, @x) ON DUPLICATE KEY UPDATE $AUTOINCR_INDEXES_INDEX_VALUE_COLUMN_NAME=@x; end \\$\\$ DELIMITER ; \u0026#34; \u0026gt;\u0026gt; $TMP_CREATE_TRIGGER_FILE fi done mysql -u$MYSQL_USER -p$MYSQL_PWD -h$MYSQL_HOST $MYSQL_DBNAME \u0026lt; $TMP_CREATE_TRIGGER_FILE rm -f $TMP_CREATE_TRIGGER_FILE # 4. 针对有自增ID的表，为每个表在$AUTOINCR_INDEXES_TABLE_NAME表中创建对应记录以保存该表的auto_increment值 for T in ${TABLES[@]} ; do autoIncrIndexValue=`mysql --batch -u$MYSQL_USER -p$MYSQL_PWD -h$MYSQL_HOST mysql -e \u0026#34;SELECT AUTO_INCREMENT FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = \u0026#39;$MYSQL_DBNAME\u0026#39; AND TABLE_NAME = \u0026#39;$T\u0026#39;;\u0026#34; | sed -n \u0026#39;1!p\u0026#39; | awk \u0026#39;{print $1}\u0026#39;` if [[ $autoIncrIndexValue != \u0026#34;NULL\u0026#34; ]]; then mysql --batch -u$MYSQL_USER -p$MYSQL_PWD -h$MYSQL_HOST $MYSQL_DBNAME -e \u0026#34;INSERT INTO $AUTOINCR_INDEXES_TABLE_NAMEVALUES (\u0026#39;$T\u0026#39;, $autoIncrIndexValue) ON DUPLICATE KEY UPDATE $AUTOINCR_INDEXES_INDEX_VALUE_COLUMN_NAME=$autoIncrIndexValue;\u0026#34; fi done # 5. 创建一个存储过程，其功能是以$AUTOINCR_INDEXES_TABLE_NAME表的记录为准，恢复每个表的auto_increment值 TMP_CREATE_PROCEDURE_FILE=\u0026#34;$(mktemp /tmp/$$_create_trigger_XXXX.sql)\u0026#34; trap \u0026#34;[ -f \u0026#34;$TMP_CREATE_PROCEDURE_FILE\u0026#34;] \u0026amp;\u0026amp; rm -f $TMP_CREATE_PROCEDURE_FILE\u0026#34; HUP INT QUIT TERM EXIT echo \u0026#34;use $MYSQL_DBNAME; drop procedure IF EXISTS $PROCEDURE_NAME; delimiter \\$\\$ create procedure $PROCEDURE_NAME() begin DECLARE done INT DEFAULT 0; DECLARE tableName CHAR(40); DECLARE indexValue INT; -- 声明游标对应的 SQL 语句 DECLARE cur CURSOR FOR select table_name, index_value from $MYSQL_DBNAME.$AUTOINCR_INDEXES_TABLE_NAME; -- 在游标循环到最后会将 done 设置为 1 DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = 1; -- 执行查询 open cur; -- 遍历游标每一行 REPEAT -- 把一行的信息存放在对应的变量中 FETCH cur INTO tableName, indexValue; if not done then -- 拼装修改auto_increment值的sql语句，并执行语句 SET @STMT := CONCAT(\\\u0026#34;ALTER TABLE $MYSQL_DBNAME.\\\u0026#34;, tableName, \\\u0026#34; AUTO_INCREMENT=\\\u0026#34;, indexValue, \\\u0026#34;;\\\u0026#34;); PREPARE STMT FROM @STMT; EXECUTE STMT; end if; UNTIL done END REPEAT; CLOSE cur; end \\$\\$ DELIMITER ; \u0026#34; \u0026gt; $TMP_CREATE_PROCEDURE_FILE mysql -u$MYSQL_USER -p$MYSQL_PWD -h$MYSQL_HOST $MYSQL_DBNAME \u0026lt; $TMP_CREATE_PROCEDURE_FILE rm -f $TMP_CREATE_PROCEDURE_FILE # 6. 修改my.cnf文件，以使mysql在启动时调用存储过程 echo \u0026#34;use $MYSQL_DBNAME; call $PROCEDURE_NAME(); \u0026#34; \u0026gt; $MYSQL_INIT_FILE sed -i -e \u0026#34;s|^\\[mysqld\\]$|[mysqld]\\ninit-file=$MYSQL_INIT_FILE|\u0026#34; /etc/my.cnf 上述代码说起来大概可以归结为以下三点：\n 将所有表的auto_increment值保存下来 利用插入后的触发器，在每次插入数据后更新保存的auto_increment值 利用init-file参数，在mysql服务启动时调用一个存储过程，该存储过程负责以保存的auto_increment值为基准，恢复每个表的auto_increment值  参考 https://mariadb.atlassian.net/browse/MDEV-6076 http://bugs.mysql.com/bug.php?id=199 http://dev.mysql.com/doc/refman/5.7/en/trigger-syntax.html http://dev.mysql.com/doc/refman/5.7/en/server-options.html#option_mysqld_init-file http://dev.mysql.com/doc/refman/5.7/en/create-procedure.html http://dev.mysql.com/doc/refman/5.7/en/cursors.html\n","permalink":"https://jeremyxu2010.github.io/2016/08/%E4%BF%9D%E5%AD%98mysql-innodb%E7%9A%84auto_increment%E5%80%BC%E5%8F%A6%E7%B1%BB%E6%96%B9%E6%A1%88/","tags":["mysql","linux","bash"],"title":"保存mysql InnoDB的auto_increment值另类方案"},{"categories":["云计算"],"contents":"公司有八九台服务器一直由我负责运维，这些服务器的配置参次不齐，主要部署了研发的持续集成环境、测试环境、性能压测环境、maven私服等一系列支撑日常开发活动的服务器。之前的办法是在这些服务器利用KVM虚拟化技术手工创建虚拟机来满足需求，最常用到的命令可能就是qemu-img、virsh了。这种办法主要存在以下的问题：\n 需要将每个主机上跑了哪些主机这些信息记录下来，最好形成表格，一旦有变更一定是同步更新表格 虚拟机创建、销毁、迁移得手工敲命令完成，即使形成一些较通用的工具脚本，还是有不少敲命令的工作量 由于公司机房安全限制，不允许机房里启动DHCP服务器，每个虚拟机镜像创建完毕之后，需要自行敲命令使用libguestfs相关命令修改虚拟机镜像文件系统中的网络配置文件，以使创建的虚拟机启动好后，自行配置好IP地址、主机名等信息。  其实早就知道针对企业内部私有云可以采用openstack，但每次一看到openstack部署那浩浩荡荡的文档就打了退堂鼓。这个周末有空，终于有时间将看过到的openstack部署过程实践一把了。我基本上是参照在CentOS7上部署openstack mitaka版的官方文档来操作的，这里就不记录详细的过程了，重点写一下安装过程中坑。\n安装环境概览 为了部署的方便，我仅在一台物理上部署openstack的Identity service(keystone)、Image service(glance)、Compute service(nova)、Networking service(neutron)、Block Storage service(cinder)、Dashboard(horizon)。由于只是测试部署的过程及试用openstack的功能，并没有部署Object Storage service(swift)。同时Networking service采用的是Provider networks的方案。\n整个部署还是比较简单的。\n首先看一下openstack各组成部分的概览，大概理解各组成部分之间的关系，根据自身需求确定好网络方案。\n然后就是按文档准备环境，这里主要就是生成安全的密码并记录下来、确定管理网络与工作网络的网络拓扑、配置时间同步服务、配置yum软件安装源、安装配置SQL数据库、安装配置NoSQL数据库、安装配置消息队列、安装配置内存缓存服务。\n然后就是按照各部分的部署文档一步步部署就是了。大部分openstack组件的安装过程无非是以下几个步骤（有的组件还需要在控制节点及计算节点分别进行安装配置）：\n 创建组件对应的SQL数据库及授予访问该数据库的用户权限 创建管理该组件在keystone中对应服务的用户 在keystone中将该组件注册为服务，并创建服务的API访问端点（公开的、内部的、管理的） 安装该组件的rpm包，修改该组件的一系列配置文件 初始化组件对应的SQL数据库的表结构 设置组件对应的系统服务开机自启并启动  在安装的过程中其实也慢慢对openstack中说到的Domain、Project、Role、User有一些感觉，后来看到了IBM的一篇文章，才对openstack中的授权模型及它的鉴权逻辑有进一步理解。\n使用openstack 还是按照文档尝试在openstack平台上启动一个虚拟机。其实在这篇文章里的所有操作都可以通过访问dashboard来操作，而且如果只是为了使用openstack，我也建议应该使用dashboard界面操作，毕竟今后使用起来会经常创建虚拟机，早点熟悉界面操作也有益处。\n创建并启动虚拟机这里有一个坑，我发现完全通过界面无法使用操作系统的安装ISO给一个虚拟机全新安装系统，每次总是报\u0026quot;no usable disks hava been found\u0026rdquo;，后来google后才知道现在只有两种解决方法：\n 要么使用传统KVM虚拟化方案，装好系统后，拿到系统的镜像文件，将镜像文件上传至openstack的镜像服务，再以此镜像创建虚拟机 要么下载操作系统的虚拟机镜像文件（如http://docs.openstack.org/image-guide/obtain-images.html），将镜像文件上传至openstack的镜像服务，再以此镜像创建虚拟机  使用体会 简单试用了openstack的功能后，记录一下自己的体会。\nopenstack作为云计算IaaS的一站式解决方案，总的来说架构还是比较清晰的，各组件之间的交互方式也比较统一(都是利用keystone这个服务注册框架来完成服务之间的交互的)，各个组件在设计初也考虑了多种底层实现方案，用户可根据自己的实际情况修改配置文件来满足需求。\n不过缺点也比较明显，我感觉如下：\n 各个逻辑组成划分得比较细，如果只是搭建一个不超过10台物理机的小型私有云环境，使用openstack就会感觉逻辑组成部分过多了，增大了部署的复杂度 各个组件在设计时考虑了太多底层实现方案，导致配置文件里的配置项相当多，每个配置项的取值也相当多，十分考验部署能力 对虚拟机libvirt细粒度的调整能力不足，比如想调整某个虚拟机的xml定义变得很复杂。 组件过多，比较消耗系统资源，如图  其实这几天我一直在思考在目前研发这个环境中，最适合的私有云管理平台是什么，可以肯定openstack肯定是不太适合。经过几天的思考，大致有一个方案，接下来我会将这个方案实践一下，如果成功，我会将这个方案写出来。\n参考 http://docs.openstack.org/mitaka/install-guide-rdo http://www.ibm.com/developerworks/cn/cloud/library/1506_yuwz_keystonev3/index.html\n","permalink":"https://jeremyxu2010.github.io/2016/08/%E8%AF%95%E7%94%A8openstack/","tags":["openstack","kvm","linux"],"title":"试用openstack"},{"categories":["云计算"],"contents":"搭建私有云环境，为了确保数据中心内部服务器的时间一致，一般建议在数据中心内部搭建NTP服务。这里将搭建NTP服务器的过程简单记录一下以备忘。\nNTP服务端设置 #安装ntp服务 yum install -y ntp echo \u0026#34; #在与上级时间服务器联系时所花费的时间，记录在driftfile参数后面的文件 driftfile /var/lib/ntp/drift #默认关闭所有的 NTP 联机服务 restrict default ignore restrict -6 default ignore #如从loopback网口请求，则允许NTP的所有操作 restrict 127.0.0.1 restrict -6 ::1 #仅允许某个网段的客户端可以通过此服务端进行网络校时 restrict 188.188.100.0 mask 255.255.255.0 nomodify notrap nopeer #中国区常用的时间服务器 server 1.cn.pool.ntp.org server 2.cn.pool.ntp.org server 3.cn.pool.ntp.org server 0.cn.pool.ntp.org server cn.pool.ntp.org #不允许第三方时间服务器修改本时间服务器的配置，查询本时间服务器的信息 restrict 1.cn.pool.ntp.org nomodify notrap nopeer noquery restrict 2.cn.pool.ntp.org nomodify notrap nopeer noquery restrict 3.cn.pool.ntp.org nomodify notrap nopeer noquery restrict 0.cn.pool.ntp.org nomodify notrap nopeer noquery restrict cn.pool.ntp.org nomodify notrap nopeer noquery #万一无法与第三方时间服务器校时，则使用本机时间 server\t127.127.1.0\t# local clock fudge\t127.127.1.0 stratum 10 #其它认证信息 includefile /etc/ntp/crypto/pw keys /etc/ntp/keys \u0026#34; \u0026gt; /etc/ntp.conf service ntpd start iptables -A INPUT -p udp -m state --state NEW -m udp --dport 123 -j ACCEPT #保存防火墙配置 service iptables save service iptables restart NTP客户端设置 #安装ntp服务 yum install -y ntp echo \u0026#34; #在与上级时间服务器联系时所花费的时间，记录在driftfile参数后面的文件 driftfile /var/lib/ntp/drift #默认关闭所有的 NTP 联机服务 restrict default ignore restrict -6 default ignore #如从loopback网口请求，则允许NTP的所有操作 restrict 127.0.0.1 restrict -6 ::1 #使用指定的时间服务器 server 188.188.100.54 #允许指定的时间服务器查询本时间服务器的信息 restrict 188.188.100.54 nomodify notrap nopeer noquery #其它认证信息 includefile /etc/ntp/crypto/pw keys /etc/ntp/keys \u0026#34; \u0026gt; /etc/ntp.conf service ntpd start 验证 等待1024s后，执行下面的命令\nntpstat 得到下面类似的输出\nsynchronised to NTP server (188.188.100.54) at stratum 4 time correct to within 87 ms polling server every 1024 s 则说明时间已校正\n执行ntpq -p可看到与上游时间服务器校正的细节，输出类似下面的\nremote refid st t when poll reach delay offset jitter ============================================================================== *188.188.100.54 202.118.1.81 3 u 762 1024 377 0.171 0.051 0.389 188.188.100.54前面有一个*号代表已与该时间服务器校正了时间。\n如需将时间同步到硬件时钟，可执行命令hwclock --systohc\n","permalink":"https://jeremyxu2010.github.io/2016/07/%E7%A7%81%E6%9C%89%E4%BA%91%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83ntp%E6%9C%8D%E5%8A%A1%E6%90%AD%E5%BB%BA/","tags":["ntp","linux"],"title":"私有云数据中心NTP服务搭建"},{"categories":["云计算"],"contents":"工作中经常发现公司机房里有些服务器上的硬盘空间不足，但还存在一些服务器上有很多空余空间，所以一直在想如何高效利用这些硬盘空间的问题。最初的解决方案是NFS，即在有空余空间的服务器上开启NFS服务器，然后需要硬盘空间的服务器通过NFS挂载过去。用过一段时间后发现存在以下问题：\n  有空余空间的服务器数量还很多，得作好记录哪个服务器由于什么用途export了哪些目录出去了，export的目录被谁挂载了。\n  NFS文件共享方式不存在数据冗余存储，主要依靠底层的存储技术如RAID来保证数据的安全。\n  后来在深度实践KVM这本书里看到了glusterfs，再加上同事也推荐让看一看glusterfs，于是周末花时间研究了下glusterfs，这里作一下记录。\n概念 一语句解释glusterfs  GlusterFS is a scalable network filesystem suitable for data-intensive tasks such as cloud storage and media streaming. GlusterFS is free and open source software and can utilize common off-the-shelf hardware.\n 核心术语  集群(Cluster) : 它是相互连接的一组主机，这些主机协同工作共同完成某一个功能，对外界来说就像一台主机一样。 可信的存储池(Trusted Storage Pool)：它是存储服务器所组成的可信网络。 服务器(Server)：实际存储数据的服务器。 卷(Volume)：Brick的逻辑集合。 分卷(SubVolume)：由多个Brick逻辑构成的卷，它是其它卷的子卷。比如在分布复制卷中每一组复制的Brick就构成了一个复制的分卷，而这些分卷又组成了分布卷。 块(Brick)：存储的基本单元，表现为服务器导出的一个目录。 客户端(Client)：挂载Volume的主机。  卷的种类  分布卷(brick count \u0026gt; 1) 复制卷(brick count = replica count \u0026amp;\u0026amp; replica count \u0026gt; 1) 分布复制卷(brick count = replica count * n \u0026amp;\u0026amp; replica count \u0026gt; 1 \u0026amp;\u0026amp; n \u0026gt; 1) 条带卷(stripe count \u0026gt; 1) 分布条带卷(brick count = stripe count * n \u0026amp;\u0026amp; stripe count \u0026gt; 1 \u0026amp;\u0026amp; n \u0026gt; 1) 分布条带复制卷(brick count = replica count * stripe count * n \u0026amp;\u0026amp; replica count \u0026gt; 1 \u0026amp;\u0026amp; stripe count \u0026gt; 1 \u0026amp;\u0026amp; n \u0026gt; 1) 条带复制卷(brick count = replica count * stripe count \u0026amp;\u0026amp; replica count \u0026gt; 1 \u0026amp;\u0026amp; stripe count \u0026gt; 1) 冗余卷(brick count = disperse count \u0026amp;\u0026amp; disperse count \u0026gt; 2 * redundancy count \u0026amp;\u0026amp; redundancy count \u0026gt;= 1)，这个相当于软件实现的RAID 分布冗余卷(brick count = disperse count * n \u0026amp;\u0026amp; disperse count \u0026gt; 2 * redundancy count \u0026amp;\u0026amp; redundancy count \u0026gt;= 1 \u0026amp;\u0026amp; n \u0026gt; 1)  用户空间文件系统工作原理 FUSE用户空间文件系统，原来只是知道，这次终于在glusterfs的官方文档上看到一个较详实的解释了。\n Being a userspace filesystem, to interact with kernel VFS, GlusterFS makes use of FUSE (File System in Userspace). For a long time, implementation of a userspace filesystem was considered impossible. FUSE was developed as a solution for this. FUSE is a kernel module that support interaction between kernel VFS and non-privileged user applications and it has an API that can be accessed from userspace. Using this API, any type of filesystem can be written using almost any language you prefer as there are many bindings between FUSE and other languages.\n  This shows a filesystem \u0026ldquo;hello world\u0026rdquo; that is compiled to create a binary \u0026ldquo;hello\u0026rdquo;. It is executed with a filesystem mount point /tmp/fuse. Then the user issues a command ls -l on the mount point /tmp/fuse. This command reaches VFS via glibc and since the mount /tmp/fuse corresponds to a FUSE based filesystem, VFS passes it over to FUSE module. The FUSE kernel module contacts the actual filesystem binary \u0026ldquo;hello\u0026rdquo; after passing through glibc and FUSE library in userspace(libfuse). The result is returned by the \u0026ldquo;hello\u0026rdquo; through the same path and reaches the ls -l command.\n  The communication between FUSE kernel module and the FUSE library(libfuse) is via a special file descriptor which is obtained by opening /dev/fuse. This file can be opened multiple times, and the obtained file descriptor is passed to the mount syscall, to match up the descriptor with the mounted filesystem.\n glusterfs工作原理 下面这段摘自官方文档，我觉得短短几段话，还是描述得挺清楚的。\n As soon as GlusterFS is installed in a server node, a gluster management daemon(glusterd) binary will be created. This daemon should be running in all participating nodes in the cluster. After starting glusterd, a trusted server pool(TSP) can be created consisting of all storage server nodes (TSP can contain even a single node). Now bricks which are the basic units of storage can be created as export directories in these servers. Any number of bricks from this TSP can be clubbed together to form a volume.\n  Once a volume is created, a glusterfsd process starts running in each of the participating brick. Along with this, configuration files known as vol files will be generated inside /var/lib/glusterd/vols/. There will be configuration files corresponding to each brick in the volume. This will contain all the details about that particular brick. Configuration file required by a client process will also be created. Now our filesystem is ready to use. We can mount this volume on a client machine very easily as follows and use it like we use a local storage:\n  mount.glusterfs \u0026lt;IP or hostname\u0026gt;:\u0026lt;volume_name\u0026gt; \u0026lt;mount_point\u0026gt;\n  IP or hostname can be that of any node in the trusted server pool in which the required volume is created.\n  When we mount the volume in the client, the client glusterfs process communicates with the servers’ glusterd process. Server glusterd process sends a configuration file (vol file) containing the list of client translators and another containing the information of each brick in the volume with the help of which the client glusterfs process can now directly communicate with each brick’s glusterfsd process. The setup is now complete and the volume is now ready for client's service.\n  When a system call (File operation or Fop) is issued by client in the mounted filesystem, the VFS (identifying the type of filesystem to be glusterfs) will send the request to the FUSE kernel module. The FUSE kernel module will in turn send it to the GlusterFS in the userspace of the client node via /dev/fuse (this has been described in FUSE section). The GlusterFS process on the client consists of a stack of translators called the client translators which are defined in the configuration file(vol file) send by the storage server glusterd process. The first among these translators being the FUSE translator which consists of the FUSE library(libfuse). Each translator has got functions corresponding to each file operation or fop supported by glusterfs. The request will hit the corresponding function in each of the translators. Main client translators include:\n   FUSE translator DHT translator- DHT translator maps the request to the correct brick that contains the file or directory required. AFR translator- It receives the request from the previous translator and if the volume type is replicate, it duplicates the request and pass it on to the Protocol client translators of the replicas. Protocol Client translator- Protocol Client translator is the last in the client translator stack. This translator is divided into multiple threads, one for each brick in the volume. This will directly communicate with the glusterfsd of each brick.    In the storage server node that contains the brick in need, the request again goes through a series of translators known as server translators, main ones being:\n   Protocol server translator POSIX translator    The request will finally reach VFS and then will communicate with the underlying native filesystem. The response will retrace the same path.\n 实操glusterfs 准备环境  三台CentOS6.8，其中两台作为Server（gfs1, gfs2），一台作为Client(gfs_client) 在三台主机上配置好/etc/hosts文件，保证使用名称可解析到正确的ping 三台服务器均配置好glusterfs的软件安装源  实操 首先在两台Server执行以下操作\n#安装glusterfs的服务器端软件包 yum -y install glusterfs glusterfs-server #启动glusterfs daemon服务 chkconfig glusterd on service glusterd start #配置glusterfs允许外界访问的防火墙规则 iptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 24007:24008 -j ACCEPT iptables -A INPUT -p udp -m state --state NEW -m udp --dport 24007:24008 -j ACCEPT iptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 49152:49155 -j ACCEPT iptables -A INPUT -p udp -m state --state NEW -m udp --dport 49152:49155 -j ACCEPT #保存防火墙配置 service iptables save service iptables restart #创建brick存储数据的目录 mkdir -p /data/gfs_b1 然后在gfs1这台Server执行命令\n#将gfs2加入到集群节点列表 gluster peer probe gfs2 #查看集群节点状态 gluster peer status #创建复制卷 gluster volume create gfs_b1 replica 2 gfs1:/data/gfs_b1 gfs2:/data/gfs_b1 #启动卷 gluster volume start gfs_b1 #查看卷状态 gluster volume info \u0026amp;\u0026amp; gluster volume status 最后在Client的主机执行以下命令\n#安装使用glusterfs的FUSE挂载方式依赖的软件包 yum -y install glusterfs glusterfs-fuse #创建挂载目录 mkdir -p /mnt/gfs_b1 #写入挂载配置 echo \u0026#34; gfs1:/gfs_b1\t/mnt/gfs_b1\tglusterfs defaults,_netdev,transport=tcp 0 0\u0026#34; \u0026gt;\u0026gt; /etc/fstab #执行挂载 mount -a 除了利用FUSE挂载，也可以使用NFS挂载，可执行以下命令\n#在Server端需要添加允许NFS Server供外部访问的防火墙规则 echo \u0026#34; gfs1:/gfs_b1 /mnt/gfs_b1 nfs defaults,_netdev,mountproto=tcp,vers=3 0 0\u0026#34; \u0026gt;\u0026gt; /etc/fstab mount -a 常用运维命令 #删除卷 gluster volume stop img gluster volume delete img #将机器移出集群 gluster peer detach 172.28.26.102 #只允许172.28.0.0的网络访问glusterfs gluster volume set img auth.allow 172.28.26.* #加入新的机器并添加到卷里(由于副本数设置为2,至少要添加2（4、6、8..）台机器) gluster peer probe 172.28.26.105 gluster peer probe 172.28.26.106 gluster volume add-brick img 172.28.26.105:/data/gluster 172.28.26.106:/data/gluster #收缩卷，收缩卷前gluster需要先移动数据到其他位置 gluster volume remove-brick img 172.28.26.101:/data/gluster/img 172.28.26.102:/data/gluster/img start # 查看收缩状态 gluster volume remove-brick img 172.28.26.101:/data/gluster/img 172.28.26.102:/data/gluster/img status # 收缩完成后提交 gluster volume remove-brick img 172.28.26.101:/data/gluster/img 172.28.26.102:/data/gluster/img commit # 平衡卷的布局及迁移已有的数据 gluster volume rebalance img start # 查看平衡状态 gluster volume rebalance img status # 取消平衡卷的操作 gluster volume rebalance img stop # 迁移卷 将172.28.26.101的数据迁移到172.28.26.107,先将172.28.26.107加入集群 gluster peer probe 172.28.26.107 gluster volume replace-brick img 172.28.26.101:/data/gluster/img 172.28.26.107:/data/gluster/img start # 查看迁移状态 gluster volume replace-brick img 172.28.26.101:/data/gluster/img 172.28.26.107:/data/gluster/img status # 数据迁移完毕后提交 gluster volume replace-brick img 172.28.26.101:/data/gluster/img 172.28.26.107:/data/gluster/img commit # 如果机器172.28.26.101出现故障已经不能运行,执行强制提交然后要求gluster马上执行一次同步 gluster volume replace-brick img 172.28.26.101:/data/gluster/img 172.28.26.107:/data/gluster/img commit -force gluster volume heal img full 进一步思考 与常见分布存储相比，优缺点 优点：\n 安装部署简单方便 隐藏了元数据的概念，元数据直接以扩展属性的方式存储在文件上 兼容POSIX标准，挂载方便 与kvm整合较好 可作基于地理位置分布的复制，见这里 基于LVM，可进行快照管理，见这里 可方便地进行配额限制，见这里 可方便地进行性能监控，见这里  缺点：\n 用户空间文件系统操作文件的效率相比内核的文件系统可能会慢一些。 每一个Brick需要与其它同卷中的Brick建立TCP长连接，为了不至于影响性能，必须限制Brick的数量，好像官方4.x版本正在想办法处理这个问题 只提供了基于文件系统的使用方式，不像Ceph那样还提供了对象存储、块设备的使用方式。（可以使用第三方项目SwiftOnFile以支持对象存储） 未像Ceph那样隔离管理网络及存储网络，可能会由于管理网络的数据传输拥堵导致存储网络性能降低。（不过我尝试，在服务器上配置双IP，服务器与客户端对主机名作不同的解析可以规避这个问题）  参考 http://gluster.readthedocs.io/en/latest/\n","permalink":"https://jeremyxu2010.github.io/2016/07/%E5%88%9D%E8%AF%86glusterfs/","tags":["glusterfs"],"title":"初识glusterfs"},{"categories":["java开发"],"contents":"今天在压力测试环境某一个服务出现crash了，经过一番检查，终于发现是由于JVM的Finalization Delay引起的，这个问题比较特殊，这里记录一下。\n这个服务是用Java写的，主要完成的功能是根据特定的指令文件生成mp4文件，用到的java库主要有javacv，这个库底层其实是使用JNI调用操作系统里安装的ffmpeg。\n检查日志文件 首先检查日志文件，发现日志里出现了OOM的报错\njava.lang.OutOfMemoryError: null at sun.misc.Unsafe.allocateMemory(Native Method) ~[na:1.7.0_79] at java.nio.DirectByteBuffer.\u0026lt;init\u0026gt;(DirectByteBuffer.java:127) ~[na:1.7.0_79] at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:306) ~[na:1.7.0_79] at org.bytedeco.javacv.Frame.\u0026lt;init\u0026gt;(Frame.java:105) ~[javacv-1.1.jar:1.1] at org.bytedeco.javacv.Java2DFrameConverter.getFrame(Java2DFrameConverter.java:712) ~[javacv-1.1.jar:1.1] at org.bytedeco.javacv.Java2DFrameConverter.getFrame(Java2DFrameConverter.java:679) ~[javacv-1.1.jar:1.1] at org.bytedeco.javacv.Java2DFrameConverter.getFrame(Java2DFrameConverter.java:673) ~[javacv-1.1.jar:1.1] at org.bytedeco.javacv.Java2DFrameConverter.convert(Java2DFrameConverter.java:62) ~[javacv-1.1.jar:1.1] 很明显这里是申请DirectByteBuffer出现了OOM，所以应该是Direct Memory申请太多了。为了确认问题，将服务跑起来，使用jconsole看了下JVM的堆内存使用情况，发现堆内存使用一直都是比较稳定的，但使用top -p ${pid}查看进程占用的内存，发现RES字段的值一直是在增长的，而且增长得很快，不到半个小时值就从原来的500M增长到1.6G。\n分析代码 接下来看一下关键代码\npublic void encodeFrame(BufferedImage image, long frameTime) { try { long t = frameTime * 1000L; if(t\u0026gt;recorder.getTimestamp()) { recorder.setTimestamp(t); } Frame frame = java2dConverter.convert(image); recorder.record(frame); } catch (FrameRecorder.Exception e) { log.error(\u0026#34;JavaCVMp4Encoder encode frame error.\u0026#34;, e); } } 业务层是不停地调用encodeFrame将每一张图片编码到mp4文件里。\n而java2dConverter.convert(image);这句代码的实现里会申请一个DirectByteBuffer。如下面的代码。\npublic Frame(int width, int height, int depth, int channels) { int pixelSize = Math.abs(depth) / 8; this.imageWidth = width; this.imageHeight = height; this.imageDepth = depth; this.imageChannels = channels; this.imageStride = ((imageWidth * imageChannels * pixelSize + 7) \u0026amp; ~7) / pixelSize; // 8-byte aligned  this.image = new Buffer[1]; ByteBuffer buffer = ByteBuffer.allocateDirect(imageHeight * imageStride * pixelSize).order(ByteOrder.nativeOrder()); switch (imageDepth) { case DEPTH_BYTE: case DEPTH_UBYTE: image[0] = buffer; break; case DEPTH_SHORT: case DEPTH_USHORT: image[0] = buffer.asShortBuffer(); break; case DEPTH_INT: image[0] = buffer.asIntBuffer(); break; case DEPTH_LONG: image[0] = buffer.asLongBuffer(); break; case DEPTH_FLOAT: image[0] = buffer.asFloatBuffer(); break; case DEPTH_DOUBLE: image[0] = buffer.asDoubleBuffer(); break; default: throw new UnsupportedOperationException(\u0026#34;Unsupported depth value: \u0026#34; + imageDepth); } } 这里ByteBuffer.allocateDirect方法申请的DirectByteBuffer并不是Java堆内存，而是直接在C堆上申请的。而DirectByteBuffer申请的C堆内存释放很特殊，并不是简单地由JVM GC完成的。\n先看一下DirectByteBuffer的定义\nclass DirectByteBuffer extends MappedByteBuffer implements DirectBuffer { ... protected static final Unsafe unsafe = Bits.unsafe(); ... private static class Deallocator implements Runnable { private static Unsafe unsafe = Unsafe.getUnsafe(); private long address; private long size; private int capacity; private Deallocator(long address, long size, int capacity) { assert (address != 0); this.address = address; this.size = size; this.capacity = capacity; } public void run() { if (address == 0) { // Paranoia  return; } unsafe.freeMemory(address); address = 0; Bits.unreserveMemory(size, capacity); } } private final Cleaner cleaner; ... DirectByteBuffer(int cap) { // package-private  super(-1, 0, cap, cap); boolean pa = VM.isDirectMemoryPageAligned(); int ps = Bits.pageSize(); long size = Math.max(1L, (long)cap + (pa ? ps : 0)); Bits.reserveMemory(size, cap); long base = 0; try { base = unsafe.allocateMemory(size); } catch (OutOfMemoryError x) { Bits.unreserveMemory(size, cap); throw x; } unsafe.setMemory(base, size, (byte) 0); if (pa \u0026amp;\u0026amp; (base % ps != 0)) { // Round up to page boundary  address = base + ps - (base \u0026amp; (ps - 1)); } else { address = base; } cleaner = Cleaner.create(this, new Deallocator(base, size, cap)); att = null; } ... } 可以看到创建DirectByteBuffer对象时实际上使用unsafe.allocateMemory申请一块C堆内存的。DirectByteBuffer对象内部有一个Cleaner cleaner，看样子应该是这个东东负责对申请申请的C堆内存进行释放的。看一下Cleaner的定义：\npublic class Cleaner extends PhantomReference\u0026lt;Object\u0026gt; { ... private static final ReferenceQueue\u0026lt;Object\u0026gt; dummyQueue = new ReferenceQueue(); private Cleaner(Object var1, Runnable var2) { super(var1, dummyQueue); this.thunk = var2; } public static Cleaner create(Object var0, Runnable var1) { return var1 == null?null:add(new Cleaner(var0, var1)); } public void clean() { if(remove(this)) { try { this.thunk.run(); } catch (final Throwable var2) { AccessController.doPrivileged(new PrivilegedAction() { public Void run() { if(System.err != null) { (new Error(\u0026#34;Cleaner terminated abnormally\u0026#34;, var2)).printStackTrace(); } System.exit(1); return null; } }); } } } ... } 原来Cleaner实际上是一个PhantomReference\n PhantomReference虚引用主要用来跟踪对象被垃圾回收器回收的活动。虚引用与软引用和弱引用的一个区别在于：虚引用必须和引用队列 （ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。\n 在这个场景里也就是说当JVM垃圾回收器准备回收某个DirectByteBuffer对象时，发现这个DirectByteBuffer对象有虚引用，就会将虚引用加入到与之关联的引用队列中。将虚引用加入到与之关联的引用队列中有什么作用？看一下Reference的实现代码\npublic abstract class Reference\u0026lt;T\u0026gt; { ... private T referent; ... static private class Lock { } private static Lock lock = new Lock(); /* List of References waiting to be enqueued. The collector adds * References to this list, while the Reference-handler thread removes * them. This list is protected by the above lock object. The * list uses the discovered field to link its elements. */ private static Reference\u0026lt;Object\u0026gt; pending = null; /* High-priority thread to enqueue pending References */ private static class ReferenceHandler extends Thread { private static void ensureClassInitialized(Class\u0026lt;?\u0026gt; clazz) { try { Class.forName(clazz.getName(), true, clazz.getClassLoader()); } catch (ClassNotFoundException e) { throw (Error) new NoClassDefFoundError(e.getMessage()).initCause(e); } } static { // pre-load and initialize InterruptedException and Cleaner classes  // so that we don\u0026#39;t get into trouble later in the run loop if there\u0026#39;s  // memory shortage while loading/initializing them lazily.  ensureClassInitialized(InterruptedException.class); ensureClassInitialized(Cleaner.class); } ReferenceHandler(ThreadGroup g, String name) { super(g, name); } public void run() { while (true) { tryHandlePending(true); } } } /** * Try handle pending {@link Reference} if there is one.\u0026lt;p\u0026gt; * Return {@code true} as a hint that there might be another * {@link Reference} pending or {@code false} when there are no more pending * {@link Reference}s at the moment and the program can do some other * useful work instead of looping. * * @param waitForNotify if {@code true} and there was no pending * {@link Reference}, wait until notified from VM * or interrupted; if {@code false}, return immediately * when there is no pending {@link Reference}. * @return {@code true} if there was a {@link Reference} pending and it * was processed, or we waited for notification and either got it * or thread was interrupted before being notified; * {@code false} otherwise. */ static boolean tryHandlePending(boolean waitForNotify) { Reference\u0026lt;Object\u0026gt; r; Cleaner c; try { synchronized (lock) { if (pending != null) { r = pending; // \u0026#39;instanceof\u0026#39; might throw OutOfMemoryError sometimes  // so do this before un-linking \u0026#39;r\u0026#39; from the \u0026#39;pending\u0026#39; chain...  c = r instanceof Cleaner ? (Cleaner) r : null; // unlink \u0026#39;r\u0026#39; from \u0026#39;pending\u0026#39; chain  pending = r.discovered; r.discovered = null; } else { // The waiting on the lock may cause an OutOfMemoryError  // because it may try to allocate exception objects.  if (waitForNotify) { lock.wait(); } // retry if waited  return waitForNotify; } } } catch (OutOfMemoryError x) { // Give other threads CPU time so they hopefully drop some live references  // and GC reclaims some space.  // Also prevent CPU intensive spinning in case \u0026#39;r instanceof Cleaner\u0026#39; above  // persistently throws OOME for some time...  Thread.yield(); // retry  return true; } catch (InterruptedException x) { // retry  return true; } // Fast path for cleaners  if (c != null) { c.clean(); return true; } ReferenceQueue\u0026lt;? super Object\u0026gt; q = r.queue; if (q != ReferenceQueue.NULL) q.enqueue(r); return true; } static { ThreadGroup tg = Thread.currentThread().getThreadGroup(); for (ThreadGroup tgn = tg; tgn != null; tg = tgn, tgn = tg.getParent()); Thread handler = new ReferenceHandler(tg, \u0026#34;Reference Handler\u0026#34;); /* If there were a special system-only priority greater than * MAX_PRIORITY, it would be used here */ handler.setPriority(Thread.MAX_PRIORITY); handler.setDaemon(true); handler.start(); // provide access in SharedSecrets  SharedSecrets.setJavaLangRefAccess(new JavaLangRefAccess() { @Override public boolean tryHandlePendingReference() { return tryHandlePending(false); } }); } ... Reference(T referent, ReferenceQueue\u0026lt;? super T\u0026gt; queue) { this.referent = referent; this.queue = (queue == null) ? ReferenceQueue.NULL : queue; } } 这里代码看着有些糊涂，并没有代码给pending这个类变量赋值，为啥ReferenceHandler这个线程执行体里又在读取它的值，但看了看private static Reference\u0026lt;Object\u0026gt; pending = null;这一行上面的注释，想了想终于明白了，原来JVM垃圾回收器将将虚引用加入到与之关联的引用队列后，JVM垃圾回收器又负责逐个将引用队列中的引用拿出来赋于pending，然后通知ReferenceHandler线程，ReferenceHandler线程拿到引用后，发现如果是Cleaner，则调用其clean方法。然后终于与DirectByteBuffer里的Deallocator接上了，最终DirectByteBuffer申请的C堆内存被释放。\n既然DirectByteBuffer申请的C堆内存释放是自动的，为啥在这个场景里会出现OOM呢？查阅java的bug记录，终于找到原因。http://bugs.java.com/bugdatabase/view_bug.do?bug_id=4857305，http://bugs.java.com/bugdatabase/view_bug.do?bug_id=4469299。\n意思是如果DirectByteBuffer创建得过于频繁，服务器的CPU太繁忙，C堆内存还是会OOM的，原因是JVM来不及进行GC及Finalization，大量对象的销毁工作被推后，最终C堆内存无法得到释放。\n解决方案 bug记录提到了3个解决方案：\n Insert occasional explicit System.gc() invocations to ensure that direct buffers are reclaimed.\n  Reduce the size of the young generation to force more frequent GCs.\n  Explicitly pool direct buffers at the application level.\n 我这里采用了第一个解决方案，代码如下：\npublic void encodeFrame(BufferedImage image, long frameTime) { try { long t = frameTime * 1000L; if(t\u0026gt;recorder.getTimestamp()) { recorder.setTimestamp(t); } Frame frame = java2dConverter.convert(image); recorder.record(frame); if(System.currentTimeMillis() - lastGCTime \u0026gt; 60000){ System.gc(); System.runFinalization(); lastGCTime = System.currentTimeMillis(); Thread.yield(); TimeUnit.SECONDS.sleep(3); } } catch (FrameRecorder.Exception e) { log.error(\u0026#34;JavaCVMp4Encoder encode frame error.\u0026#34;, e); } } 意思是说每隔1分钟显式地调用System.gc();与System.runFinalization();，并让出CPU休息3秒钟。经过长达10几个小时的测试，目前一切都正常了。\n","permalink":"https://jeremyxu2010.github.io/2016/07/jvm%E7%9A%84finalization-delay%E5%BC%95%E8%B5%B7%E7%9A%84oom/","tags":["java","nio","oom"],"title":"JVM的Finalization Delay引起的OOM"},{"categories":["devops"],"contents":"最近开始使用CentOS7了，跟CentOS6相比最大的变更可能就是守护进程由init改为了systemd。在网上看到阮一峰的“Systemd 入门教程”，觉得很好，这里转载一下。阮一峰的“Systemd 入门教程”原贴地址\n转载内容：\n Systemd 是 Linux 系统工具，用来启动守护进程，已成为大多数发行版的标准配置。 本文介绍它的基本用法，分为上下两篇。今天介绍它的主要命令，下一篇介绍如何用于实战。\n由来 历史上，Linux 的启动一直采用init进程。 下面的命令用来启动服务。\n$ sudo /etc/init.d/apache2 start 或者 $ service apache2 start\n这种方法有两个缺点。\n一是启动时间长。init进程是串行启动，只有前一个进程启动完，才会启动下一个进程。 二是启动脚本复杂。init进程只是执行启动脚本，不管其他事情。脚本需要自己处理各种情况，这往往使得脚本变得很长。\nSystemd 概述 Systemd 就是为了解决这些问题而诞生的。它的设计目标是，为系统的启动和管理提供一套完整的解决方案。 根据 Linux 惯例，字母d是守护进程（daemon）的缩写。 Systemd 这个名字的含义，就是它要守护整个系统。\n使用了 Systemd，就不需要再用init了。Systemd 取代了initd，成为系统的第一个进程（PID 等于 1），其他进程都是它的子进程。\n$ systemctl --version\n上面的命令查看 Systemd 的版本。\nSystemd 的优点是功能强大，使用方便，缺点是体系庞大，非常复杂。事实上，现在还有很多人反对使用 Systemd，理由就是它过于复杂，与操作系统的其他部分强耦合，违反\u0026quot;keep simple, keep stupid\u0026quot;的Unix 哲学。\n系统管理 Systemd 并不是一个命令，而是一组命令，涉及到系统管理的方方面面。\nsystemctl systemctl是 Systemd 的主命令，用于管理系统。\n# 重启系统 $ sudo systemctl reboot # 关闭系统，切断电源 $ sudo systemctl poweroff # CPU停止工作 $ sudo systemctl halt # 暂停系统 $ sudo systemctl suspend # 让系统进入冬眠状态 $ sudo systemctl hibernate # 让系统进入交互式休眠状态 $ sudo systemctl hybrid-sleep # 启动进入救援状态（单用户状态） $ sudo systemctl rescue systemd-analyze systemd-analyze命令用于查看启动耗时。\n# 查看启动耗时 $ systemd-analyze # 查看每个服务的启动耗时 $ systemd-analyze blame # 显示瀑布状的启动过程流 $ systemd-analyze critical-chain # 显示指定服务的启动流 $ systemd-analyze critical-chain atd.service hostnamectl hostnamectl命令用于查看当前主机的信息。\n# 显示当前主机的信息 $ hostnamectl # 设置主机名。 $ sudo hostnamectl set-hostname rhel7 3.4 localectl localectl命令用于查看本地化设置。 # 查看本地化设置 $ localectl # 设置本地化参数。 $ sudo localectl set-locale LANG=en_GB.utf8 $ sudo localectl set-keymap en_GB timedatectl timedatectl命令用于查看当前时区设置。\n# 查看当前时区设置 $ timedatectl # 显示所有可用的时区 $ timedatectl list-timezones # 设置当前时区 $ sudo timedatectl set-timezone America/New_York $ sudo timedatectl set-time YYYY-MM-DD $ sudo timedatectl set-time HH:MM:SS loginctl loginctl命令用于查看当前登录的用户。\n# 列出当前session $ loginctl list-sessions # 列出当前登录用户 $ loginctl list-users # 列出显示指定用户的信息 $ loginctl show-user ruanyf Unit 含义 Systemd 可以管理所有系统资源。不同的资源统称为 Unit（单位）。Unit 一共分成12种。\nService unit：系统服务 Target unit：多个 Unit 构成的一个组 Device Unit：硬件设备 Mount Unit：文件系统的挂载点 Automount Unit：自动挂载点 Path Unit：文件或路径 Scope Unit：不是由 Systemd 启动的外部进程 Slice Unit：进程组 Snapshot Unit：Systemd 快照，可以切回某个快照 Socket Unit：进程间通信的 socket Swap Unit：swap 文件 Timer Unit：定时器 systemctl list-units命令可以查看当前系统的所有 Unit 。\n# 列出正在运行的 Unit $ systemctl list-units # 列出所有Unit，包括没有找到配置文件的或者启动失败的 $ systemctl list-units --all # 列出所有没有运行的 Unit $ systemctl list-units --all --state=inactive # 列出所有加载失败的 Unit $ systemctl list-units --failed # 列出所有正在运行的、类型为 service 的 Unit $ systemctl list-units --type=service Unit 的状态 systemctl status命令用于查看系统状态和单个 Unit 的状态。\n# 显示系统状态 $ systemctl status # 显示单个 Unit 的状态 $ sysystemctl status bluetooth.service # 显示远程主机的某个 Unit 的状态 $ systemctl -H root@rhel7.example.com status httpd.service 除了status命令，systemctl还提供了三个查询状态的简单方法，主要供脚本内部的判断语句使用。\n# 显示某个 Unit 是否正在运行 $ systemctl is-active application.service # 显示某个 Unit 是否处于启动失败状态 $ systemctl is-failed application.service # 显示某个 Unit 服务是否建立了启动链接 $ systemctl is-enabled application.service Unit 管理 对于用户来说，最常用的是下面这些命令，用于启动和停止 Unit（主要是 service）。\n# 立即启动一个服务 $ sudo systemctl start apache.service # 立即停止一个服务 $ sudo systemctl stop apache.service # 重启一个服务 $ sudo systemctl restart apache.service # 杀死一个服务的所有子进程 $ sudo systemctl kill apache.service # 重新加载一个服务的配置文件 $ sudo systemctl reload apache.service # 重载所有修改过的配置文件 $ sudo systemctl daemon-reload # 显示某个 Unit 的所有底层参数 $ systemctl show httpd.service # 显示某个 Unit 的指定属性的值 $ systemctl show -p CPUShares httpd.service # 设置某个 Unit 的指定属性 $ sudo systemctl set-property httpd.service CPUShares=500 依赖关系 Unit 之间存在依赖关系：A 依赖于 B，就意味着 Systemd 在启动 A 的时候，同时会去启动 B。 systemctl list-dependencies命令列出一个 Unit 的所有依赖。\n$ systemctl list-dependencies nginx.service\n上面命令的输出结果之中，有些依赖是 Target 类型（详见下文），默认不会展开显示。如果要展开 Target，就需要使用\u0026ndash;all参数。\n$ systemctl list-dependencies --all nginx.service\nUnit 的配置文件 概述 每一个 Unit 都有一个配置文件，告诉 Systemd 怎么启动这个 Unit 。 Systemd 默认从目录/etc/systemd/system/读取配置文件。但是，里面存放的大部分文件都是符号链接，指向目录/usr/lib/systemd/system/，真正的配置文件存放在那个目录。 systemctl enable命令用于在上面两个目录之间，建立符号链接关系。\n$ sudo systemctl enable clamd@scan.service 等同于 $ sudo ln -s '/usr/lib/systemd/system/clamd@scan.service' '/etc/systemd/system/multi-user.target.wants/clamd@scan.service'\n如果配置文件里面设置了开机启动，systemctl enable命令相当于激活开机启动。 与之对应的，systemctl disable命令用于在两个目录之间，撤销符号链接关系，相当于撤销开机启动。\n$ sudo systemctl disable clamd@scan.service\n配置文件的后缀名，就是该 Unit 的种类，比如sshd.socket。如果省略，Systemd 默认后缀名为.service，所以sshd会被理解成sshd.service。\n配置文件的状态 systemctl list-unit-files命令用于列出所有配置文件。\n# 列出所有配置文件 $ systemctl list-unit-files # 列出指定类型的配置文件 $ systemctl list-unit-files --type=service 这个命令会输出一个列表。 $ systemctl list-unit-files UNIT FILE STATE chronyd.service enabled clamd@.service static clamd@scan.service disabled 这个列表显示每个配置文件的状态，一共有四种。 enabled：已建立启动链接 disabled：没建立启动链接 static：该配置文件没有[Install]部分（无法执行），只能作为其他配置文件的依赖 masked：该配置文件被禁止建立启动链接 注意，从配置文件的状态无法看出，该 Unit 是否正在运行。这必须执行前面提到的systemctl status命令。 $ systemctl status bluetooth.service 一旦修改配置文件，就要让 SystemD 重新加载配置文件，然后重新启动，否则修改不会生效。 $ sudo systemctl daemon-reload $ sudo systemctl restart httpd.service 配置文件的格式 配置文件就是普通的文本文件，可以用文本编辑器打开。\nsystemctl cat命令可以查看配置文件的内容。\n$ systemctl cat atd.service [Unit] Description=ATD daemon [Service] Type=forking ExecStart=/usr/bin/atd [Install] WantedBy=multi-user.target 从上面的输出可以看到，配置文件分成几个区块。每个区块的第一行，是用方括号表示的区别名，比如[Unit]。注意，配置文件的区块名和字段名，都是大小写敏感的。 每个区块内部是一些等号连接的键值对。 [Section] Directive1=value Directive2=value . . . 注意，键值对的等号两侧不能有空格。\n配置文件的区块 [Unit]区块通常是配置文件的第一个区块，用来定义 Unit 的元数据，以及配置与其他 Unit 的关系。它的主要字段如下。\nDescription：简短描述 Documentation：文档地址 Requires：当前 Unit 依赖的其他 Unit，如果它们没有运行，当前 Unit 会启动失败 Wants：与当前 Unit 配合的其他 Unit，如果它们没有运行，当前 Unit 不会启动失败 BindsTo：与Requires类似，它指定的 Unit 如果退出，会导致当前 Unit 停止运行 Before：如果该字段指定的 Unit 也要启动，那么必须在当前 Unit 之后启动 After：如果该字段指定的 Unit 也要启动，那么必须在当前 Unit 之前启动 Conflicts：这里指定的 Unit 不能与当前 Unit 同时运行 Condition\u0026hellip;：当前 Unit 运行必须满足的条件，否则不会运行 Assert\u0026hellip;：当前 Unit 运行必须满足的条件，否则会报启动失败 [Install]通常是配置文件的最后一个区块，用来定义如何启动，以及是否开机启动。它的主要字段如下。 WantedBy：它的值是一个或多个 Target，当前 Unit 激活时（enable）符号链接会放入/etc/systemd/system目录下面以 Target 名 + .wants后缀构成的子目录中 RequiredBy：它的值是一个或多个 Target，当前 Unit 激活时，符号链接会放入/etc/systemd/system目录下面以 Target 名 + .required后缀构成的子目录中 Alias：当前 Unit 可用于启动的别名 Also：当前 Unit 激活（enable）时，会被同时激活的其他 Unit [Service]区块用来 Service 的配置，只有 Service 类型的 Unit 才有这个区块。它的主要字段如下。 Type：定义启动时的进程行为。它有以下几种值。 Type=simple：默认值，执行ExecStart指定的命令，启动主进程 Type=forking：以 fork 方式从父进程创建子进程，创建后父进程会立即退出 Type=oneshot：一次性进程，Systemd 会等当前服务退出，再继续往下执行 Type=dbus：当前服务通过D-Bus启动 Type=notify：当前服务启动完毕，会通知Systemd，再继续往下执行 Type=idle：若有其他任务执行完毕，当前服务才会运行 ExecStart：启动当前服务的命令 ExecStartPre：启动当前服务之前执行的命令 ExecStartPost：启动当前服务之后执行的命令 ExecReload：重启当前服务时执行的命令 ExecStop：停止当前服务时执行的命令 ExecStopPost：停止当其服务之后执行的命令 RestartSec：自动重启当前服务间隔的秒数 Restart：定义何种情况 Systemd 会自动重启当前服务，可能的值包括always（总是重启）、on-success、on-failure、on-abnormal、on-abort、on-watchdog TimeoutSec：定义 Systemd 停止当前服务之前等待的秒数 Environment：指定环境变量 Unit 配置文件的完整字段清单，请参考官方文档。\nTarget 启动计算机的时候，需要启动大量的 Unit。如果每一次启动，都要一一写明本次启动需要哪些 Unit，显然非常不方便。Systemd 的解决方案就是 Target。\n简单说，Target 就是一个 Unit 组，包含许多相关的 Unit 。启动某个 Target 的时候，Systemd 就会启动里面所有的 Unit。从这个意义上说，Target 这个概念类似于\u0026quot;状态点\u0026rdquo;，启动某个 Target 就好比启动到某种状态。\n传统的init启动模式里面，有 RunLevel 的概念，跟 Target 的作用很类似。不同的是，RunLevel 是互斥的，不可能多个 RunLevel 同时启动，但是多个 Target 可以同时启动。\n# 查看当前系统的所有 Target $ systemctl list-unit-files --type=target # 查看一个 Target 包含的所有 Unit $ systemctl list-dependencies multi-user.target # 查看启动时的默认 Target $ systemctl get-default # 设置启动时的默认 Target $ sudo systemctl set-default multi-user.target # 切换 Target 时，默认不关闭前一个 Target 启动的进程， # systemctl isolate 命令改变这种行为， # 关闭前一个 Target 里面所有不属于后一个 Target 的进程 $ sudo systemctl isolate multi-user.target Target 与 传统 RunLevel 的对应关系如下。\nTraditional runlevel New target name Symbolically linked to\u0026hellip;\nRunlevel 0 | runlevel0.target -\u0026gt; poweroff.target Runlevel 1 | runlevel1.target -\u0026gt; rescue.target Runlevel 2 | runlevel2.target -\u0026gt; multi-user.target Runlevel 3 | runlevel3.target -\u0026gt; multi-user.target Runlevel 4 | runlevel4.target -\u0026gt; multi-user.target Runlevel 5 | runlevel5.target -\u0026gt; graphical.target Runlevel 6 | runlevel6.target -\u0026gt; reboot.target\n它与init进程的主要差别如下。\n（1）默认的 RunLevel（在/etc/inittab文件设置）现在被默认的 Target 取代，位置是/etc/systemd/system/default.target，通常符号链接到graphical.target（图形界面）或者multi-user.target（多用户命令行）。 （2）启动脚本的位置，以前是/etc/init.d目录，符号链接到不同的 RunLevel 目录 （比如/etc/rc3.d、/etc/rc5.d等），现在则存放在/lib/systemd/system和/etc/systemd/system目录。 （3）配置文件的位置，以前init进程的配置文件是/etc/inittab，各种服务的配置文件存放在/etc/sysconfig目录。现在的配置文件主要存放在/lib/systemd目录，在/etc/systemd目录里面的修改可以覆盖原始设置。\n日志管理 Systemd 统一管理所有 Unit 的启动日志。带来的好处就是，可以只用journalctl一个命令，查看所有日志（内核日志和应用日志）。日志的配置文件是/etc/systemd/journald.conf。\njournalctl功能强大，用法非常多。\n# 查看所有日志（默认情况下 ，只保存本次启动的日志） $ sudo journalctl # 查看内核日志（不显示应用日志） $ sudo journalctl -k # 查看系统本次启动的日志 $ sudo journalctl -b $ sudo journalctl -b -0 # 查看上一次启动的日志（需更改设置） $ sudo journalctl -b -1 # 查看指定时间的日志 $ sudo journalctl --since=\u0026#34;2012-10-30 18:17:16\u0026#34; $ sudo journalctl --since \u0026#34;20 min ago\u0026#34; $ sudo journalctl --since yesterday $ sudo journalctl --since \u0026#34;2015-01-10\u0026#34; --until \u0026#34;2015-01-11 03:00\u0026#34; $ sudo journalctl --since 09:00 --until \u0026#34;1 hour ago\u0026#34; # 显示尾部的最新10行日志 $ sudo journalctl -n # 显示尾部指定行数的日志 $ sudo journalctl -n 20 # 实时滚动显示最新日志 $ sudo journalctl -f # 查看指定服务的日志 $ sudo journalctl /usr/lib/systemd/systemd # 查看指定进程的日志 $ sudo journalctl _PID=1 # 查看某个路径的脚本的日志 $ sudo journalctl /usr/bin/bash # 查看指定用户的日志 $ sudo journalctl _UID=33 --since today # 查看某个 Unit 的日志 $ sudo journalctl -u nginx.service $ sudo journalctl -u nginx.service --since today # 实时滚动显示某个 Unit 的最新日志 $ sudo journalctl -u nginx.service -f # 合并显示多个 Unit 的日志 $ journalctl -u nginx.service -u php-fpm.service --since today # 查看指定优先级（及其以上级别）的日志，共有8级 # 0: emerg # 1: alert # 2: crit # 3: err # 4: warning # 5: notice # 6: info # 7: debug $ sudo journalctl -p err -b # 日志默认分页输出，--no-pager 改为正常的标准输出 $ sudo journalctl --no-pager # 以 JSON 格式（单行）输出 $ sudo journalctl -b -u nginx.service -o json # 以 JSON 格式（多行）输出，可读性更好 $ sudo journalctl -b -u nginx.serviceqq -o json-pretty # 显示日志占据的硬盘空间 $ sudo journalctl --disk-usage # 指定日志文件占据的最大空间 $ sudo journalctl --vacuum-size=1G # 指定日志文件保存多久 $ sudo journalctl --vacuum-time=1years ","permalink":"https://jeremyxu2010.github.io/2016/07/systemd-%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E8%BD%AC%E8%BD%BD/","tags":["linux","systemd"],"title":"Systemd 入门教程（转载）"},{"categories":["devops"],"contents":"工作中需要对mongodb进程进行，控制它最多使用的内存，简单想了一下，想到可以使用linux中的cgroup完成此功能，于是研究了一下cgroup，在这里记录备忘一下。\n概念 CGroup 技术被广泛用于 Linux 操作系统环境下的物理分割，是 Linux Container 技术的底层基础技术，是虚拟化技术的基础。CGroup 是 Control Groups 的缩写，是 Linux 内核提供的一种可以限制、记录、隔离进程组 (process groups) 所使用的物力资源 (如 cpu memory i/o 等等) 的机制。CGroup 是将任意进程进行分组化管理的 Linux 内核功能。CGroup 本身是提供将进程进行分组化管理的功能和接口的基础结构，I/O 或内存的分配控制等具体的资源管理功能是通过这个功能来实现的。这些具体的资源管理功能称为 CGroup 子系统或控制器。CGroup 子系统有控制内存的 Memory 控制器、控制进程调度的 CPU 控制器等。CGroup 提供了一个 CGroup 虚拟文件系统，作为进行分组管理和各子系统设置的用户接口。\n 子系统（subsystem）。一个子系统就是一个资源控制器，比如cpu子系统就是控制cpu时间分配的一个控制器。子系统必须附加（attach）到一个层级上才能起作用，一个子系统附加到某个层级以后，这个层级上的所有控制组群都受到这个子系统的控制。使用lssubsys -a命令查看内核支持的子系统。 层级（hierarchy）。子系统必须附加（attach）到一个层级上才能起作用。使用mkdir -p /cgroup/name \u0026amp;\u0026amp; mount -t cgroup -o subsystems name /cgroup/name命令创建一个层级，并把该层级挂载到目录。 控制组群（control group）。控制组群就是一组按照某种标准划分的进程。cgroups中的资源控制都是以控制组群为单位实现。一个进程可以加入到某个控制组群，也从一个进程组迁移到另一个控制组群。一个进程组的进程可以使用cgroups以控制组群为单位分配的资源，同时受到cgroups以控制组群为单位设定的限制。某一个层级上默认存在一个cgroup_path为/的控制组群，另外还可以创建树形结构的控制组群。使用cgcreate -g subsystems:cgroup_path命令可以创建控制组群。 任务（task）。任务就是系统的一个进程。控制组群所对应的目录中有一个tasks文件，将进程ID写进该文件，该进程就会受到该控制组群的限制。  另外上述4个概念还存在一些规则，如下\n 每次在系统中创建新层级时，该系统中的所有任务都是那个层级的默认 cgroup（我们称之为 root cgroup，此 cgroup 在创建层级时自动创建，后面在该层级中创建的 cgroup 都是此 cgroup 的后代）的初始成员；比如在创建其它控制组群之前，使用cat /cgroup/memory/tasks命令查看一下，就可以看到系统里所有进程的ID都在这儿。 一个子系统最多只能附加到一个层级；比如使用lssubsys -am可以看到memory子系统都附件到memory层级并挂载至/cgroup/memory了，此时就不可再使用mkdir -p /cgroup/memory2 \u0026amp;\u0026amp; mount -t cgroup -o memory memory2 /cgroup/memory2命令将memory子系统再附加到其它层级了。 一个层级可以附加多个子系统；可以使用mkdir -p /cgroup/cpu,cpuacct \u0026amp;\u0026amp; mount -t cgroup -o cpu,cpuacct cpu,cpuacct /cgroup/cpu,cpuacct命令将cpu、cpuacct附加至层级cpu,cpuacct并挂载至/cgroup/cpu,cpuacct 一个任务可以是多个 cgroup 的成员，但是这些 cgroup 必须在不同的层级；比如一个层级里某一个进程ID只能归属于唯一一个控制组群，但该进程ID还可以归属于另一个层级里的唯一一个控制组群。 系统中的进程（任务）创建子进程（任务）时，该子任务自动成为其父进程所在 cgroup 的成员。然后可根据需要将该子任务移动到不同的 cgroup 中，但开始时它总是继承其父任务的 cgroup。  设计成这样的原因如下：\n 因为某个任务可属于任一层级中的单一cgroup，所以只有一个方法可让单一子系统限制或者影响任务。这是合理的：是一个功能，而不是限制。 您可以将几个子系统分组在一起以便它们可影响单一层级中的所有任务。因为该层级中的cgroup有不同的参数设定，因此会对那些任务产生不同的影响。 有时可能需要重构层级。例如：从附加了几个子系统的层级中删除一个子系统，并将其附加到不同的层级中。 反正，如果从不同层级中分离子系统的需求降低，则您可以删除层级并将其子系统附加到现有层级中。 这个设计允许简单的cgroup使用，比如为单一层级中的具体任务设定几个参数，单一层级可以是只附加了cpu和memeory子系统的层级。 这个设计还允许高精度配置：系统中的每个任务（进程）都可以是每个层级的成员，每个层级都有单一附加的子系统。这样的配置可让系统管理员绝对控制每个单一任务的所有参数。\n 子系统 cgroups为每种可以控制的资源定义了一个子系统。典型的子系统介绍如下：\n cpu 子系统，主要限制进程的 cpu 使用率。 cpuacct 子系统，可以统计 cgroups 中的进程的 cpu 使用报告。 cpuset 子系统，可以为 cgroups 中的进程分配单独的 cpu 节点或者内存节点。 memory 子系统，可以限制进程的 memory 使用量。 blkio 子系统，可以限制进程的块设备 io。 devices 子系统，可以控制进程能够访问某些设备。 net_cls 子系统，可以标记 cgroups 中进程的网络数据包，然后可以使用 tc 模块（traffic control）对数据包进行控制。 freezer 子系统，可以挂起或者恢复 cgroups 中的进程。 ns 子系统，可以使不同 cgroups 下面的进程使用不同的 namespace。  cgconfig.conf 文件 上面都是使用命令控制层级和控制组群，而且并没有将创建的结果存储下来，系统重启后创建的结果就没了。所以系统提供了cgconfig.conf文件，cgconfig 服务启动时会读取 cgroup 配置文件 \u0026ndash; /etc/cgconfig.conf。根据配置文件的内容，cgconfig 可创建层级、挂载所需文件系统、创建 cgroup 以及为每个组群设定子系统参数。\ncgconfig.conf 文件包含两个主要类型的条目 \u0026ndash; mount 和 group。挂载条目生成并挂载层级并将其作为虚拟文件系统，同时将子系统附加到那些层级中。挂载条目使用以下语法定义：\nmount { \u0026lt;controller\u0026gt; = \u0026lt;path\u0026gt;; … } 组群条目创建 cgroup 并设定子系统参数。组群条目使用以下语法定义：\ngroup \u0026lt;name\u0026gt; { [\u0026lt;permissions\u0026gt;] \u0026lt;controller\u0026gt; { \u0026lt;param name\u0026gt; = \u0026lt;param value\u0026gt;; … } … } 请注意 permissions 部分是可选的。要为组群条目定义权限，请使用以下语法：\nperm { task { uid = \u0026lt;task user\u0026gt;; gid = \u0026lt;task group\u0026gt;; } admin { uid = \u0026lt;admin name\u0026gt;; gid = \u0026lt;admin group\u0026gt;; } } 这个表示\u0026lt;admin group\u0026gt;:\u0026lt;admin name\u0026gt;的用户可以调整该控制组群中子系统的参数，而\u0026lt;task group\u0026gt;:\u0026lt;task user\u0026gt;的用户可以读写该控制组群中的tasks文件。\n还是举个实例：\nmount { cpuset = /cgroup/cpuset; cpu = /cgroup/cpu; cpuacct = /cgroup/cpuacct; memory = /cgroup/memory; devices = /cgroup/devices; freezer = /cgroup/freezer; net_cls = /cgroup/net_cls; blkio = /cgroup/blkio; } group mysql_g1 { perm { task { uid = root; gid = sqladmin; } admin { uid = root; gid = root; } } cpu { cpu.cfs_quota_us = 50000; cpu.cfs_period_us = 100000; } cpuset { cpuset.cpus = \u0026#34;3\u0026#34;; cpuset.mems = \u0026#34;0\u0026#34;; } memory { memory.limit_in_bytes=104857600; memory.swappiness=0; } blkio { blkio.throttle.read_bps_device=\u0026#34;8:0 524288\u0026#34;; blkio.throttle.write_bps_device=\u0026#34;8:0 524288\u0026#34;; } } group mongodb_g1 { perm { task { uid = root; gid = mongodbadmin; } admin { uid = root; gid = root; } } cpu { cpu.cfs_quota_us = 50000; cpu.cfs_period_us = 100000; } } 将某个进程移动到控制组群中 有3个办法完成这个操作：\n ###cgclassify 命令### 可以运行 cgclassify 命令将进程移动到 cgroup 中，cgclassify 的语法为：cgclassify -g subsystems:path_to_cgroup pidlist，其中：  subsystems 是用逗号分开的子系统列表，或者 * 启动与所有可用子系统关联的层级中的进程。请注意：如果在多个层级中有同名的 cgroup，则 -g 选项会将该进程移动到每个组群中。 path_to_cgroup 是到其层级中的 cgroup 的路径 pidlist 是用空格分开的进程识别符（PID）列表 还可以在 pid 前面添加 \u0026ndash; sticky 选项以保证所有子进程位于同一 cgroup 中。 举个例子    cgclassify -g cpu,memory:mysql_g1 1701  ###cgred 守护进程### Cgred 是一个守护进程，它可根据在 /etc/cgrules.conf 文件中设定的参数将任务移动到 cgroup 中。/etc/cgrules.conf 文件中的条目可以使用以下两个格式之一：  user hierarchies control_group user:command hierarchies control_group 举个例子\nmaria:ftp\tdevices\t/usergroup/staff/ftp   ###cgexec 命令### 可以运行 cgexec 命令在 cgroup 中启动进程。cgexec 语法为：cgexec -g subsystems:path_to_cgroup command arguments ，其中：\n subsystems 是用逗号分开的子系统列表或者 * 启动与所有可用子系统关联的层级中的进程。请注意：如 第 2.7 节 “设置参数” 所述的cgset，如果在多个层级中有同名的 cgroup，-g 选项会在每个组群中创建进程。 path_to_cgroup 是到与该层级相关的 cgroup 的路径。 command 是要运行的命令 arguments 是该命令所有参数  举个例子\n  cgexec -g cpu:group1 lynx http://www.redhat.com 设置子系统参数 可以运行 cgset 命令设定子系统参数。cgset 的语法为：cgset -r parameter=value path_to_cgroup ，其中：\n parameter 是要设定的参数，该参数与给定 cgroup 的目录中的文件对应。 value 是为参数设定的值 path_to_cgroup 是到相对该层级 root 的 cgroup 路径  举个例子：\ncgset -r cpuset.cpus=0-1 group1 可以使用 cgset 将一个 cgroup 中的参数复制到另一个现有 cgroup 中。使用 cgset 复制参数的语法为：cgset --copy-from path_to_source_cgroup path_to_target_cgroup，其中：\n path_to_source_cgroup 是相对该层级中 root 组群，到要复制其参数的 cgroup 的路径。 path_to_target_cgroup 是相对该层级 root 组群的目的 cgroup 的路径。  举个例子：\ncgset --copy-from group1/ group2/ 获取子系统参数 可以运行 cgget 命令获取子系统参数。cgget 的语法为：cgget -r parameter list_of_cgroups，其中：\n parameter 是包含子系统值的伪文件 list_of_cgroups 是用空格分开的 cgroup 列表  举个例子：\ncgget -r cpuset.cpus -r memory.limit_in_bytes lab1 lab2 如果不知道参数名称，请使用类似命令：\ncgget -g cpuset lab1 lab2 子系统可调参数 子系统可调参数比较多，可参考https://access.redhat.com/documentation/zh-CN/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/ch-Subsystems_and_Tunable_Parameters.html\n应用实例 在我的场景里需要对MongoDB服务进行资源限制，限制MongoDB服务所使用的内存在500M以内。\n修改/etc/cgconfig.conf文件：\nmount { cpuset = /cgroup/cpuset; cpu = /cgroup/cpu; cpuacct = /cgroup/cpuacct; memory = /cgroup/memory; devices = /cgroup/devices; freezer = /cgroup/freezer; net_cls = /cgroup/net_cls; blkio = /cgroup/blkio; } group mongodb_g1 { memory { memory.limit_in_bytes=524288000; memory.swappiness=0; } } 然后修改/etc/init.d/mongod，使用cgexec启动mongod进程\n/sbin/startproc -u $MONGOD_USER -g $MONGOD_GROUP -s -e /usr/bin/cgexec -g memory:mongodb_g1 $MONGOD_BIN --config /etc/mongod.conf run 参考 https://access.redhat.com/documentation/zh-CN/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/ch01.html http://www.ibm.com/developerworks/cn/linux/1506_cgroup/index.html http://colobu.com/2015/07/23/Using-Cgroups-to-Limit-MongoDB-memory-usage/\n","permalink":"https://jeremyxu2010.github.io/2016/07/%E4%BD%BF%E7%94%A8cgroup%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%BA%90/","tags":["linux","cgroup"],"title":"使用cgroup控制系统资源"},{"categories":["容器编排"],"contents":"今天又抽时间研究了一下如何搭建docker集群，终于找到配合consul、docker-machine、swarm搭建一个简易docker集群的办法，在这里记录一下。\n创建一个consul数据库 首先需要创建一个用于swarm集群节点服务发现、健康检测的consul数据库。\n#这里`https://xxxx.mirror.aliyuncs.com`参见上一篇文件里所提及的阿里云registry加速地址 docker-machine create -d virtualbox --engine-registry-mirror=https://xxxx.mirror.aliyuncs.com consul-servers #设置docker命令连接consul-servers依赖的环境变量 eval $(docker-machine env consul-servers) #启动第一个consul server节点容器 docker run --name consul-server1 --restart=always -d progrium/consul -server -bootstrap-expect 3 -ui-dir /ui JOIN_IP=$(docker inspect -f \u0026#39;{{ .NetworkSettings.IPAddress }}\u0026#39; consul-server1) #启动第二个consul server节点容器，并加入consul集群中 docker run -d --name consul-server2 -h consul-server2 progrium/consul -server -join $JOIN_IP #启动第三个consul server节点容器，并加入consul集群中 docker run -d --name consul-server3 -h consul-server3 progrium/consul -server -join $JOIN_IP #启动第一个consul client节点容器，并加入consul集群中 docker run -d --restart=always -p 8400:8400 -p 8500:8500 -p 8600:53/udp --name consul-client1 -h consul-client1 progrium/consul -join $JOIN_IP #启动第二个consul client节点容器，并加入consul集群中 docker run -d --restart=always -p 8401:8400 -p 8501:8500 -p 8601:53/udp --name consul-client2 -h consul-client2 progrium/consul -join $JOIN_IP 这里解释一下启动容器时的一些参数 --name consul 指定容器的名称为consul --restart=always 指定当容器退出时自动重启 -p 8400:8400 将容器的8400端口映射至docker host的8400端口，这个是consul的RPC端口 -p 53:53/udp 将容器的UDP 53端口映射至docker host的UDP 53端口，这个是consul内置的DNS Server端口 -d 容器放在后台运行 -server consul在容器里以server模式运行 -bootstrap-expect 3 至少3个consul agent接入进来，则认为可以开始自启动了，设置集群当前状态为可工作。 -p 8500:8500 将容器的8500端口映射至docker host的8500端口，这个是consul的HTTP端口 -ui-dir /ui 启用consul的WebUI，访问地址为http://${docker_host_ip}:8500\n创建swarm主节点 理论上这时应该开始创建swarm相关节点了，并将swarm相关节点加入到swarm集群了。但研究docker-machine的命令行参数，发现它其实支持一条命令自动创建。\n#首先拿到`consul-servers`这个docker host IP地址，假设拿到的IP地址为${consul_client_ip} docker-machine ip consul-servers #创建swarm第一个主节点，swarm agent节点，并告知swarm使用consul的集群节点发现服务 docker-machine create -d virtualbox --engine-registry-mirror=https://xxxx.mirror.aliyuncs.com --swarm --swarm-master --swarm-opt=\u0026#34;replication\u0026#34; --swarm-discovery=\u0026#34;consul://${consul_client_ip}:8500\u0026#34; --engine-opt=\u0026#34;cluster-store=consul://${consul_client_ip}:8500\u0026#34; --engine-opt=\u0026#34;cluster-advertise=eth1:2376\u0026#34; swarm-master1 #创建swarm第二个主节点，swarm agent节点，并告知swarm使用consul的集群节点发现服务 docker-machine create -d virtualbox --engine-registry-mirror=https://xxxx.mirror.aliyuncs.com --swarm --swarm-master --swarm-opt=\u0026#34;replication\u0026#34; --swarm-discovery=\u0026#34;consul://${consul_client_ip}:8500\u0026#34; --engine-opt=\u0026#34;cluster-store=consul://${consul_client_ip}:8500\u0026#34; --engine-opt=\u0026#34;cluster-advertise=eth1:2376\u0026#34; swarm-master2 这里解释一下上面创建docker host的命令 --swarm --swarm-master 需要在docker host里创建swarm的主节点容器 --swarm-opt=\u0026quot;replication\u0026quot; 启用swarm主节点之间的复制功能 --swarm-discovery=\u0026quot;consul://${consul_client_ip}:8500\u0026quot; 指定swarm所使用的集群节点发现服务地址为consul://${consul_client_ip}:8500 --engine-opt=\u0026quot;cluster-store=consul://${consul_client_ip}:8500\u0026quot; 设置docker host所使用的集群KV数据库地址cluster-store=consul://${consul_client_ip}:8500\n最后我们看一眼这个docker host上的容器列表\n$ \u0026gt; eval $(docker-machine env swarm-master1) $ \u0026gt; docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ba1ed23055bf swarm:latest \u0026#34;/swarm join --advert\u0026#34; About an hour ago Up About an hour swarm-agent aff1d873fafc swarm:latest \u0026#34;/swarm manage --tlsv\u0026#34; About an hour ago Up About an hour swarm-agent-master 很清楚吧，一个是swarm-agent容器，一个是swarm-master容器。\n再创建一个swarm节点 docker-machine create -d virtualbox --engine-registry-mirror=https://xxxx.mirror.aliyuncs.com --swarm --swarm-discovery=\u0026#34;consul://${consul_client_ip}:8501\u0026#34; --engine-opt=\u0026#34;cluster-store=consul://${consul_client_ip}:8501\u0026#34; --engine-opt=\u0026#34;cluster-advertise=eth1:2376\u0026#34; swarm-node1 上述命令就不解释了，直接看创建的容器吧\n$ \u0026gt; eval $(docker-machine env swarm-node1) $ \u0026gt; docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 527d83c7d1a9 swarm:latest \u0026#34;/swarm join --advert\u0026#34; About an hour ago Up About an hour swarm-agent 检查docker集群 docker集群创建好了，用docker客户端连上去查看一下集群的状况。\n$ \u0026gt; eval $(docker-machine env --swarm swarm-master1) $ \u0026gt; docker info Containers: 5 Running: 5 Paused: 0 Stopped: 0 Images: 3 Server Version: swarm/1.2.3 Role: primary Strategy: spread Filters: health, port, containerslots, dependency, affinity, constraint Nodes: 3 swarm-master1: 192.168.99.109:2376 └ ID: XTQ5:VQY2:OA7Y:NJ3E:6IOV:IJEA:WGCR:YQO4:4Z7L:WZOO:T3MH:BXLE └ Status: Healthy └ Containers: 2 └ Reserved CPUs: 0 / 1 └ Reserved Memory: 0 B / 1.021 GiB └ Labels: executiondriver=, kernelversion=4.4.12-boot2docker, operatingsystem=Boot2Docker 1.11.2 (TCL 7.1); HEAD : a6645c3 - Wed Jun 1 22:59:51 UTC 2016, provider=virtualbox, storagedriver=aufs └ UpdatedAt: 2016-06-29T15:50:13Z └ ServerVersion: 1.11.2 swarm-master2: 192.168.99.110:2376 └ ID: EDG6:QK6E:EWY5:SKJK:Y2KI:YURB:C3LV:2WDR:THNP:A5VB:WLQZ:2G4S └ Status: Healthy └ Containers: 2 └ Reserved CPUs: 0 / 1 └ Reserved Memory: 0 B / 1.021 GiB └ Labels: executiondriver=, kernelversion=4.4.12-boot2docker, operatingsystem=Boot2Docker 1.11.2 (TCL 7.1); HEAD : a6645c3 - Wed Jun 1 22:59:51 UTC 2016, provider=virtualbox, storagedriver=aufs └ UpdatedAt: 2016-06-29T15:50:10Z └ ServerVersion: 1.11.2 swarm-node1: 192.168.99.111:2376 └ ID: CD43:U7OL:FIZI:Y753:RBHU:65SL:ZEXK:RIUI:HJTE:UTUG:J3IB:TUAT └ Status: Healthy └ Containers: 1 └ Reserved CPUs: 0 / 1 └ Reserved Memory: 0 B / 1.021 GiB └ Labels: executiondriver=, kernelversion=4.4.12-boot2docker, operatingsystem=Boot2Docker 1.11.2 (TCL 7.1); HEAD : a6645c3 - Wed Jun 1 22:59:51 UTC 2016, provider=virtualbox, storagedriver=aufs └ UpdatedAt: 2016-06-29T15:50:36Z └ ServerVersion: 1.11.2 Plugins: Volume: Network: Kernel Version: 4.4.12-boot2docker Operating System: linux Architecture: amd64 CPUs: 3 Total Memory: 3.063 GiB Name: swarm-master1 Docker Root Dir: Debug mode (client): false Debug mode (server): false WARNING: No kernel memory limit support 这里要注意docker-machine env --swarm swarm-master1，如果不加--swarm参数，则是设置连接swarm-master1这个docker host的环境变量，加了--swarm参数，则是设置连接swarm集群的环境变量。\n注意事项  本方案中考虑了consul集群中consul server节点的单点故障问题，创建了多个consul server节点，如其中有某个consul server节点出现故障，会自动选举出一个新的Leader consul server节点 本方案中通过多个consul client节点减少转发请求至consul server节点的延时及资源消耗，同时也确保部分consul client节点出现故障不会导致整个consul集群不可用 本方案中考虑了swarm集群中swarm manager节点单点故障问题，创建了两个互相复制的swarm manager节点，一旦发觉其中一个出现故障，可很方便地连接另一个swarm manager节点进行操作 在正式生产环境里不会将所有consul节点都部署在一个docker host里。为了确保consul server节点不出现单点故障，一般创建3-5个consul server节点，并将consul server节点部署在不同的docker host里。为了通过多个consul client节点减少转发请求至consul server节点的延时及资源消耗，一般在一个数据中心会在不同的docker host上部署多个consul client节点，一个数据中心的所有swarm节点分成几组，每组里面的所有swarm节点使用一个consul client地址 我自己研究是在一台物理机上使用docker-machine开设多个virtualbox虚机来模拟集群环境的，而如果不指定其它参数，默认docker daemon创建的容器是使用docker0虚拟交换机实现网接接入的。docker0默认仅保证单机上的容器是可通信。而一般真实环境是多个主机上建立集群的，所以可能需要采用划分独立的网段、组VLAN、基于SDN等方式确保多个主机本身可通信，同时在创建docker host时需合理指定cluster-advertise=eth1:2376参数 目前看到简单实现跨主机实现容器间联通有两种方案，一种是自定义Docker Daemon启动时的桥接网桥，见这里；另一种是使用Overlay Network，见这里。我个人是更倾向于后一种，网络安全性更可控一点，只是不知道性能如何，需实际应用场景测试一下。 在使用swarm集群时，还可以自定义调度策略及选择节点的逻辑，可参考《Docker-从入门到实践》书中介绍的“Docker Swarm项目 - 调度器“，“Docker Swarm项目 - 过滤器“  其它集群方式 事实上还是比我这个教程更简单的docker集群创建办法，可参考《Docker-从入门到实践》书中介绍的“Docker Swarm项目 - 使用DockerHub提供的服务发现功能”、“Docker Swarm项目 - 使用文件”这两个章节，但这两个办法存在比较致命的缺陷。第一个方法要求docker集群能通畅地连结国外的DockerHub服务；第2个方法不能很好适合集群节点的动态变更。所以这两种方法都不建议生产环境使用。\n参考  https://docs.docker.com/v1.10/engine/reference/commandline/daemon/ http://dockone.io/article/667 http://dockone.io/article/1298 http://yunshen0909.iteye.com/blog/2245926  ","permalink":"https://jeremyxu2010.github.io/2016/06/%E6%90%AD%E5%BB%BA%E7%AE%80%E6%98%93%E7%9A%84docker%E9%9B%86%E7%BE%A4/","tags":["linux","docker"],"title":"搭建简易的docker集群"},{"categories":["容器编排"],"contents":"花了好几天，终于看完了《Docker进阶与实战》，今天终于抽出时间来实践一把，这里把今天实战的过程记录一下。\n安装Docker 因为我使用的MacOS系统，查阅文档找到在MacOS系统中最简易安装办法如下。\n#使用Homebrew Cask安装virtualbox brew cask install virtualbox #使用Homebrew安装docker-machine brew install docker-machine #使用docker-machine创建名叫mydockerhost的docker主机 docker-machine create --driver=virtualbox mydockerhost 这里创建docker主机时要从github下载一个boot2docker.iso的镜像，下得很慢，我使用axel加速了一下\nwget https://github.com/boot2docker/boot2docker/releases/download/v1.11.2/boot2docker.iso # 这里会进行几次重定向，最后重定向到amazon上s3的一个地址，拷贝这个地址，然后中止下载，然后用axel来多线程下载 axel -o boot2docker.iso -n 20 \u0026#34;${amazon_s3_url}\u0026#34; #最后将下载的iso移至docker的缓存目录 mv boot2docker.iso ~/.docker/machine/cache/boot2docker.iso #重新创始docker主机 docker-machine create --driver=virtualbox mydockerhost #登入docker主机 docker-machine ssh mydockerhost 加速docker抓取镜像 默认安装的docker主机抓取官方镜像是从国外官方registry上下载的，天朝的网速怎么受得了。因此花了些时间找加速的registry。\n首先就是试用daocloud的加速服务。要使用daocloud的加速服务需要在daocloud上安装它的监控程序。安装过程倒是很简单，就是在docker主机上执行命令。\ncurl -sSL https://get.daocloud.io/daomonit/install.sh | sh -s ${你daocloud用户所对应的一长串字符} 但我实际测试发现daocloud的加速服务拉取像ubuntu这样的热门镜像比较快，但提取像jetty:9.2.11-jre7这样的冷门镜像就相当慢，弃之。\n然后试用灵雀云alauda.cn的加速服务，注册它的帐户后，点“免费加速器”，会看到你的加速地址，形如http://${你的帐户名}.m.alauda.cn，然后就修改docker主机里的/var/lib/boot2docker/profile文件，在它的EXTRA_ARGS参数里加入--registry-mirror=http://${你的帐户名}.m.alauda.cn，然后重启docker主机，再次登入docker主机。根据我实际测试，同样拉取像ubuntu这样的热门镜像比较快，但提取像jetty:9.2.11-jre7这样的冷门镜像就相当慢，弃之。\n最后试用阿里云的加速服务，同样注册阿里云帐户登入https://cr.console.aliyun.com之后，点击加速器，它同样给出了加速地址，形如https://xxxxx.mirror.aliyuncs.com，它也是修改EXTRA_ARGS参数法，就不赘述了。经测试无论拉取的是ubuntu之类热门镜像，还是jetty:9.2.11-jre7这样的冷门镜像，速度都相当快，点赞。\n但看过三者的功能后，觉得从三个厂商提供的功能来看是daocloud\u0026gt;alauda\u0026gt;aliyun，aliyun仅仅提供了一个镜像构建管理，而alauda还提供了服务的编排及存储卷管理，daocloud更甚之还提供了管理集群的docker主机功能。阿里云在docker这方面是落后了，好在阿里云有钱基础设施如带宽是好的。\ndocker常用命令 接下来就试用一下docker常用的一些命令\n docker pull centos:6 拉取版本为6的centos镜像 docker pull nginx 拉取版本为latest的nginx镜像 docker pull -a centos 拉取所有版本的centos镜像 docker pull registry.aliyuncs.com/alicloudhpc/toolkit 从其它registry上拉取某个镜像的latest版本 docker push ${你自己的registry地址}/my-centos:6 推送一个镜像至registry私服 docker run -t -i \u0026ndash;rm centos:6 /bin/bash 以交互式方式启动一个centos容器，可在bash里尝试各种命令操作，容器退出时会删除容器 docker run -d \u0026ndash;name=nginx nginx 以后台方式启动一个名称为nginx的nginx容器 docker logs ${containerId} 在外部查看一个容器控制台输出 docker attach ${containerId} 进入容器查看容器控制台输出，注意此时要返回外部千万不能按Ctrl+C，而应该按Ctrl-P Ctrl-Q，按Ctrl+C容器就停止了 docker run -d -p 8000:80 nginx 将容器的80映射至docker主机的8000端口 docker run -d -p 8000:80 -v /tmp/htmlfiles:/usr/share/nginx/html:ro nginx 将docker主机的/tmp/htmlfiles目录只读挂载至容器的/usr/share/nginx/html目录 docker run \u0026ndash;link db -d \u0026ndash;name=nginx nginx 启动一个名称为nginx的nginx容器，它链接一个名叫db的容器（即需要使用其它容器提供的服务） docker commit ${containerId} my-centos:6 将某个容器提交为镜像，镜像的版本为6 docker build -t my-centos:6 . 在当前目录下的Dockerfile构建镜像，镜像的名字为my-centos，版本为6  如果不涉及服务编排及docker主机集群基本就上面这些命令了\n构建自己的镜像 为了易于版本管理，一般构建自己的镜像还是推荐编写Dockerfile的办法。编写Dockerfile比较简单，熟悉linux命令就好办了，我试着也写了一个，地址在这里，写的过程中要注意控制最终镜像的大小，比如Dockerfile的行数要少一些，因为每一条命令后都会在存储上产生一个layer，安装软件包之后要注意及时清理cache等。\n然后本地环境试着构建一下\ncd my-centos6 docker build -t my-centos6 . 很好，一次就成功了。接下来到阿里云的镜像管理里试一下在线构建镜像。\n点\u0026quot;创建镜像仓库\u0026rdquo;，在出现的表单里填写该镜像的信息，其中“代码源”的地址我选的github，构建设置我选择“master”分支的代码构建镜像版本为“latest”，“6”tag的代码构建镜像版本为“6”。镜像仓库创建好了之后，点“立即构建”，两个镜像就会快速构建，速度很快。构建完毕之后，该镜像就可以在dev.aliyun.com容器Hub里搜索到。也即别人可以通过docker pull registry.aliyuncs.com/jeremyxu2010/my-centos6抓取到我制作的镜像。\n接下来的计划 今天只是简单地试用了docker的功能，接下来抽时间实践一下docker里服务编排及docker主机集群功能。\n参考  https://docs.docker.com/machine/get-started/ https://segmentfault.com/a/1190000000751601  ","permalink":"https://jeremyxu2010.github.io/2016/06/%E8%AF%95%E7%94%A8docker%E5%8A%9F%E8%83%BD/","tags":["linux","docker"],"title":"试用docker功能"},{"categories":["java开发"],"contents":"起因 以前也用Netty做到异步网络编程，用过之后也一直没想过要把Netty拿起来重新研究一翻，直到上周工作中遇到一个棘手的问题。 在我们的项目中基于netty-socketio，我们实现了一个基于WebSocket的浏览器与服务端的请求回应机制。\n这里贴一下该机制的大概的代码逻辑，真实项目比这复杂得多。\nimport com.corundumstudio.socketio.AckRequest; import com.corundumstudio.socketio.Configuration; import com.corundumstudio.socketio.SocketConfig; import com.corundumstudio.socketio.SocketIOClient; import com.corundumstudio.socketio.annotation.OnConnect; import com.corundumstudio.socketio.annotation.OnEvent; import com.corundumstudio.socketio.listener.ConnectListener; import com.corundumstudio.socketio.listener.DataListener; import com.corundumstudio.socketio.listener.DisconnectListener; import io.netty.util.concurrent.Future; import io.netty.util.concurrent.GenericFutureListener; import java.util.concurrent.TimeUnit; /** * Created by jeremy on 16/6/18. */ public class SocketIOServer { public static void main(String[] args) throws InterruptedException { SocketIOServer server = new SocketIOServer(); server.start(); TimeUnit.SECONDS.sleep(Long.MAX_VALUE); } private void start() { SocketConfig socketConfig = new SocketConfig(); socketConfig.setReuseAddress(true); Configuration config = new Configuration(); config.setHostname(\u0026#34;0.0.0.0\u0026#34;); config.setPort(8888); config.setSocketConfig(socketConfig); config.setContext(\u0026#34;/socketio\u0026#34;); com.corundumstudio.socketio.SocketIOServer server = new com.corundumstudio.socketio.SocketIOServer(config); server.addListeners(new RequestHandler()); server.startAsync().addListener(new GenericFutureListener\u0026lt;Future\u0026lt;? super Void\u0026gt;\u0026gt;() { public void operationComplete(Future\u0026lt;? super Void\u0026gt; future) throws Exception { if(future.isSuccess()){ System.out.println(\u0026#34;socketio server started.\u0026#34;); } } }); } private class RequestHandler implements ConnectListener, DataListener\u0026lt;String\u0026gt;, DisconnectListener{ public void onConnect(SocketIOClient client) { System.out.println(\u0026#34;client is connected\u0026#34;); } public void onData(SocketIOClient client, String data, AckRequest ackSender) throws Exception { System.out.println(\u0026#34;receive data\u0026#34;); //这里开始解析请求的数据,然后调用后台Controller层相关接口处理请求,处理完毕之后,向客户端返回响应结果  TimeUnit.SECONDS.sleep(20); //这里用20秒来模拟长时间的耗时操作  client.sendEvent(\u0026#34;data\u0026#34;, \u0026#34;data response\u0026#34;); } public void onDisconnect(SocketIOClient client) { System.out.println(\u0026#34;client is disconnected\u0026#34;); } } } 最开始功能一切正常，但最近经过压力测试发现，当请求压力上来之后，很多请求在规则的响应时间内都出现超时失败了。\n分析原因后发现有问题的关键代码如下\nSystem.out.println(\u0026#34;receive data\u0026#34;); //这里开始解析请求的数据,然后调用后台Controller层相关接口处理请求,处理完毕之后,向客户端返回响应结果  TimeUnit.SECONDS.sleep(20); //这里用20秒来模拟长时间的耗时操作  client.sendEvent(\u0026#34;data\u0026#34;, \u0026#34;data response\u0026#34;); 这里实际上是直接使用Netty的NIO线程进行，所以如果业务处理很耗时的话，所以NIO线程都在阻塞等待业务处理完成，这个时候其它请求的IO操作就得不到及时处理。\n解决这个问题先 处理这个问题也比较简单，就是将业务的处理放到业务线程池里处理。如下面的代码\nprivate final ExecutorService businessExecutor = Executors.newFixedThreadPool(10); private class RequestHandler implements ConnectListener, DataListener\u0026lt;String\u0026gt;, DisconnectListener{ public void onConnect(SocketIOClient client) { System.out.println(\u0026#34;client is connected\u0026#34;); } public void onData(SocketIOClient client, String data, AckRequest ackSender) throws Exception { businessExecutor.submit(new ProcessRequestTask(client, data)); } public void onDisconnect(SocketIOClient client) { System.out.println(\u0026#34;client is disconnected\u0026#34;); } } private class ProcessRequestTask implements Runnable { private final String data; private final SocketIOClient client; public ProcessRequestTask(SocketIOClient client, String data) { this.client = client; this.data = data; } public void run() { try { System.out.println(\u0026#34;receive data\u0026#34;); //这里开始解析请求的数据,然后调用后台Controller层相关接口处理请求,处理完毕之后,向客户端返回响应结果  TimeUnit.SECONDS.sleep(20); //这里用20秒来模拟长时间的耗时操作  client.sendEvent(\u0026#34;data\u0026#34;, \u0026#34;data response\u0026#34;); } catch (InterruptedException e) { e.printStackTrace(); } } } 这样Netty的NIO线程就不会由于业务逻辑的处理而阻塞了。\nNetty的线程模型 就着这个问题重新复习一下Netty的线程模型\n总的来说Netty采用的是Reactor主从多线程模型，工作原理是服务端用于接收客户端连接的是一个独立的NIO线程池。Acceptor接收到客户端TCP连接请求处理完成后（可能包含接入认证等），将新创建的SocketChannel注册到IO线程池（sub reactor线程池）的某个IO线程上，由它负责SocketChannel的读写和编解码工作。Acceptor线程池仅仅只用于对接收到的连接作有限的处理，处理完毕后如果链路建立成功，就将链路注册到后端subReactor线程池的IO线程上，由IO线程负责后续的IO操作。\n它的线程模型如下图所示：\n这里结合代码解释一下这里说到的概念\n先看一下普通的服务端启动代码\nServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup).channel(NioServerSocketChannel.class) .option(ChannelOption.SO_REUSEADDR, true) .childOption(ChannelOption.TCP_NODELAY, true) .handler(new CheckAcceptHandler()) .childHandler(new ChannelInitializer\u0026lt;SocketChannel\u0026gt;() { @Override public void initChannel(SocketChannel ch) throws Exception { ch.pipeline().addLast(\u0026#34;decoder\u0026#34;, new LineBasedFrameDecoder(4096)); ch.pipeline().addLast(\u0026#34;stringDecoder\u0026#34;, new StringDecoder(StandardCharsets.UTF_8)); ch.pipeline().addLast(\u0026#34;lineEncoder\u0026#34;, new LineEncoder(StandardCharsets.UTF_8)); ch.pipeline().addLast(\u0026#34;logicHandler\u0026#34;, new ServerLogicalHandler()); } }); final int listenPort = 8888; b.bind(listenPort).sync(); 这里bossGroup就是上面说到的负责接收客户端连接的线程池，workGroup就是subReactor线程池，handler(new ChannelInitializer(){...})就是在指定masterReactor线程池里对接收的连接如何处理的逻辑，childHandler(new ChannelInitializer(){...})就是在指定subReactor线程池里对连接后续的IO操作如何处理的逻辑。workGroup的类型是NioEventLoopGroup，而一个NioEventLoopGroup其实是n个NioEventLoop的组合，经过大量经验测算，这个n设置为CPU核心数的两倍整体IO效率较高，所以在Netty里这个n默认就是CPU核心数的两倍。NioEventLoopGroup里NioEventLoop的数目是有限，而Netty本身同时要处理成千上万连接的IO操作。很容易得出结论在Netty里一个NioEventLoop是同时处理多个连接的IO操作的。\n 一个NioEventLoop聚合了一个多路复用器Selector，因此可以处理成百上千的客户端连接，Netty的处理策略是每当有一个新的客户端接入，则从NioEventLoop线程组中顺序获取一个可用的NioEventLoop，当到达数组上限之后，重新返回到0，通过这种方式，可以基本保证各个NioEventLoop的负载均衡。一个客户端连接只注册到一个NioEventLoop上，这样就避免了多个IO线程去并发操作它。\n  Netty通过串行化设计理念降低了用户的开发难度，提升了处理性能。利用线程组实现了多个串行化线程水平并行执行，线程之间并没有交集，这样既可以充分利用多核提升并行处理能力，同时避免了线程上下文的切换和并发保护带来的额外性能损耗。\n 业务操作耗费太多CPU怎么办 Netty本身是通过串行化设计理念来处理万千上万连接的IO操作的。这里就带来一个问题，有一些业务操作就是要耗费太多CPU时间，如何保证处理这样的业务操作时不阻塞Netty的NIO线程？\nNetty本身提供了EventExecutorGroup作为业务操作线程池，使用示例如下：\nprivate final EventExecutorGroup businessGroup = new DefaultEventExecutorGroup(4); private final EventLoopGroup bossGroup = new NioEventLoopGroup(); private final EventLoopGroup workerGroup = new NioEventLoopGroup(); private void start() throws InterruptedException { ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup).channel(NioServerSocketChannel.class) .option(ChannelOption.SO_REUSEADDR, true) .childOption(ChannelOption.TCP_NODELAY, true) .handler(new CheckAcceptHandler()) .childHandler(new ChannelInitializer\u0026lt;SocketChannel\u0026gt;() { @Override public void initChannel(SocketChannel ch) throws Exception { ch.pipeline().addLast(\u0026#34;decoder\u0026#34;, new LineBasedFrameDecoder(4096)); ch.pipeline().addLast(\u0026#34;stringDecoder\u0026#34;, new StringDecoder(StandardCharsets.UTF_8)); ch.pipeline().addLast(\u0026#34;lineEncoder\u0026#34;, new LineEncoder(StandardCharsets.UTF_8)); ch.pipeline().addLast(businessGroup, \u0026#34;logicHandler\u0026#34;, new ServerLogicalHandler()); } }); final int listenPort = 8888; b.bind(listenPort).sync(); } 可以看到ChannelPipeline的add相关方法均提供了一个重载方法，用来指定Handler将在哪个线程池里运行，如果不指定默认就是在Netty的NIO线程池里。\n问题又来了，前面说过Netty是采用串行化设计理念处理连接上的IO操作的，这样做可以避免线程竞争，线程上下文切换带来的额外性能损耗。那如果NIO线程与业务线程同时往channel里写数据，这个不是破坏了Netty的串行化理念。刚开始我也这么想，可后来看到Netty底层的write方法才发现并不存在这个问题。\nprivate void write(Object msg, boolean flush, ChannelPromise promise) { AbstractChannelHandlerContext next = findContextOutbound(); final Object m = pipeline.touch(msg, next); EventExecutor executor = next.executor(); if (executor.inEventLoop()) { if (flush) { next.invokeWriteAndFlush(m, promise); } else { next.invokeWrite(m, promise); } } else { AbstractWriteTask task; if (flush) { task = WriteAndFlushTask.newInstance(next, m, promise); } else { task = WriteTask.newInstance(next, m, promise); } safeExecute(executor, task, promise, m); } } 这里会判断当前EventExecutor是否是NIO线程，如果是则执行Write操作，否则仅仅是创建一个异步Task，执行该异步Task，该异步Task会到该连接对应的NIO线程里去执行，最终保证还是一个NIO线程串行化地处理该连接上的IO操作。\n当然如果不想用EventExecutorGroup，也可以像最上面的示例一样，将长时间的任务丢进自己的业务线程池里处理。\nNetty源码研究 花了几天时间结合李林峰的帖子自己翻阅Netty的源码，有了不少收获。Netty框架的源代码解读，李林峰的帖子已经把比较重要写了一遍，这里就不赘述了。这里将我发现的几处亮点记录下来以备忘。\n MultithreadEventExecutorGroup是一个多线程的线程池，它提供了一个next方法供外部获取一个EventExecutor来提交Task。而为了MultithreadEventExecutorGroup内部的EventExecutor能比较均衡地干活，每次调用next方法实际上是使用EventExecutorChooser帮助选择一个EventExecutor的，下面是EventExecutorChooser的实现代码  public EventExecutorChooser newChooser(EventExecutor[] executors) { if (isPowerOfTwo(executors.length)) { return new PowerOfTowEventExecutorChooser(executors); } else { return new GenericEventExecutorChooser(executors); } } private static boolean isPowerOfTwo(int val) { return (val \u0026amp; -val) == val; } private static final class PowerOfTowEventExecutorChooser implements EventExecutorChooser { private final AtomicInteger idx = new AtomicInteger(); private final EventExecutor[] executors; PowerOfTowEventExecutorChooser(EventExecutor[] executors) { this.executors = executors; } @Override public EventExecutor next() { return executors[idx.getAndIncrement() \u0026amp; executors.length - 1]; } } private static final class GenericEventExecutorChooser implements EventExecutorChooser { private final AtomicInteger idx = new AtomicInteger(); private final EventExecutor[] executors; GenericEventExecutorChooser(EventExecutor[] executors) { this.executors = executors; } @Override public EventExecutor next() { return executors[Math.abs(idx.getAndIncrement() % executors.length)]; } } 这里针对executors的数目是2的倍数的情况，采用了位操作替换了普通的取模操作。\n Netty内部实现中有很多实例对象内部的状态变更比较频繁，而且这些变更也有可能是由多个线程造成的，为了最小化多线程修改状态所生产的性能开销，同时保证线程安全，采用了大量非阻塞同步的CAS指令实现，这种乐观锁的策略比互斥同步的悲观锁性能高不少。如下面的代码  public abstract class SingleThreadEventExecutor extends AbstractScheduledEventExecutor { ... private volatile int state = ST_NOT_STARTED; ... private static final AtomicIntegerFieldUpdater\u0026lt;SingleThreadEventExecutor\u0026gt; STATE_UPDATER; static{ AtomicIntegerFieldUpdater\u0026lt;SingleThreadEventExecutor\u0026gt; updater = PlatformDependent.newAtomicIntegerFieldUpdater(SingleThreadEventExecutor.class, \u0026#34;state\u0026#34;); if (updater == null) { updater = AtomicIntegerFieldUpdater.newUpdater(SingleThreadEventExecutor.class, \u0026#34;state\u0026#34;); } STATE_UPDATER = updater; } ... public boolean isShuttingDown() { return STATE_UPDATER.get(this) \u0026gt;= ST_SHUTTING_DOWN; } ... private void startThread() { if (STATE_UPDATER.get(this) == ST_NOT_STARTED) { if (STATE_UPDATER.compareAndSet(this, ST_NOT_STARTED, ST_STARTED)) { doStartThread(); } } } ... }  NioEventLoop线程除了对多个连接的IO操作进行处理外，本身还需要处理一些定时任务，因而会有以下的代码  protected void run() { for (;;) { ... final int ioRatio = this.ioRatio; if (ioRatio == 100) { processSelectedKeys(); runAllTasks(); } else { final long ioStartTime = System.nanoTime(); processSelectedKeys(); final long ioTime = System.nanoTime() - ioStartTime; runAllTasks(ioTime * (100 - ioRatio) / ioRatio); } ... } } /** * Poll all tasks from the task queue and run them via {@link Runnable#run()} method. This method stops running * the tasks in the task queue and returns if it ran longer than {@code timeoutNanos}. */ protected boolean runAllTasks(long timeoutNanos) { fetchFromScheduledTaskQueue(); Runnable task = pollTask(); if (task == null) { return false; } final long deadline = ScheduledFutureTask.nanoTime() + timeoutNanos; long runTasks = 0; long lastExecutionTime; for (;;) { try { task.run(); } catch (Throwable t) { logger.warn(\u0026#34;A task raised an exception.\u0026#34;, t); } runTasks ++; // Check timeout every 64 tasks because nanoTime() is relatively expensive.  // XXX: Hard-coded value - will make it configurable if it is really a problem.  if ((runTasks \u0026amp; 0x3F) == 0) { lastExecutionTime = ScheduledFutureTask.nanoTime(); if (lastExecutionTime \u0026gt;= deadline) { break; } } task = pollTask(); if (task == null) { lastExecutionTime = ScheduledFutureTask.nanoTime(); break; } } this.lastExecutionTime = lastExecutionTime; return true; } run方法里那段代码逻辑很简单，就是处理一些SelectionKey后，按比例抽出一些CPU时间进行队列中任务的处理。runAllTasks方法里从队列里取出任务处理，而且为了在指定的时间内跳出处理任务的逻辑，而且考虑到ScheduledFutureTask.nanoTime()方法比较耗时，每处理64个任务后检查一次，这里为了数字比较高效，又采用了位操作if ((runTasks \u0026amp; 0x3F) == 0)\n NioEventLoop的run方法里每次loop都需要处理一些SelectionKey，正常办法是使用selector.selectedKeys()拿到SelectionKey的Set，然后依次处理就好了，但Netty为了高效是拿到SelectionKey的数组进行处理的  ... private SelectedSelectionKeySet selectedKeys; ... try { SelectedSelectionKeySet selectedKeySet = new SelectedSelectionKeySet(); Class\u0026lt;?\u0026gt; selectorImplClass = Class.forName(\u0026#34;sun.nio.ch.SelectorImpl\u0026#34;, false, PlatformDependent.getSystemClassLoader()); // Ensure the current selector implementation is what we can instrument.  if (!selectorImplClass.isAssignableFrom(selector.getClass())) { return selector; } Field selectedKeysField = selectorImplClass.getDeclaredField(\u0026#34;selectedKeys\u0026#34;); Field publicSelectedKeysField = selectorImplClass.getDeclaredField(\u0026#34;publicSelectedKeys\u0026#34;); selectedKeysField.setAccessible(true); publicSelectedKeysField.setAccessible(true); selectedKeysField.set(selector, selectedKeySet); publicSelectedKeysField.set(selector, selectedKeySet); selectedKeys = selectedKeySet; logger.trace(\u0026#34;Instrumented an optimized java.util.Set into: {}\u0026#34;, selector); } catch (Throwable t) { selectedKeys = null; logger.trace(\u0026#34;Failed to instrument an optimized java.util.Set into: {}\u0026#34;, selector, t); } private void processSelectedKeys() { if (selectedKeys != null) { processSelectedKeysOptimized(selectedKeys.flip()); } else { processSelectedKeysPlain(selector.selectedKeys()); } } package io.netty.channel.nio; import java.nio.channels.SelectionKey; import java.util.AbstractSet; import java.util.Iterator; final class SelectedSelectionKeySet extends AbstractSet\u0026lt;SelectionKey\u0026gt; { private SelectionKey[] keysA; private int keysASize; private SelectionKey[] keysB; private int keysBSize; private boolean isA = true; SelectedSelectionKeySet() { keysA = new SelectionKey[1024]; keysB = keysA.clone(); } @Override public boolean add(SelectionKey o) { if (o == null) { return false; } if (isA) { int size = keysASize; keysA[size ++] = o; keysASize = size; if (size == keysA.length) { doubleCapacityA(); } } else { int size = keysBSize; keysB[size ++] = o; keysBSize = size; if (size == keysB.length) { doubleCapacityB(); } } return true; } private void doubleCapacityA() { SelectionKey[] newKeysA = new SelectionKey[keysA.length \u0026lt;\u0026lt; 1]; System.arraycopy(keysA, 0, newKeysA, 0, keysASize); keysA = newKeysA; } private void doubleCapacityB() { SelectionKey[] newKeysB = new SelectionKey[keysB.length \u0026lt;\u0026lt; 1]; System.arraycopy(keysB, 0, newKeysB, 0, keysBSize); keysB = newKeysB; } SelectionKey[] flip() { if (isA) { isA = false; keysA[keysASize] = null; keysBSize = 0; return keysA; } else { isA = true; keysB[keysBSize] = null; keysASize = 0; return keysB; } } @Override public int size() { if (isA) { return keysASize; } else { return keysBSize; } } @Override public boolean remove(Object o) { return false; } @Override public boolean contains(Object o) { return false; } @Override public Iterator\u0026lt;SelectionKey\u0026gt; iterator() { throw new UnsupportedOperationException(); } } 看到这里我都惊了，看来Netty团队开发人员连数组与Set两者之间遍历的性能差别都注意到了。\n使用Netty进行网络编程注意事项 读了下述的帖子后，发现使用Netty进行网络编程有不少注意事项，这里列举出来以备忘。\n 操作系统的最大句柄数修改  vim /etc/security/limits.conf * soft　nofile　1000000 * hard　nofile　1000000  尽量不要在Netty的I/O线程上处理业务，业务处理就扔到业务线程池里 IdleStateHandler，ReadTimeoutHandler，WriteTimeoutHandler处理时延要可控，防止时延不可控导致的NioEventLoop被意外阻塞。 合理的心跳周期，具体的心跳周期并没有统一的标准，180S也许是个不错的选择 合理设置接收和发送缓冲区容量，用好AdaptiveRecvByteBufAllocator 使用Netty的内存池，同时注意完成ByteBuf的解码工作之后必须显式的调用ReferenceCountUtil.release(msg)对接收缓冲区ByteBuf进行内存释放  childOption(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT)  NIO线程里尽量异步打日志 调整TCP层面的接收和发送缓冲区大小设置，在Netty中分别对应ChannelOption的SO_SNDBUF和SO_RCVBUF，通常建议值为128K或者256K 对于时延敏感的应用场景需关闭SO_TCPNODELAY Linux内核版本如果大于2.6.35，可考虑开启RPS以实现软中断均衡在多个cpu上，提升网络并行处理性能 尽量使用“零拷贝” 采用高性能的序列化框架，比如Protobuf、thrift 正确地使用互斥同步  始终使用wait循环来调用wait方法，永远不要在循环之外调用wait方法。原因是尽管条件并不满足被唤醒条件，但是由于其它线程意外调用notifyAll()方法会导致被阻塞线程意外唤醒，此时执行条件并不满足，它将破坏被锁保护的约定关系，导致约束失效，引起意想不到的结果； 唤醒线程，应该使用notify还是notifyAll，当你不知道究竟该调用哪个方法时，保守的做法是调用notifyAll唤醒所有等待的线程。从优化的角度看，如果处于等待的所有线程都在等待同一个条件，而每次只有一个线程可以从这个条件中被唤醒，那么就应该选择调用notify   如果一个变量虽然被多线程访问，但只是一个线程写、其它线程读，可考虑只用volatile关键字来保证线程安全  总结 通过这几天阅读帖子及Netty的源码，对Netty框架的理解深入了许多，现在回头一想以前使用Netty做的网络程序还有不少优化空间。另外这里十分感谢李林峰写的一系列Netty相关的帖子，质量相当高。\n参考 http://www.infoq.com/cn/articles/netty-threading-model http://www.infoq.com/cn/articles/netty-server-create http://www.infoq.com/cn/articles/netty-concurrent-programming-analysis http://www.infoq.com/cn/articles/the-multithreading-of-netty-cases http://www.infoq.com/cn/articles/the-multithreading-of-netty-cases-part02 http://www.infoq.com/cn/articles/netty-elegant-exit-mechanism-and-principles http://www.infoq.com/cn/articles/netty-high-performance http://www.infoq.com/cn/articles/netty-million-level-push-service-design-points\n","permalink":"https://jeremyxu2010.github.io/2016/06/netty%E6%A1%86%E6%9E%B6%E7%A0%94%E7%A9%B6/","tags":["java","netty","nio"],"title":"Netty框架研究"},{"categories":["java开发"],"contents":"今天遇到一个很有趣的问题，由于业务要求，需要懒初始化一个实例变量。\n简单方法 很顺手就写出下面的代码。\npublic class LazyFieldInitializer { private Object obj = null; public LazyFieldInitializer(){ } public void someOp(){ if(obj == null){ obj = new Object(); } } public void otherOp(){ } public static void main(String[] args) { LazyFieldInitializer instance = new LazyFieldInitializer(); instance.someOp(); } } 但这种方法存在问题，线程不安全，当两个线程同时调用someOp方法，obj变量被初始化了两次。\n加个锁吧 public class LazyFieldInitializer { private Object obj = null; public LazyFieldInitializer(){ } public void someOp(){ synchronized (this) { if (obj == null) { obj = new Object(); } } } public void otherOp(){ } public static void main(String[] args) { LazyFieldInitializer instance = new LazyFieldInitializer(); instance.someOp(); } } 这种方法虽说没问题，就是效率不高，每次执行someOp方法都会锁this。\n双重校验 好了，来个双重校验吧\npublic class LazyFieldInitializer { private Object obj = null; public LazyFieldInitializer(){ } public void someOp(){ if (obj == null) { synchronized (this) { if(obj == null) { obj = new Object(); } } } } public void otherOp(){ } public static void main(String[] args) { LazyFieldInitializer instance = new LazyFieldInitializer(); instance.someOp(); } } 这次没问题了吧。很可惜还是有问题，关键在obj = new Object();这句，这并非是一个原子操作，事实上在 JVM 中这句话大概做了下面 3 件事情。\n 给obj分配内存 调用Object的构造函数来初始化成员变量 将obj对象指向分配的内存空间（执行完这步obj就为非null了）  这个就是JVM很有特色的指令重排序优化。也就是说上面的第二步和第三步的顺序是不能保证的，最终的执行顺序可能是 1-2-3 也可能是 1-3-2。如果是后者，则在3执行完毕、2 未执行之前，被另一个线程二抢占了，这时 instance 已经是非 null 了（但却没有初始化），这个线程拿着这个obj引用去干活，自然就会出问题。\n规避指令重排序优化的办法  使用volatile关键字禁止指令重排序优化  public class LazyFieldInitializer { private volatile Object obj = null; public LazyFieldInitializer(){ } public void someOp(){ if (obj == null) { synchronized (this) { if (obj == null) { obj = new Object(); } } } } public void otherOp(){ } public static void main(String[] args) { LazyFieldInitializer instance = new LazyFieldInitializer(); instance.someOp(); } } volatile关键字在这里有两层含义，一个是禁止JVM对该变量的指令重排序优化，另一个是使这个变量的修改对其它线程可见。\nJava单例 查阅JVM的指令重排序优化相关文章，还看到Java单例写法的文章，这里小小总结一下。\n/** * Created by jeremy on 16/6/11. * 懒汉模式, 线程不安全 */ public class Singleton1 { private static Singleton1 instance; private Singleton1(){ } public void sayHello(){ System.out.println(\u0026#34;hello\u0026#34;); } public static Singleton1 getInstance(){ if(instance == null){ instance = new Singleton1(); } return instance; } public static void main(String[] args) { Singleton1.getInstance().sayHello(); } } /** * Created by jeremy on 16/6/11. * 懒汉模式, 线程安全 */ public class Singleton2 { private static Singleton2 instance; private Singleton2(){ } public void sayHello(){ System.out.println(\u0026#34;hello\u0026#34;); } public static synchronized Singleton2 getInstance(){ if(instance == null){ instance = new Singleton2(); } return instance; } public static void main(String[] args) { Singleton2.getInstance().sayHello(); } } /** * Created by jeremy on 16/6/11. * 饿汉模式, 类变量类加载时初始化, 线程安全 */ public class Singleton3 { private static Singleton3 instance = new Singleton3(); private Singleton3(){ } public void sayHello(){ System.out.println(\u0026#34;hello\u0026#34;); } public static Singleton3 getInstance(){ return instance; } public static void main(String[] args) { Singleton3.getInstance().sayHello(); } } /** * Created by jeremy on 16/6/11. * 饿汉模式, 类变量类加载时在类的静态初始化块里初始化, 线程安全 */ public class Singleton4 { private static Singleton4 instance = null; static { instance = new Singleton4(); } private Singleton4(){ } public void sayHello(){ System.out.println(\u0026#34;hello\u0026#34;); } public static Singleton4 getInstance(){ return instance; } public static void main(String[] args) { Singleton4.getInstance().sayHello(); } } /** * Created by jeremy on 16/6/11. * 懒汉模式, 采用内部静态类，线程安全 */ public class Singleton5 { private static class Singleton5Holder { private static final Singleton5 instance = new Singleton5(); } private Singleton5(){ } public void sayHello(){ System.out.println(\u0026#34;hello\u0026#34;); } public static Singleton5 getInstance(){ return Singleton5Holder.instance; } public static void main(String[] args) { Singleton5.getInstance().sayHello(); } } /** * Created by jeremy on 16/6/11. * 懒汉模式, 枚举实现, 线程安全 */ public enum Singleton6 { INSTANCE; Singleton6(){ } public void sayHello(){ System.out.println(\u0026#34;hello\u0026#34;); } public static void main(String[] args) { Singleton6.INSTANCE.sayHello(); } } /** * Created by jeremy on 16/6/11. * 懒汉模式, 双重校验, 采用volatile规避指令重排序优化, 线程安全 */ public class Singleton7 { private static volatile Singleton7 instance; private Singleton7(){ } public void sayHello(){ System.out.println(\u0026#34;hello\u0026#34;); } public static Singleton7 getInstance(){ if(instance == null){ synchronized (Singleton7.class){ if(instance == null){ instance = new Singleton7(); } } } return instance; } public static void main(String[] args) { Singleton7.getInstance().sayHello(); } } 总结 Java还是有不少暗坑的，写代码时得小心了。记得大学时看过Java Puzzlers，那上面列举过不少这样的Java陷阱，抽时间要重读一读这本书了。\n","permalink":"https://jeremyxu2010.github.io/2016/06/%E5%AE%9E%E4%BE%8B%E5%8F%98%E9%87%8F%E7%9A%84%E6%87%92%E5%88%9D%E5%A7%8B%E5%8C%96/","tags":["java","concurrent"],"title":"实例变量的懒初始化"},{"categories":["java开发"],"contents":"工作中需要将大量文件从一台服务器传输至另一台服务器，最开始是直接使用基础的TCP编程搞定的。但后来业务上要求两台服务器间只能走HTTP协议，而且还要保证传输过去的文件的完整性。想了下，最后基于WebSocket协议完成了该功能。\n思路  服务器端侦听某端口，接受WebSocket请求，后面可用nginx作反向代理，外部看到的将是80端口 客户端连接服务器的WebSocket地址，连接成功后，首先传送一个NEW_FILE的数据包，里面带上要传输的文件名 服务器端收到NEW_FILE包后，解析出文件名，并创建目标文件，再回复ACK_NEW_FILE的数据包 客户端收到ACK_NEW_FILE的数据包后，检查回应的code，如是成功码，则启动一个线程，该线程负责将源文件的数据封装成多个FILE_DATA数据包，传送这些FILE_DATA数据至服务器端 服务器端接收FILE_DATA数据包，解析出里面的文件数据，将文件数据写入文件 客户端发送完源文件数据后，再传送一个FILE_END数据包，该文件包中带上源文件的MD5值 服务器端收到FILE_END数据包后，比对源文件的MD5值与目标文件的MD5值，如相同，则认为传输成功，并返回ACK_FILE_END数据包，里面带上成功码 客户端收到ACK_FILE_END数据包，检查回应的code，如是成功码，则认为传输成功，否则认为传输失败。  具体实现 以下为示例的简易代码，项目中的代码比这组织得更完善一些。该实现使用了WebSocket的Java实现Java-WebSocket与Java NIO。\nFilePacket.java\nimport java.nio.ByteBuffer; import java.nio.ByteOrder; import java.nio.charset.StandardCharsets; /** * Created by jeremy on 16/6/11. */ public class FilePacket { public static final int P_NEW_FILE = 0x01; public static final int P_ACK_NEW_FILE = 0x02; public static final int P_FILE_DATA = 0x03; public static final int P_FILE_END = 0x04; public static final int P_ACK_FILE_END = 0x05; public static final int SUCCESS_CODE = 0; public static final int ERROR_CODE = -1; private static final int TYPE_LEN = 1; private int type; private final ByteBuffer buffer; public FilePacket(ByteBuffer buffer) { this.buffer = buffer; } public static FilePacket constructNewFilePacket(String fileName) { byte[] bytes = fileName.getBytes(StandardCharsets.UTF_8); ByteBuffer buffer = ByteBuffer.allocate(TYPE_LEN + 4 + bytes.length); buffer.order(ByteOrder.BIG_ENDIAN); buffer.put((byte)P_NEW_FILE); buffer.putInt(bytes.length); buffer.put(bytes); buffer.flip(); return new FilePacket(buffer); } public static FilePacket constructAckNewFilePacket(int code) { ByteBuffer buffer = ByteBuffer.allocate(TYPE_LEN + 1); buffer.order(ByteOrder.BIG_ENDIAN); buffer.put((byte)P_ACK_NEW_FILE); buffer.put((byte)code); buffer.flip(); return new FilePacket(buffer); } public static FilePacket constructFileEndPacket(String digest) { byte[] bytes = digest.getBytes(StandardCharsets.UTF_8); ByteBuffer buffer = ByteBuffer.allocate(TYPE_LEN + 4 + bytes.length); buffer.order(ByteOrder.BIG_ENDIAN); buffer.put((byte)P_FILE_END); buffer.putInt(bytes.length); buffer.put(bytes); buffer.flip(); return new FilePacket(buffer); } public static FilePacket constructAckFileEndPacket(int code) { ByteBuffer buffer = ByteBuffer.allocate(TYPE_LEN + 1); buffer.order(ByteOrder.BIG_ENDIAN); buffer.put((byte)P_ACK_FILE_END); buffer.put((byte)code); buffer.flip(); return new FilePacket(buffer); } public static FilePacket parseByteBuffer(ByteBuffer buffer){ FilePacket p = new FilePacket(buffer); p.parseType(); return p; } private void parseType() { this.type = (int)this.buffer.get(); } public ByteBuffer getBuffer() { return buffer; } public int getType() { return type; } } FileServer.java\nimport org.java_websocket.WebSocket; import org.java_websocket.handshake.ClientHandshake; import org.java_websocket.server.WebSocketServer; import javax.xml.bind.DatatypeConverter; import java.io.IOException; import java.net.InetSocketAddress; import java.net.UnknownHostException; import java.nio.ByteBuffer; import java.nio.channels.ByteChannel; import java.nio.charset.StandardCharsets; import java.nio.file.*; import java.security.MessageDigest; import java.util.EnumSet; import java.util.HashMap; import java.util.Map; import java.util.concurrent.ConcurrentHashMap; import java.util.concurrent.ConcurrentMap; /** * Created by jeremy on 16/6/11. */ public class FileServer extends WebSocketServer { private ConcurrentMap\u0026lt;WebSocket, Map\u0026lt;String, Object\u0026gt;\u0026gt; clients = new ConcurrentHashMap\u0026lt;WebSocket, Map\u0026lt;String, Object\u0026gt;\u0026gt;(); public FileServer(int port) throws UnknownHostException { super(new InetSocketAddress( port )); } @Override public void onOpen(WebSocket webSocket, ClientHandshake clientHandshake) { clients.put(webSocket, new HashMap\u0026lt;String, Object\u0026gt;()); } @Override public void onClose(WebSocket webSocket, int i, String s, boolean b) { clients.remove(webSocket); } @Override public void onMessage(WebSocket webSocket, String s) { // do nothing  } @Override public void onMessage(WebSocket conn, ByteBuffer message) { FilePacket p = FilePacket.parseByteBuffer(message); Map\u0026lt;String, Object\u0026gt; params; ByteChannel fileChannel; MessageDigest md; switch (p.getType()) { case FilePacket.P_NEW_FILE: try{ int fileNameLen = p.getBuffer().getInt(); byte[] fileNameBytes = new byte[fileNameLen]; p.getBuffer().get(fileNameBytes); String fileName = new String(fileNameBytes, StandardCharsets.UTF_8); System.out.println(\u0026#34;receive file request : \u0026#34; + fileName); Path filePath = Paths.get(\u0026#34;/tmp/otherdir\u0026#34;, fileName); fileChannel = Files.newByteChannel(filePath, EnumSet.of(StandardOpenOption.CREATE, StandardOpenOption.WRITE)); params = clients.get(conn); params.put(\u0026#34;fileChannel\u0026#34;, fileChannel); md = MessageDigest.getInstance(\u0026#34;MD5\u0026#34;); params.put(\u0026#34;md\u0026#34;, md); System.out.println(\u0026#34;server accept file request: \u0026#34; + fileName); FilePacket ackP = FilePacket.constructAckNewFilePacket(FilePacket.SUCCESS_CODE); conn.send(ackP.getBuffer()); } catch (Exception e){ System.out.println(\u0026#34;server deny file request\u0026#34;); FilePacket ackP = FilePacket.constructAckNewFilePacket(FilePacket.ERROR_CODE); conn.send(ackP.getBuffer()); } break; case FilePacket.P_FILE_DATA: params = clients.get(conn); fileChannel = (ByteChannel) params.get(\u0026#34;fileChannel\u0026#34;); md = (MessageDigest)params.get(\u0026#34;md\u0026#34;); try { p.getBuffer().mark(); md.update(p.getBuffer()); p.getBuffer().reset(); fileChannel.write(p.getBuffer()); } catch (IOException e){ try { fileChannel.close(); } catch (IOException ignore) { } conn.close(); } break; case FilePacket.P_FILE_END: params = clients.get(conn); fileChannel = (ByteChannel) params.get(\u0026#34;fileChannel\u0026#34;); md = (MessageDigest)params.get(\u0026#34;md\u0026#34;); try { byte[] digest = md.digest(); String localDigest = DatatypeConverter.printHexBinary(digest).toUpperCase(); int digestBytesLen = p.getBuffer().getInt(); byte[] digestBytes = new byte[digestBytesLen]; p.getBuffer().get(digestBytes); String remoteDigest = new String(digestBytes, StandardCharsets.UTF_8); System.out.println(\u0026#34;receive file end, digest : \u0026#34; + remoteDigest); FilePacket ackP; if(localDigest.equals(remoteDigest)){ System.out.println(\u0026#34;file digests are same, send success ack code\u0026#34;); ackP = FilePacket.constructAckFileEndPacket(FilePacket.SUCCESS_CODE); } else { System.out.println(\u0026#34;file digests are not same, send error ack code\u0026#34;); ackP = FilePacket.constructAckFileEndPacket(FilePacket.ERROR_CODE); } conn.send(ackP.getBuffer()); } finally { try { fileChannel.close(); } catch (IOException ignore) { } } break; } } @Override public void onError(WebSocket webSocket, Exception e) { } public static void main(String[] args) throws UnknownHostException, InterruptedException { FileServer s = new FileServer( 8888 ); s.start(); System.out.println( \u0026#34;FileServer started on port: \u0026#34; + s.getPort() ); Thread.sleep(Long.MAX_VALUE); } } FileClient.java\nimport org.java_websocket.client.WebSocketClient; import org.java_websocket.drafts.Draft_17; import org.java_websocket.handshake.ServerHandshake; import javax.xml.bind.DatatypeConverter; import java.net.URI; import java.net.URISyntaxException; import java.nio.ByteBuffer; import java.nio.ByteOrder; import java.nio.channels.ByteChannel; import java.nio.file.Files; import java.nio.file.Path; import java.nio.file.Paths; import java.nio.file.StandardOpenOption; import java.security.MessageDigest; import java.util.EnumSet; import java.util.concurrent.atomic.AtomicBoolean; /** * Created by jeremy on 16/6/11. */ public class FileClient implements Runnable{ private final String wsUrl; private final Path filePath; private WebSocketClient wsclient; private volatile AtomicBoolean running = new AtomicBoolean(false); public FileClient(String wsUrl, Path filePath) { this.wsUrl = wsUrl; this.filePath = filePath; } public static void main(String[] args) throws InterruptedException { FileClient fileClient = new FileClient(\u0026#34;ws://127.0.0.1:8888\u0026#34;, Paths.get(\u0026#34;/tmp/onedir\u0026#34;, \u0026#34;test1.txt\u0026#34;)); fileClient.start(); fileClient.await(); } private void await() { while(running.get()){ synchronized (running) { try { running.wait(2000L); } catch (InterruptedException e) { e.printStackTrace(); } } } } private void start() { Thread t = new Thread(this); t.start(); running.set(true); } public void run() { try { wsclient = new WebSocketClient(new URI(this.wsUrl), new Draft_17()) { @Override public void onOpen(ServerHandshake serverHandshake) { String fileName = FileClient.this.filePath.getFileName().toString(); System.out.println(\u0026#34;request send file : \u0026#34; + fileName); FilePacket p = FilePacket.constructNewFilePacket(fileName); this.send(p.getBuffer().array()); } @Override public void onMessage(String s) { // do nothing  } @Override public void onMessage(ByteBuffer bytes) { FilePacket p = FilePacket.parseByteBuffer(bytes); int code; switch (p.getType()) { case FilePacket.P_ACK_NEW_FILE: code = (int)p.getBuffer().get(); if(FilePacket.SUCCESS_CODE == code){ System.out.println(\u0026#34;server accept file request\u0026#34;); startSendFileData(); } break; case FilePacket.P_ACK_FILE_END: code = (int)p.getBuffer().get(); if(FilePacket.SUCCESS_CODE == code){ System.out.println(\u0026#34;server save file sucessfully\u0026#34;); wsclient.close(); } break; } } @Override public void onClose(int i, String s, boolean b) { stop(); } @Override public void onError(Exception e) { stop(); } }; wsclient.connect(); } catch (URISyntaxException e) { e.printStackTrace(); stop(); } } private void stop(){ running.set(false); synchronized (running){ running.notify(); } } private void startSendFileData() { Runnable runnable = new Runnable() { public void run() { try { ByteChannel fileChannel = Files.newByteChannel(FileClient.this.filePath, EnumSet.of(StandardOpenOption.READ)); ByteBuffer buffer = ByteBuffer.allocate(1 + 4096); buffer.order(ByteOrder.BIG_ENDIAN); MessageDigest md = MessageDigest.getInstance(\u0026#34;MD5\u0026#34;); int bytesRead = -1; buffer.clear();//make buffer ready for write  buffer.put((byte)FilePacket.P_FILE_DATA); while((bytesRead = fileChannel.read(buffer)) != -1){ buffer.flip(); //make buffer ready for read  buffer.mark(); buffer.get(); //skip a byte  md.update(buffer); buffer.reset(); FileClient.this.wsclient.getConnection().send(buffer); buffer.clear(); //make buffer ready for write  buffer.put((byte)FilePacket.P_FILE_DATA); } byte[] digest = md.digest(); String digestInHex = DatatypeConverter.printHexBinary(digest).toUpperCase(); System.out.println(\u0026#34;send file finished, digest: \u0026#34; + digestInHex); FilePacket p = FilePacket.constructFileEndPacket(digestInHex); FileClient.this.wsclient.getConnection().send(p.getBuffer()); } catch (Exception e) { wsclient.close(); } } }; new Thread(runnable).start(); } } 注意事项  为了清除内存byte数组拷贝，全部使用的是Java NIO的Buffer，所以要注意flip、clear、mark、reset、compact的用法，用惯了Netty的Buffer，再用Java NIO的Buffer还真是不习惯 服务器端与客户端传输了int，为了避免大小端问题，最好显式设置ByteOrder，buffer.order(ByteOrder.BIG_ENDIAN); 为了提高文件操作效率，全部使用Java NIO File API，特别要注意打开文件的方式，ByteChannel fileChannel = Files.newByteChannel(FileClient.this.filePath, EnumSet.of(StandardOpenOption.READ));，这个跟Old File API有些不一样，在打开文件Channel时必须指定Channel的操作方式，详见java.nio.file.StandardOpenOption  ","permalink":"https://jeremyxu2010.github.io/2016/06/%E9%80%9A%E8%BF%87websocket%E4%BC%A0%E8%BE%93%E6%96%87%E4%BB%B6/","tags":["java","nio","websocket"],"title":"通过WebSocket传输文件"},{"categories":["nodejs开发"],"contents":"这几天使用WebDriver写了个网页黑盒测试脚本，使用的是NodeJS SDK，但脚本运行时间比较长时，感觉node进程的内存占用越来越多，应该是内存泄露。这里把分析的过程记录一下。\n原始代码 我的需求是打开一个网页，然后隔一段时间做一下鼠标移动操作，因此写了个简单的测试代码如下\ntestLeak1.js\n\u0026#39;use strict\u0026#39;; const webdriver = require(\u0026#39;selenium-webdriver\u0026#39;), By = webdriver.By; const driver = new webdriver.Builder() .forBrowser(\u0026#39;chrome\u0026#39;) .build(); driver.get(\u0026#39;https://www.baidu.com/\u0026#39;); const interval = 50; driver.call(function* () { let el = yield driver.findElement(By.id(\u0026#39;kw\u0026#39;)); while(true){ yield driver.actions().mouseMove(el).perform(); yield driver.sleep(interval); } }); 然后运行它\nnode --max_semi_space_size=30 --max_old_space_size=60 --optimize_for_size ./test/testLeak1.js 为了让node进程不至于使用过多进程，我设置好几个v8的参数，v8的参数其实还是很容易查看说明的，可直接使用命令查看\n$ node --v8-options | sed -n \u0026#39;/max_semi_space_size/N; /max_semi_space_size/p\u0026#39; --max_semi_space_size (max size of a semi-space (in MBytes), the new space consists of twosemi-spaces) type: int default: 0  \u0026ndash;max_semi_space_size=30 设置半个new space的大小为30M(我这个压力测试会频繁申请释放内存，所以将new space设置大一点) \u0026ndash;max_old_space-size=60 设置old space的大小为60M \u0026ndash;optimize_for_size 在占用内存与程序执行速度间取得平衡  使用rrdtool查看heap堆大小的变化情况 最开始我使用node-inspector查看node进程的heap内存占用情况。操作如下：\nnpm install -g node-inspector 然后以调用方式启动node进程\nnode --debug --max_semi_space_size=30 --max_old_space_size=60 --optimize_for_size ./test/testLeak1.js 再打开node-inspector\n./node_modules/.bin/node-inspector 再用浏览器打开http://127.0.0.1:8080/?port=5858，在chrome的开发者工具里可以对node进程Heap进行dump。\n但发现使用node-inspector每次抓取heap快照时会引起GC。所以我使用rrdtool将使用的heap大小记录到rrd文件里。\n在js代码的末尾加入代码\nconst rrdtool = require(\u0026#39;rrdtool\u0026#39;); const db = rrdtool.create(__dirname + \u0026#39;/testLeak1.rrd\u0026#39;, { start: rrdtool.now(), step: 1, force: true }, [ \u0026#39;DS:heap_used:GAUGE:1:U:U\u0026#39;, \u0026#39;RRA:AVERAGE:0.5:3:2400\u0026#39; ]); function updateHeapUsed(){ db.update({\u0026#34;heap_used\u0026#34; : process.memoryUsage().heapUsed}); } setInterval(updateHeapUsed, 1000); 然后定时调用rrdtool生成图片\nwatch -n 5 rrdtool graph /Users/jeremy/dev/git/webdriverdemo/test/testLeak1.png --start now-10m --title \u0026#34;Used Heap Size\u0026#34; --width 800 --height 600 --font DEFAULT:14:monospace DEF:heap_used=/Users/jeremy/dev/git/webdriverdemo/test/testLeak1.rrd:heap_used:AVERAGE AREA:heap_used#ff0000 再用chrome浏览器打开此图片，当然可使用浏览器插件定时作一下刷新\nheap使用情况如下图\n可以看到持续不断地有内存泄露，而且程序运行不到8分钟就OOM退出了。\n再用node-inspector抓几个Heap快照对比一下，发现内存泄露都与ManagedPromise有关。\nManagedPromise是包含在selenium-webdriver库里的，源码在这里。\n阅读了上述的promise.js的源码，WebDriver官方自己实现一个ManagedPromise的原因如下\n The promise module is centered around the {@linkplain ControlFlow}, a class that coordinates the execution of asynchronous tasks. The ControlFlow allows users to focus on the imperative commands for their script without worrying about chaining together every single asynchronous action, which can be tedious and verbose. APIs may be layered on top of the control flow to read as if they were synchronous. For instance, the core {@linkplain ./webdriver.WebDriver WebDriver} API is built on top of the control flow, allowing users to write\n driver.get('http://www.google.com/ncr'); driver.findElement({name: 'q'}).sendKeys('webdriver'); driver.findElement({name: 'btnGn'}).click();   instead of\n driver.get('http://www.google.com/ncr') .then(function() { return driver.findElement({name: 'q'}); }) .then(function(q) { return q.sendKeys('webdriver'); }) .then(function() { return driver.findElement({name: 'btnG'}); }) .then(function(btnG) { return btnG.click(); });  而且并没有发现与之相关的内存泄露报告\n于是怀疑是WebDriver提供的Generator函数执行器有问题，改用co试试\n用co驱动Generator函数 \u0026#39;use strict\u0026#39;; const webdriver = require(\u0026#39;selenium-webdriver\u0026#39;), By = webdriver.By; const co = require(\u0026#39;co\u0026#39;); const driver = new webdriver.Builder() .forBrowser(\u0026#39;chrome\u0026#39;) .build(); const interval = 50; co(function * (){ yield driver.get(\u0026#39;https://www.baidu.com/\u0026#39;); let el = yield driver.findElement(By.id(\u0026#39;kw\u0026#39;)); while(true){ yield driver.actions().mouseMove(el).perform(); yield driver.sleep(interval); } }); const rrdtool = require(\u0026#39;rrdtool\u0026#39;); const db = rrdtool.create(__dirname + \u0026#39;/testLeak2.rrd\u0026#39;, { start: rrdtool.now(), step: 1, force: true }, [ \u0026#39;DS:heap_used:GAUGE:1:U:U\u0026#39;, \u0026#39;RRA:AVERAGE:0.5:3:2400\u0026#39; ]); function updateHeapUsed(){ db.update({\u0026#34;heap_used\u0026#34; : process.memoryUsage().heapUsed}); } setInterval(updateHeapUsed, 1000); 还是泄露地厉害，如下图\n使用bluebird重写循环逻辑 在网上找了下原因，最终找到有人也提出了相同的问题。我分析了下，觉得原因是这样的。无论co还是driver.call，它们作为Generator函数的执行器，都是将Generator函数里的每个yield后面的promise连结成一串的，而这将导致这些promise对象无法被GC。在我的场景里本身就是一个死循环，不停有promise对象加入到这个串里，最终导致进程OOM退出。于是我尝试使用bluebird重写循环逻辑\n\u0026#39;use strict\u0026#39;; const webdriver = require(\u0026#39;selenium-webdriver\u0026#39;), By = webdriver.By; const Promise = require(\u0026#39;bluebird\u0026#39;); const promiseFor = Promise.method(function(condition, action) { if (!condition()) return; return action().then(promiseFor.bind(null, condition, action)); }); const driver = new webdriver.Builder() .forBrowser(\u0026#39;chrome\u0026#39;) .build(); driver.get(\u0026#39;https://www.baidu.com/\u0026#39;); const el = driver.findElement(By.id(\u0026#39;kw\u0026#39;)); const interval = 50; function logErr(e){ console.log(e); } promiseFor(function() { return true; }, function() { return Promise.delay(interval).then(function(){ return driver.actions().mouseMove(el).perform().catch(logErr); }).catch(logErr); }); const rrdtool = require(\u0026#39;rrdtool\u0026#39;); const db = rrdtool.create(__dirname + \u0026#39;/testLeak3.rrd\u0026#39;, { start: rrdtool.now(), step: 1, force: true }, [ \u0026#39;DS:heap_used:GAUGE:1:U:U\u0026#39;, \u0026#39;RRA:AVERAGE:0.5:3:2400\u0026#39; ]); function updateHeapUsed(){ db.update({\u0026#34;heap_used\u0026#34; : process.memoryUsage().heapUsed}); } setInterval(updateHeapUsed, 1000); 如上图所示，这次终于正常了。\n总结 这次诊断NodeJS进程内存泄露问题很周折，但还是学到了不少东西，这里总结一下：\n 尽管ES6并没有规定一个Generator函数状态机里封装状态的个数，但实际操作时不要试图往Generator函数里塞进无尽的状态，目前来说Generator函数的执行器，比如co执行这样的Generator函数是会出问题的，最好采用promiseFor这样的方案来实现相同的功能。 为了安全，Node进程运行时最好按需要指定max_old_space_size参数，如果不指定，max_old_space_size的默认值很大，如果该进程申请内存又比较频繁的话，Node进程占用的内存会变得相当高。 分析NodeJS内存泄露的成因时，可利用node-inspector抓几个Heap快照对比一下，从对比的增量中一般可以看到究竟是什么产生了泄露 查看NodeJS进程的GC行为时，可以加--log_gc、--trace_gc_verbose之类的参数，但在控制台上看到的数字不太直观，也可采用生成rrd文件的方式，改明我也写一下npm库，供大家更直观的查看NodeJS的GC状况。 尽管可以设置--expose_gc参数，然后程序里调用global.gc();显式地进行GC操作，但最好不要这么干，正确的做法还是应该调整--max_semi_space_size、--max_old_space_size、--optimize_for_size参数，让NodeJS运行时寻找更合适的时机进行GC。 存储并查看时序相关的数据，使用rrdtool还是比较方便的。抽空要再看看rrdtool的文档  参考 https://blog.eood.cn/node-js_gc https://developer.chrome.com/devtools/docs/javascript-memory-profiling http://erikcorry.blogspot.ru/2012/11/memory-management-flags-in-v8.html https://github.com/tj/co/issues/180 http://stackoverflow.com/questions/24660096/correct-way-to-write-loops-for-promise\n","permalink":"https://jeremyxu2010.github.io/2016/05/%E5%A4%84%E7%90%86%E4%B8%80%E4%B8%AAnodejs%E7%A8%8B%E5%BA%8F%E5%86%85%E5%AD%98%E6%B3%84%E9%9C%B2%E7%9A%84%E9%97%AE%E9%A2%98/","tags":["nodejs","webdriver","bluebird","rrd"],"title":"处理一个NodeJS程序内存泄露的问题"},{"categories":["nodejs开发"],"contents":"工作中需要对web界面进行测试，在网上找了找解决方案，最终找到了Selenium WebDriver。\nWebDriver简介  The primary new feature in Selenium 2.0 is the integration of the WebDriver API. WebDriver is designed to provide a simpler, more concise programming interface in addition to addressing some limitations in the Selenium-RC API. Selenium-WebDriver was developed to better support dynamic web pages where elements of a page may change without the page itself being reloaded. WebDriver’s goal is to supply a well-designed object-oriented API that provides improved support for modern advanced web-app testing problems.\n  Selenium-WebDriver makes direct calls to the browser using each browser’s native support for automation. How these direct calls are made, and the features they support depends on the browser you are using. Information on each ‘browser driver’ is provided later in this chapter.\n  For those familiar with Selenium-RC, this is quite different from what you are used to. Selenium-RC worked the same way for each supported browser. It ‘injected’ javascript functions into the browser when the browser was loaded and then used its javascript to drive the AUT within the browser. WebDriver does not use this technique. Again, it drives the browser directly using the browser’s built in support for automation.\n 上面的官方介绍，我简单提练一下：\n WebDriver API相对于Selenium Remote Control API来说，虽然同样是控制浏览器，但它的编程接口更加简洁 WebDriver可以应对那些网页本身不重新加载的动态网页。 Selenium Remote Control是采用向浏览器注入javascript脚本来控制浏览器的，但WebDriver与之不同，它是直接使用浏览器内置的自动化支持来控制浏览器的。  WebDriver实际上就像它的名字一样，向上屏蔽各厂商浏览器的差异，提供了一个统一的编程API，方便广大程序员控制浏览器的行为。\nWebDriver的Driver 即然要屏蔽各厂商浏览器的差异，那么各厂商自然需要根据WebDriver规范作出各自的实现。WebDriver官方文档就列出各实现：HtmlUnit Driver、Firefox Driver、InternetExplorerDriver、ChromeDriver、Opera Driver、iOS Driver、Android Driver。这些Driver各有优缺点及各自适用的场景，具体可看官方文档说明。其实一看这些名字就知道是什么意思，要控制哪种浏览器就需要下载安装对应的Driver。比如我这里是Mac OSX系统，而且想控制该系统上的Chrome浏览器，那么就下载chromedriver_mac32.zip(注意该Driver对你的Chrome浏览器有版本要求，要求版本必须是v46-50这个范围)，将该压缩包里的可执行文件放到PATH环境变量目录中，比如放到/usr/local/bin目录中。\nWebDriver的SDK的API介绍 官方还很贴心地为WebDriver提供了更主流语言的SDK。支持的语言有Java、C#、Python、Ruby、Perl、PHP、JavaScript。但我感觉这种测试相关的编程语言最好还是用脚本语言合适一点，改起来很方便，不需要时时编译。因此我最后选择了JavaScript SDK。安装过程见下面的命令：\n//前提是先安装好NodeJS mkdir test \u0026amp;\u0026amp; cd test npm init //这里根据提示一步步初始化一个新的NodeJS项目 npm install selenium-webdriver --save //安装WebDriver JavaScript SDK的npm依赖 使用WebDriver控制浏览器 var webdriver = require(\u0026#39;selenium-webdriver\u0026#39;), By = require(\u0026#39;selenium-webdriver\u0026#39;).By, until = require(\u0026#39;selenium-webdriver\u0026#39;).until; var driver = new webdriver.Builder() .forBrowser(\u0026#39;chrome\u0026#39;) .build(); driver.get(\u0026#39;http://www.google.com/ncr\u0026#39;); driver.findElement(By.name(\u0026#39;q\u0026#39;)).sendKeys(\u0026#39;webdriver\u0026#39;); driver.findElement(By.name(\u0026#39;btnG\u0026#39;)).click(); driver.wait(until.titleIs(\u0026#39;webdriver - Google Search\u0026#39;), 1000); driver.quit(); 上面是一个最简单的例子，它以沙箱方式打开一个Chrome窗口，然后访问http://www.google.com/ncr，再在搜索框中输入webdriver，再点击搜索按钮，最后等待浏览器显示出搜索结果页面后关闭浏览器窗口。\n这个小例子确实简单了一些，接下来我们看一下WebDriver的其它一些常用的API。\n定位UI元素  根据ID定位：driver.findElement(By.id(\u0026lsquo;eleID\u0026rsquo;)); 根据Class类名定位：driver.findElements(By.className(\u0026ldquo;eleCls\u0026rdquo;)) 根据tag名定位：driver.findElement(By.tagName(\u0026lsquo;iframe\u0026rsquo;)); 根据name属性定位：driver.findElement(By.name(\u0026lsquo;eleName\u0026rsquo;)); 根据链接的文字定位：driver.findElement(By.linkText(\u0026lsquo;linkText\u0026rsquo;)); 根据链接的部分文字定位：driver.findElement(By.linkText(\u0026lsquo;partialLinkText\u0026rsquo;)); 根据CSS3的css selector定位：driver.findElement(By.css('#food span.dairy.aged\u0026rsquo;)); 根据XPath定位：driver.findElements(By.xpath(\u0026quot;//input\u0026rdquo;));  这么多种定位UI元素的办法，总有一款可以适应你的需求。我个人比较喜欢使用css selector来定位元素。要得到一个元素的css selector也很简单，只需要使用Chrome的开发者工具查看这个元素，然后在这个元素上右键，点击Copy selector就得到了(当然如有可能最好对得到的css selector简写一下)。\n对UI元素的操作  取得元素的text values: driver.findElement(By.id(\u0026lsquo;elementID\u0026rsquo;)).getText(); 查找多个元素：driver.findElement(By.tagName(\u0026ldquo;select\u0026rdquo;)).findElements(By.tagName(\u0026ldquo;option\u0026rdquo;)); 清空input元素的内容：driver.findElement(By.id(\u0026lsquo;nameInput\u0026rsquo;)).clear(); 向input元素输入文字：driver.findElement(By.id(\u0026lsquo;nameInput\u0026rsquo;)).sendKeys(\u0026lsquo;abcd\u0026rsquo;); 向文件input元素指定文字：driver.findElement(By.id(\u0026lsquo;fileInput\u0026rsquo;)).sendKeys('/tmp/somefile.txt\u0026rsquo;); 点击按钮：driver.findElement(By.id(\u0026lsquo;submit\u0026rsquo;).click(); 提交表单：driver.findElement(By.id(\u0026lsquo;submit\u0026rsquo;).submit();  在窗口或Frame间移动  切换到窗口: driver.switchTo().window(\u0026lsquo;windowName\u0026rsquo;); 切换到Frame: driver.switchTo().frame(\u0026lsquo;frameName\u0026rsquo;); 取得当前窗口的Handle: driver.getWindowHandle(); 列出所有浏览器窗口的Handles: driver.getAllWindowHandles();  操作Alert窗口  点击Alert窗口中的OK：driver.switchTo().alert().accept(); 点击Alert窗口中的Cancel：driver.switchTo().alert().dismiss(); 向Alert窗口输入文字：driver.switchTo().alert().sendKeys(\u0026lsquo;abcd\u0026rsquo;);  操作浏览器的导航及地址栏  导航到某个URL：driver.navigate().to(\u0026lsquo;http://www.baidu.com\u0026rsquo;);或driver.get(\u0026lsquo;http://www.baidu.com\u0026rsquo;); 导航后退：driver.navigate().back(); 导航前进：driver.navigate().forward(); 导航刷新：driver.navigate().refresh();  操作Cookie  得到所有Cookie：driver.manage().getCookies(); 得到某一个Cookie：driver.manage().getCookie(\u0026lsquo;cookieName\u0026rsquo;); 删除所有Cookie：driver.manage().deleteAllCookies(); 删除某一个Cookie：driver.manage().deleteCookie(\u0026lsquo;cookieName\u0026rsquo;); 添加一个Cookie：driver.manage().addCookie(\u0026lsquo;cookieName\u0026rsquo;, \u0026lsquo;cookieValue\u0026rsquo;);  操作窗口  设置窗口位置：driver.manage().window().setPosition(100, 100); 设置窗口大小：driver.manage().window().setSize(1280, 800); 最大化窗口：driver.manage().window().maximize();  高级用户接口  移动鼠标至某个UI元素：driver.actions().mouseMove(ele).perform(); 鼠标按下：driver.actions().mouseDown().perform(); 鼠标抬起：driver.actions().mouseUp().perform(); 拖拽鼠标：driver.actions().dragAndDrop(ele, {x: 100, y: 80}).perform(); 鼠标点击：driver.actions().click().perform(); 鼠标双击：driver.actions().doubleClick().perform(); 按键按下：driver.actions().keyDown(Key.CONTROL).perform(); 按键抬起：driver.actions().keyUp(Key.CONTROL).perform(); 发送按钮：driver.actions().sendKeys().perform(Key.chord(Key.CONTROL, \u0026lsquo;c\u0026rsquo;));  上述这些在actions()与perform()之间的操作是可以串行执行的，如driver.actions().mouseMove(ele).click().perform();\n操作等待  显式等待：driver.sleep(2000); Wait for Expected Condition: driver.wait(until.titleIs(\u0026lsquo;webdriver - Google Search\u0026rsquo;), 5000);  上述Wait for Expected Condition的意思是说等待Condition满足，但如果等待的时间超过指定的值Condition还是没有满足，则抛出异常。第一种方式傻傻地等也不太好，因此一般也推荐使用第二种办法来做操作等待。这样可以尽可能快地完成测试的操作序列。\nJavaScript SDK内置了很多方便产生Condition的方法，如：\n until.ableToSwitchToFrame(\u0026lsquo;frameName\u0026rsquo;); until.alertIsPresent(); until.titleIs(\u0026lsquo;test\u0026rsquo;); until.titleContains(\u0026lsquo;test\u0026rsquo;); until.titleMatches(/test/); until.elementLocated(By.css('.test-cls\u0026rsquo;)); until.elementsLocated(By.css('.test-cls\u0026rsquo;)); until.stalenessOf(ele); until.elementIsVisible(ele); until.elementIsNotVisible(ele); until.elementIsEnabled(ele); until.elementIsDisabled(ele); until.elementIsSelected(ele); until.elementIsNotSelected(ele); unitl.elementTextIs(ele, \u0026lsquo;test\u0026rsquo;); until.elementTextContains(ele, \u0026lsquo;test\u0026rsquo;); until.elementTextMatches(ele, /test/);  上述这些方法含义很明确了，看方法名就可以了。另外自己也可以写产生Condition的方法，如下面的代码：\n//产生是否可以切换至第二个窗口Condition的方法 function ableToSwitchToSecondWindow() { return new Condition(\u0026#39;to be able to switch to second window\u0026#39;, function (driver) { return driver.getAllWindowHandles().then(function(winHandles){ if(winHandles.length === 2) { return true; } else { throw new NoSuchWindowError(\u0026#39;second window is not present\u0026#39;); } }, function(e){ if (!(e instanceof error.NoSuchWindowError)) { throw e; } }); }); }; driver.wait(ableToSwitchToSecondWindow(), 5000); WebDriver JavaScript API大概就上面这些内容了，还是比较简单的。其实我感觉官方的文档还是写得太简略了，只需要有个大致印象，真要查找特别API接口时直接查看selenium-webdriver/lib目录下的源码就好了，npm包的另一好处是基本也不用太写文档，源码即文档。\n特别要注意的地方 绝大部分接口返回值都是Promise 这也是说最前面那个例子本来应该要像下面这样写的\nvar webdriver = require(\u0026#39;selenium-webdriver\u0026#39;), By = require(\u0026#39;selenium-webdriver\u0026#39;).By, until = require(\u0026#39;selenium-webdriver\u0026#39;).until; var driver = new webdriver.Builder() .forBrowser(\u0026#39;chrome\u0026#39;) .build(); driver.get(\u0026#39;http://www.google.com/ncr\u0026#39;) .then(function(){ return driver.findElement(By.name(\u0026#39;q\u0026#39;)).sendKeys(\u0026#39;webdriver\u0026#39;); }) .then(function(){ return driver.findElement(By.name(\u0026#39;btnG\u0026#39;)).click(); }) .then(function(){ return driver.wait(until.titleIs(\u0026#39;webdriver - Google Search\u0026#39;), 1000); }) .then(function(){ return driver.quit(); }); 但这样写就成then hell了，看起来仅仅比那个著名的callback hell好一点点，但仍然很难看。幸好ES6推出了Generator函数，大神也写了co，现在终于可以比较好地解决Promise的then hell问题了。详见我之前关于Generator函数的日志。而且WebDriver JavaScript API自已还提供Generator函数的执行器，连co模块都不用导入了。总之现在可以写成这样了：\nvar webdriver = require(\u0026#39;selenium-webdriver\u0026#39;), By = require(\u0026#39;selenium-webdriver\u0026#39;).By, until = require(\u0026#39;selenium-webdriver\u0026#39;).until; var driver = new webdriver.Builder() .forBrowser(\u0026#39;chrome\u0026#39;) .build(); driver.call(function * () { yield driver.get(\u0026#39;http://www.google.com/ncr\u0026#39;); yield driver.findElement(By.name(\u0026#39;q\u0026#39;)).sendKeys(\u0026#39;webdriver\u0026#39;); yield driver.findElement(By.name(\u0026#39;btnG\u0026#39;)).click(); yield driver.wait(until.titleIs(\u0026#39;webdriver - Google Search\u0026#39;), 1000); yield driver.quit(); }); 虽然JavaScript方法都是异步的，有了Generator函数，至少在形式上很像同步的写法了。\n控制NodeJS主线程 凡是上述使用driver的脚本，其实是交给Driver执行去了，一旦NodeJS将这些脚本交给Driver了，NodeJS主线程的工作就完成了，NodeJS主线程的事件队列里没有其它事件需要处理，因此NodeJS主线程就退出了。但有时我们想在用户自动按Ctrl+C结束脚本执行后做一些清理工作，比如关闭打开的浏览器窗口。于是想了点办法，于是写了下面的代码：\nvar webdriver = require(\u0026#39;selenium-webdriver\u0026#39;), By = require(\u0026#39;selenium-webdriver\u0026#39;).By, until = require(\u0026#39;selenium-webdriver\u0026#39;).until; var driver = new webdriver.Builder() .forBrowser(\u0026#39;chrome\u0026#39;) .build(); driver.call(function * () { while(true){ yield driver.get(\u0026#39;http://www.google.com/ncr\u0026#39;); yield driver.findElement(By.name(\u0026#39;q\u0026#39;)).sendKeys(\u0026#39;webdriver\u0026#39;); yield driver.findElement(By.name(\u0026#39;btnG\u0026#39;)).click(); yield driver.wait(until.titleIs(\u0026#39;webdriver - Google Search\u0026#39;), 1000); } }); var running = true; var rl = null; function shutdown(exitCode){ running = false; if(rl){ rl.close(); } //nodejs主线程退出时一定关闭打开的浏览器  driver.quit().then(function(){ process.exit(exitCode); }, function(e){ process.exit(exitCode); }); } //block to nodejs\u0026#39;s main thread (function wait () { if(running){ setTimeout(wait, 500); } })(); //Windows平台需要这样监听Ctrl+C事件 if (process.platform === \u0026#34;win32\u0026#34;) { rl = require(\u0026#34;readline\u0026#34;).createInterface({ input: process.stdin, output: process.stdout }); rl.on(\u0026#34;SIGINT\u0026#34;, function () { shutdown(0); }); } process.on(\u0026#39;SIGINT\u0026#39;, function() { shutdown(0); }); 关键是使用一个递归调用保持NodeJS主线程的事件队列里一直有事件，另外用户按了Ctrl+C后主动关闭浏览器。\n同时进行多个测试 一开始并不知道WebDriver JavaScript SDK支持多个测试同时进行，因此还搞了个主进程控制多个子进程的实现。主要代码如下：\nparent.js\nvar child_process = require(\u0026#39;child_process\u0026#39;); var process = require(\u0026#39;process\u0026#39;); var co = require(\u0026#39;co\u0026#39;); var child_processes = []; var child_count = process.argv[2]; var running = true; function sleep(ms){ return new Promise(function(resolve, reject){ setTimeout(function(){ if(running){ resolve(); } else { reject(); } }, ms); }); } function restartChildProcess(j){ return function(){ console.log(\u0026#39;restart child process \u0026#39; + j); co(startChildProcess(j)); }; } function * startChildProcess(j){ console.log(\u0026#39;start child process \u0026#39; + j); var p = child_process.exec(\u0026#39;node \u0026#39; + __dirname + \u0026#39;/child.js \u0026#39; + j + \u0026#39; 2\u0026gt;\u0026amp;1\u0026#39; , function(err, stdout,stderr){ if(err){ p.kill(); var idx = child_processes.indexOf(p); if(idx \u0026gt; -1){ child_processes.splice(idx, 1); } p = null; if(running){ process.nextTick(restartChildProcess(j)); } } }); child_processes.push(p); yield sleep(20000); } co(function * (){ for(var j=0; j\u0026lt;child_count; j++){ if(running){ yield * startChildProcess(j); } } }).then(function(){ console.log(\u0026#39;all child processes started\u0026#39;); }); function shutdown(){ running = false; for(var i=0; i\u0026lt;child_processes.length; i++){ child_processes[i].kill(); } } //block to nodejs\u0026#39;s main thread (function wait () { if(running){ setTimeout(wait, 500); } })(); if (process.platform === \u0026#34;win32\u0026#34;) { var rl = require(\u0026#34;readline\u0026#34;).createInterface({ input: process.stdin, output: process.stdout }); rl.on(\u0026#34;SIGINT\u0026#34;, function () { console.log(\u0026#34;Caught interrupt signal\u0026#34;); shutdown(); }); } process.on(\u0026#39;SIGINT\u0026#39;, function() { console.log(\u0026#34;Caught interrupt signal\u0026#34;); shutdown(); }); child.js\n\u0026#39;use strict\u0026#39;; const webdriver = require(\u0026#39;selenium-webdriver\u0026#39;), By = webdriver.By, until = webdriver.until; const process = require(\u0026#39;process\u0026#39;); var driver = new webdriver.Builder().forBrowser(\u0026#39;chrome\u0026#39;).build(); driver.call(function * (){ try { ... } catch (e){ //发生异常时打印异常并退出nodejs\u0026#39;s main thread  console.log(e); shutdown(-1); } }); var running = true; var rl = null; function shutdown(exitCode){ running = false; if(rl){ rl.close(); } //nodejs主线程退出时一定关闭打开的浏览器  driver.quit().then(function(){ process.exit(exitCode); }, function(e){ process.exit(exitCode); }); } //block to nodejs\u0026#39;s main thread (function wait () { if(running){ setTimeout(wait, 500); } })(); if (process.platform === \u0026#34;win32\u0026#34;) { rl = require(\u0026#34;readline\u0026#34;).createInterface({ input: process.stdin, output: process.stdout }); rl.on(\u0026#34;SIGINT\u0026#34;, function () { shutdown(0); }); } process.on(\u0026#39;SIGINT\u0026#39;, function() { shutdown(0); }); 这样写虽然也能解决问题，但每个测试示例都要对应一个node进程，而且还需要一个父node进程，进程数多了之后进程间切换开销也很大。\n后面翻阅selenium-webdriver的源码，在它的examples里找到了parallel_flows.js，原来WebDriver JavaScript SDK本身也是支持多个测试同时进行的。于是改进了原来的代码，如下：\n\u0026#39;use strict\u0026#39;; const webdriver = require(\u0026#39;selenium-webdriver\u0026#39;), By = webdriver.By, until = webdriver.until; const flow_interval = 30000; let drivers = {}; let running = true; function restartFlow(flowNo){ return function(){ if(running) { console.log(\u0026#39;restart flow \u0026#39; + flowNo); console.log(\u0026#39;quit flow \u0026#39; + flowNo + \u0026#39;\\\u0026#39;s driver\u0026#39;); try { drivers[flowNo].controlFlow().reset(); } catch(e){ console.log(\u0026#39;reset flow \u0026#39; + flowNo + \u0026#39;\\\u0026#39;s controlFlow failed: %s\u0026#39;, e); } drivers[flowNo].quit().then(function(){ drivers[flowNo] = null; runOneFlow(flowNo, true); }, function(e){ console.log(\u0026#39;quit flow \u0026#39; + flowNo + \u0026#39; failed: %s\u0026#39;, e); }); } }; } function runOneFlow(flowNo, noSleep){ console.log(\u0026#39;start flow \u0026#39; + flowNo); let flow = new webdriver.promise.ControlFlow() .on(\u0026#39;uncaughtException\u0026#39;, function(e) { //console.log(\u0026#39;uncaughtException in flow %d: %s\u0026#39;, flowNo, e);  process.nextTick(restartFlow(flowNo)); }); let driver = new webdriver.Builder(). forBrowser(\u0026#39;chrome\u0026#39;). setControlFlow(flow). // Comment out this line to see the difference.  build(); drivers[flowNo]=driver; driver.call(function* () { // Position and resize window so it\u0026#39;s easy to see them running together.  yield driver.manage().window().setSize(1280, 800); yield driver.manage().window().setPosition(90 * flowNo, 80 * flowNo); if(!noSleep){ yield driver.sleep(flow_interval * flowNo); } ... }); } for (let i = 0; i \u0026lt; session_count; i++) { runOneFlow(i); } function quitAllDrivers(){ for(let i=0; i\u0026lt;drivers.length; i++){ try { drivers[i].controlFlow().reset(); } catch (e) { console.log(\u0026#39;reset flow \u0026#39; + i + \u0026#39;\\\u0026#39;s controlFlow failed: %s\u0026#39;, e); } drivers[i].quit().then(undefined, function(e){ console.log(\u0026#39;quit flow \u0026#39; + i + \u0026#39; failed: %s\u0026#39;, e); }); } } var rl = null; function shutdown(){ running = false; if(rl){ rl.close(); } //进程退出时关闭打开的浏览器  quitAllDrivers(); } //block to nodejs\u0026#39;s main thread (function wait () { if(running){ setTimeout(wait, 500); } })(); if (process.platform === \u0026#34;win32\u0026#34;) { rl = require(\u0026#34;readline\u0026#34;).createInterface({ input: process.stdin, output: process.stdout }); rl.on(\u0026#34;SIGINT\u0026#34;, function () { shutdown(); }); } process.on(\u0026#39;SIGINT\u0026#39;, function() { shutdown(); }); 终于是运行多个测试终于只有一个node进程了。\n经验教训 以后使用第三方重要库决不能只看它给出的文档，还是应该仔细看一看人家给出的使用示例。\n参考 http://www.seleniumhq.org/docs/03_webdriver.jsp http://www.seleniumhq.org/docs/04_webdriver_advanced.jsp https://github.com/SeleniumHQ/selenium/tree/master/javascript/node/selenium-webdriver/lib\n","permalink":"https://jeremyxu2010.github.io/2016/05/web%E7%95%8C%E9%9D%A2%E6%B5%8B%E8%AF%95%E5%AE%9E%E8%B7%B5%E4%B9%8Bselenium-webdriver/","tags":["nodejs","webdriver","es6"],"title":"WEB界面测试实践之Selenium WebDriver"},{"categories":["java开发"],"contents":"今天看到jdeferred文档中一个关于Asynchronous Servlet的例子，如下\n@WebServlet(value = \u0026#34;/AsyncServlet\u0026#34;, asyncSupported = true) public class AsyncServlet extends HttpServlet { private static final long serialVersionUID = 1L; private ExecutorService executorService = Executors.newCachedThreadPool(); private DeferredManager dm = new DefaultDeferredManager(executorService); protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { final AsyncContext actx = request.startAsync(request, response); dm.when(new Callable\u0026lt;String\u0026gt;() { @Override public String call() throws Exception { if (actx.getRequest().getParameter(\u0026#34;fail\u0026#34;) != null) { throw new Exception(\u0026#34;oops!\u0026#34;); } Thread.sleep(2000); return \u0026#34;Hello World!\u0026#34;; } }).then(new DoneCallback\u0026lt;String\u0026gt;() { @Override public void onDone(String result) { actx.getRequest().setAttribute(\u0026#34;message\u0026#34;, result); actx.dispatch(\u0026#34;/hello.jsp\u0026#34;); } }).fail(new FailCallback\u0026lt;Throwable\u0026gt;() { @Override public void onFail(Throwable exception) { actx.getRequest().setAttribute(\u0026#34;exception\u0026#34;, exception); actx.dispatch(\u0026#34;/error.jsp\u0026#34;); } }); } } 突然想到在以前工作中经常前端向后端提交了一个长时间任务，为了良好的用户体验，前端还需要定时获取该任务的进度信息。之前的方案如下：\n 前端提交任务创建需要的信息至后台，后台为该任务创建对应Task，仅将该Task的ID返回至前端 后端向线程池提交该任务对应的Task Runnable，该Runnable的执行体里以任务的进度信息更新该Task的progress字段 前端定时发AJAX请求凭借Task的ID取进度  以前我一直有个疑问：就为了更新进度信息，浏览器要不停地向后端发请求，是不是代价太大了。曾经也尝试过以一个WebSocket请求代替轮寻询AJAX请求，但还是觉得比较麻烦。\n今天看到异步Servlet，又想起以前看过的监控AJAX下载进度的例子，感觉可以有另一种解决方案。直接粘代码吧。\n首先是获取任务进度的后端代码\npackage personal.xxj.servlet; import org.jdeferred.DeferredManager; import org.jdeferred.DoneCallback; import org.jdeferred.FailCallback; import org.jdeferred.impl.DefaultDeferredManager; import javax.servlet.AsyncContext; import javax.servlet.ServletException; import javax.servlet.http.HttpServlet; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import java.io.IOException; import java.util.Random; import java.util.concurrent.Callable; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; /** * Created by jeremy on 16/5/15. */ public class GetTaskProgressServlet extends HttpServlet { private ExecutorService executorService = Executors.newCachedThreadPool(); private DeferredManager dm = new DefaultDeferredManager(executorService); private Random random = new Random(); @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { final AsyncContext actx = request.startAsync(request, response); actx.setTimeout(Long.MAX_VALUE); dm.when(new Callable\u0026lt;Void\u0026gt;() { @Override public Void call() throws Exception { HttpServletResponse resp = (HttpServletResponse) actx.getResponse(); resp.setContentType(\u0026#34;text/html\u0026#34;); resp.setCharacterEncoding(\u0026#34;UTF-8\u0026#34;); resp.setContentLength(100); try { for (int i = 0; i \u0026lt; 100; i++) { Thread.sleep(random.nextInt(10) * 10); resp.getWriter().write(\u0026#34;*\u0026#34;); resp.getWriter().flush(); } } catch (Throwable e){ e.printStackTrace(); } finally { actx.complete(); } return null; } }); } } \u0026lt;web-app xmlns=\u0026#34;http://xmlns.jcp.org/xml/ns/javaee\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd\u0026#34; version=\u0026#34;3.1\u0026#34;\u0026gt; \u0026lt;display-name\u0026gt;Java Web Demo\u0026lt;/display-name\u0026gt; \u0026lt;servlet\u0026gt; \u0026lt;servlet-name\u0026gt;GetTaskProgressServlet\u0026lt;/servlet-name\u0026gt; \u0026lt;servlet-class\u0026gt;personal.xxj.servlet.GetTaskProgressServlet\u0026lt;/servlet-class\u0026gt; \u0026lt;async-supported\u0026gt;true\u0026lt;/async-supported\u0026gt; \u0026lt;/servlet\u0026gt; \u0026lt;servlet-mapping\u0026gt; \u0026lt;servlet-name\u0026gt;GetTaskProgressServlet\u0026lt;/servlet-name\u0026gt; \u0026lt;url-pattern\u0026gt;/api/getTaskProgress\u0026lt;/url-pattern\u0026gt; \u0026lt;/servlet-mapping\u0026gt; \u0026lt;/web-app\u0026gt; 可以看到这里用到了jdeferred与Asynchronous Servlet，工作逻辑就是模拟一个任务在慢慢地执行，每执行1%则向response里打印一个*。\n为啥一定要用Asynchronous Servlet？最大的原因是不想这些长时间运行的任务占用http线程，但又想持有请求响应上下文，可以在任务运行过程中输出合理的响应。\n这里有几点要注意：\n 像actx.setTimeout(Long.MAX_VALUE)这样根据实际场景设置超时时间，默认好像才30秒，对于一个长时间任务来说太短了 resp.setContentType，resp.setCharacterEncoding，resp.setContentLength最后都调用一遍，以免前端由于收到不这样响应头，非得接收完整的响应内容后才触发XMLHttpRequest的progress事件。(唉，入坑数小时，说多都是泪) 每向response里打印一个*后需要调用resp.getWriter().flush();，尽快将响应刷回客户端。 任务完成后要保证actx.complete();得到调用。 注册异步servlet时，在web.xml里需要\u0026lt;async-supported\u0026gt;true\u0026lt;/async-supported\u0026gt;  然后是前端代码\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Task Progress Demo\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1 id=\u0026#34;output\u0026#34;\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var xhr = new XMLHttpRequest(); xhr.timeout = Number.MAX_VALUE; xhr.open(\u0026#39;GET\u0026#39;, \u0026#39;/javawebdemo/api/getTaskProgress\u0026#39;); xhr.onprogress = function(event){ if (event.lengthComputable) { var percentComplete = parseInt((100 * event.loaded) / event.total); document.getElementById(\u0026#34;output\u0026#34;).innerHTML = \u0026#34;Task\u0026#39;s progress is \u0026#34; + percentComplete + \u0026#34;%\u0026#34;; } }; xhr.onerror = function(){ document.getElementById(\u0026#34;output\u0026#34;).innerHTML = \u0026#34;Task\u0026#39;s execution is failed\u0026#34;; }; xhr.send(); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 前端代码倒没有太多要注意的地方，只有一点要注意设置xhr.timeout。\n本例使用了Servlet 3.0 API及HTML5中的XMLHttpRequest 2，XMLHttpRequest 2现在较新的主流浏览器都支持。\n另外我查阅XMLHttpRequest 2的文档时还发现在XMLHttpRequest 2里不仅可以监控下载的进度，也可以监控上传的进度，参见XMLHttpRequest.upload的progress事件。\nXMLHttpRequest 2还可以上传文件，接收二进制数据，参见这里，真是强大地不要不要的。\n","permalink":"https://jeremyxu2010.github.io/2016/05/%E8%8E%B7%E5%8F%96%E5%90%8E%E5%8F%B0%E4%BB%BB%E5%8A%A1%E8%BF%9B%E5%BA%A6%E7%9A%84%E5%8F%A6%E7%B1%BB%E5%8A%9E%E6%B3%95/","tags":["java","html5","ajax","servlet","jdeferred"],"title":"获取后台任务进度的另类办法"},{"categories":["web开发"],"contents":"今天结合前面说到的前后端开发知识，做一个小工程，这个小工程并不完全具体的业务功能，但该工程包括前后端，可以作为以后复杂工程的起点。\n前端代码 前端代码稍微复杂一点，就先从前端代码开始。\nfrontend/js/demo6.js\nimport React from \u0026#39;react\u0026#39;; import ReactDOM from \u0026#39;react-dom\u0026#39;; import ReactRouter from \u0026#39;react-router\u0026#39;; import { Router, Route, IndexRoute, browserHistory } from \u0026#39;react-router\u0026#39;; class App extends React.Component{ render(){ return \u0026lt;div\u0026gt;{this.props.children}\u0026lt;/div\u0026gt;; } } class Home extends React.Component{ render(){ return \u0026lt;h1\u0026gt;Home Page\u0026lt;/h1\u0026gt;; } } class About extends React.Component{ render(){ return \u0026lt;h1\u0026gt;About Page\u0026lt;/h1\u0026gt;; } } class Features extends React.Component{ render(){ return \u0026lt;h1\u0026gt;Features Page\u0026lt;/h1\u0026gt;; } } document.body.innerHTML = \u0026#39;\u0026lt;div id=\u0026#34;reactHolder\u0026#34;\u0026gt;\u0026lt;/div\u0026gt;\u0026#39;; ReactDOM.render( \u0026lt;Router history={browserHistory}\u0026gt; \u0026lt;Route path=\u0026#39;/\u0026#39; component={App}\u0026gt; \u0026lt;IndexRoute component={Home} /\u0026gt; \u0026lt;Route path=\u0026#39;about\u0026#39; component={About} /\u0026gt; \u0026lt;Route path=\u0026#39;features\u0026#39; component={Features} /\u0026gt; \u0026lt;Route path=\u0026#39;*\u0026#39; component={Home} /\u0026gt; \u0026lt;/Route\u0026gt; \u0026lt;/Router\u0026gt;, document.getElementById(\u0026#39;reactHolder\u0026#39;) ); 可以看到就是定义了几个React组件，并用react-router定义了一个很简的路由，这个路由的history使用的是browserHistory。react的用法可参考这里，react-router的用法可参考这里\n写前端代码编译脚本 webpack.config.js\nconst webpack = require(\u0026#34;webpack\u0026#34;); const HtmlWebpackPlugin = require(\u0026#39;html-webpack-plugin\u0026#39;); module.exports = { entry: { demo6 : __dirname + \u0026#39;/frontend/js/demo6.js\u0026#39; }, output: { path: __dirname + \u0026#39;/public\u0026#39;, publicPath: \u0026#39;/\u0026#39;, filename: \u0026#39;js/[name].js\u0026#39;, hotUpdateMainFilename: \u0026#39;hot-update/[hash].hot-update.json\u0026#39;, chunkFilename: \u0026#39;js/chunks/[name].js\u0026#39;, hotUpdateChunkFilename: \u0026#39;hot-update/chunks/[id].[hash].hot-update.js\u0026#39;, }, plugins: [ new webpack.SourceMapDevToolPlugin({ test: /\\.(js|css|less)($|\\?)/i, filename: \u0026#39;[file].map\u0026#39; }), new HtmlWebpackPlugin({ title: \u0026#39;demo6\u0026#39;, filename: \u0026#39;index.html\u0026#39;, hash : true, chunks : [\u0026#39;demo6\u0026#39;] }) ], module: { loaders: [ {test: /\\.(js|jsx)$/, loaders: [\u0026#34;babel\u0026#34;]} ] }, resolve: { extensions: [\u0026#39;\u0026#39;, \u0026#39;.js\u0026#39;, \u0026#39;.jsx\u0026#39;] }, devtool: \u0026#39;eval\u0026#39; }; .babelrc\n{ \u0026#34;presets\u0026#34;: [\u0026#34;react\u0026#34;, \u0026#34;es2015\u0026#34;] } 上面的webpack编译配置很简单，就是配置把frontend/js/demo6.js编译到public/js/demo6.js，同时生成public/index.html，其引用生成的public/js/demo6.js\n利用npm-scripts来进行webpack的调用\n \u0026quot;scripts\u0026quot;: { \u0026quot;wpack\u0026quot;: \u0026quot;./node_modules/.bin/webpack --watch --progress\u0026quot; }, 后端代码 backend/server.js\n\u0026#34;use strict\u0026#34;; const koa = require(\u0026#39;koa\u0026#39;); const serve = require(\u0026#39;koa-static\u0026#39;); const sendfile = require(\u0026#39;koa-sendfile\u0026#39;) const path = require(\u0026#39;path\u0026#39;); const Promise = require(\u0026#39;bluebird\u0026#39;); const fs = require(\u0026#39;fs\u0026#39;); const statAsync = Promise.promisify(fs.stat); const app = koa(); app.use(serve(__dirname + \u0026#39;/../public\u0026#39;)); app.use(function *(next){ let p = path.resolve(__dirname, \u0026#39;..\u0026#39;, \u0026#39;public\u0026#39;, this.path); let stats = null; try{ stats = yield statAsync(p); }catch(ignore){} if (!stats) { try { stats = yield sendfile(this, path.resolve(__dirname, \u0026#39;..\u0026#39;, \u0026#39;public\u0026#39;, \u0026#39;index.html\u0026#39;)); }catch(ignore){} if (!stats) this.throw(404); } }); const port = 5000; app.listen(port); console.log(\u0026#34;server started on port \u0026#34; + port); 因为前端使用了browserHistory路由，后端要实现类似nginx的try_files逻辑，详情见这里，如果后端是用Java写法，可以考虑使用TryFilesFilter\n这里使用bluebird的promisify方法将NodeJS风格的API fs.stat 转化成返回Promise对象的方法，这个是为配合koa的yield而为，详见这里\n同样利用npm-scripts启动后端server\n \u0026quot;scripts\u0026quot;: { \u0026quot;serve\u0026quot;: \u0026quot;node ./backend/server.js\u0026quot; }, 运行测试 打开两个终端，在一个里面执行npm run serve启动后端server，在另一个里面执行npm run wpack启动webpack对前端代码进行编译。最后使用浏览器分别访问http://127.0.0.1:5000/、http://127.0.0.1:5000/about、http://127.0.0.1:5000/features，即可看到路由切换的效果。\n本篇源代码地址\n","permalink":"https://jeremyxu2010.github.io/2016/05/%E7%8E%B0%E4%BB%A3web%E5%BC%80%E5%8F%91%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B_07/","tags":["web","html5","javascript","nodejs"],"title":"现代Web开发系列教程_07"},{"categories":["java开发"],"contents":"Java 7中提供了java.nio.file.WatchService用来监听文件系统目录变更，用起来还是比较简单的，在这里记录一下。\n创建一个WatchService 代码如下：\nWatchService watcher = FileSystems.getDefault().newWatchService(); 当然一个WatchService是关联着操作系统资源的，需要完全的关闭，所以一般像下面这样写：\nWatchService watcher = null; try { watcher = FileSystems.getDefault().newWatchService(); ... } finally { if(watcher != null){ try { watcher.close(); } catch (Exception ignore){} } } 注册监听一个目录 Path dir = Paths.get(\u0026#34;/somewhere\u0026#34;); WatchKey key = dir.register(watcher, ENTRY_CREATE, ENTRY_DELETE, ENTRY_MODIFY); 这里这个key即与这个dir关联，以后dir里一旦发生监听的事件，则从watcher就可以poll或take到这个key\n循环从WatchService里取出有信号的WatchKey for (;;) { // wait for key to be signalled  WatchKey key; try { key = watcher.take(); } catch (InterruptedException x) { return; } for (WatchEvent\u0026lt;?\u0026gt; event: key.pollEvents()) { WatchEvent.Kind kind = event.kind(); // TBD - provide example of how OVERFLOW event is handled  if (kind == OVERFLOW) { continue; } // Context for directory entry event is the file name of entry  WatchEvent\u0026lt;Path\u0026gt; ev = (WatchEvent\u0026lt;T\u0026gt;)event; Path name = ev.context(); ... } // reset key and remove from set if directory no longer accessible  boolean valid = key.reset(); if (!valid) { break; } } WatchKey被cancel或WatchService被close时，key.reset()会返回false, 此时应该跳出循环。\n递归监听目录 上述的代码很简单了，跟Java原生NIO的思想差不多。不过经我实验，dir.register(watcher, ENTRY_CREATE, ENTRY_DELETE, ENTRY_MODIFY);只会监听该目录下一级的变更事件，子目录下的变更就监听不到了。例如在/somewhere目录下建一个目录test，再在test下建一个文件test.txt，此时就监听不到了。简单写了个递归监听某个目录下所有变更的例子，如下\nimport java.nio.file.*; import static java.nio.file.StandardWatchEventKinds.*; import static java.nio.file.LinkOption.*; import java.nio.file.attribute.*; import java.io.*; import java.util.*; /** * Created by jeremy on 16/5/12. */ public class WatchDir { private final WatchService watcher; private final Map\u0026lt;WatchKey,Path\u0026gt; keys; private boolean trace = false; @SuppressWarnings(\u0026#34;unchecked\u0026#34;) static \u0026lt;T\u0026gt; WatchEvent\u0026lt;T\u0026gt; cast(WatchEvent\u0026lt;?\u0026gt; event) { return (WatchEvent\u0026lt;T\u0026gt;)event; } /** * Register the given directory with the WatchService */ private void register(Path dir) throws IOException { WatchKey key = dir.register(watcher, ENTRY_CREATE, ENTRY_DELETE, ENTRY_MODIFY); if (trace) { Path prev = keys.get(key); if (prev == null) { System.out.format(\u0026#34;register: %s\\n\u0026#34;, dir); } else { if (!dir.equals(prev)) { System.out.format(\u0026#34;update: %s -\u0026gt; %s\\n\u0026#34;, prev, dir); } } } keys.put(key, dir); } /** * Register the given directory, and all its sub-directories, with the * WatchService. */ private void registerAll(final Path start) throws IOException { // register directory and sub-directories  Files.walkFileTree(start, new SimpleFileVisitor\u0026lt;Path\u0026gt;() { @Override public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs) throws IOException { register(dir); return FileVisitResult.CONTINUE; } }); } /** * Creates a WatchService and registers the given directory */ WatchDir(Path dir) throws IOException { this.watcher = FileSystems.getDefault().newWatchService(); this.keys = new HashMap\u0026lt;WatchKey,Path\u0026gt;(); System.out.format(\u0026#34;Scanning %s ...\\n\u0026#34;, dir); registerAll(dir); System.out.println(\u0026#34;Done.\u0026#34;); // enable trace after initial registration  this.trace = true; } /** * Process all events for keys queued to the watcher */ void processEvents() { for (;;) { // wait for key to be signalled  WatchKey key; try { key = watcher.take(); } catch (InterruptedException x) { return; } Path dir = keys.get(key); if (dir == null) { System.err.println(\u0026#34;WatchKey not recognized!!\u0026#34;); continue; } for (WatchEvent\u0026lt;?\u0026gt; event: key.pollEvents()) { WatchEvent.Kind kind = event.kind(); // TBD - provide example of how OVERFLOW event is handled  if (kind == OVERFLOW) { continue; } // Context for directory entry event is the file name of entry  WatchEvent\u0026lt;Path\u0026gt; ev = cast(event); Path name = ev.context(); Path child = dir.resolve(name); // print out event  System.out.format(\u0026#34;%s: %s\\n\u0026#34;, event.kind().name(), child); // if directory is created, and watching recursively, then  // register it and its sub-directories  if (kind == ENTRY_CREATE) { try { if (Files.isDirectory(child, NOFOLLOW_LINKS)) { registerAll(child); } } catch (IOException x) { // ignore to keep sample readbale  } } } // reset key and remove from set if directory no longer accessible  boolean valid = key.reset(); if (!valid) { keys.remove(key); // all directories are inaccessible  if (keys.isEmpty()) { break; } } } } static void usage() { System.err.println(\u0026#34;usage: java WatchDir dir\u0026#34;); System.exit(-1); } public static void main(String[] args) throws IOException { // parse arguments  if (args.length == 0 || args.length \u0026gt; 1) usage(); // register directory and process its events  Path dir = Paths.get(args[0]); new WatchDir(dir).processEvents(); } } Path name = ev.context();拿到的仅仅只是相对于dir的Path，并不是绝对路径，为了拼出绝对路径，没办法只能建了一个Map keys，用来维护WatchKey与dir的映射关系。本以为这样写，面对一个巨大的目录，Map keys将会很大，性能不好。实测监听一个100多G的目录，并没占用太多内存，进程使用的文件句柄数也正常得很，而且性能还比较高。\n希望Java以后的版本能直接在WatchEvent拿到变更ENTRY的绝对路径就好了。\n","permalink":"https://jeremyxu2010.github.io/2016/05/java%E7%9B%91%E5%90%AC%E7%9B%AE%E5%BD%95%E6%96%87%E4%BB%B6%E5%8F%98%E6%9B%B4/","tags":["java","nio","filesystem"],"title":"Java监听目录文件变更"},{"categories":["web开发"],"contents":"什么是同源 浏览器安全的基石是\u0026quot;同源政策\u0026rdquo;，所有浏览器都实行这个政策。所谓两个网页\u0026quot;同源\u0026quot;指的两个网页的\u0026quot;协议相同\u0026rdquo;、\u0026ldquo;域名相同\u0026rdquo;、\u0026ldquo;端口相同\u0026rdquo;。\n浏览器为什么遵循同源政策 同源政策的目的，是为了保证用户信息的安全，防止恶意的网站窃取数据。设想这样一种情况：A网站是一家银行，用户登录以后，又去浏览其他网站。如果其他网站可以读取A网站的 Cookie，将会产生严重的信息安全问题。\n不同源的两个网页有哪些限制  各自无法读取对方的Cookie、LocalStorage 和 IndexDB 各自无法操作对方的DOM 各自无法发送AJAX请求至对方的地址  如何规避限制 虽然上述限制是必要的，但是有时很不方便，合理的用途也受到影响，下面说一下如何规避。\n不同源页面之间共享Cookie 如果两个网页一级域名相同，只是二级域名不同，浏览器允许通过设置document.domain共享 Cookie。示例如下\nhttp://a.test.com:8000/test1.html\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Test1\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; document.domain = \u0026#39;test.com\u0026#39;; document.cookie = \u0026#34;test1=hello;domain=.test.com\u0026#34;; \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; http://b.test.com:8000/test2.html\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Test2\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; document.domain = \u0026#39;test.com\u0026#39;; var allCookie = document.cookie; console.log(allCookie); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 在上面的示例里，先用浏览器在一个标签页里访问http://a.test.com:8000/test1.html，再在另一个标签页里访问http://b.test.com:8000/test2.html，可以发现在test2.html里可以访问test1.html里设置的Cookie。这种方法虽然简单，但LocalStorage 和 IndexDB 无法通过这种方法规避同源政策。\n不同源的父子页面之间互访JS对象、DOM对象 正常情况下两个页面本身也没有互操作DOM的需求，但在使用iframe窗口或window.open打开窗口时，经常存在父窗口需要与子窗口互访JS对象、DOM对象。这个时候如果父子窗口刚好不满足同源政策，这种互访操作将无法进行。\n同样如果这两个网页一级域名相同，只是二级域名不同，浏览器允许通过设置document.domain允许这种互访操作。示例如下\nhttp://a.test.com:8000/test1.html\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Test1\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; document.domain = \u0026#39;test.com\u0026#39;; var myJsVar1 = \u0026#39;test1JsVar\u0026#39;; window.setTimeout(function(){ console.log(document.getElementById(\u0026#34;myIFrame\u0026#34;).contentWindow.document); console.log(document.getElementById(\u0026#34;myIFrame\u0026#34;).contentWindow.myJsVar2); }, 2000); \u0026lt;/script\u0026gt; \u0026lt;iframe id=\u0026#34;myIFrame\u0026#34; src=\u0026#34;http://b.test.com:8000/test2.html\u0026#34;/\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; http://b.test.com:8000/test2.html\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Test2\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; document.domain = \u0026#39;test.com\u0026#39;; var myJsVar2 = \u0026#39;test2JsVar\u0026#39;; window.setTimeout(function(){ console.log(parent.document); console.log(parent.myJsVar1); }, 2000); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 使用window.postMessage在不同源的父子页面间传递消息 不同源的父子页面间传递消息，除了使用document.domain方案，其实还存在其它3种方案：\n 片段识别符（fragment identifier） 通过window.name属性 通过window.postMessage方法  其中前两种方法限制较多，而且感觉像是奇技淫巧，这里就不介绍了。这里重点说一下window.postMessage方法。\nHTML5为了解决不同源页面间消息传递的问题，引入了一个全新的API：跨文档通信 API（Cross-document messaging）。\n这个API为window对象新增了一个window.postMessage方法，允许跨窗口通信，不论这两个窗口是否同源。示例如下\nhttp://a.test.com:8000/test1.html\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Test1\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; window.addEventListener(\u0026#39;message\u0026#39;, function(e) { console.log(e.data); },false); window.setTimeout(function(){ document.getElementById(\u0026#34;myIFrame\u0026#34;).contentWindow.postMessage(\u0026#39;say hello to test2\u0026#39;, \u0026#39;http://b.test.com:8000\u0026#39;); }, 2000); \u0026lt;/script\u0026gt; \u0026lt;iframe id=\u0026#34;myIFrame\u0026#34; src=\u0026#34;http://b.test.com:8000/test2.html\u0026#34;/\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; http://b.test.com:8000/test2.html\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Test2\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; window.addEventListener(\u0026#39;message\u0026#39;, function(e) { console.log(e.data); },false); window.setTimeout(function(){ window.parent.postMessage(\u0026#39;say hello to test1\u0026#39;, \u0026#39;http://a.test.com:8000\u0026#39;); }, 2000); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; postMessage方法的第一个参数是具体的信息内容，第二个参数是接收消息的窗口的源（origin），即\u0026quot;协议 + 域名 + 端口\u0026rdquo;。也可以设为*，表示不限制域名，向所有窗口发送。\nmessage事件的事件对象event，提供以下三个属性。\n event.source：发送消息的窗口 event.origin: 消息发向的网址 event.data: 消息内容  可以通过使用event.source属性拿到发送消息的窗口句柄，进而再使用postMessage向之传递消息。event.origin属性可以过滤不是发给本窗口的消息，如下\nwindow.addEventListener(\u0026#39;message\u0026#39;, function(e) { if (e.origin !== \u0026#39;http://a.test.com:8000\u0026#39;) return; e.source.postMessage(\u0026#39;Hello\u0026#39;, event.origin); },false); 使用postMessage，花点心思，操作非同源页面的LocalStorage也可能了，如下\nhttp://a.test.com:8000/test1.html\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Test1\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; window.setTimeout(function(){ var obj = { name: \u0026#39;Jack\u0026#39; }; document.getElementById(\u0026#34;myIFrame\u0026#34;).contentWindow.postMessage(JSON.stringify({key: \u0026#39;storage\u0026#39;, data: obj}), \u0026#39;http://a.test.com:8000\u0026#39;); }, 2000); \u0026lt;/script\u0026gt; \u0026lt;iframe id=\u0026#34;myIFrame\u0026#34; src=\u0026#34;http://b.test.com:8000/test2.html\u0026#34;/\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; http://b.test.com:8000/test2.html\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Test2\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; window.addEventListener(\u0026#39;message\u0026#39;, function(e) { var payload = JSON.parse(e.data); localStorage.setItem(payload.key, JSON.stringify(payload.data)); },false); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 使用JSONP向不同源的站点发送AJAX请求 JSONP是服务器与客户端跨源通信的常用方法。最大特点就是简单适用，老式浏览器全部支持，服务器改造非常小。\n它的基本思想是，网页通过添加一个\u0026lt;script\u0026gt;元素，向服务器请求JSON数据，这种做法不受同源政策限制；服务器收到请求后，将数据放在一个指定名字的回调函数里传回来。\n这种方式用起来很简单，甚至jQuery都提供了一种请求类型jsonp，缺陷是请求的服务端必须进行改造，需要以jsonp的方式返回响应。\n/UsingYQLandJSONP $.ajax({ type: \u0026#34;get\u0026#34;, url: \u0026#34;http://b.test.com:8000/api/getUserInfo\u0026#34;, dataType: \u0026#34;jsonp\u0026#34;,//指定以jsonp方式執行  data: { userId : 3 }, success: function(res){ alert(res.msg); }, error: function(){ alert(\u0026#39;fail\u0026#39;); } }); 使用CORS向不同源的站点发送AJAX请求 CORS是跨源资源分享（Cross-Origin Resource Sharing）的缩写。它是W3C标准，是跨源AJAX请求的根本解决方法。相比JSONP只能发GET请求，CORS允许任何类型的请求。\nCORS请求分成两类：简单请求（simple request）和非简单请求（not-so-simple request）。\n简单CORS请求 只要同时满足以下两大条件，就属于简单请求。\n  请求方法是以下三种方法之一：\n HEAD GET POST    HTTP的头信息包含以下几种字段：\n Accept Accept-Language Content-Language Last-Event-ID Content-Type：只限于三个值application/x-www-form-urlencoded、multipart/form-data、text/plain    简单请求的特征是浏览器本身就可以不依赖于CORS成功发送请求至服务端。比如一个JSONP请求可以被看作是一个简单CORS GET请求。一个普通的表单提交请求可以被看作是一个简单的CORS POST请求。\n凡是不同时满足上面两个条件，就属于非简单请求。\n简单CORS请求流程 浏览器发现这次跨源AJAX请求是简单请求，就自动在头信息之中，添加一个Origin字段。\nGET /api/getUserInfo HTTP/1.1 Origin: http://a.test.com:8000 Host: b.test.com Accept-Language: en-US Connection: keep-alive User-Agent: Mozilla/5.0... 如果Origin指定的源，不在许可范围内，服务器会返回一个正常的HTTP回应。浏览器发现，这个回应的头信息没有包含Access-Control-Allow-Origin字段，浏览器就知道出错了，从而抛出一个错误，被XMLHttpRequest的onerror回调函数捕获。\n如果Origin指定的域名在许可范围内，服务器返回的响应，会多出几个头信息字段。\nAccess-Control-Allow-Origin: http://a.test.com:8000 Access-Control-Allow-Credentials: true Access-Control-Expose-Headers: FooBar Content-Type: text/html; charset=utf-8 上面的头信息之中，有三个与CORS请求相关的字段，都以Access-Control-开头。\n  Access-Control-Allow-Origin 该字段是必须的。它的值要么是请求时Origin字段的值，要么是一个*，表示接受任意域名的请求。\n  Access-Control-Allow-Credentials 该字段可选。它的值是一个布尔值，表示是否允许发送Cookie。设为true，即表示服务器明确许可，Cookie可以包含在请求中，一起发给服务器。这个值也只能设为true，如果服务器不要浏览器发送Cookie，删除该字段即可。 默认情况下，Cookie和HTTP认证信息不包括在CORS请求之中，要将Cookie和HTTP认证信息包含到CORS请求里发送到服务端，首先发送AJAX请求时需打开withCredentials属性。\nvar xhr = new XMLHttpRequest(); xhr.withCredentials = true; 其次服务端必须返回\nAccess-Control-Allow-Credentials: true 上述两个条件都满足，CORS请求才会成功。如果其中只有一个为true，则请求会失败。\n  Access-Control-Expose-Headers 该字段可选。CORS请求时，XMLHttpRequest对象的getResponseHeader()方法只能拿到6个基本字段：Cache-Control、Content-Language、Content-Type、Expires、Last-Modified、Pragma。如果想拿到其他字段，就必须在Access-Control-Expose-Headers里面指定。上面的例子指定，getResponseHeader(\u0026lsquo;FooBar\u0026rsquo;)可以返回FooBar字段的值。\n  非简单CORS请求 非简单请求是那种对服务器有特殊要求的请求，比如请求方法是PUT或DELETE，或者Content-Type字段的类型是application/json。\n非简单请求的CORS请求，会在正式通信之前，增加一次HTTP查询请求，称为\u0026quot;预检\u0026quot;请求（preflight）。\n浏览器先询问服务器，当前网页所在的域名是否在服务器的许可名单之中，以及可以使用哪些HTTP动词和头信息字段。只有得到肯定答复，浏览器才会发出正式的XMLHttpRequest请求，否则就报错。\n示例如下\n首先浏览器发送一个非简单的CORS请求。\nvar url = \u0026#39;http://b.test.com:8000/api/createUser\u0026#39;; var xhr = new XMLHttpRequest(); xhr.open(\u0026#39;PUT\u0026#39;, url, true); xhr.setRequestHeader(\u0026#39;X-Custom-Header\u0026#39;, \u0026#39;value\u0026#39;); xhr.send(); 浏览器发现，这是一个非简单请求，就自动发出一个\u0026quot;预检\u0026quot;请求，要求服务器确认可以这样请求。下面是这个\u0026quot;预检\u0026quot;请求的HTTP头信息。\nOPTIONS /cors HTTP/1.1 Origin: http://a.test.com:8000 Access-Control-Request-Method: PUT Access-Control-Request-Headers: X-Custom-Header Host: b.test.com Accept-Language: en-US Connection: keep-alive User-Agent: Mozilla/5.0... \u0026ldquo;预检\u0026quot;请求用的请求方法是OPTIONS，表示这个请求是用来询问的。头信息里面，关键字段是Origin，表示请求来自哪个源。 除了Origin字段，\u0026ldquo;预检\u0026quot;请求的头信息包括两个特殊字段。\n Access-Control-Request-Method 该字段是必须的，用来列出浏览器的CORS请求会用到哪些HTTP方法，上例是PUT。 Access-Control-Request-Headers 该字段是一个逗号分隔的字符串，指定浏览器CORS请求会额外发送的头信息字段，上例是X-Custom-Header  服务器收到\u0026quot;预检\u0026quot;请求以后，检查了Origin、Access-Control-Request-Method和Access-Control-Request-Headers字段以后，确认允许跨源请求，就可以做出回应。\nHTTP/1.1 200 OK Date: Mon, 01 Dec 2008 01:15:39 GMT Server: Apache/2.0.61 (Unix) Access-Control-Allow-Origin: http://a.test.com:8000 Access-Control-Allow-Methods: GET, POST, PUT Access-Control-Allow-Headers: X-Custom-Header Content-Type: text/html; charset=utf-8 Content-Encoding: gzip Content-Length: 0 Keep-Alive: timeout=2, max=100 Connection: Keep-Alive Content-Type: text/plain 上面的HTTP回应中，关键的是Access-Control-Allow-Origin字段，表示http://a.test.com:8000可以请求数据。该字段也可以设为星号，表示同意任意跨源请求。\n如果浏览器否定了\u0026quot;预检\u0026quot;请求，会返回一个正常的HTTP回应，但是没有任何CORS相关的头信息字段。这时，浏览器就会认定，服务器不同意预检请求，因此触发一个错误，被XMLHttpRequest对象的onerror回调函数捕获。\n服务器回应的其他CORS相关字段如下\nAccess-Control-Allow-Methods: GET, POST, PUT Access-Control-Allow-Headers: X-Custom-Header Access-Control-Allow-Credentials: true Access-Control-Max-Age: 1728000 CORS\u0026quot;预检\u0026quot;响应字段意义如下：\n Access-Control-Allow-Methods 该字段必需，它的值是逗号分隔的一个字符串，表明服务器支持的所有跨域请求的方法。注意，返回的是所有支持的方法，而不单是浏览器请求的那个方法。这是为了避免多次\u0026quot;预检\u0026quot;请求。 Access-Control-Allow-Headers 如果浏览器请求包括Access-Control-Request-Headers字段，则Access-Control-Allow-Headers字段是必需的。它也是一个逗号分隔的字符串，表明服务器支持的所有头信息字段，不限于浏览器在\u0026quot;预检\u0026quot;中请求的字段。 Access-Control-Allow-Credentials 该字段与简单请求时的含义相同。 Access-Control-Max-Age 该字段可选，用来指定本次预检请求的有效期，单位为秒。上面结果中，有效期是20天（1728000秒），即允许缓存该条回应1728000秒（即20天），在此期间，不用发出另一条预检请求。  一旦服务器通过了\u0026quot;预检\u0026quot;请求，以后每次浏览器正常的CORS请求，就都跟简单请求一样，会有一个Origin头信息字段。服务器的回应，也都会有一个Access-Control-Allow-Origin头信息字段。\n\u0026ldquo;预检\u0026quot;请求之后，浏览器的正常CORS请求\nPUT /api/createUser HTTP/1.1 Origin: http://a.test.com:8000 Host: b.test.com X-Custom-Header: value Accept-Language: en-US Connection: keep-alive User-Agent: Mozilla/5.0... 上面头信息的Origin字段是浏览器自动添加的。然后服务器正常的回应Access-Control-Allow-Origin。\nAccess-Control-Allow-Origin: http://a.test.com:8000 Content-Type: text/html; charset=utf-8 总结   如果非同源页面间消息传递，应该优选window.postMessage方案。如果两个网页一级域名相同，只是二级域名不同，也可以采用document.domain方案。\n  AJAX请求非同源站点，应该优选CORS方案，如果方便对服务端接口进行改造，也可以使用JSONP方案\n  ","permalink":"https://jeremyxu2010.github.io/2016/05/web%E8%B7%A8%E5%9F%9F%E6%80%BB%E7%BB%93/","tags":["web","html5","javascript","cors","jsonp"],"title":"Web跨域总结"},{"categories":["java开发"],"contents":"今天在工作中遇到一个很奇怪的问题。使用java.io.File进行文件操作抛出了FileNotFoundException，代码如下\n//但事实上在`/somewhere`目录下存在文件`测试.txt` FileInputStream fin = new FileInputStream(new File(\u0026#34;/somewhere/测试.txt\u0026#34;)); 在网络找了下，发现一个可能是由于路径中包括非ASCII字符，详见这里\n最后找到了解决方案\nInputStream fin = Files.newInputStream(Paths.get(\u0026#34;/somewhere/测试.txt\u0026#34;)); 看来Java老的那一套File操作接口确实问题多多啊。这里将Java 7引入的新File操作接口复习一下以备忘。\nPath接口 这个接口表示一个文件在文件系统中的定位器。常见与文件路径相关的操作都可以在这个接口里找到。详细文档参见这里\n注意以下两点\n Path对象一般由java.nio.file.Paths的两个get静态方法得来，并不是new出来的 如果与已有的库交互要用到java.io.File，可以使用它的toFile方法得到一个File对象  Files接口 Files接口里基本上都是静态方法，所有与文件相关的操作都可以在这个接口里找到。详细文档参见这里\n为了便于更好地使用Files进行文件操作，这里列举经常用到的静态方法，并与使用java.io.File作个参照。\n Files.exists(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs (new File(\u0026quot;/somewhere/somefile.txt\u0026quot;)).exists() Files.newInputStream(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs new FileInputStream(new File(\u0026quot;/somewhere/somefile.txt\u0026quot;)) Files.newOutputStream(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs new FileOutputStream(new File(\u0026quot;/somewhere/somefile.txt\u0026quot;)) Files.newByteChannel(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs (new FileInputStream(new File(\u0026quot;xxx\u0026quot;)).getChannel() Files.newDirectoryStream(Paths.get(\u0026quot;/somewhere\u0026quot;)) vs (new File(\u0026quot;/somewhere\u0026quot;)).listFiles()  Files.createFile(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;) vs (new File(\u0026quot;/somewhere/somefile.txt\u0026quot;)).createNewFile() Files.createDirectory(Paths.get(\u0026quot;/somewhere\u0026quot;)) vs (new File(\u0026quot;/somewhere\u0026quot;)).mkdir() Files.createDirectories(Paths.get(\u0026quot;/somewhere\u0026quot;)) vs (new File(\u0026quot;/somewhere\u0026quot;)).mkdirs() Files.createAndCheckIsDirectory(Paths.get(\u0026quot;/somewhere\u0026quot;)) vs (new File(\u0026quot;/somewhere\u0026quot;)).mkdirs(); boolean success = (new File(\u0026quot;/somewhere\u0026quot;)).exists() \u0026amp;\u0026amp; (new File(\u0026quot;/somewhere\u0026quot;)).isDirectory() Files.createTempFile(Paths.get(\u0026quot;/somewhere\u0026quot;), \u0026quot;XXXX\u0026quot;, null) vs File.createTempFile(\u0026quot;XXXX\u0026quot;, null, new File(\u0026quot;/somewhere\u0026quot;)) Files.createTempFile(\u0026quot;XXXX\u0026quot;, null) vs File.createTempFile(\u0026quot;XXXX\u0026quot;, null) Files.createTempDirectory(Paths.get(\u0026quot;/somewhere\u0026quot;), \u0026quot;XXXX\u0026quot;) vs - Files.createTempDirectory(\u0026quot;XXXX\u0026quot;) vs - Files.createSymbolicLink(Paths.get(\u0026quot;/somewhere/link.txt\u0026quot;), Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs - Files.createLink(Paths.get(\u0026quot;/somewhere/link.txt\u0026quot;), Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs - Files.delete(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs (new File(\u0026quot;/somewhere/somefile.txt\u0026quot;)).delete() Files.deleteIfExists(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs - Files.copy(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;), Paths.get(\u0026quot;/somewhere/somefile2.txt\u0026quot;)) vs - Files.move(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;), Paths.get(\u0026quot;/somewhere/somefile2.txt\u0026quot;)) vs (new File(\u0026quot;/somewhere/somefile.txt\u0026quot;)).renameTo(new File(\u0026quot;/somewhere/somefile2.txt\u0026quot;)) Files.readSymbolicLink(Paths.get(\u0026quot;/somewhere/link.txt\u0026quot;)) | - Files.isSameFile(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;), Paths.get(\u0026quot;/somewhere/somefile2.txt\u0026quot;)) | - Files.isHidden(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs (new File(\u0026quot;/somewhere/somefile.txt\u0026quot;)).isHidden() Files.getFileAttributeView(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;), AclFileAttributeView.class) vs - Files.readAttributes(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;), PosixFileAttributes.class) vs - Files.setAttribute(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;), \u0026quot;dos:hidden\u0026quot;, true) vs - Files.getAttribute(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;), \u0026quot;unix:uid\u0026quot;) | - Files.readAttributes(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;), \u0026quot;posix:permissions,owner,size\u0026quot;) vs - Files.getPosixFilePermissions(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs - Files.setPosixFilePermissions(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;), new HashSet(){ {this.add(PosixFilePermission.OWNER_READ);this.add(PosixFilePermission.OWNER_WRITE);this.add(PosixFilePermission.OWNER_EXECUTE);} }) vs - Files.getOwner(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs - UserPrincipal joe = lookupService.lookupPrincipalByName(\u0026quot;joe\u0026quot;);Files.setOwner(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;), joe) vs - Files.isSymbolicLink(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs - Files.isDirectory(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs (new File(\u0026quot;/somewhere/somefile.txt\u0026quot;)).isDirectory() Files.isRegularFile(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs (new File(\u0026quot;/somewhere/somefile.txt\u0026quot;)).isFile() Files.getLastModifiedTime(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs (new File(\u0026quot;/somewhere/somefile.txt\u0026quot;)).lastModified() Files.setLastModifiedTime(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;), FileTime.fromMillis(System.currentTimeMillis())) vs (new File(\u0026quot;/somewhere/somefile.txt\u0026quot;)).setLastModified(System.currentTimeMillis()) Files.size(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs (new File(\u0026quot;/somewhere/somefile.txt\u0026quot;)).length() Files.notExists(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs !(new File(\u0026quot;/somewhere/somefile.txt\u0026quot;)).exists() Files.isAccessible(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs - Files.isReadable(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs (new File(\u0026quot;/somewhere/somefile.txt\u0026quot;)).canRead() Files.isWritable(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs (new File(\u0026quot;/somewhere/somefile.txt\u0026quot;)).canWrite() Files.isExecutable(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs (new File(\u0026quot;/somewhere/somefile.txt\u0026quot;)).canExecute() Files.walkFileTree(Paths.get(\u0026quot;/somewhere\u0026quot;), new FileVisitor\u0026lt;Path\u0026gt;(){…}) vs - Files.newBufferedReader(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs new InputStreamReader(new FileInputStream(new File(\u0026quot;/somewhere/somefile.txt\u0026quot;))) Files.newBufferedWriter(Paths.get(\u0026quot;/somewhere/somefile.txt\u0026quot;)) vs new OutputStreamWriter(new FileOutputStream(new File(\u0026quot;/somewhere/somefile.txt\u0026quot;))) Files.list(Paths.get(\u0026quot;/somewhere\u0026quot;)) vs (new File(\u0026quot;/somewhere\u0026quot;)).listFiles()  有一些Stream与Path相互copy、一些重载的方法没有列在上面。\n官方为什么又发明一个新的文件操作API呢？我想了下，感觉大概是以下原因\n 静态方法比对象方法更易用 Files的静态方法实现得更跨平台 对于文件属和权限的操作更方便易用 Files的静态方法在文件操作出错时能更好地给予外界信息供诊断 Files的静态方法处理比较大的目录时，因为使用了Visitor模式，比老的方式效率更高 Files的move方法操作更原子性 老的API已被广泛使用，不好直接对其作大幅修改  另外原来的deleteOnExit建议换成下面的写法\nOutputStream out = Files.newOutputStream(Paths.get(\u0026#34;test.tmp\u0026#34;), StandardOpenOption.DELETE_ON_CLOSE); ... out.close(); ","permalink":"https://jeremyxu2010.github.io/2016/05/java-nio-file%E6%93%8D%E4%BD%9C/","tags":["java","nio"],"title":"Java NIO File操作"},{"categories":["devops"],"contents":"一般来说Web应用程序的开发者不太关心网络限速的问题。所以通常写的程序逻辑基本认为用户提交上来的数据速率越快越好；用户下载文件时，下载越快越好。但现实情况是服务器的带宽不是无限的，通常我们并不希望某一个用户的极速下载导致其它用户感觉此Web应用程序不可用。这样就带来了网络速率的需求。我在实际工作中大概总结出好几种限速办法，在这里记录以备忘。\nngx_http_core_module限制下载速率 最简单是直接使用ngx_http_core_module中的limit_rate、limit_rate_after指令，如下\nlocation /flv/ { alias /www/flv/; limit_rate_after 500k; limit_rate 50k; } limit_rate可限制响应传输至浏览器客户端的速率，limit_rate_after表示浏览器客户端下载多少后才可以执行限速(使下载小文件不受限，下载大文件才限速)。\nlimit_rate还有一种配合后端被代理服务器的用户，如下\nlocation /download/ { proxy_pass http://127.0.0.1:8080/download/; # proxied server return \u0026quot;X-Accel-Limit-Rate\u0026quot; response header } 后端被代理服务器可返回X-Accel-Limit-Rate响应头，nginx将根据这个响应头设置的值进行限速。这样就可以灵活控制限速的逻辑（比如有些用户下载不限速，有些用户下载限速，而且限速的数值也可根据不同用户身份而不同）\nnginx-upload-module限制上传速率 location /upload { # 转到后台处理URL,表示Nginx接收完上传的文件后，然后交给后端处理的地址 upload_pass @backend; # 临时保存路径, 可以使用散列 # 上传模块接收到的文件临时存放的路径， 1 表示方式，该方式是需要在/tmp/nginx_upload下创建以0到9为目录名称的目录，上传时候会进行一个散列处理。 upload_store /tmp/nginx_upload; # 上传文件的权限，rw表示读写 r只读 upload_store_access user:rw group:rw all:rw; set $upload_field_name \u0026quot;file\u0026quot;; # upload_resumable on; # 这里写入http报头，pass到后台页面后能获取这里set的报头字段 upload_set_form_field \u0026quot;${upload_field_name}_name\u0026quot; $upload_file_name; upload_set_form_field \u0026quot;${upload_field_name}_content_type\u0026quot; $upload_content_type; upload_set_form_field \u0026quot;${upload_field_name}_path\u0026quot; $upload_tmp_path; # Upload模块自动生成的一些信息，如文件大小与文件md5值 upload_aggregate_form_field \u0026quot;${upload_field_name}_md5\u0026quot; $upload_file_md5; upload_aggregate_form_field \u0026quot;${upload_field_name}_size\u0026quot; $upload_file_size; # 允许的字段，允许全部可以 \u0026quot;^.*$\u0026quot; upload_pass_form_field \u0026quot;^.*$\u0026quot;; # upload_pass_form_field \u0026quot;^submit$|^description$\u0026quot;; # 每秒字节速度控制，0表示不受控制，默认0, 128K upload_limit_rate 0; # 如果pass页面是以下状态码，就删除此次上传的临时文件 upload_cleanup 400 404 499 500-505; # 打开开关，意思就是把前端脚本请求的参数会传给后端的脚本语言，比如：http://192.168.1.251:9000/upload/?k=23,后台可以通过POST['k']来访问。 upload_pass_args on; } location @backend { proxy_pass http://127.0.0.1:8080/process_upload; } upload_limit_rate即可对上传速率进行限制。\nngx_stream_proxy_module限制上传下载速率 server { listen 81; proxy_pass 127.0.0.1:8081; proxy_download_rate 200k; proxy_upload_rate 200k; } 使用ngx_stream_proxy_module的好处时只要是tcp或udp协议且使用nginx作反向代理，都可以限速。proxy_download_rate可限制下载速率，proxy_upload_rate可限制上传速率。\nJava使用Guava的RateLimiter进行限速 上面说的全是使用nginx配置的方式进行限速，当有很特殊需求时，我们也可以使用程序来限速，如Java可使用Guava的RateLimiter进行限速。\nRateLimiter 从概念上来讲，速率限制器会在可配置的速率下分配许可证。如果必要的话，每个acquire() 会阻塞当前线程直到许可证可用后获取该许可证。一旦获取到许可证，不需要再释放许可证。\nRateLimiter使用的是一种叫令牌桶的流控算法，RateLimiter会按照一定的频率往桶里扔令牌，线程拿到令牌才能执行，比如你希望自己的应用程序QPS不要超过1000，那么RateLimiter设置1000的速率后，就会每秒往桶里扔1000个令牌。（这个跟nginx的ngx_http_limit_req_module中用到的leaky bucket是一个意思）\nRateLimiter经常用于限制对一些物理资源或者逻辑资源的访问速率。与Semaphore 相比，Semaphore 限制了并发访问的数量而不是使用速率。\nRateLimiter几个关键的方法  static RateLimiter create(double permitsPerSecond) 根据指定的稳定吞吐率创建RateLimiter，这里的吞吐率是指每秒多少许可数（通常是指QPS，每秒多少查询） static RateLimiter create(double permitsPerSecond, long warmupPeriod, TimeUnit unit) 根据指定的稳定吞吐率和预热期来创建RateLimiter，这里的吞吐率是指每秒多少许可数（通常是指QPS，每秒多少个请求量），在这段预热时间内，RateLimiter每秒分配的许可数会平稳地增长直到预热期结束时达到其最大速率。（只要存在足够请求数来使其饱和） double acquire(int permits) 从RateLimiter获取指定许可数，该方法会被阻塞直到获取到请求 void setRate(double permitsPerSecond) 动态更新RateLimite的稳定速率，参数permitsPerSecond 由构造RateLimiter的工厂方法提供。 boolean tryAcquire(int permits) 从RateLimiter 获取许可数，如果该许可数可以在无延迟下的情况下立即获取得到的话 boolean tryAcquire(int permits, long timeout, TimeUnit unit) 从RateLimiter 获取指定许可数如果该许可数可以在不超过timeout的时间内获取得到的话，或者如果无法在timeout 过期之前获取得到许可数的话，那么立即返回false （无需等待）  使用示例 限制写入response的速率不超过200kB/s\nRateLimiter limiter = RateLimiter.create(1024*200); while(....){ byte[] bytes = ... limiter.acquire(bytes.length); response.getWriter().write(bytes); } ","permalink":"https://jeremyxu2010.github.io/2016/05/web%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E9%99%90%E9%80%9F%E6%96%B9%E6%B3%95/","tags":["java","nginx","guava"],"title":"Web应用程序限速方法"},{"categories":["web开发"],"contents":"虽然经常用koa作为NodeJS Web项目的框架，但一直都是只知道怎么做，但并不知道它究竟是怎么实现的。今天花了些时间来研究它，在这里记录一下。\nGenerator函数 Generator函数形式 Generator函数是ES6提供的一种异步编程解决方案，语法行为与传统函数完全不同。Generator函数相当是一个状态机，封装了多个内部状态。执行Generator函数会返回一个遍历器对象，也就是说，Generator函数除了状态机，还是一个遍历器对象生成函数。返回的遍历器对象，可以依次遍历Generator函数内部的每一个状态。\nfunction* helloWorldGenerator() { yield \u0026#39;hello\u0026#39;; yield \u0026#39;world\u0026#39;; return \u0026#39;ending\u0026#39;; } var hw = helloWorldGenerator(); 在上面的例子里，helloWorldGenerator就是一个Generator函数，Generator函数的function关键字与函数名之间有一个星号，同时函数体内部使用yield语句，定义不同的内部状态，helloWorldGenerator这个Generator函数有3个状态：hello、world、ending。hw就是这个Generator函数产生的遍历器。\n下面使用hw这个遍历器对Generator函数的内部状态进行遍历\nconsole.log(JSON.stringify(hw.next())); // {value: \u0026#34;hello\u0026#34;, done: false}  console.log(JSON.stringify(hw.next())); // {\u0026#34;value\u0026#34;:\u0026#34;world\u0026#34;,\u0026#34;done\u0026#34;:false}  console.log(JSON.stringify(hw.next())); // {\u0026#34;value\u0026#34;:\u0026#34;ending\u0026#34;,\u0026#34;done\u0026#34;:true} yield语句的返回值 yield句本身没有返回值，或者说总是返回undefined。next方法可以带一个参数，该参数就会被当作上一个yield语句的返回值。于是自然就可以写出下面的代码。\nfunction doSomeThing(){ return \u0026#39;xxj\u0026#39;; } function doAnotherThing(name){ return \u0026#39;hello, \u0026#39; + name; } function* f(){ var name = yield doSomeThing(); var greeting = yield doAnotherThing(name); return greeting; } var gen = f(); var state1 = gen.next(); var state2 = gen.next(state1.value); var state3 = gen.next(state2.value); console.log(state3.value); // hello, xxj  上面的例子比较无聊，这种同步的方法使用Generator函数来调用并没有什么意义。请耐下心继续看下去。\nyield*语句 如果在Generater函数内部，调用另一个Generator函数，默认情况下是没有效果的。\nfunction* foo() { yield \u0026#39;a\u0026#39;; yield \u0026#39;b\u0026#39;; } function* bar() { yield \u0026#39;x\u0026#39;; foo(); yield \u0026#39;y\u0026#39;; } for (let v of bar()){ console.log(v); } // \u0026#34;x\u0026#34; // \u0026#34;y\u0026#34; 直接用yield也达不到期望的效果\nfunction* inner() { yield \u0026#39;hello!\u0026#39;; } function* outer1() { yield \u0026#39;open\u0026#39;; yield inner(); yield \u0026#39;close\u0026#39;; } var gen = outer1() gen.next().value // \u0026#34;open\u0026#34; gen.next().value // 返回一个遍历器对象 gen.next().value // \u0026#34;close\u0026#34; 可以看到第2次next()得到的value是一个遍历器对象，并没有得到“hello!”\n必须要用yield*才可以\nfunction* inner() { yield \u0026#39;hello!\u0026#39;; } function* outer2() { yield \u0026#39;open\u0026#39; yield* inner() yield \u0026#39;close\u0026#39; } var gen = outer2() gen.next().value // \u0026#34;open\u0026#34; gen.next().value // \u0026#34;hello!\u0026#34; gen.next().value // \u0026#34;close\u0026#34; co模块 从上面的例子可以看到，目前这种使用Generator函数的方式并没有解决什么问题，而且开发者还得自己不停地调用next方法才可以驱使Generator函数工作。那么是否有一种自动执行机制来驱使Generator函数呢？大招终于来了，co模块。\nco模块是著名程序员TJ Holowaychuk于2013年6月发布的一个小工具，用于Generator函数的自动执行。\n使用co模块时，Generator就相当于一个容纳n个异步操作的容器，co模块负责自动推动这个容器内的异步操作逐个执行。使用co的前提条件是，Generator函数的yield命令后面，只能是Promise对象、Thunk函数、数组、对象、Generator函数、Generator函数的遍历器(当然数组、对象的属性键值还是必须为这5种类型)。\n举个例子\ncallback的写法\nvar fs = require(\u0026#39;fs\u0026#39;); function readFiles(callback){ fs.readFile(\u0026#39;/etc/fstab\u0026#39;, function(error, data){ if (error) reject(error); var f1 = data; fs.readFile(\u0026#39;/etc/fstab\u0026#39;, function(error, data){ if (error) reject(error); f2 = data; console.log(f1.toString()); console.log(f2.toString()); callback(f1, f2); }); }); } readFiles(function(f1, f2){ console.log(\u0026#39;finished\u0026#39;); }); Genertor函数配合co模块的写法\nvar fs = require(\u0026#39;fs\u0026#39;); var co = require(\u0026#39;co\u0026#39;); var readFile = function (fileName){ return new Promise(function (resolve, reject){ fs.readFile(fileName, function(error, data){ if (error) reject(error); resolve(data); }); }); }; var gen = function* (){ var f1 = yield readFile(\u0026#39;/etc/fstab\u0026#39;); var f2 = yield readFile(\u0026#39;/etc/shells\u0026#39;); console.log(f1.toString()); console.log(f2.toString()); }; co(gen).then(function(){ console.log(\u0026#39;finished\u0026#39;); }); 这个例子比较简单，但已经可以看出Genertor函数配合co模块将原来可能要使用多个callback的代码一下就优化得像同步代码一样简单了。所要付出的代价仅仅是要求Generator函数的yield命令后面，只能是Promise对象、Thunk函数、数组、对象、Generator函数、Generator函数的遍历器(当然数组、对象的属性键值还是必须为这5种类型)。这个要求也不是很难实现，事实上很多第三库的接口已经是返回Promise对象了，即使不是的，也可以用new Promise(function (resolve, reject){...});自行封装一个，很简单。\nkoa的源码解读 先看一下koa最简单的使用示例。\nconst app = require(\u0026#39;koa\u0026#39;)(); app.use(function *(next) { console.log(1); yield next; console.log(4); }); app.use(function *(next) { console.log(2); yield next; console.log(3); }); app.use(function *() { this.body = \u0026#39;Hello World\u0026#39;; }); app.listen(3000); 可以看到use方法的3个参数都是Genertor函数，在koa里将这些Generator函数叫做middleware。\n再看一下use方法\napp.use = function(fn){ if (!this.experimental) { // es7 async functions are not allowed,  // so we have to make sure that `fn` is a generator function  assert(fn \u0026amp;\u0026amp; \u0026#39;GeneratorFunction\u0026#39; == fn.constructor.name, \u0026#39;app.use() requires a generator function\u0026#39;); } debug(\u0026#39;use %s\u0026#39;, fn._name || fn.name || \u0026#39;-\u0026#39;); this.middleware.push(fn); return this; }; 很简单，就是将这些个Generator函数塞进this.middleware这个数组。\n再看一下app.listen方法\napp.listen = function(){ debug(\u0026#39;listen\u0026#39;); var server = http.createServer(this.callback()); return server.listen.apply(server, arguments); }; 也很简单，就是创建一个http server, 监听某个地址，所有http请求交由this.callback处理。\n再看一下app.callback方法\napp.callback = function(){ if (this.experimental) { console.error(\u0026#39;Experimental ES7 Async Function support is deprecated. Please look into Koa v2 as the middleware signature has changed.\u0026#39;) } var fn = this.experimental ? compose_es7(this.middleware) : co.wrap(compose(this.middleware)); var self = this; if (!this.listeners(\u0026#39;error\u0026#39;).length) this.on(\u0026#39;error\u0026#39;, this.onerror); return function(req, res){ res.statusCode = 404; var ctx = self.createContext(req, res); onFinished(res, ctx.onerror); fn.call(ctx).then(function () { respond.call(ctx); }).catch(ctx.onerror); } }; co.wrap(compose(this.middleware));这一句很关键，首先compose(this.middleware)是使用koa-compose将this.middleware里所有的Generator函数组装成一个Generator函数，这个Generator函数容纳了所有middleware里的异步操作。然后co.wrap将这个超级Generator函数转换成一个返回Promise对象的函数fn。\nvar ctx = self.createContext(req, res);创建处理http请求的上下文。onFinished(res, ctx.onerror);处理http请求处理完毕后的后续事宜。fn.call(ctx)以刚创建http请求上下文作为this，调用刚才得到的函数fn。刚才说过这个函数的返回值是一个Promise。respond.call(ctx);在这个Promise的then方法里根据ctx上下文里保存的信息写回response。很简单吧，一切都那么自然。\n再回过头看一下koa-compose。\nfunction compose(middleware){ return function *(next){ if (!next) next = noop(); var i = middleware.length; while (i--) { next = middleware[i].call(this, next); } return yield *next; } } /** * Noop. * * @api private */ function *noop(){} 跟最初的想象一样，就是将middleware里所有的Generator函数组装成一个超级Generator函数。再回头看看koa的使用示例，这下终于明白next原来是下一个Generator函数(middleware)的遍历器。\nconst app = require(\u0026#39;koa\u0026#39;)(); app.use(function *(next) { console.log(1); yield next; console.log(4); }); app.use(function *(next) { console.log(2); yield next; console.log(3); }); app.use(function *() { this.body = \u0026#39;Hello World\u0026#39;; }); app.listen(3000); 当初看示例的时候一直有个疑问，隐隐觉得next应该是Generator函数的遍历器，但在一个Generator函数内部调用另一个Generator函数，应该是要使用yield*的，为啥官方示例却没有用过yield*，但功能却正常呢？\n于是做了个实验\nvar co = require(\u0026#39;co\u0026#39;); var compose = require(\u0026#39;koa-compose\u0026#39;); var i = 0; function promiseA (){ console.log(++i); return Promise.resolve(true); } function *middleware1 (next){ yield next; } function *middleware2 (next){ yield promiseA(); yield promiseA(); } var middleware = [middleware1, middleware2]; var gen = compose(middleware); var iter1 = gen(); while(!(iter1.next().done)){ } console.log(\u0026#39;finished\u0026#39;); // finished 这样功能是不正常的。但改成下面这样工作正常了。\nvar co = require(\u0026#39;co\u0026#39;); var compose = require(\u0026#39;koa-compose\u0026#39;); var i = 0; function promiseA (){ console.log(++i); return Promise.resolve(true); } function *middleware1 (next){ yield *next; } function *middleware2 (next){ yield promiseA(); yield promiseA(); } var middleware = [middleware1, middleware2]; var gen = compose(middleware); var iter1 = gen(); while(!(iter1.next().done)){ } console.log(\u0026#39;finished\u0026#39;); // 1 // 2 // finished 再配合co模块，功能才正常。\nvar co = require(\u0026#39;co\u0026#39;); var compose = require(\u0026#39;koa-compose\u0026#39;); var i = 0; function promiseA (){ console.log(++i); return Promise.resolve(true); } function *middleware1 (next){ yield next; } function *middleware2 (next){ yield promiseA(); yield promiseA(); } var middleware = [middleware1, middleware2]; var gen = compose(middleware); var fn = co.wrap(gen); fn().then(function(){ console.log(\u0026#39;finished\u0026#39;); }); 原因终于出来了，原来是co模块可以处理yield后面带Generator函数遍历器的场景。再看看co模块的代码。\nfunction co(gen) { var ctx = this; var args = slice.call(arguments, 1); // we wrap everything in a promise to avoid promise chaining,  // which leads to memory leak errors.  // see https://github.com/tj/co/issues/180  return new Promise(function(resolve, reject) { if (typeof gen === \u0026#39;function\u0026#39;) gen = gen.apply(ctx, args); if (!gen || typeof gen.next !== \u0026#39;function\u0026#39;) return resolve(gen); onFulfilled(); /** * @param {Mixed} res * @return {Promise} * @api private */ function onFulfilled(res) { var ret; try { ret = gen.next(res); } catch (e) { return reject(e); } next(ret); return null; } /** * @param {Error} err * @return {Promise} * @api private */ function onRejected(err) { var ret; try { ret = gen.throw(err); } catch (e) { return reject(e); } next(ret); } /** * Get the next value in the generator, * return a promise. * * @param {Object} ret * @return {Promise} * @api private */ function next(ret) { if (ret.done) return resolve(ret.value); var value = toPromise.call(ctx, ret.value); if (value \u0026amp;\u0026amp; isPromise(value)) return value.then(onFulfilled, onRejected); return onRejected(new TypeError(\u0026#39;You may only yield a function, promise, generator, array, or object, \u0026#39; + \u0026#39;but the following object was passed: \u0026#34;\u0026#39; + String(ret.value) + \u0026#39;\u0026#34;\u0026#39;)); } }); } /** * Convert a `yield`ed value into a promise. * * @param {Mixed} obj * @return {Promise} * @api private */ function toPromise(obj) { if (!obj) return obj; if (isPromise(obj)) return obj; if (isGeneratorFunction(obj) || isGenerator(obj)) return co.call(this, obj); if (\u0026#39;function\u0026#39; == typeof obj) return thunkToPromise.call(this, obj); if (Array.isArray(obj)) return arrayToPromise.call(this, obj); if (isObject(obj)) return objectToPromise.call(this, obj); return obj; } 核心就4个函数。\n co函数将Generator函数或Generator遍历器转换为Promise对象。(要求Generator函数的yield命令后面，只能是Promise对象、Thunk函数、数组、对象、Generator函数、Generator函数的遍历器) onFulfilled函数调用Generator函数的next方法，将得到的状态传给next函数。 next函数调用toPromise函数将状态里的value转换成Promise对象，再在Promise对象的then方法里调用onFulfilled函数，以推动Generator函数进入下一个状态。 toPromise对状态里的value(即yield后跟着的值)进行转换，将之转换为Promise对象。这个可以看到当obj是Generator函数或Generator遍历器时，又去调用co函数了。  大神的代码确实不简单，短短几个函数就把这么复杂的问题解决了。\n参考文档 阮一峰的ECMAScript 6 入门 - Generator 函数 阮一峰的ECMAScript 6 入门 - 异步操作和Async函数 koa源码 koa-compose源码 co源码\n","permalink":"https://jeremyxu2010.github.io/2016/05/koa%E6%A1%86%E6%9E%B6%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/","tags":["web","javascript","nodejs"],"title":"koa框架源码解读"},{"categories":["java开发"],"contents":"很长时间都是在用Netty进行网络编程，Java原生NIO的很多概念都忘得差不多了，今天在工作中遇到要使用ByteBuffer，发现竟然已经不会用了，这里将NIO中Buffer的概念再梳理一遍以备忘。\nBuffer的基本用法 Java NIO中的Buffer用于和NIO通道进行交互。如你所知，数据是从通道读入缓冲区，从缓冲区写入到通道中的。\n缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成NIO Buffer对象，并提供了一组方法，用来方便的访问该块内存。\n使用Buffer读写数据一般遵循以下四个步骤：\n 写入数据到Buffer 调用flip()方法 从Buffer中读取数据 调用clear()方法或者compact()方法  当向buffer写入数据时，buffer会记录下写了多少数据。一旦要读取数据，需要通过flip()方法将Buffer从写模式切换到读模式。在读模式下，可以读取之前写入到buffer的所有数据。\n一旦读完了所有的数据，就需要清空缓冲区，让它可以再次被写入。有两种方式能清空缓冲区：调用clear()或compact()方法。clear()方法会清空整个缓冲区。compact()方法只会清除已经读过的数据。任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面。\n示例：\nFileInputStream aFileIn = new FileInputStream(aFile); FileChannel inChannel = aFileIn.getChannel(); //create buffer with capacity of 4096 bytes ByteBuffer buf = ByteBuffer.allocate(4096); int bytesRead = -1; while ((bytesRead = inChannel.read(buf)) != -1) { buf.flip(); //make buffer ready for read  while(buf.hasRemaining()){ System.out.print((char) buf.get()); // read 1 byte at a time  } buf.clear(); //make buffer ready for writing } aFileIn.close(); Buffer的capacity,position和limit NIO的Buffer与Netty的不一样，它是单指针的，Buffer在不同模式下，指针的含义不同。\n为了理解Buffer的工作原理，需要熟悉它的三个属性：\n capacity position limit  position和limit的含义取决于Buffer处在读模式还是写模式。不管Buffer处在什么模式，capacity的含义总是一样的。\n capacity  作为一个内存块，Buffer有一个固定的大小值，也叫“capacity”.你只能往里写capacity个byte、long，char等类型。一旦Buffer满了，需要将其清空（通过读数据或者清除数据）才能继续写数据往里写数据。\n position  当你写数据到Buffer中时，position表示当前的位置。初始的position值为0.当一个byte、long等数据写到Buffer后， position会向前移动到下一个可插入数据的Buffer单元。position最大可为capacity – 1.\n当读取数据时，也是从某个特定位置读。当将Buffer从写模式切换到读模式，position会被重置为0. 当从Buffer的position处读取数据时，position向前移动到下一个可读的位置。\n limit  在写模式下，Buffer的limit表示你最多能往Buffer里写多少数据。 写模式下，limit等于Buffer的capacity。\n当切换Buffer到读模式时， limit表示你最多能读到多少数据。因此，当切换Buffer到读模式时，limit会被设置成写模式下的position值。换句话说，你能读到之前写入的所有数据（limit被设置成已写数据的数量，这个值在写模式下就是position）\nBuffer的类型 Java NIO 有以下Buffer类型\n ByteBuffer CharBuffer DoubleBuffer FloatBuffer IntBuffer LongBuffer ShortBuffer  这些Buffer类型代表了不同的数据类型, 可以通过char，short，int，long，float 或 double类型来操作缓冲区中的字节。\nBuffer的分配 要想获得一个Buffer对象首先要进行分配。 每一个Buffer类都有一个allocate方法。下面是一个分配48字节capacity的ByteBuffer的例子。\nByteBuffer buf = ByteBuffer.allocate(48); 向Buffer中写数据 写数据到Buffer有两种方式：\n 从Channel写到Buffer。 通过Buffer的put()方法写到Buffer里。  从Channel写到Buffer的例子\nint bytesRead = inChannel.read(buf); //read into buffer. 通过put方法写Buffer的例子\nbuf.put(127); 还有很多重载的put方法。\nflip方法 flip方法可以将Buffer从写模式切换到读模式。调用flip()方法会将position设回0，并将limit设置成之前position的值。\n从Buffer中读取数据 从Buffer中读取数据有两种方式：\n 从Buffer读取数据到Channel。 使用get()方法从Buffer中读取数据。  从Buffer读取数据到Channel的例子：\n//read from buffer into channel. int bytesWritten = inChannel.write(buf); 使用get()方法从Buffer中读取数据的例子\nbyte aByte = buf.get(); 还有很多重载的get方法。\nclear与compact方法 一旦读完Buffer中的数据，需要让Buffer准备好再次被写入。可以通过clear()或compact()方法来完成。\n如果调用的是clear()方法，position将被设回0，limit被设置成 capacity的值。换句话说，Buffer 被清空了。Buffer中的数据并未清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。\n如果Buffer中有一些未读的数据，调用clear()方法，数据将“被遗忘”，意味着不再有任何标记会告诉你哪些数据被读过，哪些还没有。\n如果Buffer中仍有未读的数据，且后续还需要这些数据，但是此时想要先先写些数据，那么使用compact()方法。\ncompact()方法将所有未读的数据拷贝到Buffer起始处。然后将position设到最后一个未读元素正后面。limit属性依然像clear()方法一样，设置成capacity。现在Buffer准备好写数据了，但是不会覆盖未读的数据。\n其它重要方法  mark()与reset()方法  通过调用Buffer.mark()方法，可以标记Buffer中的一个特定position。之后可以通过调用Buffer.reset()方法恢复到这个position。例如：\nbuffer.mark(); //call buffer.get() a couple of times, e.g. during parsing. buffer.reset(); //set position back to mark. ","permalink":"https://jeremyxu2010.github.io/2016/05/java-nio%E4%B8%ADbuffer%E4%BD%BF%E7%94%A8%E5%A4%87%E5%BF%98/","tags":["java","nio"],"title":"Java NIO中Buffer使用备忘"},{"categories":["nodejs开发"],"contents":"今天一上班，顺手点了一次构建整个项目，结果发现项目中的javascript编译报错，而且报的错莫名其秒。\nundefined is not iterable! 搜遍互联网才在babel的twitter上找到了这个问题的说明。\nIf you are getting an undefined is not iterable!error, pleasenpm install babel-types (to use v6.8.1). If necessary, clear node_modules\n看情况应该是babel相关的依赖自动升级导致的错误，这里鄙视一下NodeJS生态里的npmjs.com上的库，质量真的是参差不齐，明明安装的是兼容的版本，可实际上很有可能由于某个依赖的升级导致整个项目编译失败。\n实际上我之前已发现了这个问题，当时的方案是在package.json里将所有依赖的包指定一个确定的版本号，如下如示：\n \u0026quot;dependencies\u0026quot;: { \u0026quot;babel-polyfill\u0026quot;: \u0026quot;6.3.14\u0026quot;, \u0026quot;before-unload\u0026quot;: \u0026quot;2.0.0\u0026quot;, \u0026quot;bootstrap\u0026quot;: \u0026quot;3.3.4\u0026quot;, \u0026quot;bowser\u0026quot;: \u0026quot;1.0.0\u0026quot;, \u0026quot;browser-audio\u0026quot;: \u0026quot;1.0.2\u0026quot;, \u0026quot;classnames\u0026quot;: \u0026quot;2.1.2\u0026quot;, \u0026quot;cropper\u0026quot;: \u0026quot;0.11.0\u0026quot;, \u0026quot;extend\u0026quot;: \u0026quot;3.0.0\u0026quot;, \u0026quot;fancybox\u0026quot;: \u0026quot;3.0.0\u0026quot;, \u0026quot;favicon-notification\u0026quot;: \u0026quot;0.1.4\u0026quot;, \u0026quot;flux\u0026quot;: \u0026quot;2.0.3\u0026quot;, \u0026quot;fullscreen\u0026quot;: \u0026quot;1.0.0\u0026quot;, \u0026quot;immutable\u0026quot;: \u0026quot;3.7.5\u0026quot;, \u0026quot;inherits\u0026quot;: \u0026quot;2.0.1\u0026quot;, \u0026quot;jquery\u0026quot;: \u0026quot;1.11.3\u0026quot;, \u0026quot;jquery-textrange\u0026quot;: \u0026quot;1.3.3\u0026quot;, \u0026quot;jwt-decode\u0026quot;: \u0026quot;1.4.0\u0026quot;, \u0026quot;keymirror\u0026quot;: \u0026quot;0.1.1\u0026quot;, ... } 原以为这样依赖的版本号就固定了。但实际上在NodeJS生态里大量第三方库其package.json文件是这样的：\n\u0026quot;dependencies\u0026quot;: { \u0026quot;acorn\u0026quot;: \u0026quot;^3.0.0\u0026quot;, \u0026quot;async\u0026quot;: \u0026quot;^1.3.0\u0026quot;, \u0026quot;clone\u0026quot;: \u0026quot;^1.0.2\u0026quot;, \u0026quot;enhanced-resolve\u0026quot;: \u0026quot;^2.2.0\u0026quot;, \u0026quot;interpret\u0026quot;: \u0026quot;^1.0.0\u0026quot;, \u0026quot;loader-runner\u0026quot;: \u0026quot;^2.1.0\u0026quot;, \u0026quot;loader-utils\u0026quot;: \u0026quot;^0.2.11\u0026quot;, \u0026quot;memory-fs\u0026quot;: \u0026quot;~0.3.0\u0026quot;, \u0026quot;mkdirp\u0026quot;: \u0026quot;~0.5.0\u0026quot;, \u0026quot;node-libs-browser\u0026quot;: \u0026quot;^1.0.0\u0026quot;, \u0026quot;object-assign\u0026quot;: \u0026quot;^4.0.1\u0026quot;, \u0026quot;source-map\u0026quot;: \u0026quot;^0.5.3\u0026quot;, \u0026quot;supports-color\u0026quot;: \u0026quot;^3.1.0\u0026quot;, \u0026quot;tapable\u0026quot;: \u0026quot;~0.2.3\u0026quot;, \u0026quot;uglify-js\u0026quot;: \u0026quot;~2.6.0\u0026quot;, \u0026quot;watchpack\u0026quot;: \u0026quot;^1.0.0\u0026quot;, \u0026quot;webpack-sources\u0026quot;: \u0026quot;^0.1.0\u0026quot;, \u0026quot;yargs\u0026quot;: \u0026quot;^3.31.0\u0026quot; } 可以看到~表示该依赖可能会自动更新至最近的minor版本，^表示该依赖可能会自动更新至最近的major版本。\n这样就存在问题了，这里我用示例简单说明一下。\n最开始项目是这样的，其中A使用^依赖于B\nproj 1.0.0 A 1.1.0 B 1.2.0 某一天B的维护者发布了一个新的版本1.3.0，但他并没有经过完备的测试来保证一定是与1.2.0版本是兼容的。项目的维护者又手贱地执行了下npm install或npm install C，执行后，依赖树就变成下面这样了。\nproj 1.0.0 A 1.1.0 B 1.3.0 然后项目编译时就失败了，或者编译成功，但在浏览器中运行出错了，悲剧。\n怎么办？还好查到了npmjs.com官方针对这个问题的说明，详见这里\nnpm shrinkwrap的作用就是以项目为根，将项目依赖树上所有第三方库版本固定。 使用上还是比较简单的，就是执行npm shrinkwrap命令，就会在package.json旁边上一个npm-shrinkwrap.json，以后再执行npm install，就会安装npm-shrinkwrap.json里描述的确切版本。\n不过这里还有一个小坑，官方文档里说明如下：\nSince npm shrinkwrap is intended to lock down your dependencies for production use, devDependencies will not be included unless you explicitly set the --dev flag when you run npm shrinkwrap. 也就是默认不会锁定devDependencies的版本，除非执行npm shrinkwrap带上--dev参数。我建议执行npm shrinkwrap还是带上--dev参数，否则很有可能某天一个开发依赖库版本小升个版本号，你的项目又悲剧了。\n","permalink":"https://jeremyxu2010.github.io/2016/05/%E9%94%81%E5%AE%9Anodejs%E9%A1%B9%E7%9B%AE%E7%9A%84%E4%BE%9D%E8%B5%96%E5%BA%93/","tags":["nodejs","npm"],"title":"锁定NodeJS项目的依赖库"},{"categories":["devops"],"contents":"工作中经常要用到nginx，这里将使用nginx最常要用到的技巧记录下来以备忘。\n安装 在linux或mac下安装nginx还是很简单的，我一般都是直接下载源代码编译安装。这里要注意，configure时它会提示缺少某些开发库，按照它说明的安装上就可以编译了。另外我一般是将nginx的源码目录留下来，以免以后在用的过程中缺少某个module，需要重新编译安装。\nmkdir build/ tar xf nginx-x.y.z.tar.gz -C build/ cd build/nginx-x.y.z ./configure --prefix=/opt/nginx make \u0026amp;\u0026amp; make install 查看版本信息、启动、停止、检查配置文件、重加载配置、分割日志文件 查看版本信息\n/opt/nginx/sbin/nginx -V 启动\n/opt/nginx/sbin/nginx \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp; 停止\n/opt/nginx/sbin/nginx -s stop 检查配置文件\n/opt/nginx/sbin/nginx -t 重加载配置\n/opt/nginx/sbin/nginx -s reload 分割日志文件\nmv /opt/nginx/logs/access.log /opt/nginx/logs/access_20160430.log \u0026amp;\u0026amp; mv /opt/nginx/logs/error.log /opt/nginx/logs/error_20160430.log /opt/nginx/sbin/nginx -s reopen 当然一般也不会像上面这样启停nginx，一般会用initscripts, 可参考initscripts。日志的滚动也使用logrotate来完成，可参考使用logrotate管理nginx日志文件\n配置 配置文件的组成，这里摘录一下nginx官方文档的说明\nnginx consists of modules which are controlled by directives specified in the configuration file. Directives are divided into simple directives and block directives. A simple directive consists of the name and parameters separated by spaces and ends with a semicolon (;). A block directive has the same structure as a simple directive, but instead of the semicolon it ends with a set of additional instructions surrounded by braces ({ and }). If a block directive can have other directives inside braces, it is called a context (examples: events, http, server, and location). Directives placed in the configuration file outside of any contexts are considered to be in the main context. The events and http directives reside in the main context, server in http, and location in server.\n常用指令 error_log 配置错误日志输出到哪儿及日志的输出级别，详见这里\nevents 配置连接如何被处理，详见这里与这里\ninclude 包含其它配置文件，用于有效地分割配置文件，详见这里\npid 指定pid文件位置，详见这里\nthread_pool 设置线程池，详见这里\nuser 指定nginx运行时的用户身份，详见这里\nworker_processes 指定worker进程的数目，详见这里\nworker_connections 指定worker进程最大的连接数，详见这里\nclient_body_buffer_size 指定读取客户端请求体的buffer大小，详见这里\nclient_max_body_size 指定可读取客户端请求体的最大大小，详见这里\nclient_body_timeout 读取客户端请求体时，多久未传输任何数据，则认为请求超时了，详见这里\nclient_header_buffer_size 指定读取客户端请求头的buffer大小，详见这里\nclient_header_timeout 读取客户端请求头时，多久未传输任何数据，则认为请求超时了，详见这里\nalias 指定location使用的路径，与root类似，但不改变文件的跟路径，仅适用文件系统的路径，详见这里\ndefault_type 指定默认的MIME type，详见这里\nerror_page 为错误响应码指定响应客户端的URI，详见这里\ninternal 指定某个location仅内部可用，详见这里\nlimit_except 限制某个location里允许的HTTP方法，详见这里\nlimit_rate 限制响应发回客户端的速度，一般用于限速，详见这里\nlimit_rate_after 响应发回客户端传输多大之后开始限速，详见这里\nlisten 指定server监听的地址及端口，详见这里\nlocation 指定相对于某个URI的配置，详见这里\nlocation的优先级顺序比较复杂，见官方文档的三段话\nA location can either be defined by a prefix string, or by a regular expression. Regular expressions are specified with the preceding “~*” modifier (for case-insensitive matching), or the “~” modifier (for case-sensitive matching). To find location matching a given request, nginx first checks locations defined using the prefix strings (prefix locations). Among them, the location with the longest matching prefix is selected and remembered. Then regular expressions are checked, in the order of their appearance in the configuration file. The search of regular expressions terminates on the first match, and the corresponding configuration is used. If no match with a regular expression is found then the configuration of the prefix location remembered earlier is used.\nIf the longest matching prefix location has the “^~” modifier then regular expressions are not checked.\nAlso, using the “=” modifier it is possible to define an exact match of URI and location. If an exact match is found, the search terminates.\nresolver 指定用于查找upstream servers的DNS服务器，详见这里\nroot 指定响应请求的根目录，详见这里\nserver 为某个虚拟主机指定配置，详见这里\nserver_name 为虚拟主机指定主机名，详见这里\ntcp_nodelay 是否开启socket的TCP_NODELAY选项，详见这里\ntry_files 按指定的顺序检查请求的文件是否存在(请求文件的路径是根据root或alias得出的)，如找到，则直接响应请求，否则内部重定向至最后一个参数指定的uri，详见这里\ntypes 根据文件名的后缀决定输出的MIME type，详见这里\nallow 指定允许访问的IP地址或网络，详见这里\ndeny 指定拒绝访问的IP地址或网络，详见这里\nautoindex 是否开启列目录输出，详见这里\ncharset 添加指定的编译至响应头的Content-Type属性，详见这里\nempty_gif 输出一个1x1的透明gif图片，一般为占位图片，详见这里\ngzip 是否开启gzip响应，详见这里\ngzip_comp_level 设置gzip压缩的级别，级别越高压缩得越小，但越耗cpu，详见这里\ngzip_disable 如果User-Agent请求头匹配指定的正则表达式，则禁用gzip，详见这里\ngzip_min_length 当响应体超过这个大小才进行gzip压缩，详见这里\ngzip_types 针对哪些MIME type才进行gzip压缩，详见这里\nindex 指定哪些文件被作为索引页，详见这里\nlimit_conn_zone 定义限制连接数的数据区，详见这里\nlimit_conn 定义限制的连接数，详见这里\nlimit_req_zone 定义限制请求数的数据区，详见这里\nlimit_req 定义限制的请求数，详见这里\n限制请求数的逻辑比较复杂，参见这里\naccess_log 设置访问日志，详见这里\nlog_format 设置日志的格式，详见这里\nproxy_pass 设置代理的协议及地址，详见这里\nproxy_redirect 设置代理服务器Location及Refresh响应头里应作的替换，详见这里\nvalid_referers 设置合法的Referer请求头，详见这里\nreturn 停止处理，直接返回响应码至客户端，详见这里\nif if判断，详见这里\nrewrite 重写URL，详见这里\n这里注意重写URL时如果加上flag, 意义不一样。\nlast相当于重写URL后，该URL重新开始location匹配搜索\nbreak相当于中断在当前location里的rewrite处理\nredirect是302临时重定向\npermanent是301永久重定向\nssl 是否为指定的虚拟主机开启HTTPS协议，详见这里\nssl_certificate ssl的证书，详见这里\nssl_certificate_key ssl的私钥文件，详见这里\nupstream 定义一组upstream servers，详见这里\n示例 nginx官方有一个完整的示例\n其它示例：\n虚拟主机的示例\nhttp { server { listen 80; server_name www.domain1.com; access_log logs/domain1.access.log main; location / { index index.html; root /var/www/domain1.com/htdocs; } } server { listen 80; server_name www.domain2.com; access_log logs/domain2.access.log main; location / { index index.html; root /var/www/domain2.com/htdocs; } } } 负载均衡的示例\nhttp { upstream myproject { server 127.0.0.1:8000 weight=3;: server 127.0.0.1:8001; server 127.0.0.1:8002; server 127.0.0.1:8003; } server { listen 80; server_name www.domain.com; location / { proxy_pass http://myproject; } } } 更多 官方完整的指令列表\n官方完整的变量列表\n官方完整的内置模块列表\n使用Nginx的X-Accel-Redirect实现下载的示例\n使用mod_zip实现打包下载的示例\nnginx反向代理WebSockets的示例\nnginx反向代理WebSockets的示例\nnginx利用image_filter动态生成缩略图的示例\nnginx使用tcp代理实现HA的示例\n增强nginx ssl安全性的教程\n","permalink":"https://jeremyxu2010.github.io/2016/05/nginx%E4%BD%BF%E7%94%A8%E5%A4%87%E5%BF%98/","tags":["nginx","web"],"title":"nginx使用备忘"},{"categories":["web开发"],"contents":"作为一个从事多年Java Web开发的程序员，面对现如今NodeJS开发Web后端程序一直十分感兴趣，于是花了点时间研究了下，本篇就主要说一说我在项目中应用NodeJS开发后端的具体步骤。实在是受Java Web后端开发影响太大了，我使用NodeJS开发后端程序还是采用了普通Java MVC分层架构，可能与一般的NodeJS程序员的做法不太一样。\n数据访问层 首先定义一个全局唯一的数据访问对象。\nsrc/server/dao/DB.js\n\u0026#34;use strict\u0026#34;; const orm = require(\u0026#34;orm\u0026#34;); const qOrm = require(\u0026#39;q-orm\u0026#39;); const transaction = require(\u0026#39;orm-transaction\u0026#39;); const config = require(\u0026#39;../config.js\u0026#39;); function defineSchemas(db){ db.schemas = { qUser : db.qDefine(\u0026#34;users\u0026#34;, { id : Number, username : String, pwd : String }) }; } const DB = qOrm.qConnect({ host: config.db.host, database: config.db.database, user: config.db.user, password: config.db.pwd, protocol: \u0026#39;mysql\u0026#39;, port: config.db.port, query: { reconnect : true, pool: true, debug: false } }).then(function (db) { db.use(transaction); defineSchemas(db); return db; }).fail(function (err) { throw err; }); module.exports = DB; 可以看到DB还是比较简单的，就是按orm的基本用法定义了数据库连接与数据库中的schema, 这里有一点不同的是我比较喜欢返回Promise对象，避免写过多callback，因此采用了q-orm。一般的数据访问层都提供了数据库事务的处理，在NodeJS里，我没找到太多选择，只找到orm-transaction，同样不太喜欢它默认给出的callback用法，简单封装了一个返回Promise的工具方法。\nTransaction.js\nmodule.exports = function(db){ return new Promise(function(resolve, reject){ db.transaction(function (err, t) { if(!err){ resolve({ qCommit : function (){ return new Promise(function(resolve2, reject2){ t.commit(function(err){ if(!err){ resolve2(); } else { reject2(err); } }); }); }, qRollback : function (){ return new Promise(function(resolve2, reject2){ t.rollback(function(err){ if(!err){ resolve2(); } else { reject2(err); } }); }); } }); } else { reject(err); } }); }); }; 业务Service层 这样简直写一个业务Service层实现，有Java基础的同学一看一定觉得很熟悉。\nsrc/server/service/UserService.js\n\u0026#34;use strict\u0026#34;; const DB = require(\u0026#39;../dao/DB.js\u0026#39;); const Transaction = require(\u0026#39;../dao/Transaction.js\u0026#39;); module.exports = { findAll : function *(){ let db = yield DB; let users = yield db.schemas.qUser.qAll(); return Promise.resolve(users); }, findByUsername : function *(username){ let db = yield DB; let user = yield db.schemas.qUser.qOne({username : username}); return Promise.resolve(user); }, updateUserPwd : function *(username, pwd){ let db = yield DB; let user = yield db.schemas.qUser.qOne({username : username}); user.pwd = pwd; let transaction = yield Transaction(db); try { yield user.qSave(); yield transaction.qCommit(); } catch (err){ console.log(err); yield transaction.qRollback(); } } }; 因为数据访问层都是返回的Promise，这里就可以很方便使用Generator函数与yield来书写代码逻辑了，避免了JavaScript里大量的callback，Generator函数与yield的用法可以参考阮一峰的异步操作的同步化表达。\nController层 虽然NodeJS里叫这个为route，但我还是习惯按Java的玩法叫它Controller，我使用了koa-router，还是比较好用的。\nsrc\\server\\controller\\UserController.js\n\u0026#34;use strict\u0026#34;; const Router = require(\u0026#39;koa-router\u0026#39;); const UserService = require(\u0026#39;../service/UserService.js\u0026#39;); const UserController = new Router({ prefix: \u0026#39;/users\u0026#39; }); UserController.get(\u0026#39;/\u0026#39;, function *(){ let users = yield UserService.findAll(); this.body = users; }); UserController.get(\u0026#39;/:username\u0026#39;, function *(){ let user = yield UserService.findByUsername(this.params.username); this.body = user; }); UserController.post(\u0026#39;/:username\u0026#39;, function *(){ yield UserService.updateUserPwd(this.params.username, this.request.body.pwd); this.body = {success : true}; }); module.exports = UserController; 中间件容器 我使用koa作为这个小Web项目的中间件容器，简单写一个启动器。\nsrc\\server\\app.js\nconst koa = require(\u0026#39;koa\u0026#39;); const app = koa(); const json = require(\u0026#39;koa-json\u0026#39;); const bodyParser = require(\u0026#39;koa-bodyparser\u0026#39;); const UserController = require(\u0026#39;./controller/UserController.js\u0026#39;); app.use(json({ pretty: false, param: \u0026#39;pretty\u0026#39; })); app.use(bodyParser()); app.use(UserController.routes()).use(UserController.allowedMethods()); app.listen(3000); 最后执行命令node src\\server\\app.js, 这个小Web项目就跑起来了。\n总结 个人感觉使用NodeJS写简单的Web后端程序确实比用Java简单了不少，最关键是不用编译，异常地快。\n本篇源代码地址\n","permalink":"https://jeremyxu2010.github.io/2016/04/%E7%8E%B0%E4%BB%A3web%E5%BC%80%E5%8F%91%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B_06/","tags":["javascript","nodejs","webmvc"],"title":"现代Web开发系列教程_06"},{"categories":["web开发"],"contents":"本篇不开发新的功能，不过对目前的编译环境重新整理一下。\n区别开发编译与生产编译 在webpack.config.js中添加读取环境变量NODE_ENV\n... var isProduction = (process.env.NODE_ENV === \u0026#39;production\u0026#39;); ... 编译出css文件 ... var ExtractTextPlugin = require(\u0026#39;extract-text-webpack-plugin\u0026#39;); ... var plugins_options = [ ... new ExtractTextPlugin(\u0026#39;css/[name].css\u0026#39;, {allChunks: true}), ... ]; var loaders = [ ... { test: /\\.css$/, loader: ExtractTextPlugin.extract(\u0026#34;style-loader\u0026#34;, \u0026#34;css-loader\u0026#34; + (isProduction ? \u0026#39;\u0026#39; : \u0026#39;?sourceMap\u0026#39;) + \u0026#34;!postcss-loader\u0026#34;) },{ test: /\\.less$/, loader: ExtractTextPlugin.extract(\u0026#34;style-loader\u0026#34;, \u0026#34;css-loader\u0026#34; + (isProduction ? \u0026#39;\u0026#39; : \u0026#39;?sourceMap\u0026#39;) + \u0026#34;!postcss-loader!less-loader\u0026#34; + (isProduction ? \u0026#39;\u0026#39; : \u0026#39;?sourceMap\u0026#39;)) } ... ]; var webpackConfig = { ... module: { ... postcss: function () { return [autoprefixer]; } } }; 非生产编译模式，编译出sourcemap，以方便调试 if(!isProduction){ plugins_options.push(new webpack.SourceMapDevToolPlugin({ test: /\\.(js|css|less)($|\\?)/i, filename: \u0026#39;[file].map\u0026#39; })); } 生产编译模式压缩、去注释、优化排序 if(isProduction){ plugins_options.push(new webpack.optimize.UglifyJsPlugin({ compress: { warnings: false }, output: { comments: false } })); plugins_options.push(new webpack.optimize.OccurenceOrderPlugin()); } 非生产编译模式，启用模块热替换 if(!isProduction){ plugins_options.push(new webpack.HotModuleReplacementPlugin()); } 非生产编译模式，编译出的js带webpack_dev_client if(!isProduction){ entries.webpack_dev_client = \u0026#39;webpack-dev-server/client?http://0.0.0.0:5000\u0026#39;; entries.webpack_hot_dev_server = \u0026#39;webpack/hot/only-dev-server\u0026#39;; } webpack-dev-server的配置 var devServer_options = { host : \u0026#39;0.0.0.0\u0026#39;, port: 5000, contentBase: \u0026#34;.\u0026#34;, progress: true, hot: true, inline: true, stats: { colors: true }, noInfo: true, historyApiFallback: true }; var webpackConfig = { ... devServer: devServer_options, ... }; 非生产编译模式，使用eval方式生成sourcemap，这个速度最快 if(!isProduction){ webpackConfig.devtool = \u0026lsquo;eval\u0026rsquo;; }\n使用eslint对js进行静态检查 var loaders = [ ... { test: /\\.(js|jsx)$/, exclude: [/node_modules/], // exclude any and all files in the node_modules folder  loaders: [\u0026#39;babel-loader\u0026#39;, \u0026#39;eslint-loader\u0026#39;, \u0026#39;strict-loader\u0026#39;] } ... ]; 安装一系列npm包 npm install webpack-dev-server --global npm install webpack-dev-server extract-text-webpack-plugin eslint-loader eslint strict-loader eslint-plugin-react babel-eslint style-loader css-loader postcss-loader postcss less-loader less autoprefixer file-loader url-loader img-loader --save-dev 最后在package.json里加入两行npm script  \u0026quot;scripts\u0026quot;: { \u0026quot;serve-dev\u0026quot;: \u0026quot;./node_modules/.bin/webpack-dev-server --progress\u0026quot;, \u0026quot;build-prod\u0026quot;: \u0026quot;NODE_ENV=production ./node_modules/.bin/webpack --progress\u0026quot; } 以后执行npm run serve-dev就直接打开了webpack-dev-server了，开发终于不用再依赖于nginx了，直接访问http://127.0.0.1:5000/demo4/demo4.html就可以看到页面效果，而且可以修改了代码后还可以热替换。 如果只想编译出最优化的代码，输入npm run build-prod就好了。\n前端应该会玩了吧，下篇我将开始说后端了。\n本篇源代码地址\n","permalink":"https://jeremyxu2010.github.io/2016/04/%E7%8E%B0%E4%BB%A3web%E5%BC%80%E5%8F%91%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B_05/","tags":["web","html5","javascript"],"title":"现代Web开发系列教程_05"},{"categories":["web开发"],"contents":"本篇使用redux结合react重写刚才那个很简单的hello world示例。\nredux的理念 redux有三个重要的理念：单一数据源、状态是只读的、使用纯函数转换状态。具体见链接\n安装redux与react-redux npm install redux react-redux --save 状态转换纯函数 web-src/js/components/GreetingConstant.js\nexport const CHANGE_NAME = \u0026#39;CHANGE_NAME\u0026#39;; web-src/js/reducers/GreetingReducer.js\nimport {CHANGE_NAME} from \u0026#39;../constants/GreetingConstant.js\u0026#39; const initialState = { name: \u0026#39;\u0026#39;, output: \u0026#39;\u0026#39; } export function GreetingReducer(state = initialState, action) { if (typeof state === \u0026#39;undefined\u0026#39;) { return initialState; } switch(action.type) { case CHANGE_NAME: return Object.assign({}, state, { name : action.name, output: \u0026#39;Hello, \u0026#39; + action.name }); default: return state; } }; 这两个文件很简单，GreetingConstant.js里定义了action类型的常量，GreetingReducer.js就是一个普通纯函数，它的工作就是根据action转换state。\nactionCreator action是一个纯对象，其中保存了用来转换state的信息，一般包括type类型及其它参数，官方是这样定义的Actions are payloads of information that send data from your application to your store.。\nactionCreator则是产生action的方法。\nweb-src/js/actions/GreetingAction.js\nimport { CHANGE_NAME } from \u0026#39;../constants/GreetingConstant.js\u0026#39; export function changeName(name) { return { type : CHANGE_NAME, name: name }; } 组件 在redux与react项目中，组件分为Presentational Components与Container Components，有的地方叫Dump Components与Smart Components\n    Presentational Components Container Components     Purpose How things look (markup, styles) How things work (data fetching, state updates)   Aware of Redux No Yes   To read data Read data from props Subscribe to Redux state   To change data Invoke callbacks from props Dispatch Redux actions   Are written By hand Usually generated by React Redux    简单来说Presentational Components是完全根据props属性决定行为与展现的组件，完成不感知redux的存在。Container Component则负责从state中抽取属性，分发redux's action，这里一般会用到redux的connect方法，还是看下面的代码。\nweb-src/js/components/GreetingComponent.js，这个就是一个Presentational Components组件\nimport React from \u0026#39;react\u0026#39; const noop = function(){}; class GreetingComponent extends React.Component{ constructor(props){ super(props); this._changeName = this.changeName.bind(this); } changeName(e){ this.props.changeName(e.target.value); } render() { return ( \u0026lt;div\u0026gt; \u0026lt;input value={this.props.name} onChange={this._changeName}/\u0026gt;\u0026lt;br/\u0026gt; \u0026lt;label\u0026gt;{this.props.output}\u0026lt;/label\u0026gt; \u0026lt;/div\u0026gt; ); } }; GreetingComponent.propTypes = { changeName: React.PropTypes.func.isRequired, name: React.PropTypes.string.isRequired, output: React.PropTypes.string.isRequired }; GreetingComponent.defaultProps = { changeName: noop, name: \u0026#39;\u0026#39;, output: \u0026#39;\u0026#39; }; export default GreetingComponent; web-src/js/containers/GreetingContainer.js，这个就是一个Container Component组件\nimport { connect } from \u0026#39;react-redux\u0026#39; import GreetingComponent from \u0026#39;../components/GreetingComponent.js\u0026#39; import {changeName} from \u0026#39;../actions/GreetingAction.js\u0026#39; const mapStateToProps = (state) =\u0026gt; { return { name: state.name, output: state.output } } const mapDispatchToProps = (dispatch) =\u0026gt; { return { changeName: (name) =\u0026gt; { dispatch(changeName(name)) } } } const GreetingContainer = connect( mapStateToProps, mapDispatchToProps )(GreetingComponent) export default GreetingContainer web-src/js/components/GreetingApp.js，这个就是一个Presentational Components组件\nimport React from \u0026#39;react\u0026#39; import GreetingContainer from \u0026#39;../containers/GreetingContainer.js\u0026#39; export default class GreetingApp extends React.Component{ render(){ return \u0026lt;GreetingContainer/\u0026gt; } } 使用Provider将state与组件关联起来 web-src/js/entries/demo3.js\nimport React from \u0026#39;react\u0026#39; import { render } from \u0026#39;react-dom\u0026#39; import { Provider } from \u0026#39;react-redux\u0026#39; import { createStore } from \u0026#39;redux\u0026#39; import {GreetingReducer} from \u0026#39;../reducers/GreetingReducer.js\u0026#39; import GreetingApp from \u0026#39;../components/GreetingApp.js\u0026#39; let store = createStore(GreetingReducer) render( \u0026lt;Provider store={store}\u0026gt; \u0026lt;GreetingApp /\u0026gt; \u0026lt;/Provider\u0026gt;, document.getElementById(\u0026#39;reactHolder\u0026#39;) ) 这里用到了redux的Provider，它会把store附属到组件树的context上，其子组件就都可以访问到store了。\n本篇源代码地址\n","permalink":"https://jeremyxu2010.github.io/2016/04/%E7%8E%B0%E4%BB%A3web%E5%BC%80%E5%8F%91%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B_04/","tags":["web","html5","javascript"],"title":"现代Web开发系列教程_04"},{"categories":["web开发"],"contents":"现在我们使用React重写昨天的hello world示例。本篇涉及了很多react的知识，如果不清楚，建议先看看react官方文档\n安装react及babel npm install react react-dom --save npm install babel-core babel-loader babel-preset-react --save-dev 修改js代码及模板文件 demo2.js\nvar React = require(\u0026#39;react\u0026#39;); var ReactDOM = require(\u0026#39;react-dom\u0026#39;); var Greeting = React.createClass({ render : function(){ return \u0026lt;h1\u0026gt;Hello, world!\u0026lt;/h1\u0026gt;; } }); ReactDOM.render( \u0026lt;Greeting /\u0026gt;, document.getElementById(\u0026#39;reactHolder\u0026#39;) ); base.html\n\u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;\u0026lt;%= htmlWebpackPlugin.options.title || \u0026#39;Webpack App\u0026#39;%\u0026gt;\u0026lt;/title\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;IE=edge,chrome=1\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;renderer\u0026#34; content=\u0026#34;webkit\u0026#34;\u0026gt; \u0026lt;% for (var css in htmlWebpackPlugin.files.css) { %\u0026gt; \u0026lt;link href=\u0026#34;\u0026lt;%= htmlWebpackPlugin.files.css[css] %\u0026gt;\u0026#34; rel=\u0026#34;stylesheet\u0026#34;\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body class=\u0026#34;body-cls\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;reactHolder\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;% for (var chunk in htmlWebpackPlugin.files.chunks) { %\u0026gt; \u0026lt;script src=\u0026#34;\u0026lt;%= htmlWebpackPlugin.files.chunks[chunk].entry %\u0026gt;\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 修改webpack编译脚本 webpack.config.js\nvar webpack = require(\u0026quot;webpack\u0026quot;); var HtmlWebpackPlugin = require('html-webpack-plugin'); var output_options = { path: 'public', publicPath: '/demo2/', filename: 'js/[name].js', hotUpdateMainFilename: 'hot-update/[name].[hash].hot-update.js', chunkFilename: 'js/chunks/[name].js', hotUpdateChunkFilename: 'hot-update/chunks/[name].[hash].hot-update.js', }; var createHtmlDef = function(opts){ return new HtmlWebpackPlugin({ title: 'demo2', filename: opts.path, template: 'web-src/html/base.html', chunks: opts.chunks, hash: true, inject: false }); }; var plugins_options = [ createHtmlDef({path: 'demo2.html', chunks: ['vendor', 'demo2']}) ]; var entries = { demo2 : __dirname + '/web-src/js/demo2.js', vendor : ['react', 'react-dom'] }; var loaders = [ {test: /\\.(js|jsx)$/, loaders: [\u0026quot;babel\u0026quot;]}, ]; var webpackConfig = { entry: entries, output: output_options, plugins: plugins_options, module: { loaders: loaders } }; module.exports = webpackConfig; 因为是使用babel来编译jsx语法的js文件，需要对babel进行一些配置\n.babalrc\n{ \u0026quot;presets\u0026quot;: [\u0026quot;react\u0026quot;] } 把这个小demo部署到nginx里吧\nln -sf demo2/public /usr/local/var/www/demo2 最后在浏览器里访问http://127.0.0.1/demo2/demo2.html，在页面上就可以看到Hello World了。\n转换成ES6写法 react新版本已经推荐采用ES6写法了，也得用上吧。\nnpm install babel-preset-es2015 --save-dev 修改.babelrc文件\n{ \u0026quot;presets\u0026quot;: [\u0026quot;react\u0026quot;, \u0026quot;es2015\u0026quot;] } 修改demo2.js\nimport React from \u0026#39;react\u0026#39; import ReactDOM from \u0026#39;react-dom\u0026#39; class Greeting extends React.Component { render(){ return \u0026lt;h1\u0026gt;Hello, world!\u0026lt;/h1\u0026gt;; } } ReactDOM.render( \u0026lt;Greeting /\u0026gt;, document.getElementById(\u0026#39;reactHolder\u0026#39;) ); 去除编译后的js大量重复内容 在webpack.config.js里作一点点修改\nvar plugins_options = [ createHtmlDef({path: 'demo2.html', chunks: ['vendor', 'demo2']}), new webpack.optimize.CommonsChunkPlugin({ name: \u0026quot;vendor\u0026quot;, minChunks: Infinity, }) ]; 这里其实有个技巧，我研究了很久才想明白，其它人我不告诉的。CommonsChunkPlugin的工作原理相当于把指定的chunks里每个chunk引用的所有资源先抽出来，然后对这些重复资源进行统计，看每个重复资源被n个chunk引用，如果n\u0026gt;=minChunks设置的值，则将该资源移至common chunk里，如果n\u0026lt;minChunks，则看该资源name指定的chunk引用不，如果引用就只将该资源返还给name指定的chunk，否则返还给所有引用它的chunk。因此\n new webpack.optimize.CommonsChunkPlugin({ name: \u0026quot;vendor\u0026quot;, minChunks: Infinity, }) var entries = { demo2 : __dirname + '/web-src/js/demo2.js', vendor : ['react', 'react-dom'] }; 相当于把引用的第三方资源抽到vendor.js里了。\n本篇源代码地址\n","permalink":"https://jeremyxu2010.github.io/2016/04/%E7%8E%B0%E4%BB%A3web%E5%BC%80%E5%8F%91%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B_03/","tags":["web","html5","javascript"],"title":"现代Web开发系列教程_03"},{"categories":["web开发"],"contents":"引言 本文从一个很小的前端工程说起，慢慢推导出我目前多个项目前端实践的工程结构。\n为Web前端项目建工程 Web工程需要建工程吗？不是建个目录就可以开搞的吗？你一定会这么问。但事实上面对越来越复杂的前端页面逻辑，真的有必要好好组织一下前端的代码结构。同时前端代码的依赖管理、编译方式真的需要考量一下。\nmkdir -p demo1 cd demo1 npm install # 回答一系列回答后，一个npm工程就建好了 创建前端源代码目录及编译后产物目录 从Java带来的习惯，我是不习惯将源代码与编译后的产物放在一起的，还是分开一点好。\nmkdir web-src #前端源代码目录 mkdir public #前端源代码编译后产物目录 来一个Hello World 在web-src目录下写一个JS版的hello world\nweb-src/js/demo1.js\nconsole.log(\u0026#39;Hello World!\u0026#39;); 写编译脚本 在demo1目录中执行命令安装webpack\nnpm install webpack --global #--global是安装到系统全局 npm install webpack --save-dev #--save-dev是安装到node_modules目录下，并修改package.json文件，添加此开发依赖 在demo1目录建一个webpack.config.js文件，在这里配置如何使用webpack编译。\nwebpack.config.js\nvar webpack = require(\u0026#34;webpack\u0026#34;); var output_options = { path: \u0026#39;public\u0026#39;, publicPath: \u0026#39;/demo1/\u0026#39;, filename: \u0026#39;js/[name].js\u0026#39;, hotUpdateMainFilename: \u0026#39;hot-update/[hash].hot-update.json\u0026#39;, chunkFilename: \u0026#39;js/chunks/[name].js\u0026#39;, hotUpdateChunkFilename: \u0026#39;hot-update/chunks/[id].[hash].hot-update.js\u0026#39;, }; var entries = { demo1 : __dirname + \u0026#39;/web-src/js/demo1.js\u0026#39; }; var webpackConfig = { entry: entries, output: output_options }; module.exports = webpackConfig; 这时在demo1目录下执行webpack命令，就会在public/js目录下生成编译后的js文件\nwebpack 你打开public/js/demo1.js一看，肯定吓一跳，就一个简单的hello world，怎么就编出这么大的文件，这个是脱裤子放屁。如果这个系列你继续看下去，你会发现这一些也是值得的。\n生成html页面 光编出js文件，如果没法在浏览器里查看运行效果也是白搭，我不建议自己手工写html页面来引用生成的js文件，还是让webpack来做吧。\n安装webpack生成html的插件\nnpm install html-webpack-plugin --save-dev 在web-src/html目录下创建html的模板文件\n\u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;\u0026lt;%= htmlWebpackPlugin.options.title || \u0026#39;Webpack App\u0026#39;%\u0026gt;\u0026lt;/title\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;IE=edge,chrome=1\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;renderer\u0026#34; content=\u0026#34;webkit\u0026#34;\u0026gt; \u0026lt;% for (var css in htmlWebpackPlugin.files.css) { %\u0026gt; \u0026lt;link href=\u0026#34;\u0026lt;%= htmlWebpackPlugin.files.css[css] %\u0026gt;\u0026#34; rel=\u0026#34;stylesheet\u0026#34;\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body class=\u0026#34;body-cls\u0026#34;\u0026gt; \u0026lt;% for (var chunk in htmlWebpackPlugin.files.chunks) { %\u0026gt; \u0026lt;script src=\u0026#34;\u0026lt;%= htmlWebpackPlugin.files.chunks[chunk].entry %\u0026gt;\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 在webpack.config.js文件加入几行\n... var HtmlWebpackPlugin = require(\u0026#39;html-webpack-plugin\u0026#39;); var createHtmlDef = function(opts){ return new HtmlWebpackPlugin({ title: \u0026#39;demo1\u0026#39;, filename: opts.path, template: \u0026#39;web-src/html/base.html\u0026#39;, chunks: opts.chunks, hash: true, inject: false }); }; var plugins_options = [ createHtmlDef({path: \u0026#39;demo1.html\u0026#39;, chunks: [\u0026#39;demo1\u0026#39;]}) ]; var webpackConfig = { ... plugins: plugins_options, ... }; ... 然后在demo1目录下执行webpack命令，即完成编译\nwebpack 部署到nginx里 我本机安装了nginx，因此就直接把这个小demo部署到nginx里吧\nln -sf demo1/public /usr/local/var/www/demo1 最后在浏览器里访问http://127.0.0.1/demo1/demo1.html，打开chrome的控制台，应该就可以看到Hello World了。这么费劲才搞了个hello world, 确实很无聊，下篇我们在页面上做点好玩的。本篇源代码地址。\nwebpack的一些概念 本篇里出现了不少webpack的概念，这里简单介绍一下，详细的可以参考官方文档\nwebpack里有三个概念：入口文件（entry），分块（chunk），模块（module）。\nmodule指各种资源文件，如js、css、图片、svg、scss、less等等，一切资源皆被当做模块。\nchunk：包含一个或者多个资源，必须可以被其它资源用require依赖。\nentry：入口，也是包含了一个或者多个资源，它是一种特殊的chunk，不是一定得能被其它资源依赖，通常是使用html标签直接引入到页面里的。\n比如本篇中项目执行webpack命令的输出如下\nHash: 7bd5b3403fe5f918821c Version: webpack 1.13.0 Time: 528ms Asset Size Chunks Chunk Names js/demo1.js 1.42 kB 0 [emitted] demo1 demo1.html 318 bytes [emitted] [0] ./web-src/js/demo1.js 29 bytes {0} [built] Child html-webpack-plugin for \u0026quot;demo1.html\u0026quot;: + 3 hidden modules 这里可以看到entry里配置的名称为demo1的entry生成了一个同名的chunk, 该chunk的编号为0，该chunk最后编译生成了js/demo1.js。而通过HtmlWebpackPlugin插件生成了demo1.html，该插件的配置里写明了生成的html页面需要引入名称为demo1的chunk，因此最后生成的html页面里就用script标签引入了js/demo1.js，并且由于hash: true，引入的url后面还加了hash，以避免浏览器缓存js。\n事实上除了定义entry的方式间接声明一个chunk外，还有另一种方法，这个后面会遇到，到时再说吧。\n","permalink":"https://jeremyxu2010.github.io/2016/04/%E7%8E%B0%E4%BB%A3web%E5%BC%80%E5%8F%91%E6%95%99%E7%A8%8B%E7%B3%BB%E5%88%97_02/","tags":["web","html5","javascript"],"title":"现代Web开发教程系列_02"},{"categories":["web开发"],"contents":"##引言\n工作大概9年了，就算不算上在学校里的写网页的经历，其实从事与Web前端有关的开发已经8年。这8年我自己也随着Web开发技术的革新，也在不停地更新这方面的知识体系。最近在想，是时候对目前所掌握的Web开发技术进行一个小结了。于是就有了这个系列的开始，这个系列开篇会概览性地说一下Web开发几个重要的工具或技术，后面则会在以demo示例的方式讲一讲在真实项目中是如何实践它们的。\n很多年前，那个时候写前端页面真的很简单，基本上抓起一个编辑器就可以开始写html了，当时页面真的很简单，就是一堆html标签，有个别事件处理，动画效果，几行javascript代码就搞定了。但事实上随着现在Web端开发技术的发展，Web前端页面变得越来越复杂了，甚至还有人十分推崇One Page Application，即在一个页面搞定所有的用户界面交互。后端服务则变得越来越简单了，服务端渲染技术用的人越来越少了，后端基本上就成了提供restful api的server了。\n编译工具链 面对Web前端越来越复杂的页面逻辑，以前在页面里随意写javascript代码的方式必须更新了，javascript代码必须被更合理地管理，于是产生了不少前端代码模块依赖系统，比较出名的是cmd, amd, umd。javascript代码被合理地拆分后，面临一个问题，文件过多，而用户浏览网页时不可能等待下载如此多的脚本文件下载，所以必须将拆分后的脚本文件进行合并，合并之后最好还能不影响调试，如果是生产环境最好还能对脚本文件去重、压缩等一系统优化。经过几个项目的实践，我最终还是选择了插件众多，功能强大的webpack作为Web前端代码的编译工具。\n界面库 很多年前，我做Web开发也谈不上界面库，一度认为html标准的那些标签就是界面库，抓起来就用。后来发现有时候一些html标签及其附带的javascript代码、css代码频繁重复，于是就想起是应该封装部分组件了，当时Web开发社区里也出现了不少jquery插件，基本上很多功能很复杂的组件都可以封装到一个jquery插件里去，同时也出现了像extjs那样很成体系的前端框架。后来又出现了bootstrap这种前端css框架，写样式更简单了。但一直觉得前端的技术不成体系，一个项目做下来，前端无非是jquery+n个jquery插件+bootstrap样式框架+其它杂七杂八的业务js代码。多个项目做下来，最大的收获就是积累了不少jquery插件的用法，还有了解了不少浏览器的避坑方案。直到发现了React，才发现界面库可以如此简单高效，而且不像extjs那样有超高的约束性。\n前端分层框架 我是从java Web开发过来的，很早就知道架构分层的概念，后端很早就有了Controller + Service + Model分层的概念，但前端分层却是从“One Page Application”开始流行才开始，渐渐地MVVM成了前端分层的标配。这方面我倒不是特关心，当然目前与React配合最好的还是redux了。\n与后端通讯的技术 最常用的当时是直接发GET, POST请求，稍高级一点采用AJAX方式发GET, POST请求，但如果你能用起来websocket的话，我建议还是可以大量地采用websocket，实时双工是很大的优势。\n","permalink":"https://jeremyxu2010.github.io/2016/04/%E7%8E%B0%E4%BB%A3web%E5%BC%80%E5%8F%91%E6%95%99%E7%A8%8B%E7%B3%BB%E5%88%97_01/","tags":["web","html5","javascript"],"title":"现代Web开发教程系列_01"},{"categories":["工具"],"contents":"家里的路由器TL-WR941D还是多年前买的，之前一直用着还挺稳定的，只不过有时觉得网速有点慢。最近却频频遇到问题，一会儿ping国外某个IP丢包率奇高，一会儿DNS经常解析域名失败。之前就听说现在OpenWRT已经很稳定了，今天周末在家没什么事儿，决定刷OpenWRT算了。\n下载对应的刷机包 刷之前先进TP-Link的Web管理控制台看了下版本，发现是TL-WR941D v6版，因此下载对应的刷机包, 登入TP-Link的Web管理控制台，在更新系统那里选择该刷机包，直接刷入就可以了。\n配置OpenWRT 使用有线将电脑与路由器接好，然后执行命令\ntelnet 192.168.1.1 #登入OpenWRT后，因为我家是使用的adsl，所以执行下面的命令设置好wan口 uci set network.wan.proto=pppoe uci set network.wan.username=\u0026#39;${your adsl login name}\u0026#39; uci set network.wan.proto=\u0026#39;${your adsl password}\u0026#39; uci commit network ifup wan #现在安装OpenWRT Web图形管理界面 opkg update opkg install luci luci-i18n-chinese /etc/init.d/uhttpd start /etc/init.d/uhttpd enable 然后使用浏览器访问http://192.168.1.1，这时可以直接登录进去，不需要输入密码，首先在“System-System-Language”里将语言修改为“chinese”, 保存，刷新页面，这里页面就变成中文了。\n然后在“系统-管理权”这里给路由器设个密码，以后命令行登入ssh root@192.168.1.1或Web控制台http://192.168.1.1登录就需要输入密码了。\n在我这里OpenWRT默认没有打开WiFi，同时登入Web控制台，在“网络-无线”这里对无线网络进行设置后，启动该无线网络就可以了。\n","permalink":"https://jeremyxu2010.github.io/2016/04/tl-wr941d%E8%B7%AF%E7%94%B1%E5%99%A8%E5%88%B7openwrt%E5%A4%87%E5%BF%98/","tags":["OpenWRT","linux"],"title":"TL-WR941D路由器刷OpenWRT备忘"},{"categories":["java开发"],"contents":"今天的工作涉及了不少JVM底层的知识，趁着今天刚翻阅资料，还记得一些内容，将我常用的JVM知识整理一下。\nJVM组成 JVM = 类加载器 classloader + 执行引擎 execution engine + 运行时数据区域 runtime data area\n类加载器 classloader 类加载器classloader的作用就是装载.class文件，当然类文件不一定是以.class文件的方式存储。\n类加载器classloader有两种装载类的时机：\n1. 隐式：运行过程中，碰到new方式生成对象时，隐式调用classLoader装载类到JVM 2. 显式：通过class.forname()动态加载类到JVM  类加载器的工作方式 类的加载过程采用双亲委托机制，这种机制能更好的保证 Java 平台的安全。 该模型要求除了顶层的Bootstrap class loader启动类加载器外，其余的类加载器都应当有自己的父类加载器。子类加载器和父类加载器不是以继承（Inheritance）的关系来实现，而是通过组合（Composition）关系来复用父加载器的代码。每个类加载器都有自己的命名空间。（由该加载器及所有父类加载器所加载的类组成，在同一个命名空间中，不会出现类的完整名字（包括类的包名）相同的两个类；在不同的命名空间中，有可能会出现类的完整名字（包括类的包名）相同的两个类）\n双亲委派模型的工作过程如下： 1. 当前 ClassLoader 首先从自己已经加载的类的缓存中查询是否此类已经加载，如果已经加载则直接返回原来已经加载的类。 2. 当前 classLoader 的缓存中没有找到被加载的类的时候，委托父类加载器去加载，父类加载器采用同样的策略，首先查看自己的缓存，然后委托父类的父类去加载，一直到 bootstrap ClassLoader. 3. 当所有的父类加载器都没有加载的时候，再由当前的类加载器加载，并将其放入它自己的缓存中，以便下次有加载请求的时候直接返回。\n这种工作方式的原因： 主要是为了安全性，避免用户自己编写的类动态替换 Java 的一些核心类，比如String，同时也避免了重复加载，因为 JVM中区分不同类，不仅仅是根据类名，相同的 class 文件被不同的 ClassLoader加载就是不同的两个类，如果相互转型的话会抛java.lang.ClassCaseException.\n类加载器的层次结构 1. Bootstrap class loader: 当运行 java 虚拟机时，这个类加载器被创建，它负责加载虚拟机的核心类库，如 java.lang.* 等。例如 java.lang.Object 就是由根类加载器加载的。 2. Extension class loader: 这个加载器加载出了基本 API 之外的一些拓展类。 3. AppClass Loader: 加载应用程序和程序员自定义的类。 4. User-defined Class Loader: 用户定制的类加载器，Java 提供了抽象类java.lang.ClassLoader，所有用户自定义的类加载器应该继承 ClassLoader 类。  执行引擎 JVM Runtime 执行引擎JVM Runtime的作用是执行字节码，或者执行本地方法\nGC 垃圾回收机制是由垃圾收集器Garbage Collection GC来实现的，GC是后台的守护进程。它的特别之处是它是一个低优先级进程，但是可以根据内存的使用情况动态的调整他的优先级。因此，它是在内存中低到一定限度时才会自动运行，从而实现对内存的回收。这就是垃圾回收的时间不确定的原因。\n因为GC也是进程，也要消耗CPU等资源，如果GC执行过于频繁会对java的程序的执行产生较大的影响，所以JVM中GC是不定期的，同时采用分代的方式进行对象的收集，以缩短GC对应用造成的暂停。\nGC只能回收通过new关键字申请的堆内内存，但是堆上的内存并不完全是通过new申请分配的。还有一些本地方法（一般是调用的C方法）。这部分“特殊的内存”如果不手动释放，就会导致内存泄露，gc是无法回收这部分内存的。 所以需要在finalize中用本地方法(native method)如free操作等，再使用gc方法。\n运行时数据区域 runtime data area JVM 运行时数据区 (JVM Runtime Area) 其实就是指 JVM在运行期间，其对JVM内存空间的划分和分配。JVM在运行时将数据划分为了6个区域来存储。 程序员写的所有程序都被加载到运行时数据区域中，不同类别存放在heap, java stack, native method stack, PC register, method area.\n1. PC程序计数器 一块较小的内存空间，可以看做是当前线程所执行的字节码的行号指示器, NAMELY存储每个线程下一步将执行的JVM指令，如该方法为native的，则PC寄存器中不存储任何信息。Java 的多线程机制离不开程序计数器，每个线程都有一个自己的PC，以便完成不同线程上下文环境的切换。 2. java虚拟机栈 与 PC 一样，java 虚拟机栈也是线程私有的。每一个 JVM 线程都有自己的java 虚拟机栈，这个栈与线程同时创建，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 3. 本地方法栈 与虚拟机栈的作用相似，虚拟机栈为虚拟机执行执行java方法服务，而本地方法栈则为虚拟机使用到的本地方法服务。 4. Java堆 被所有线程共享的一块存储区域，在虚拟机启动时创建，它是JVM用来存储对象实例以及数组值的区域，可以认为Java中所有通过new创建的对象的内存都在此分配。 Java堆在JVM启动的时候就被创建，堆中储存了各种对象，这些对象被Garbage Collector（垃圾回收器）所管理。这些对象无需、也无法显示地被销毁。 Java堆分为两块：新生代New Generation和旧生代Old Generation 5. 方法区 方法区和堆区域一样，是各个线程共享的内存区域，它用于存储每一个类的结构信息，例如运行时常量池，成员变量和方法数据，构造函数和普通函数的字节码内容，还包括一些在类、实例、接口初始化时用到的特殊方法。当开发人员在程序中通过Class对象中的getName、isInstance等方法获取信息时，这些数据都来自方法区。 方法区也是全局共享的，在虚拟机启动时候创建。在一定条件下它也会被GC。这块区域对应Permanent Generation 持久代。 XX：PermSize指定大小。 6. 运行时常量池 其空间从方法区中分配，存放的为类中固定的常量信息、方法和域的引用信息。  执行引擎中的GC主要是对Java堆空间进行内存回收，而Java中的垃圾回收技术还很复杂，这里简单记录一下\n垃圾回收技术 Java堆的分代 由于GC需要消耗一些资源和时间的，Java在对对象的生命周期特征进行分析后，采用了分代的方式来进行对象的收集，即按照新生代、旧生代的方式来对对象进行收集，以尽可能的缩短GC对应用造成的暂停. heap 的组成有三区域/世代：\n1. 新生代 Young Generation 1.1 Eden Space 任何新进入运行时数据区域的实例都会存放在此 1.2 S0 Suvivor Space 存在时间较长，经过垃圾回收没有被清除的实例，就从Eden 搬到了S0 1.3 S1 Survivor Space 同理，存在时间更长的实例，就从S0 搬到了S1 2. 旧生代 Old Generation/tenured 存在时间更长的实例，对象多次回收没被清除，就从S1 搬到了tenured 3. Perm 存放运行时数据区的方法区  JVM收集器 上面有7中收集器，分为两块，上面为新生代收集器，下面是老年代收集器。如果两个收集器之间存在连线，就说明它们可以搭配使用。\n Serial(串行GC)收集器  Serial收集器是一个新生代收集器，单线程执行，使用复制算法。它在进行垃圾收集时，必须暂停其他所有的工作线程(用户线程)。是Jvm client模式下默认的新生代收集器。对于限定单个CPU的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。\n ParNew(并行GC)收集器  ParNew收集器其实就是serial收集器的多线程版本，除了使用多条线程进行垃圾收集之外，其余行为与Serial收集器一样。\n Parallel Scavenge(并行回收GC)收集器  Parallel Scavenge收集器也是一个新生代收集器，它也是使用复制算法的收集器，又是并行多线程收集器。parallel Scavenge收集器的特点是它的关注点与其他收集器不同，CMS等收集器的关注点是尽可能地缩短垃圾收集时用户线程的停顿时间，而parallel Scavenge收集器的目标则是达到一个可控制的吞吐量。吞吐量= 程序运行时间/(程序运行时间 + 垃圾收集时间)，虚拟机总共运行了100分钟。其中垃圾收集花掉1分钟，那吞吐量就是99%。\n Serial Old(串行GC)收集器  Serial Old是Serial收集器的老年代版本，它同样使用一个单线程执行收集，使用“标记-整理”算法。主要使用在Client模式下的虚拟机。\n Parallel Old(并行GC)收集器  Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记-整理”算法。\n CMS(并发GC)收集器  CMS(Concurrent Mark Sweep)收集器是一种以获取最短回收停顿时间为目标的收集器。CMS收集器是基于“标记-清除”算法实现的\n G1收集器  G1(Garbage First)收集器是JDK1.7提供的一个新收集器，G1收集器基于“标记-整理”算法实现，也就是说不会产生内存碎片。还有一个特点之前的收集器进行收集的范围都是整个新生代或老年代，而G1将整个Java堆(包括新生代，老年代)\nClient、Server模式默认GC    JVM模式 新生代GC方式 老年代和持久代GC方式     Client Serial 串行GC Serial Old 串行GC   Server Parallel Scavenge 并行回收GC\tParallel Old 并行GC    Sun/oracle JDK GC组合方式    JVM参数　 新生代GC方式 老年代和持久代GC方式     -XX:+UseSerialGC Serial 串行GC Serial Old 串行GC   -XX:+UseParallelGC Parallel Scavenge 并行回收GC Serial Old 并行GC   -XX:+UseConcMarkSweepGC ParNew 并行GC CMS 并发GC, 当出现“Concurrent Mode Failure”时,采用Serial Old 串行GC   -XX:+UseParNewGC ParNew 并行GC Serial Old 串行GC   -XX:+UseParallelOldGC Parallel Scavenge 并行回收GC\tParallel Old 并行GC   -XX:+UseConcMarkSweepGC -XX:-UseParNewGC Serial 串行GC CMS 并发GC, 当出现“Concurrent Mode Failure”时,采用Serial Old 串行GC    不同世代采用不同回收方法 通俗的回收方法： 1. 引用计数法。简单但速度很慢。缺陷是：不能处理循环引用的情况。 2. 停止-复制(stop and copy)。效率低，需要的空间大，优点，不会产生碎片。 3. 标记 - 清除算法 (mark and sweep)。速度较快，占用空间少，标记清除后会产生大量的碎片。\n分代回收： 1. 在新生代中，使用“停止-复制”算法进行Minor GC，将新生代内存分为2部分，1部分Eden区较大，1部分Survivor比较小，并被划分为两个等量的部分。每次进行清理时，将Eden区和一个Survivor中仍然存活的对象拷贝到 另一个Survivor中，然后清理掉Eden和刚才的Survivor。 由于绝大部分的对象都是短命的，甚至存活不到Survivor中，所以，Eden区与Survivor的比例较大，HotSpot默认是8:1，即分别占新生代的80%，10%，10%。如果一次回收中，Survivor+Eden中存活下来的内存超过了10%，则需要将一部分对象分配到 老年代。用-XX:SurvivorRatio参数来配置Eden区域Survivor区的容量比值。 2. 老年代存储的对象比年轻代多得多，而且不乏大对象，对老年代进行内存清理时，如果使用停止-复制算法，则相当低效。一般，老年代用的算法是标记-整理算法，即：标记出仍然存活的对象（存在引用的），将所有存活的对象向一端移动，以保证内存的连续。在发生Minor GC时，虚拟机会检查每次晋升进入老年代的大小是否大于老年代的剩余空间大小，如果大于，则直接触发一次Full GC。 3. 永久代的回收有两种：常量池中的常量，无用的类信息，常量的回收很简单，没有引用了就可以被回收。对于无用的类进行回收，必须保证3点：1）类的所有实例都已经被回收 2）加载类的ClassLoader已经被回收 3）类对象的Class对象没有被引用（即没有通过反射引用该类的地方） 永久代的回收并不是必须的，可以通过参数来设置是否对类进行回收。HotSpot提供-Xnoclassgc进行控制\n常用JVM参数  -Xms或-XX:InitialHeapSize 初始堆容量 -Xmx或-XX:MaxHeapSize 最大堆容量 -Xmn或-XX:NewSize -XX:MaxNewSize 新生代容量 -XX:SurvivorRatio 新生代中eden的比例，如果设置为8，意味着新生代中eden占据80%的空间，两个survivor分别占据10% -XX:NewRatio 年轻代(包括Eden和两个Survivor区)与年老代的比值(除去持久代) -XX:PermSize 初始持久代容量 -XX:MaxPermSize 最大持久代容量 -XX:MaxDirectMemorySize 最大Java NIO Direct Buffer容量 -XX:+PrintGCDetails 让jvm在每次发生gc的时候打印日志，利于分析gc的原因和状况 -Xss 每个线程的堆栈大小，JDK5.0以后每个线程堆栈大小为1M，在相同物理内存下,减小这个值能生成更多的线程，一般小的应用， 如果栈不是很深， 应该是128k够用的大的应用建议使用256k。这个选项对性能影响比较大，需要严格的测试。 -XX:+DisableExplicitGC 关闭System.gc() -XX:MaxTenuringThreshold 垃圾最大年龄，如果设置为0的话,则年轻代对象不经过Survivor区,直接进入年老代。对于年老代比较多的应用,可以提高效率.如果将此值设置为一个较大值,则年轻代对象会在Survivor区进行多次复制,这样可以增加对象再年轻代的存活 时间,增加在年轻代即被回收的概率，该参数只有在串行GC时才有效. -Xnoclassgc 禁用持久代垃圾回收 -XX:+PrintGCDateStamps 打印GC发生的时间信息 -Xloggc:filename 指定gc日志存储路径，如-Xloggc:/tmp/gc.log -XX:+HeapDumpOnOutOfMemoryError 当出现OOM时将Heap dump出来 -XX:HeapDumpPath Heap dump出来时保存的目录 -XX:ErrorFile=targetDir/hs_err_pid_%p.log JVM crash日志的存储路径 -XX:+CMSPermGenSweepingEnabled -XX:+CMSClassUnloadingEnabled 为了避免Perm区满引起的Full GC，建议开启CMS回收Perm区选项 -XX:CMSInitiatingOccupancyFraction=80 默认CMS是在tenured generation沾满68%的时候开始进行CMS收集，如果你的年老代增长不是那么快，并且希望降低CMS次数的话，可以适当调高此值 -XX:+ExplicitGCInvokesConcurrent 调用System.gc()时触发CMS Full GC，需配合CMS使用 -verbose:gc 记录 GC 运行以及运行时间，一般用来查看 GC 是否是应用的瓶颈 -XX:+UseG1GC 使用G1垃圾回收算法 -XX:PretenureSizeThreshold　直接晋升到老年代对象的大小，设置这个参数后，大于这个参数的对象将直接在老年代分配 -XX:UseAdaptiveSizePolicy　动态调整java堆中各个区域的大小以及进入老年代的年龄 -XX:ParallelGCThreads　设置并行GC进行内存回收的线程数 -XX:GCTimeRatio　GC时间占总时间的比列，默认值为99，即允许1%的GC时间，仅在使用Parallel Scavenge 收集器时有效 -XX:MaxGCPauseMillis　设置GC的最大停顿时间，在Parallel Scavenge 收集器下有效 -XX:+UseCMSCompactAtFullCollection　由于CMS收集器会产生碎片，此参数设置在垃圾收集器后是否需要一次内存碎片整理过程，仅在CMS收集器时有效 -XX:CMSFullGCBeforeCompaction　设置CMS收集器在进行n次垃圾收集后再进行一次内存碎片整理过程，通常与UseCMSCompactAtFullCollection参数一起使用 -XX:+CMSParallelRemarkEnabled　降低CMS标记停顿 -XX:MinHeapFreeRatio　指定 jvm heap 在使用率小于 n 的情况下,　heap进行收缩,　Xmx==Xms的情况下无效, 如:-XX:MinHeapFreeRatio=30 -XX:MaxHeapFreeRatio　指定 jvm heap 在使用率大于 n 的情况下,　heap进行扩张,　Xmx==Xms的情况下无效, 如:-XX:MaxHeapFreeRatio=70 -XX:+UseGCLogFileRotation　开启回转日志文件 -XX:GCLogFileSize　设置单个文件最大的文件大小 -XX:NumberOfGCLogFiles　设置回转日志文件的个数 -XX:+PrintFlagsFinal 打印出几乎所有的JVM支持的参数以及他们的默认值 -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8090 开启Java远程调试 -Dcom.sun.management.jmxremote=true -Djava.rmi.server.hostname=${ip} -Dcom.sun.management.jmxremote.port=${port} -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false 允许jconsole接入 -verbose:[class|gc|jni] 开启指定类型的日志输出  ","permalink":"https://jeremyxu2010.github.io/2016/04/jvm%E5%BA%95%E5%B1%82%E7%9F%A5%E8%AF%86%E5%A4%87%E5%BF%98/","tags":["java"],"title":"JVM底层知识备忘"},{"categories":["工具"],"contents":"sed是一个非交互式的流编辑器（stream editor）。所谓非交互式，是指使用sed只能在命令行下输入编辑命令来编辑文本，然后在屏幕上查看输出；而所谓流编辑器，是指sed每次只从文件（或输入）读入一行，然后对该行进行指定的处理，并将结果输出到屏幕（除非取消了屏幕输出又没有显式地使用打印命令），接着读入下一行。整个文件像流水一样被逐行处理然后逐行输出。\n工作中经常会使用sed命令对文件进行各种操作，之前一直对它的工作原理不是很了解，只不过在网上抄一些命令完成操作，有时遇到了问题，就问一问身边的“脚本小王子”，基本上都可以搞定。今天下班了决定对sed命令深入学习一下。\n工作原理 核心逻辑 sed一次处理一行内容。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”（pattern space），接着用sed命令处理缓冲区(pattern space)中的内容，处理完成后，把缓冲区(pattern space)的内容送往屏幕。接着清空缓冲区(pattern space)，处理下一行，这样不断重复，直到文件末尾。\nsed里有两个空间：pattern space与hold space。\npattern space（模式空间）相当于车间sed把流内容在这里处理；\nhold space（保留空间）相当于仓库，加工的半成品在这里临时储存（当然加工完的成品也在这里存储）。\nsed处理每一行的逻辑：\n1. 先读入一行，去掉尾部换行符，存入pattern space，执行编辑命令。 2. 处理完毕，除非加了-n参数，把现在的pattern space打印出来，在后边打印曾去掉的换行符。 3. 把pattern space内容给hold space，把pattern space置空。 4. 接着读下一行，处理下一行。  命令组织形式 sed最重要的命令组织形式可以概括为*** [address[,address]][!]{cmd} ***\naddress address可以是一个数字，也可以是一个模式，你可以通过逗号要分隔两个address 表示两个address的区间\nman手册上对address的理解比较全面\nAddresses Sed commands can be given with no addresses, in which case the command will be executed for all input lines; with one address, in which case the command will only be executed for input lines which match that address; or with two addresses, in which case the command will be executed for all input lines which match the inclusive range of lines starting from the first address and continuing to the second address. Three things to note about address ranges: the syntax is addr1,addr2 (i.e., the addresses are separated by a comma); the line which addr1 matched will always be accepted, even if addr2 selects an earlier line; and if addr2 is a regexp, it will not be tested against the line that addr1 matched. After the address (or address-range), and before the command, a ! may be inserted, which specifies that the command shall only be executed if the address (or address-range) does not match. The following address types are supported: number Match only the specified line number (which increments cumulatively across files, unless the -s option is specified on the command line). first~step Match every step'th line starting with line first. For example, ``sed -n 1~2p'' will print all the odd-numbered lines in the input stream, and the address 2~5 will match every fifth line, starting with the second. first can be zero; in this case, sed operates as if it were equal to step. (This is an extension.) $ Match the last line. /regexp/ Match lines matching the regular expression regexp. \\cregexpc Match lines matching the regular expression regexp. The c may be any character. GNU sed also supports some special 2-address forms: 0,addr2 Start out in \u0026quot;matched first address\u0026quot; state, until addr2 is found. This is similar to 1,addr2, except that if addr2 matches the very first line of input the 0,addr2 form will be at the end of its range, whereas the 1,addr2 form will still be at the beginning of its range. This works only when addr2 is a regular expression. addr1,+N Will match addr1 and the N lines following addr1. addr1,~N Will match addr1 and the lines following addr1 until the next line whose input line number is a multiple of N. ! 表示匹配成功后是否执行命令\ncmd 要执行的命令。\n下面摘录一下sed man文档中的常用命令（其中删除了较复杂的与label有关的命令）\n不可接受address的命令  } The closing bracket of a { } block. 可接受零个或一个address的命令  = Print the current line number. a \\ text Append text, which has each embedded newline preceded by a backslash. i \\ text Insert text, which has each embedded newline preceded by a backslash. r filename Append text read from filename. R filename Append a line read from filename. Each invocation of the command reads a line from the file. This is a GNU extension. 可接受address范围的命令  { Begin a block of commands (end with a }). c \\ text Replace the selected lines with text, which has each embedded newline preceded by a backslash. d Delete pattern space. Start next cycle. D If pattern space contains no newline, start a normal new cycle as if the d command was issued. Otherwise, delete text in the pattern space up to the first newline, and restart cycle with the resul- tant pattern space, without reading a new line of input. h H Copy/append pattern space to hold space. g G Copy/append hold space to pattern space. n N Read/append the next line of input into the pattern space. p Print the current pattern space. P Print up to the first embedded newline of the current pattern space. s/regexp/replacement/ Attempt to match regexp against the pattern space. If successful, replace that portion matched with replacement. The replacement may contain the special character \u0026amp; to refer to that portion of the pattern space which matched, and the special escapes \\1 through \\9 to refer to the corresponding matching sub-expressions in the regexp. w filename Write the current pattern space to filename. W filename Write the first line of the current pattern space to filename. This is a GNU extension. x Exchange the contents of the hold and pattern spaces. y/source/dest/ Transliterate the characters in the pattern space which appear in source to the corresponding character in dest. 常用命令解析 sed -n '1p' test.txt 打印第一行，这条命令其实应该理解为sed -n '1 p' test.txt, 其中1是一个address，这条命令实际是说按照address的说明，仅第一行被作为要操作的address范围，那么在这个范围里每一行就执行p命令，同时-n说明不要把处理的模式空间内容打印出来，于是最后就打印了第一行。\nsed -i 's/abcd/efgh/g' test.txt 将文件中所有的abcd替换成efgh，这条命令没有address范围，那么address范围默认就是整个文件范围，这里对整个文件范围里每一行执行s/abcd/efgh/g命令，即将每一行里的abcd替换成efgh, 同时因为有/g选项，一行里如果出现多个abcd, 就每一个都会替换。-i参数说明将直接修改文件，而不仅仅将结果打印到标准输出里(注意MAC OSX下要达成相同效果要写-i '')。\nsed '{/This/{/fish/d}}' test.txt 删除文件中即有This也有fish的行，这条命令没有address范围，那么address范围默认就是整个文件范围，这里对整个文件范围里每一行执行{/This/{/fish/d}}命令，这是个嵌套命令，意思是先匹配/This/，匹配成功的行再尝试匹配/fish/，如果又匹配成功，则删除该行。\nsed '{/This/d; /fish/d}' test.txt 删除文件中有This或fish的行，这条命令与上面那条很像，但逻辑很不一样。这条命令没有address范围，那么address范围默认就是整个文件范围，这里对整个文件范围里每一行执行{/This/d; /fish/d}命令，这也是个嵌套命令，意思是针对当前行，先匹配/This/，如果匹配成功，则删除该行，否则再尝试匹配/fish/，如果匹配成功，则删除该行。\nsed -i '/abcd/,/efgh/ s/xxx/yyyy/g' test.txt 这条命令就很好理解了，它有address范围，在文件里先匹配/abcd/，以匹配的行为范围的起点，再在文件里匹配/efgh/，以匹配的行为范围的终点，在这个范围里执行s/xxx/yyyy/g命令，这个就不用再解释了。\n小结 还有一些关于hold space的高级用法，我平时没怎么用到，不过只要头脑里有pattern space与hold space的概念，按sed核心的执行逻辑推演一下，还是可以看懂那些高级用法的，至于要熟练运用的话就得靠多练了。附上sed常用命令及中文解释\nPS MAC OSX里记得需要使用brew install gnu-sed安装GNU版的sed，然后使用gsed， 自带的BSD版本sed功能实在弱了点。\n","permalink":"https://jeremyxu2010.github.io/2016/04/sed%E5%91%BD%E4%BB%A4%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%91%BD%E4%BB%A4%E5%A4%87%E5%BF%98/","tags":["linux","sed"],"title":"sed命令工作原理及命令备忘"},{"categories":["java开发"],"contents":"目前在做的java项目里有一个需求，已经将用户在进行一个业务操作的操作行为记录下来了，形成了这些操作行为的指令文件，然后需要将这些指令文件编码为mp4视频。项目之前用的是xuggle来完成的，不过xuggle项目好像有四五年没有更新了，甚至我将OSX升级至10.11之后，xuggle就没法在我本机编译通过了，报了一大堆的错。上xuggle的github仓库一看，人家也说不维护了，推荐使用https://github.com/artclarke/humble-video了，不过我尝试了下，依然没能把humble-video在我本机编译通过。看来得找其它解决方案了。上网搜索过后，找到两个替代方案jcodec和javacv，对比编码性能后，最终选择了javacv，纯java方案相对于jni方案性能差得不是一星半点啊。不过在使用javacv过程中还是遇到了不少坑，在这里分享一下，也可以帮助一下正在这些坑里的兄弟们。\n首先参照javacv的文档，在pom.xml里添加\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.bytedeco\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;javacv\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 然后快速地写了个JavaCVMp4Encoder\npackage test; public class JavaCVMp4Encoder implements Mp4Encoder { private String fileName; private FFmpegFrameRecorder recorder; private static final double FRAME_RATE = 25.0; private static final double MOTION_FACTOR = 1; private static final Java2DFrameConverter java2dConverter; private static final Logger log = LoggerFactory.getLogger(JavaCVMp4Encoder.class); public JavaCVMp4Encoder(){ this.java2dConverter = new Java2DFrameConverter(); } @Override public void make(String fileName) { this.fileName = fileName; } @Override public void configVideo(int width, int height) { recorder = new FFmpegFrameRecorder(this.fileName, width, height); recorder.setVideoCodec(avcodec.AV_CODEC_ID_H264); recorder.setFrameRate(FRAME_RATE); /* * videoBitRate这个参数很重要，当然越大，越清晰，但最终的生成的视频也越大。查看一个资料，说均衡考虑建议设为videoWidth*videoHeight*frameRate*0.07*运动因子，运动因子则与视频中画面活动频繁程度有关，如果很频繁就设为4，不频繁则设为1 */ recorder.setVideoBitrate((int)((width*height*FRAME_RATE)*MOTION_FACTOR*0.07)); recorder.setPixelFormat(avutil.AV_PIX_FMT_YUV420P); recorder.setFormat(\u0026#34;mp4\u0026#34;); try { recorder.start(); } catch (FrameRecorder.Exception e) { log.error(\u0026#34;JavaCVMp4Encoder configure video error.\u0026#34;, e); } } @Override public void encodeFrame(BufferedImage image, long timestamp) { try { recorder.record(java2dConverter.convert(image)); } catch (FrameRecorder.Exception e) { log.error(\u0026#34;JavaCVMp4Encoder encode frame error.\u0026#34;, e); } } @Override public void close() { if(recorder != null){ try { recorder.stop(); } catch (FrameRecorder.Exception e) { log.error(\u0026#34;JavaCVMp4Encoder stop error.\u0026#34;, e); } try { recorder.release(); } catch (FrameRecorder.Exception e) { log.error(\u0026#34;JavaCVMp4Encoder release error.\u0026#34;, e); } } } } 顺手写了个测试案例，还好是可以工作的\npackage test; public JavaCVMp4EncoderTest { public static void main(String[] args){ Mp4Encoder encoder = new JavaCVMp4Encoder(); encoder.make(\u0026#34;/tmp/test.mp4\u0026#34;); encoder.configVideo(1024, 768); BufferedImage img = new BufferedImage(1024, 768, BufferedImage.TYPE_3BYTE_BGR); Java2DFrameConverter java2dConverter = new Java2DFrameConverter(); Graphics2D g2 = (Graphics2D)img.getGraphics(); for (int i = 0; i \u0026lt;= 25 * 20; i++) { g2.setColor(Color.white); g2.fillRect(0, 0, width, height); g2.setPaint(Color.black); g2.drawString(\u0026#34;frame \u0026#34; + i, 10, 25); encoder.encodeFrame(java2dConverter.convert(img), System.currentTimeMillis()); } encoder.close(); } } 不过不久就发现在项目中转出的录像播放得太快了，检查代码发现JavaCVMp4Encoder的encodeFrame方法的第二个参数timestamp并没有用到，但在项目中进行mp4编码时，实际上是对每一帧指定的时间戳的，于是修改encodeFrame方法\n@Override public void encodeFrame(BufferedImage image, long timestamp) { try { long t = timestamp * 1000L; if (t \u0026gt; recorder.getTimestamp()) { recorder.setTimestamp(t); } recorder.record(java2dConverter.convert(image)); } catch (FrameRecorder.Exception e) { log.error(\u0026#34;JavaCVMp4Encoder encode frame error.\u0026#34;, e); } } 终于转出的视频不再飞快播放了。\n又过了好几天，在正式环境上运行着，又出问题，进行mp4编码的Java进程crash了。crash日志时仅报了一下跟jni调用相关的错。\nStack: [0x00007f1932fb4000,0x00007f19330b5000], sp=0x00007f19330b2d88, free space=1019k Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code) C [libswscale.so.3+0x52f41] sws_getCachedContext+0x1471 [error occurred during error reporting (printing native stack), id 0xb] Java frames: (J=compiled Java code, j=interpreted, Vv=VM code) j org.bytedeco.javacpp.swscale.sws_scale(Lorg/bytedeco/javacpp/swscale$SwsContext;Lorg/bytedeco/javacpp/PointerPointer;Lorg/bytedeco/javacpp/IntPointer;IILorg/bytedeco/javacpp/PointerPointer;Lorg/bytedeco/javacpp/IntPointer;)I+0 j org.bytedeco.javacv.FFmpegFrameRecorder.recordImage(IIIIII[Ljava/nio/Buffer;)Z+570 j org.bytedeco.javacv.FFmpegFrameRecorder.record(Lorg/bytedeco/javacv/Frame;I)V+70 j org.bytedeco.javacv.FFmpegFrameRecorder.record(Lorg/bytedeco/javacv/Frame;)V+3 在网上查阅了很久，终于找到一个线索，说是跟下面的代码相关\nif ( (uintptr_t)dst[0]%16 || (uintptr_t)dst[1]%16 || (uintptr_t)dst[2]%16 || (uintptr_t)src[0]%16 || (uintptr_t)src[1]%16 || (uintptr_t)src[2]%16 || dstStride[0]%16 || dstStride[1]%16 || dstStride[2]%16 || dstStride[3]%16 || srcStride[0]%16 || srcStride[1]%16 || srcStride[2]%16 || srcStride[3]%16 ) { static int warnedAlready=0; int cpu_flags = av_get_cpu_flags(); if (HAVE_MMXEXT \u0026amp;\u0026amp; (cpu_flags \u0026amp; AV_CPU_FLAG_SSE2) \u0026amp;\u0026amp; !warnedAlready){ av_log(c, AV_LOG_WARNING, \u0026#34;Warning: data is not aligned! This can lead to a speedloss\\n\u0026#34;); warnedAlready=1; } } 意思是视频的宽度必须是16的倍数，否则ffmpeg可能因为无法对齐而crash。这么重要的事情，在ffmpeg文档上竟然从来没提出。但经我实际测试，发现视频的宽度必须是32的倍数，高度必须是2的倍数，于是写了点代码修正了width与height，然后问题就解决了。\nint width = ...; int height = ...; if (width % 32 != 0) { int j = width % 32; if (j \u0026lt;= 16) { width = width - (width % 32); } else { width = width + (32 - width / 32); } } if (height % 2 != 0) { int j = height % 4; switch (j) { case 1: height = height - 1; break; case 3: height = height + 1; break; } } Mp4Encoder encoder = new JavaCVMp4Encoder(); encoder.make(\u0026#34;/tmp/test.mp4\u0026#34;); encoder.configVideo(width, height); BufferedImage img = new BufferedImage(width, height, BufferedImage.TYPE_3BYTE_BGR); ","permalink":"https://jeremyxu2010.github.io/2016/04/javacv%E7%BC%96%E7%A0%81mp4%E8%A7%86%E9%A2%91/","tags":["java","ffmpeg","javacv","h264"],"title":"javacv编码mp4视频"},{"categories":["工具"],"contents":"作为一个开发者，本时有工作中肯定有技术上各种点点滴滴想记下来，毕竟好记忆不如烂笔头。当然使用一个笔记工具完全可以满足这个需求。但同时我在想能否将这些点点滴滴不仅记起来，同时这些知识也成为展现自己技术能力的一张名片呢？自然而然就想到很多大牛的技术博客。进而发现了github+hexo的组合，但我在使用中发现就为了写篇技术博文还得开chrome, iTerm2, Sublime 3三个程序，而且还在这三个程序间频繁切换了，太痛苦了有没有。不能在一个程序里搞定所有的事情吗？经过反复探索，终于找到了一个办法，那就是hexo-admin。下面简单记述一下我的实施步骤。\n安装hexo npm install hexo-cli -g  初始化博客工程  cd ~/dev/git/ hexo init blog cd blog npm install 新建blog对应的github仓库 在上述图中填写名称时一定要填写你的github帐户的git用户名+github.io, 比如我的github帐户的github用户为abcd，则应该填写abcd.github.io\n将blog推送至github npm install hexo-deployer-git --save 编辑_config.yml文件，在里面填写好git部署的连接信息\ndeploy: type: git repo: git@github.com:abcd/abcd.github.io.git branch: master message: \u0026quot;博客更新: {{ now('YYYY-MM-DD HH:mm:ss') }}\u0026quot; 执行命令将blog推送至github\nhexo deploy -g 等待片刻后就可以访问http://abcd.github.io查看你自己的博客了。\n使用hexo-admin搞定所有工作 发现没有，在上面的步骤里，你至少打开了两个程序：iTerm2、Chrome。如果你还想对Markdown文件进行编辑，少不了还得打开一个类似于Sublime 3的程序。现在我们用hexo-admin一次搞定所有工作。\n安装hexo-admin npm install hexo-admin -g 设置部署命令 在blog目录下新建一个文件deploy_cmd.sh，在其中添加如下内容\n#!/bin/bash /usr/local/bin/node /usr/local/bin/hexo deploy -g 2\u0026gt;\u0026amp;1 给这个文件添加可执行权限chmod +x /Users/abcd/dev/git/blog/deploy_cmd.sh\n在_config.yml文件中添加如下内容\nadmin: deployCommand: /Users/abcd/dev/git/blog/deploy_cmd.sh 注意这里文件的路径你得自己修改正确哦\n试用hexo-admin的功能 执行命令启动本地server\nhexo server -d 这里应该就可以使用Chrome访问http://127.0.0.1:4000/admin，在这里可以查看你写的博文，可以新建博文，编辑博文，甚至可以在这里直接部署blog至github。\n最后设置hexo的本地server随用户启动登录自动启动 按Ctrl-C结束刚才的hexo本地server， 然后新建文件~/Library/LaunchAgents/hexo-admin.plist，内容如下\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE plist PUBLIC \u0026#34;-//Apple//DTD PLIST 1.0//EN\u0026#34; \u0026#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd\u0026#34;\u0026gt; \u0026lt;plist version=\u0026#34;1.0\u0026#34;\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;Disabled\u0026lt;/key\u0026gt; \u0026lt;false/\u0026gt; \u0026lt;key\u0026gt;KeepAlive\u0026lt;/key\u0026gt; \u0026lt;true/\u0026gt; \u0026lt;key\u0026gt;Label\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;hexo-admin\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;WorkingDirectory\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;/Users/abcd/dev/git/blog\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ProgramArguments\u0026lt;/key\u0026gt; \u0026lt;array\u0026gt; \u0026lt;string\u0026gt;/usr/local/bin/node\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;/usr/local/bin/hexo\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;server\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;-d\u0026lt;/string\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;key\u0026gt;RunAtLoad\u0026lt;/key\u0026gt; \u0026lt;false/\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/plist\u0026gt; 启动hexo-admin launchctl load ~/Library/LaunchAgents/hexo-admin.plist 以后这个hexo本地服务器就可以自启动了。\n总结 至此，以后就可以在Chrome里访问http://127.0.0.1:4000/admin进行博文的撰写及发布了，发布完毕之后可访问http://abcd.github.io看一看blog的真实效果。\n","permalink":"https://jeremyxu2010.github.io/2016/04/%E5%BC%80%E5%8F%91%E8%80%85%E7%9A%84%E5%8D%9A%E5%AE%A2%E5%86%99%E4%BD%9C%E7%8E%AF%E5%A2%83/","tags":["blog","hexo"],"title":"开发者的博客写作环境"},{"categories":["工具"],"contents":"买一个国外的VPS主机 我是在digitalocean.com上买的，选的操作系统为CentOS 7\n服务端操作 启用TCP BBR yum upgrade -y wget --no-check-certificate https://github.com/teddysun/across/raw/master/bbr.sh \u0026amp;\u0026amp; chmod +x bbr.sh \u0026amp;\u0026amp; ./bbr.sh # 这里会重启服务器 系统参数优化 tee /etc/sysctl.conf \u0026lt;\u0026lt; EOF # max open files fs.file-max = 1024000 # max read buffer net.core.rmem_max = 67108864 # max write buffer net.core.wmem_max = 67108864 # default read buffer net.core.rmem_default = 65536 # default write buffer net.core.wmem_default = 65536 # max processor input queue net.core.netdev_max_backlog = 4096 # max backlog net.core.somaxconn = 4096 # resist SYN flood attacks net.ipv4.tcp_syncookies = 1 # reuse timewait sockets when safe net.ipv4.tcp_tw_reuse = 1 # turn off fast timewait sockets recycling net.ipv4.tcp_tw_recycle = 0 # short FIN timeout net.ipv4.tcp_fin_timeout = 30 # short keepalive time net.ipv4.tcp_keepalive_time = 1200 # outbound port range net.ipv4.ip_local_port_range = 10000 65000 # max SYN backlog net.ipv4.tcp_max_syn_backlog = 4096 # max timewait sockets held by system simultaneously net.ipv4.tcp_max_tw_buckets = 5000 # TCP receive buffer net.ipv4.tcp_rmem = 4096 87380 67108864 # TCP write buffer net.ipv4.tcp_wmem = 4096 65536 67108864 # turn on path MTU discovery net.ipv4.tcp_mtu_probing = 1 EOF sysctl -p ulimit -SHn 1024000 tee -a /etc/security/limits.conf \u0026lt;\u0026lt; EOF * soft nofile 512000 * hard nofile 1024000 EOF tee -a /etc/profile \u0026lt;\u0026lt; EOF ulimit -SHn 1024000 EOF 安装shadowsocks-libev服务端并启动 wget -q -O /etc/yum.repos.d/librehat-shadowsocks-epel-7.repo https://copr.fedorainfracloud.org/coprs/librehat/shadowsocks/repo/epel-7/librehat-shadowsocks-epel-7.repo yum install -y mbedtls libsodium mbedtls-devel libsodium-devel gettext gcc autoconf libtool automake make asciidoc xmlto c-ares-devel libev-devel pcre-devel wget -q -O shadowsocks-libev-3.1.3.tar.gz https://github.com/shadowsocks/shadowsocks-libev/releases/download/v3.1.3/shadowsocks-libev-3.1.3.tar.gz tar xf shadowsocks-libev-3.1.3.tar.gz pushd shadowsocks-libev-3.1.3 \u0026amp;\u0026gt; /dev/null ./configure \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install popd \u0026amp;\u0026gt; /dev/null tee /etc/shadowsocks-libev/config.json \u0026lt;\u0026lt; EOF { \u0026#34;server\u0026#34;:\u0026#34;0.0.0.0\u0026#34;, \u0026#34;server_port\u0026#34;:443, \u0026#34;password\u0026#34;:\u0026#34;yourpassword\u0026#34;, \u0026#34;timeout\u0026#34;:600, \u0026#34;method\u0026#34;:\u0026#34;chacha20\u0026#34; } EOF tee /etc/systemd/system/shadowsocks-libev.service \u0026lt;\u0026lt; EOF [Unit] Description=shadowsocks-libev [Service] TimeoutStartSec=0 ExecStart=/usr/local/bin/ss-server -c /etc/shadowsocks-libev/config.json [Install] WantedBy=multi-user.target EOF systemctl enable shadowsocks-libev systemctl restart shadowsocks-libev 客户端操作 在MAC上安装shadowsocks客户端并启动 brew install shadowsocks-libev tee /usr/local/etc/shadowsocks-libev.json \u0026lt;\u0026lt; EOF { \u0026#34;server\u0026#34;:\u0026#34;yourserverip\u0026#34;, \u0026#34;server_port\u0026#34;:443, \u0026#34;local_address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;local_port\u0026#34;:1080, \u0026#34;password\u0026#34;:\u0026#34;yourpassword\u0026#34;, \u0026#34;timeout\u0026#34;:600, \u0026#34;method\u0026#34;:\u0026#34;chacha20\u0026#34; } EOF brew services restart shadowsocks-libev #启动客户端 现在本机上就有翻墙的SOCKS5代理，地址为127.0.0.1:1080\n安装privoxy 有很多程序本身不支持SOCKS5代理，但支持HTTP代理，这里使用privoxy将SOCKS5代理转成HTTP代理，并根据actionsfile规则自动决定是否使用SOCKS5代理\nbrew install privoxy python@2 tee -a /usr/local/etc/privoxy/config \u0026lt;\u0026lt; EOF listen-address 0.0.0.0:8118 actionsfile gfwlist.action EOF pip2 install gfwlist2privoxy # 安装gfwlist2privoxy # 下面两个网站虽然不在gfwlist.txt列表里，但希望走SOCKS5代理 tee user_rule.txt \u0026lt;\u0026lt; EOF !this is a user defined rule .google.com.hk .privoxy.org EOF SOCKS_PROXY=127.0.0.1:1080 curl -s --socks5 $SOCKS_PROXY -o gfwlist.txt \u0026#39;https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt\u0026#39; python2 -m gfwlist2privoxy.main -i gfwlist.txt -f /usr/local/etc/privoxy/gfwlist.action -p $SOCKS_PROXY -t socks5 --user-rule user_rule.txt # 使用gfwlist2privoxy生成gfwlist.action文件，gfwlist.action文件很好理解，里面全部是需要走SOCKS5代理的域名列表，可根据需求自行编辑，编辑完毕之后重启privoxy即可生效 brew services restart privoxy #启动privoxy 现在本机上就有自动翻墙的HTTP代理，地址为127.0.0.1:8118\nMAC系统全局使用HTTP代理 接下来在”系统偏好设置“-”网络“-”高级“-”代理“里设置，将Web Proxy和Secure Web Proxy都设置为127.0.0.1:8118\n命令行中翻墙 echo \u0026#39; alias proxy_on=\u0026#34;export http_proxy=http://127.0.0.1:8118;export https_proxy=http://127.0.0.1:8118;\u0026#34; alias proxy_off=\u0026#34;unset http_proxy;unset https_proxy;\u0026#34; \u0026#39; \u0026gt;\u0026gt; ~/.zshrc 以后执行命令如果需要用翻墙，则在执行前执行proxy_on, 执行命令结束后执行proxy_off\n避免DNS污染 brew install pcap_dnsproxy vim /usr/local/etc/pcap_DNSproxy/Config.ini [Listen] Operation Mode = Proxy [Local DNS] Local Main = 1 Local Routing = 1 [Proxy] SOCKS Proxy = 1 SOCKS IPv4 Address = 127.0.0.1:1080 修改 Operation Mode 为Proxy以确保其只为本机提供代理域名解析请求服务，修改 Local Main、Local Routing 为1以确保国内网站获得更好的 DNS 解析结果。修改SOCKS Proxy为以SocksV5代理进行DNS请求。\n设置pcap_DNSproxy开机自启\nsudo cp -fv /usr/local/opt/pcap_dnsproxy/*.plist /Library/LaunchDaemons sudo chown root /Library/LaunchDaemons/homebrew.mxcl.pcap_dnsproxy.plist sudo launchctl load /Library/LaunchDaemons/homebrew.mxcl.pcap_dnsproxy.plist 进入系统偏好设置 - 网络 - 高级 - DNS，将 DNS 服务器设置为 127.0.0.1 即可\n参考  https://teddysun.com/489.html https://github.com/iMeiji/shadowsocks_install/wiki/shadowsocks-optimize https://github.com/snachx/gfwlist2privoxy http://cckpg.blogspot.com/2011/06/privoxy.html  ","permalink":"https://jeremyxu2010.github.io/2016/04/mac%E7%BF%BB%E5%A2%99%E7%AE%80%E6%98%93%E6%95%99%E7%A8%8B/","tags":["linux","fuckgfw"],"title":"MAC翻墙简易教程"},{"categories":["工具"],"contents":"这个我自己的vim配置，以备我在一台全新的MAC电脑上恢复原来的vim配置\n 安装vim  brew install vim --with-lua  安装vundle  mkdir -p ~/.vim/bundle git clone https://github.com/gmarik/vundle.git ~/.vim/bundle/vundle  克隆vim-config工程并进行vundle插件安装  git clone https://github.com:jeremyxu2010/vim-config.git mv vim-config/vimrc ~/.vimrc vim +BundleInstall +qall ","permalink":"https://jeremyxu2010.github.io/2016/04/%E6%88%91%E7%9A%84vim%E9%85%8D%E7%BD%AE/","tags":["linux","vim"],"title":"我的VIM配置"},{"categories":["devops"],"contents":"今天的工作需要将一个很大的文件传输出远程主机上，远程主机只开启了sshd服务，仅允许ssh登录，不允许安装其它软件，到远程主机的网络很不稳定。\n首先尝试使用scp，由于网络很不稳定，传输20~30M，网络就断了，然后又从头重新传。后来想到rsync貌似可以使用ssh通道，于是写了下面的脚本。\n#!/bin/bash # rsync_copy.sh export RSYNC_RSH=\u0026#34;ssh -i /home/test/.ssh/id_rsa -c arcfour -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o LogLevel=quiet -o ServerAliveInterval=15 -o ServerAliveCountMax=2\u0026#34; rsyncSrcFile=$1 rsyncDestFile=$2 rsyncSuccess=-1 while [ $rsyncSuccess -ne 0 ] do rsync -avq --partial --inplace $rsyncSrcFile $rsyncDestFile rsyncSuccess=$? done 这个脚本的两个参数格式均可以是 /home/test/a.iso 或 root@192.168.3.4:/root/a.iso\n#执行前需要作ssh密钥无密码登录 ssh-copy-id -i /home/test/.ssh/id_rsa root@192.168.3.4 #执行下面的命令，然后就可以登出去happy了，明天早上再登入远程主机检查文件，一切ok了 ./rsync_copy.sh /home/test/a.iso root@192.168.3.4:/root/a.iso \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp; ","permalink":"https://jeremyxu2010.github.io/2015/06/rsync%E9%85%8D%E5%90%88ssh%E5%9C%A8%E4%B8%8D%E7%A8%B3%E5%AE%9A%E7%BD%91%E7%BB%9C%E4%B8%8B%E4%BC%A0%E8%BE%93%E5%A4%A7%E6%96%87%E4%BB%B6/","tags":["rsync","scp"],"title":"rsync配合ssh在不稳定网络下传输大文件"},{"categories":["devops"],"contents":"朋友公司一网站被DDOS攻击了，不得已在机房呆了两天作防护工作，才算临时解决了问题。想着自己公司线上也运行着一个系统，担心有一天也会被攻击，还是提前作一下DDOS防护吧。线上系统用的是nginx，于是我采用了比较成熟的fail2ban+nginx防护方案。\n首先安装配置fail2ban\nzypper addrepo http://download.opensuse.org/repositories/home:Peuserik/SLE_11_SP2/home:Peuserik.repo zypper refresh zypper install fail2ban vim /etc/fail2ban/jail.conf [DEFAULT] #设置忽略内网访问及某些安全网段的访问， 网段之间以空格分隔 ignoreip = 127.0.0.1/8 11.11.11.1/24 xx.xx.xx.xxx/28 ... #设置ssh登录防护 [ssh-iptables] enabled = true filter = sshd action = iptables[name=SSH, port=ssh, protocol=tcp] logpath = /var/log/sshd.log maxretry = 5 ... #设置nginx防护ddos攻击 [xxx-get-dos] enabled=true port=http,https filter=nginx-bansniffer action=iptables[name=xxx, port=http, protocol=tcp] logpath=/opt/nginx/logs/xxx_access.log maxretry=100 findtime=60 bantime=300 ... vim /etc/fail2ban/filter.d/nginx-bansniffer.conf [Definition] failregex = \u0026lt;HOST\u0026gt; -.*- .*HTTP/1.* .* .*$ ignoreregex = /etc/init.d/fail2ban start nginx设置\nvim /opt/nginx/conf/nginx.conf .... if ($http_user_agent ~* (Siege|http_load|fwptt)) { return 404; } #空agent if ($http_user_agent ~ ^$) { return 404; } #请求方式限制 if ($request_method !~ ^(GET|HEAD|POST)$) { return 403; } .... /etc/init.d/nginx restart 这样设置后发现fail2ban对正常请求也ban了，仔细检查后发现线上应用加载的静态资源过多，而nginx对这些静态资源也会记录访问日志，这样访问日志中就存在大量同一ip来的请求。于是决定对于静态资源，不记录访问日志。\nvim /opt/nginx/conf/nginx.conf ... location ~* /xxx/.+\\.(gif|jpg|png|js|css)$ { root /opt/jetty/webapps/; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; expires 10d; access_log /dev/null; } location ~* \\.(gif|jpg|png|js|css)$ { root /opt/jetty/webapps/root/; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; expires 10d; access_log /dev/null; } ... 这里特别需要注意nginx的location匹配规则，刚开始我把上面两个location的位置弄反了，一直有问题，后来发现nginx对于相同优先级的匹配符是从上往下匹配的，一旦匹配某个规则，则进行某个规则的处理。所以匹配规则一定要按先特殊后通用的顺序摆列。\n附上nginx的location匹配规则的简述。\nlocation匹配命令\n~ #波浪线表示执行一个正则匹配，区分大小写 ~* #表示执行一个正则匹配，不区分大小写 ^~ #^~表示普通字符匹配，如果该选项匹配，只匹配该选项，不匹配别的选项，一般用来匹配目录 = #进行普通字符精确匹配 @ #\u0026quot;@\u0026quot; 定义一个命名的 location，使用在内部定向时，例如 error_page, try_files location 匹配的优先级(与location在配置文件中的顺序无关)\n= 精确匹配会第一个被处理。如果发现精确匹配，nginx停止搜索其他匹配。 普通字符匹配，正则表达式规则和长的块规则将被优先和查询匹配，也就是说如果该项匹配还需去看有没有正则表达式匹配和更长的匹配。 ^~ 则只匹配该规则，nginx停止搜索其他匹配，否则nginx会继续处理其他location指令。 最后匹配理带有\u0026quot;~\u0026quot;和\u0026quot;~*\u0026quot;的指令，如果找到相应的匹配，则nginx停止搜索其他匹配；当没有正则表达式或者没有正则表达式被匹配的情况下，那么匹配程度最高的逐字匹配指令会被使用。 ","permalink":"https://jeremyxu2010.github.io/2015/06/%E4%BD%BF%E7%94%A8fail2ban%E8%BF%9B%E8%A1%8Cddos%E9%98%B2%E6%8A%A4/","tags":["ddos","fail2ban","nginx"],"title":"使用fail2ban进行DDOS防护"},{"categories":["devops"],"contents":"linux server上服务一般持续长久运行，以致服务的日志文件随着时间越来越大，如果日志处理得不好甚至有可能占满磁盘。幸好找到了logrotate这个程序来处理。\n下面是安装配置过程\nzypper in -y logrotate #虽然logrotate是通过cron来运行的 cat /etc/cron.daily/logrotate #!/bin/sh /usr/sbin/logrotate /etc/logrotate.conf EXITVALUE=$? if [ $EXITVALUE != 0 ]; then /usr/bin/logger -t logrotate \u0026#34;ALERT exited abnormally with [$EXITVALUE]\u0026#34; fi exit 0 #而logrotate.conf里会引用/etc/logrotate.d目录 cat /etc/logrotate.conf ... include /etc/logrotate.d ... #在/etc/logrotate.d目录中新建一个处理nginx日志文件的配置文件 vim /etc/logrotate.d/nginx /opt/nginx/logs/*.log { daily dateext compress rotate 3 sharedscripts postrotate if [ -f /opt/nginx/logs/nginx.pid ]; then kill -USR1 `cat /opt/nginx/logs/nginx.pid` fi endscript } 还可以直接手动执行\nlogrotate -d -f /etc/logrotate.d/nginx ","permalink":"https://jeremyxu2010.github.io/2015/04/%E4%BD%BF%E7%94%A8logrotate%E8%BF%9B%E8%A1%8C%E6%97%A5%E5%BF%97%E5%88%86%E5%89%B2%E5%8F%8A%E6%BB%9A%E5%8A%A8%E5%A4%84%E7%90%86/","tags":["linux","logrotate"],"title":"使用logrotate进行日志分割及滚动处理"},{"categories":["web开发"],"contents":"公司研发的系统为B/S架构，用户使用浏览器访问系统时，使用浏览器自带工具查看，对图片的请求数极多，多为小图片。\n今天想对这个现状进行改善，网上查到一种雪碧图的方案，其实就是使用工具将数量很多的小图片拼成一张大图片，然后css里都引用这张大图片，并指定显示该图片的某一个区域，但这个方案需要手工作很多处理。\n于是就想到能不能用目前比较成熟的grunt对前端样式文件自动进行处理，自动生成雪碧图，自动修改样式文件。一搜索果然找到了方案，下面为Gruntfile.js文件的片断：\nmodule.exports = function(grunt) { // Project configuration.  grunt.initConfig({ // 自动雪碧图  sprite: { options: { // 映射CSS中背景路径，支持函数和数组，默认为 null  imagepath_map: null, // 各图片间间距，如果设置为奇数，会强制+1以保证生成的2x图片为偶数宽高，默认 0  padding: 0, // 是否使用 image-set 作为2x图片实现，默认不使用  useimageset: false, // 是否以时间戳为文件名生成新的雪碧图文件，如果启用请注意清理之前生成的文件，默认不生成新文件  newsprite: false, // 给雪碧图追加时间戳，默认不追加  spritestamp: true, // 在CSS文件末尾追加时间戳，默认不追加  cssstamp: true, // 默认使用二叉树最优排列算法  algorithm: \u0026#39;binary-tree\u0026#39;, // 默认使用`pixelsmith`图像处理引擎  engine: \u0026#39;pixelsmith\u0026#39; }, sprite_module1: { //只对module1目录进行自动生成雪碧图处理  options : { // sprite背景图源文件夹，只有匹配此路径才会处理，默认 images/slice/  imagepath: \u0026#39;module1/images/\u0026#39;, // 雪碧图输出目录，注意，会覆盖之前文件！默认 images/  spritedest: \u0026#39;module1/images/\u0026#39; }, files: [{ // 启用动态扩展  expand: true, // css文件源的文件夹  cwd: \u0026#39;module1/\u0026#39;, // 匹配规则  src: [\u0026#39;**/*.css\u0026#39;, \u0026#39;!**/*.sprite.css\u0026#39;], // 导出css和sprite的路径地址  dest: \u0026#39;module1/\u0026#39;, // 导出的css名  ext: \u0026#39;.sprite.css\u0026#39;, extDot: \u0026#39;last\u0026#39; }] } } }); // 加载包含 \u0026#34;sprite\u0026#34; 任务的插件。  // grunt.loadNpmTasks(\u0026#39;grunt-css-sprite\u0026#39;); //因为希望生成的雪碧图为.sprite.png结尾，对原来的grunt-css-sprite作了些改动，于是手动加载grunt_tasks  grunt.loadTasks(\u0026#39;grunt_tasks\u0026#39;); grunt.registerTask(\u0026#39;default\u0026#39;, [\u0026#39;sprite\u0026#39;]); }; package.json\n{ \u0026#34;name\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;x.x.x\u0026#34;, \u0026#34;devDependencies\u0026#34;: { \u0026#34;grunt\u0026#34;: \u0026#34;0.4.5\u0026#34;, \u0026#34;async\u0026#34;: \u0026#34;0.9.0\u0026#34;, \u0026#34;spritesmith\u0026#34;: \u0026#34;1.3.0\u0026#34; } } grunt_tasks目录结构如下：\ngrunt_tasks - sprite_libs - css-spritesmith.js - imageSetSpriteCreator.js - place_after.png - place_before.png - sprite.js 其中\nsprite.js 从 https://github.com/laoshu133/grunt-css-sprite 工程里得来\ncss-spritesmith.js、imageSetSpriteCreator.js、place_after.png、place_before.png 从 https://github.com/laoshu133/css-spritesmith 工程里得来，但修改了css-spritesmith.js文件\n... sliceData.timestamp = options.spritestamp ? timeNow : \u0026#39;\u0026#39;; sliceData.timestamp = options.spritestamp ? (\u0026#39;?\u0026#39;+timeNow) : \u0026#39;\u0026#39;; sliceData.imgDest = fixPath(path.join(options.spritedest, cssFilename + \u0026#39;.sprite.png\u0026#39;)); sliceData.spriteImg = fixPath(path.join(options.spritepath, cssFilename + \u0026#39;.sprite.png\u0026#39;)) + sliceData.timestamp; sliceData.retinaImgDest = fixPath(sliceData.imgDest.replace(/\\.png$/, \u0026#39;@2x.sprite.png\u0026#39;)); sliceData.retinaSpriteImg = fixPath(path.join(options.spritepath, cssFilename + \u0026#39;@2x.sprite.png\u0026#39;)) + sliceData.timestamp; ... ","permalink":"https://jeremyxu2010.github.io/2015/03/%E4%BD%BF%E7%94%A8grunt%E5%AF%B9css%E4%B8%AD%E7%9A%84background%E5%9B%BE%E7%89%87%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E9%9B%AA%E7%A2%A7%E5%9B%BE/","tags":["grunt","nodejs","css"],"title":"使用grunt对css中的background图片自动生成雪碧图"},{"categories":["nodejs开发"],"contents":"今天一个朋友问我一个问题，他想做一个win32的桌面应用程序，而且还希望程序能做出web页面那种漂亮的效果，可目前项目组的成员全是以前做前端的一批人，怎么办？\n我想了一下，几乎毫不犹豫地推荐了node-webkit, 但又想起前段时间看到的atom-shell，于是也推荐了下atom-shell，随手写了个atom-shell的例子给他。\npackage.json\n{ \u0026#34;name\u0026#34; : \u0026#34;pingdemo\u0026#34;, \u0026#34;version\u0026#34; : \u0026#34;0.1.0\u0026#34;, \u0026#34;main\u0026#34; : \u0026#34;main.js\u0026#34; } main.js\nvar app = require(\u0026#39;app\u0026#39;); // Module to control application life. var BrowserWindow = require(\u0026#39;browser-window\u0026#39;); // Module to create native browser window.  // Report crashes to our server. require(\u0026#39;crash-reporter\u0026#39;).start(); // Keep a global reference of the window object, if you don\u0026#39;t, the window will // be closed automatically when the javascript object is GCed. var mainWindow = null; // Quit when all windows are closed. app.on(\u0026#39;window-all-closed\u0026#39;, function() { if (process.platform != \u0026#39;darwin\u0026#39;) app.quit(); }); // This method will be called when atom-shell has done everything // initialization and ready for creating browser windows. app.on(\u0026#39;ready\u0026#39;, function() { // Create the browser window.  mainWindow = new BrowserWindow({width: 800, height: 600}); // and load the index.html of the app.  mainWindow.loadUrl(\u0026#39;file://\u0026#39; + __dirname + \u0026#39;/index.html\u0026#39;); // Emitted when the window is closed.  mainWindow.on(\u0026#39;closed\u0026#39;, function() { // Dereference the window object, usually you would store windows  // in an array if your app supports multi windows, this is the time  // when you should delete the corresponding element.  mainWindow = null; }); }); index.html\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Ping Demo\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Ping Demo\u0026lt;/h1\u0026gt; \u0026lt;input id=\u0026#34;addr\u0026#34; type=\u0026#34;text\u0026#34; name=\u0026#34;addr\u0026#34; value=\u0026#34;www.baidu.com\u0026#34;/\u0026gt; \u0026lt;input id=\u0026#34;ping\u0026#34; type=\u0026#34;button\u0026#34; name=\u0026#34;ping\u0026#34; value=\u0026#34;Ping\u0026#34;/\u0026gt; \u0026lt;div id=\u0026#34;ping-result\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;ping.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ping.js\ntry { var child_process = require(\u0026#39;child_process\u0026#39;); window.$ = window.jQuery = require(\u0026#39;./jquery\u0026#39;); } catch(e) {console.log(e)} $(\u0026#39;#ping\u0026#39;).on(\u0026#39;click\u0026#39;, function (){ var opts = { encoding: \u0026#39;utf8\u0026#39;, timeout: 2000, maxBuffer: 200*1024, killSignal: \u0026#39;SIGKILL\u0026#39;, cwd: null, env: null }; var cmd = \u0026#39;ping -t 1 -c 1 \u0026#39; + $(\u0026#39;#addr\u0026#39;).val(); var child = child_process.exec(cmd, opts, function(error, stdout, stderr) { $(\u0026#39;#ping-result\u0026#39;).empty(); if(stdout.length \u0026gt; 0){ $(\u0026#39;#ping-result\u0026#39;).append(\u0026#39;\u0026lt;pre\u0026gt;\u0026#39; + stdout + \u0026#39;\u0026lt;/pre\u0026gt;\u0026#39;); } if(stderr.length \u0026gt; 0){ $(\u0026#39;#ping-result\u0026#39;).append(\u0026#39;\u0026lt;pre\u0026gt;\u0026#39; + stderr + \u0026#39;\u0026lt;/pre\u0026gt;\u0026#39;); } }); }); jquery.js从code.jquery.com/jquery-1.11.0.min.js下载过来重命名即可\n上述所有文件放在一个PingDemo目录中\n最后执行\n/Applications/Atom.app/Contents/MacOS/Atom ./PingDemo/ 想想java调用外部程序还要考虑那么多东西，这东西真心简单啊。\n","permalink":"https://jeremyxu2010.github.io/2014/06/atom-shell%E5%B0%8F%E4%BE%8B%E5%AD%90/","tags":["nodejs","atom-shell","node-webkit"],"title":"atom-shell小例子"},{"categories":["devops"],"contents":"上线的服务器有时会被人攻击，导致服务不可用，今天安装配置了nagios对上线服务器进行监控，简单记录一下\n#安装必要的软件包 yum install -y gcc glibc glibc-common gd gd-devel xinetd openssl-devel #创建nagios用户及授予目录权限 useradd -s /sbin/nologin nagios mkdir /usr/local/nagios chown -R nagios.nagios /usr/local/nagios #安装nagios tar xf nagios-4.0.7.tar.gz cd nagios-4.0.7 ./configure --prefix=/usr/local/nagios make all make install \u0026amp;\u0026amp; make install-init \u0026amp;\u0026amp; make install-commandmode \u0026amp;\u0026amp; make install-config chkconfig --add nagios \u0026amp;\u0026amp; chkconfig --level 35 nagios on \u0026amp;\u0026amp; chkconfig --list nagios #安装nagios-plugins tar xf nagios-plugins-2.0.2.tar.gz cd nagios-plugins-2.0.2 ./configure --prefix=/usr/local/nagios make \u0026amp;\u0026amp; make install #安装apache tar xf httpd-2.2.23.tar.gz cd httpd-2.2.23 ./configure --prefix=/usr/local/apache2 make \u0026amp;\u0026amp; make install #安装php tar xf php-5.5.13.tar.gz cd php-5.5.13 ./configure --prefix=/usr/local/php --with-apxs2=/usr/local/apache2/bin/apxs make \u0026amp;\u0026amp; make install 接下来配置apache\n#生成nagios密码文件 /usr/local/apache2/bin/htpasswd -c /usr/local/nagios/etc/htpasswd admin vim /usr/local/apache2/conf/httpd.conf ... User nagios Group nagios ... \u0026lt;IfModule dir_module\u0026gt; DirectoryIndex index.html index.php \u0026lt;/IfModule\u0026gt; ... AddType application/x-httpd-php .php ... #setting for nagios ScriptAlias /nagios/cgi-bin \u0026#34;/usr/local/nagios/sbin\u0026#34; \u0026lt;Directory \u0026#34;/usr/local/nagios/sbin\u0026#34;\u0026gt; AuthType Basic Options ExecCGI AllowOverride None Order allow,deny Allow from all AuthName \u0026#34;Nagios Access\u0026#34; AuthUserFile /usr/local/nagios/etc/htpasswd //用于此目录访问身份验证的文件 Require valid-user \u0026lt;/Directory\u0026gt; Alias /nagios \u0026#34;/usr/local/nagios/share\u0026#34; \u0026lt;Directory \u0026#34;/usr/local/nagios/share\u0026#34;\u0026gt; AuthType Basic Options None AllowOverride None Order allow,deny Allow from all AuthName \u0026#34;nagios Access\u0026#34; AuthUserFile /usr/local/nagios/etc/htpasswd Require valid-user \u0026lt;/Directory\u0026gt; 启动apache\nvim /etc/init.d/httpd #!/bin/sh # # Startup script for the Apache Web Server # # chkconfig: 345 85 15 # description: Apache is a World Wide Web server. It is used to serve \\ # HTML files and CGI. # processname: httpd # pidfile: /usr/local/apache2/logs/httpd.pid # config: /usr/local/apache2/conf/httpd.conf # Source function library. . /etc/rc.d/init.d/functions # See how we were called. case \u0026#34;$1\u0026#34; in start) echo -n \u0026#34;Starting httpd: \u0026#34; daemon /usr/local/apache2/bin/httpd -DSSL echo touch /var/lock/subsys/httpd ;; stop) echo -n \u0026#34;Shutting down http: \u0026#34; killproc httpd echo rm -f /var/lock/subsys/httpd rm -f /usr/local/apache2/logs/httpd.pid ;; status) status httpd ;; restart) $0 stop $0 start ;; reload) echo -n \u0026#34;Reloading httpd: \u0026#34; killproc httpd -HUP echo ;; *) echo \u0026#34;Usage: $0{start|stop|restart|reload|status}\u0026#34; exit 1 esac exit 0 chmod +x /etc/init.d/httpd chkconfig httpd on /etc/init.d/httpd start 接下来配置nagios\n#确保admin用户登录后有权限查看信息 vim /usr/local/nagios/etc/cgi.cfg ... default_user_name=admin authorized_for_system_information=nagiosadmin,admin authorized_for_configuration_information=nagiosadmin,admin authorized_for_system_commands=admin authorized_for_all_services=nagiosadmin,admin authorized_for_all_hosts=nagiosadmin,admin authorized_for_all_service_commands=nagiosadmin,admin authorized_for_all_host_commands=nagiosadmin,admin ... #修改nagios主配置文件，将主机的定义都放在/usr/local/nagios/etc/hosts目录中 mkdir /usr/local/nagios/etc/hosts vim /usr/local/nagios/etc/nagios.cfg ... cfg_dir=/usr/local/nagios/etc/hosts ... #添加一个自定义命令 vim /usr/local/nagios/etc/objects/command.cfg ... # \u0026#39;check_custom_http\u0026#39; command definition define command{ command_name check_custom_http command_line $USER1$/check_http -4 -N -H $ARG1$ -u $ARG2$ } # \u0026#39;check_dns\u0026#39; command definition define command{ command_name check_dns command_line $USER1$/check_dns -v -H $ARG1$ -a $ARG2$ -w $ARG3$ -c $ARG4$ } ... #定义主机组 vim /usr/local/nagios/etc/hosts/group.cfg define hostgroup{ hostgroup_name groupname1 alias groupname1 members server1 #server1必须在/etc/hosts里有对应的映射 } #定义主机server1 vim /usr/local/nagios/etc/hosts/server1.cfg define host{ use linux-server host_name server1 alias server1 address xx.xx.xx.xx notification_period 24x7 } define service{ use local-service ; Name of service template to use host_name server1 service_description PING check_command check_ping!100.0,20%!500.0,60% ; 延时100ms丢包率大于20%时，则发出警告通知; 延时500ms丢包率大于60%时，则发出严重错误通知 } ;需要做好本机使用的DNS设置，在/etc/resolv.conf文件中定义 define service{ use local-service ; Name of service template to use host_name server1 service_description DNS check_command check_dns!xxx.test.com!xx.xx.xx.xx!4!10 ;连续解析域名发生4次错误，则发出警告通知；连续解析域名发生10次错误，则发出严重错误通知； } define service{ use local-service ; Name of service template to use host_name server1 service_description HTTP check_command check_custom_http!xxx.abc.com!/somepath/path1 ;注意这里的参数要以!分隔 } define service{ use local-service ; Name of service template to use host_name server1 service_description SSH check_command check_ssh } #配置监控出现问题时要通知的联系人 vim /usr/local/nagios/etc/objects/contacts.cfg define contact{ contact_name user1 ; Short name of user use generic-contact ; Inherit default values from generic-contact template (defined above) alias user1 ; Full name of user email user1@abc.com ; \u0026lt;\u0026lt;***** CHANGE THIS TO YOUR EMAIL ADDRESS ****** } define contact{ contact_name user2 ; Short name of user use generic-contact ; Inherit default values from generic-contact template (defined above) alias user2 ; Full name of user email user2@abc.com ; \u0026lt;\u0026lt;***** CHANGE THIS TO YOUR EMAIL ADDRESS ****** } define contactgroup{ contactgroup_name admins alias Nagios Administrators members user1,user2 } 重启nagios\n/etc/init.d/nagios restart 刚才发现nagios监控到服务器异常也没有发邮件通知，查了一下，还需要配置mail命令可发送邮件\nyum install mail vim /etc/mail.rc ... set from=abc-noreply@abc.com set smtp=smtp.abc.com set smtp-auth-user=abc-noreply@abc.com set smtp-auth-password=somepwd set smtp-auth=login ... ","permalink":"https://jeremyxu2010.github.io/2014/06/nagios%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/","tags":["nagios","监控"],"title":"nagios安装配置"},{"categories":["web开发"],"contents":"很早就听人提过grunt，我的概念里一直认为它是一个类似java界maven的东西，帮助开发人员从频繁地编译、配置管理等工作中解放出来。今天比较有空，就尝试使用一下这个东西，看看它是不是真的那么好用。\n首先安装nodejs\n#安装Homebrew ruby -e \u0026#34;$(curl -fsSL https://raw.github.com/Homebrew/homebrew/go/install)\u0026#34; #安装nodejs brew install node #安装grunt-cli npm install -g grunt-cli 切换到工程目录，安装3个nodejs模块\nnpm install grunt --save-dev npm install grunt-contrib-uglify --save-dev npm install grunt-contrib-htmlmin --save-dev 在工程目录中新建一个Gruntfile.js文件\nmodule.exports = function(grunt) { // Project configuration.  grunt.initConfig({ uglify: { build: { options: { preserveComments: false, compress: { drop_console: true }, banner: \u0026#39;/*! compress js file date : \u0026#39; + \u0026#39;\u0026lt;%= grunt.template.today(\u0026#34;yyyy-mm-dd\u0026#34;) %\u0026gt; */\u0026#39; }, files: [{ expand: true, cwd: \u0026#39;src/\u0026#39;, src: \u0026#39;**/*.js\u0026#39;, dest: \u0026#39;build/\u0026#39; }] } }, htmlmin: { build: { options: { // Target options  removeComments: true, collapseWhitespace: true }, files: [{ expand: true, cwd: \u0026#39;src/\u0026#39;, src: \u0026#39;**/*.html\u0026#39;, dest: \u0026#39;build/\u0026#39; }] } } }); // 加载包含 \u0026#34;uglify\u0026#34; 任务的插件。  grunt.loadNpmTasks(\u0026#39;grunt-contrib-uglify\u0026#39;); // 加载包含 \u0026#34;htmlmin\u0026#34; 任务的插件。  grunt.loadNpmTasks(\u0026#39;grunt-contrib-htmlmin\u0026#39;); // 默认被执行的任务列表。  grunt.registerTask(\u0026#39;default\u0026#39;, [\u0026#39;uglify\u0026#39;, \u0026#39;htmlmin\u0026#39;]); }; 这个文件需要理解一下每个Gruntfile(和Grunt插件)都使用下面这个基本格式，并且所有Grunt代码都必须指定在这个函数里面：\nmodule.exports = function(grunt) { // 在这里处理Grunt相关的事情 } 这个函数里面的内容一般会有一个项目配置、加载多个任务的插件、多个自定义任务，每个任务里又可以定义多个目标，每个任务和每个目标都可以有options配置，配置遵循就近原则（离目标越近,其优先级越高），大概形式如下：\n // 项目配置 grunt.initConfig({ task1: { options: { }, target1: { options: { } }, target2: { } }， task2: { target1: { }, target2: { } } ..... }); // 加载提供\u0026quot;task1\u0026quot;任务的插件 grunt.loadNpmTasks('task1_node_module_name'); // 加载提供\u0026quot;task2\u0026quot;任务的插件 grunt.loadNpmTasks('task2_node_module_name'); .... grunt.registerTask('task3', ['task1:target1', 'task2']); grunt.registerTask('default', ['task1', 'task2']); 然后就可以使用grunt task1:target1, grunt task2(这个会执行task2下的所有目标), grunt task3来执行了, 其中名称叫default的自定义任务比较特殊，当直接执行grunt时，会执行这个任务。\n当然还一些高级特性，这个看看官方文档就清楚了，比如数据属性、多种多样的文件描述、项目脚手架等。\n了解地差不多了，我准备把前两个写的那个pingdemo使用grunt来构建试试看，期间还稍微看了下bower。\n安装bower\nnpm install -g bower 在项目根目录下创建bower.json文件\n{ \u0026#34;name\u0026#34;: \u0026#34;pingdemo\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;, \u0026#34;main\u0026#34;: \u0026#34;build/main.js\u0026#34;, \u0026#34;ignore\u0026#34;: [ \u0026#34;**/*.txt\u0026#34; ], \u0026#34;dependencies\u0026#34;: { \u0026#34;jquery\u0026#34;: \u0026#34;2.1.1\u0026#34; }, \u0026#34;private\u0026#34;: true } 安装项目依赖的外部js文件\nbower install 这样它会自动从github上下载jquery。\n最后放出重新整理过工程结构的pingdemo地址\nhttps://github.com/jeremyxu2010/pingdemo\n","permalink":"https://jeremyxu2010.github.io/2014/05/%E5%88%9D%E8%AF%86grunt/","tags":["grunt","bower"],"title":"初识grunt"},{"categories":["devops"],"contents":"工作需要将某个具有外网IP的server的某个端口映射到某个内网IP的server的相同端口上。\n首先想到使用NAT，命令如下\necho \u0026#34;1\u0026#34; \u0026gt; /proc/sys/net/ipv4/ip_forward iptables -t nat -I PREROUTING -d $outterIP -p tcp --dport $outterPort -j DNAT --to-destination $innerIP:$innerPort iptables -I FORWARD -p tcp -m state --state NEW,RELATED,ESTABLISHED -d $innerIP --dport $innerPort -j ACCEPT iptables -I FORWARD -p tcp -m state --state NEW,RELATED,ESTABLISHED -s $innerIP --sport $innerPort -j ACCEPT iptables -t nat -I POSTROUTING -s $innerIP -j SNAT --to-source $outterIP 后面发现NAT映射失败，仔细检查发现由于$outterIP并不是$innerIP的网关，从$innerIP回来的数据包直接从其网关传输走了，无法到达$outterIP所在的server, 即SNAT无法正常工作。\n最后想了想，还是直接用ssh port forwarding了，命令如下\nssh -Nfq -c arcfour -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o LogLevel=quiet -o ServerAliveInterval=15 -o ServerAliveCountMax=3 -L 0.0.0.0:8118:192.168.9.85:8118 -i /root/.ssh/id_rsa root@127.0.0.1 效率方面估计会比直接NAT端口映射差一点，但我也能接受了\n最后附一张iptables数据包流转图\n","permalink":"https://jeremyxu2010.github.io/2014/04/linux%E4%B8%8B%E8%BF%9B%E8%A1%8C%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84/","tags":["linux","network","iptables","ssh"],"title":"linux下进行端口映射"},{"categories":["nodejs开发"],"contents":"上一篇文章写了一个简单的PingDemo应用，今天参照atom-shell的文档将那个应用打包到mac应用，试了多久终于成功了，记录一下。\natom-shell的文档上讲mac下应用是这样的\n To distribute your app with atom-shell, you should name the folder of your app as app, and put it under atom-shell's resources directory (on OS X it is Atom.app/Contents/Resources/, and on Linux and Windows it is resources/), like this:\n  On Mac OS X:\n atom-shell/Atom.app/Contents/Resources/app/ ├── package.json ├── main.js └── index.html  On Windows and Linux:\n atom-shell/resources/app ├── package.json ├── main.js └── index.html  Then execute Atom.app (or atom on Linux, and atom.exe on Windows), and atom-shell will start as your app. The atom-shelldirectory would then be your distribution that should be delivered to final users.\n 但实际场景应用打包时，一般要求重命名Atom.app为自定义的名称，我按照上述打完包后，直接将Atom.app重命名为PingDemo.app后，再运行PingDemo.app，提示\nYou can’t open the application “PingDemo.app” because it may be damaged or incomplete. 在网上搜索了半天，终于找到解决方案\ncp -r Atom.app PingDemo.app mkdir -p PingDemo.app/Contents/Resources/app/ cp pingDemoApp/{index.html,jquery.js,main.js,package.json,ping.js} PingDemo.app/Contents/Resources/app/ 使用Property List Editor打开PingDemo.app/Contents/Info.plist, 将CFBundleName属性值修改为PingDemo, 添加一个属性CFBundleExecutable，值为Atom，如下图\n保存之后，就可以打开PingDemo.app这个应用了。\n当然如果想修改应用图标，可替换PingDemo.app/Contents/Resources/atom.icns这个图标\n","permalink":"https://jeremyxu2010.github.io/2014/03/atom-shell_mac%E7%89%88%E5%BA%94%E7%94%A8%E6%89%93%E5%8C%85/","tags":["atom-shell","nodejs","macos"],"title":"atom-shell_mac版应用打包"},{"categories":["devops"],"contents":"工作上遇到需要在linux下设置电信网通双线路IP地址，操作系统为Suse Linux Enterprise Linux 11 SP2，简要记录下步骤：\n 编辑/etc/sysconfig/network/ifcfg-eth0, /etc/sysconfig/network/ifcfg-eth1, 设置两个网卡的IP地址，eth0为电信的，eth1为网通的  BOOTPROTO='static' BROADCAST='' ETHTOOL_OPTIONS='' IPADDR='${telecomip}/${telecomnetmask}' MTU='' NAME='Ethernet Card 0' NETMASK='' NETWORK='' REMOTE_IPADDR='' STARTMODE='auto' USERCONTROL='no'  在路由表配置文件中添加两个命名路由表  echo \u0026#34; 252 tel 251 cnc \u0026#34; \u0026gt;\u0026gt; /etc/iproute2/rt_tables  写一个脚本，并设置开机自启动  #!/bin/bash /sbin/ip route flush table tel /sbin/ip route add default via ${telecomgw} dev eth0 src ${telecomip} table tel /sbin/ip rule add from ${telecomip} table tel /sbin/ip route flush table cnc /sbin/ip route add default via ${cncgw} dev eth2 src ${cncip} table cnc /sbin/ip rule add from ${cncip} table cnc ","permalink":"https://jeremyxu2010.github.io/2014/03/linux%E4%B8%8B%E8%AE%BE%E7%BD%AE%E7%94%B5%E4%BF%A1%E7%BD%91%E9%80%9A%E5%8F%8C%E7%BA%BF%E8%B7%AFip%E5%9C%B0%E5%9D%80/","tags":["linux","network","iproute2"],"title":"linux下设置电信网通双线路IP地址"},{"categories":["java开发"],"contents":"今天遇到一个很狗血的问题，一个功能在开发环境没有问题，但在生产环境出错了。\n代码如下：\n... File tmpFile = new File(fileTmpPath); File newFileTarget = new File(filePath); tmpFile.renameTo(newFileTarget); // 修改新文件的权限 FileManageHelper.chmod(newFileTarget); .... 最后报错信息提示执行chmod命令失败，但这个代码在开发环境没有问题啊。\n仔细查找原因发现jdk的renameTo方法介绍如下：\n/** * Renames the file denoted by this abstract pathname. * * \u0026lt;p\u0026gt; Many aspects of the behavior of this method are inherently * platform-dependent: The rename operation might not be able to move a * file from one filesystem to another, it might not be atomic, and it * might not succeed if a file with the destination abstract pathname * already exists. The return value should always be checked to make sure * that the rename operation was successful. * * \u0026lt;p\u0026gt; Note that the {@link java.nio.file.Files} class defines the {@link * java.nio.file.Files#move move} method to move or rename a file in a * platform independent manner. * * @param dest The new abstract pathname for the named file * * @return \u0026lt;code\u0026gt;true\u0026lt;/code\u0026gt; if and only if the renaming succeeded; * \u0026lt;code\u0026gt;false\u0026lt;/code\u0026gt; otherwise * * @throws SecurityException * If a security manager exists and its \u0026lt;code\u0026gt;{@link * java.lang.SecurityManager#checkWrite(java.lang.String)}\u0026lt;/code\u0026gt; * method denies write access to either the old or new pathnames * * @throws NullPointerException * If parameter \u0026lt;code\u0026gt;dest\u0026lt;/code\u0026gt; is \u0026lt;code\u0026gt;null\u0026lt;/code\u0026gt; */ The rename operation might not be able to move a file from one filesystem to another 也就是说如果文件是从一个文件系统将文件move到另一个文件系统有可能失败，正好开发环境上tmpFile与newFileTarget在同一个文件系统中，而在生产环境中由于HA方案的原因这两个文件在不同的文件系统。\n教训：一定要检查File的相关操作的返回值，如setLastModified, setReadOnly, setWritable, setReadable, setExecutable, createNewFile, delete, mkdir, mkdirs。\n","permalink":"https://jeremyxu2010.github.io/2014/03/%E7%89%B9%E5%88%AB%E8%A6%81%E6%A3%80%E6%9F%A5java%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%E7%9A%84%E8%BF%94%E5%9B%9E%E5%80%BC/","tags":["java","filesystem"],"title":"特别要检查Java文件操作相关方法的返回值"},{"categories":["web开发"],"contents":"今天项目中需要跨浏览器地播放视频，在网上找了一下，找到了video.js，记录一下video.js的简单用法。\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; ... \u0026lt;!--引入video.js的样式文件 --\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; href=\u0026#34;css/video-js.css\u0026#34; /\u0026gt; ... \u0026lt;!--如果没有使用Modernizr，则使用以下代码做shiv --\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; document.createElement(\u0026#39;video\u0026#39;);document.createElement(\u0026#39;audio\u0026#39;);document.createElement(\u0026#39;track\u0026#39;); \u0026lt;/script\u0026gt; ... \u0026lt;!--引入video.js的脚本文件 --\u0026gt; \u0026lt;script src=\u0026#34;js/video.js\u0026#34; type=\u0026#34;text/javascript\u0026#34; charset=\u0026#34;utf-8\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!--指定videojs的flash文件 --\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; videojs.options.flash.swf = \u0026#34;js/video-js.swf\u0026#34;; \u0026lt;/script\u0026gt; ... \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; ... \u0026lt;video id=\u0026#34;sample_video\u0026#34; preload=\u0026#34;none\u0026#34; class=\u0026#34;video-js vjs-default-skin vjs-big-play-centered\u0026#34; data-setup=\u0026#39;{ \u0026#34;controls\u0026#34;: true, \u0026#34;autoplay\u0026#34;: false, \u0026#34;preload\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;poster\u0026#34;: \u0026#34;images/sample_video_poster.png\u0026#34;, \u0026#34;width\u0026#34;: 852, \u0026#34;height\u0026#34;: 480 }\u0026#39;\u0026gt; \u0026lt;source src=\u0026#34;videos/sample_video.mp4\u0026#34; type=\u0026#39;video/mp4\u0026#39; /\u0026gt; \u0026lt;!--如果浏览器不兼容HTML5则使用flash播放 --\u0026gt; \u0026lt;object id=\u0026#34;sample_video\u0026#34; class=\u0026#34;vjs-flash-fallback\u0026#34; width=\u0026#34;852\u0026#34; height=\u0026#34;480\u0026#34; type=\u0026#34;application/x-shockwave-flash\u0026#34; data=\u0026#34;http://releases.flowplayer.org/swf/flowplayer-3.2.1.swf\u0026#34;\u0026gt; \u0026lt;param name=\u0026#34;movie\u0026#34; value=\u0026#34;http://releases.flowplayer.org/swf/flowplayer-3.2.1.swf\u0026#34; /\u0026gt; \u0026lt;param name=\u0026#34;allowfullscreen\u0026#34; value=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;param name=\u0026#34;flashvars\u0026#34; value=\u0026#39;config={\u0026#34;playlist\u0026#34;:[\u0026#34;images/sample_video_poster.png\u0026#34;, {\u0026#34;url\u0026#34;: \u0026#34;videos/sample_video.mp4\u0026#34;,\u0026#34;autoPlay\u0026#34;:false,\u0026#34;autoBuffering\u0026#34;:true}]}\u0026#39; /\u0026gt; \u0026lt;!--视频图片. --\u0026gt; \u0026lt;img src=\u0026#34;images/sample_video.png\u0026#34; width=\u0026#34;852\u0026#34; height=\u0026#34;480\u0026#34; alt=\u0026#34;Poster Image\u0026#34; title=\u0026#34;No video playback capabilities.\u0026#34; /\u0026gt; \u0026lt;/object\u0026gt; \u0026lt;/video\u0026gt; ... \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var myPlayer = null; $(document).ready(function() { ... if(!myPlayer) { // Using the video\u0026#39;s ID or element  myPlayer = videojs(\u0026#34;video_center_video\u0026#34;); } // After you have references to your players you can...(example)  myPlayer.play(); // Starts playing the video for this player. ... }); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 上面的用法是tag标签的使用办法，官方文档里还写了使用js初始化的办法，很简单，可参照https://github.com/videojs/video.js/blob/stable/docs/guides/setup.md\n使用video.js有一个好处就是video标签或flashvars中可以指定多个视频源，它会根据浏览器自动寻找合适的视频进行播放。\n不过今天使用video.js的时候发现一个问题，当设置了preload为auto之后，在chrome下首次刷新网页网络请求会出现一个错误。\nRequest URL: http://xxxxx/yyyy.mp4 Request Headers CAUTION: Provisional headers are shown. Accept-Encoding: identity;q=1, *;q=0 Cache-Control: max-age=0 Range: bytes=0- Referer: http://xxxxx User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.131 Safari/537.36 暂时将preload设置为none规避此问题。\n默认的video.js的样式不太好看，顺便附上从锤子网http://www.smartisan.com/爬下来的样式文件。\n/*! Video.js Default Styles (http://videojs.com) Version 4.6.1 Create your own skin at http://designer.videojs.com */.vjs-default-skin{color:#ccc}@font-face{font-weight:400;font-style:normal}.vjs-default-skin .vjs-slider{outline:0;position:relative;cursor:pointer;padding:0;background:url(../images/bg_player_icon_v.png) 0 -210px repeat-x}.vjs-default-skin .vjs-slider:focus{-webkit-box-shadow:0 0 2em #fff;-moz-box-shadow:0 0 2em #fff;box-shadow:0 0 2em #fff}.vjs-default-skin .vjs-slider-handle{font-family:VideoJS;font-size:1em;line-height:1;text-align:center;text-shadow:0 0 1em #fff;position:absolute;top:-13px;left:0;background:url(../images/bg_player_icon_v.png) 0 -90px no-repeat;width:33px;height:33px}.vjs-default-skin .vjs-control-bar{display:none;position:absolute;bottom:0;left:0;right:0;height:39px;padding:3px 0 0;background-color:#000}.vjs-default-skin.vjs-has-started .vjs-control-bar{display:block;visibility:visible;opacity:1;-webkit-transition:visibility .1s,opacity .1s;-moz-transition:visibility .1s,opacity .1s;-o-transition:visibility .1s,opacity .1s;transition:visibility .1s,opacity .1s}.vjs-default-skin.vjs-has-started.vjs-user-inactive.vjs-playing .vjs-control-bar{display:block;visibility:hidden;opacity:0;-webkit-transition:visibility 1s,opacity 1s;-moz-transition:visibility 1s,opacity 1s;-o-transition:visibility 1s,opacity 1s;transition:visibility 1s,opacity 1s}.vjs-default-skin.vjs-controls-disabled .vjs-control-bar{display:none}.vjs-default-skin.vjs-using-native-controls .vjs-control-bar{display:none}.vjs-default-skin.vjs-error .vjs-control-bar{display:none}@media \\0screen{.vjs-default-skin.vjs-user-inactive.vjs-playing .vjs-control-bar :before{content:\u0026#34;\u0026#34;}}.vjs-default-skin .vjs-control{outline:0;position:relative;float:left;text-align:center;margin:0;padding:0;height:3em;width:4em}.vjs-default-skin .vjs-control:before{font-family:VideoJS;font-size:1.5em;position:absolute;top:0;left:0;width:100%;height:100%;text-align:center;text-shadow:1px 1px 1px rgba(0,0,0,.5)}.vjs-default-skin .vjs-control:focus:before,.vjs-default-skin .vjs-control:hover:before{text-shadow:0 0 1em #fff}.vjs-default-skin .vjs-control-text{border:0;clip:rect(0 0 0 0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}.vjs-default-skin .vjs-play-control{width:5em;cursor:pointer;background:url(../images/bg_player_icon.png) 27px 0 no-repeat}.vjs-default-skin.vjs-playing .vjs-play-control{background:url(../images/bg_player_icon.png) -33px 0 no-repeat}.vjs-default-skin .vjs-playback-rate .vjs-playback-rate-value{font-size:1.5em;line-height:2;position:absolute;top:0;left:0;width:100%;height:100%;text-align:center;text-shadow:1px 1px 1px rgba(0,0,0,.5)}.vjs-default-skin .vjs-playback-rate.vjs-menu-button .vjs-menu .vjs-menu-content{width:4em;left:-2em;list-style:none}.vjs-default-skin .vjs-mute-control,.vjs-default-skin .vjs-volume-menu-button{cursor:pointer;float:right;background:url(../images/bg_player_icon.png) -104px 0 no-repeat}.vjs-default-skin .vjs-mute-control.vjs-vol-0,.vjs-default-skin .vjs-volume-menu-button.vjs-vol-0{background:url(../images/bg_player_icon.png) -164px 0 no-repeat}.vjs-default-skin .vjs-mute-control.vjs-vol-1:before,.vjs-default-skin .vjs-volume-menu-button.vjs-vol-1:before{background:url(../images/bg_player_icon.png) -104px 0 no-repeat}.vjs-default-skin .vjs-mute-control.vjs-vol-2:before,.vjs-default-skin .vjs-volume-menu-button.vjs-vol-2:before{background:url(../images/bg_player_icon.png) -104px 0 no-repeat}.vjs-default-skin .vjs-volume-control{width:5em;float:right;top:2px;top:4px\\0}.vjs-default-skin .vjs-volume-bar{width:5em;height:.6em;margin:1.1em auto 0}.vjs-default-skin .vjs-volume-menu-button .vjs-menu-content{height:2.9em}.vjs-default-skin .vjs-volume-level{position:absolute;top:0;left:0;height:.5em;width:100%;background:url(../images/bg_player_icon_v.png) 0 -150px repeat-x}.vjs-default-skin .vjs-volume-bar .vjs-volume-handle{width:14px;height:14px;left:4.5em;background:url(../images/bg_player_icon_v.png) 0 0 no-repeat;top:-4px}.vjs-default-skin .vjs-volume-handle:before{font-size:.9em;top:-.2em;left:-.2em;width:1em;height:1em}.vjs-default-skin .vjs-volume-menu-button .vjs-menu .vjs-menu-content{width:6em;left:-4em}.vjs-default-skin .vjs-progress-control{position:absolute;left:0;right:0;width:auto;font-size:.3em;height:6px;top:-6px;-webkit-transition:all .4s;-moz-transition:all .4s;-o-transition:all .4s;transition:all .4s}.vjs-default-skin:hover .vjs-progress-control{font-size:.9em;-webkit-transition:all .2s;-moz-transition:all .2s;-o-transition:all .2s;transition:all .2s}.vjs-default-skin .vjs-progress-holder{height:100%}.vjs-default-skin .vjs-progress-holder .vjs-play-progress,.vjs-default-skin .vjs-progress-holder .vjs-load-progress{position:absolute;display:block;height:100%;margin:0;padding:0;width:0;left:0;top:0}.vjs-default-skin .vjs-play-progress{background:url(../images/bg_player_icon_v.png) 0 -150px repeat-x}.vjs-default-skin .vjs-load-progress{background:url(../images/bg_player_icon_v.png) 0 -180px repeat-x}.vjs-default-skin.vjs-live .vjs-time-controls,.vjs-default-skin.vjs-live .vjs-time-divider,.vjs-default-skin.vjs-live .vjs-progress-control{display:none}.vjs-default-skin.vjs-live .vjs-live-display{display:block}.vjs-default-skin .vjs-live-display{display:none;font-size:1em;line-height:3em}.vjs-default-skin .vjs-time-controls{font-size:1em;line-height:36px}.vjs-default-skin .vjs-current-time{float:left}.vjs-default-skin .vjs-duration{float:left}.vjs-default-skin .vjs-remaining-time{display:none;float:left}.vjs-default-skin .vjs-time-controls,.vjs-default-skin .vjs-duration,.vjs-default-skin .vjs-duration{top:3px\\0;top:1px}.vjs-time-divider{float:left;line-height:36px}.vjs-default-skin .vjs-fullscreen-control{width:3.8em;cursor:pointer;float:right}.vjs-default-skin .vjs-fullscreen-control{background:url(../images/bg_player_icon.png) -220px 0 no-repeat}.vjs-default-skin.vjs-fullscreen .vjs-fullscreen-control{background:url(../images/bg_player_icon.png) -284px 0 no-repeat}.vjs-default-skin .vjs-big-play-button{left:50%;top:50%;margin:-26.5px 0 0 -22.5px;display:block;z-index:2;position:absolute;width:45px;height:53px;cursor:pointer;opacity:1;background:url(../images/bg_player_play.png) left top no-repeat}.vjs-default-skin .vjs-mute-control.vjs-vol-0:hover,.vjs-default-skin .vjs-mute-control.vjs-vol-1:hover,.vjs-default-skin .vjs-mute-control.vjs-vol-2:hover,.vjs-default-skin .vjs-mute-control.vjs-vol-3:hover,.vjs-default-skin .vjs-mute-control:hover,.vjs-default-skin.vjs-fullscreen .vjs-fullscreen-control:hover,.vjs-default-skin .vjs-fullscreen-control:hover,.vjs-default-skin.vjs-playing .vjs-play-control:hover,.vjs-default-skin .vjs-play-control:hover,.vjs-default-skin .vjs-big-play-button:hover{opacity:.6;-moz-transition:opacity .3s ease;-webkit-transition:opacity .3s ease;-o-transition:opacity .3s ease;transition:opacity .3s ease}.vjs-default-skin.vjs-big-play-centered .vjs-big-play-button{left:50%;margin-left:-2.1em;top:50%;margin-top:-1.4000000000000001em}.vjs-default-skin.vjs-controls-disabled .vjs-big-play-button{display:none}.vjs-default-skin.vjs-has-started .vjs-big-play-button{display:none}.vjs-default-skin.vjs-using-native-controls .vjs-big-play-button{display:none}.vjs-default-skin:hover .vjs-big-play-button,.vjs-default-skin .vjs-big-play-button:focus{outline:0;border-color:#fff}.vjs-error .vjs-big-play-button{display:none}.vjs-error-display{display:none}.vjs-error .vjs-error-display{display:block;position:absolute;left:0;top:0;width:100%;height:100%}.vjs-error .vjs-error-display:before{content:\u0026#39;X\u0026#39;;font-family:Arial;font-size:4em;color:#666;line-height:1;text-shadow:.05em .05em .1em #000;text-align:center;vertical-align:middle;position:absolute;top:50%;margin-top:-.5em;width:100%}.vjs-error-display div{position:absolute;font-size:1.4em;text-align:center;bottom:1em;right:1em;left:1em}.vjs-error-display a,.vjs-error-display a:visited{color:#F4A460}.vjs-loading-spinner{background:url(../images/bg_player_loading.png) no-repeat;display:none;position:absolute;top:50%;left:50%;font-size:4em;line-height:1;width:61px;height:61px;margin-left:-30.5px;margin-top:-30.5px;opacity:.75;-webkit-animation:spin 1.5s infinite linear;-moz-animation:spin 1.5s infinite linear;-o-animation:spin 1.5s infinite linear;animation:spin 1.5s infinite linear}.video-js.vjs-error .vjs-loading-spinner{display:none!important;-webkit-animation:none;-moz-animation:none;-o-animation:none;animation:none}@-moz-keyframes spin{0%{-moz-transform:rotate(0deg)}100%{-moz-transform:rotate(359deg)}}@-webkit-keyframes spin{0%{-webkit-transform:rotate(0deg)}100%{-webkit-transform:rotate(359deg)}}@-o-keyframes spin{0%{-o-transform:rotate(0deg)}100%{-o-transform:rotate(359deg)}}@keyframes spin{0%{transform:rotate(0deg)}100%{transform:rotate(359deg)}}.vjs-default-skin .vjs-menu-button{float:right;cursor:pointer}.vjs-default-skin .vjs-menu{display:none;position:absolute;bottom:0;left:0;width:0;height:0;margin-bottom:3em;border-left:2em solid transparent;border-right:2em solid transparent;border-top:1.55em solid #000;border-top-color:rgba(7,40,50,.5)}.vjs-default-skin .vjs-menu-button .vjs-menu .vjs-menu-content{display:block;padding:0;margin:0;position:absolute;width:10em;bottom:1.5em;max-height:15em;overflow:auto;left:-5em;background-color:#07141e;background-color:rgba(7,20,30,.7);-webkit-box-shadow:-.2em -.2em .3em rgba(255,255,255,.2);-moz-box-shadow:-.2em -.2em .3em rgba(255,255,255,.2);box-shadow:-.2em -.2em .3em rgba(255,255,255,.2)}.vjs-default-skin .vjs-menu-button:hover .vjs-menu{display:block}.vjs-default-skin .vjs-menu-button ul li{list-style:none;margin:0;padding:.3em 0;line-height:1.4em;font-size:1.2em;text-align:center;text-transform:lowercase}.vjs-default-skin .vjs-menu-button ul li.vjs-selected{background-color:#000}.vjs-default-skin .vjs-menu-button ul li:focus,.vjs-default-skin .vjs-menu-button ul li:hover,.vjs-default-skin .vjs-menu-button ul li.vjs-selected:focus,.vjs-default-skin .vjs-menu-button ul li.vjs-selected:hover{outline:0;color:#111;background-color:#fff;background-color:rgba(255,255,255,.75);-webkit-box-shadow:0 0 1em #fff;-moz-box-shadow:0 0 1em #fff;box-shadow:0 0 1em #fff}.vjs-default-skin .vjs-menu-button ul li.vjs-menu-title{text-align:center;text-transform:uppercase;font-size:1em;line-height:2em;padding:0;margin:0 0 .3em;font-weight:700;cursor:default}.vjs-default-skin .vjs-subtitles-button:before{content:\u0026#34;\\e00c\u0026#34;}.vjs-default-skin .vjs-captions-button:before{content:\u0026#34;\\e008\u0026#34;}.vjs-default-skin .vjs-captions-button:focus .vjs-control-content:before,.vjs-default-skin .vjs-captions-button:hover .vjs-control-content:before{-webkit-box-shadow:0 0 1em #fff;-moz-box-shadow:0 0 1em #fff;box-shadow:0 0 1em #fff}.video-js{background-color:#000;position:relative;padding:0;font-size:10px;vertical-align:middle;font-weight:400;font-style:normal;font-family:Arial,sans-serif;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.video-js .vjs-tech{position:absolute;top:0;left:0;width:100%;height:100%}.video-js:-moz-full-screen{position:absolute}body.vjs-full-window{padding:0;margin:0;height:100%;overflow-y:auto}.video-js.vjs-fullscreen{position:fixed;overflow:hidden;z-index:1000;left:0;top:0;bottom:0;right:0;width:100%!important;height:100%!important;_position:absolute}.video-js:-webkit-full-screen{width:100%!important;height:100%!important}.video-js.vjs-fullscreen.vjs-user-inactive{cursor:none}.vjs-poster{background-repeat:no-repeat;background-position:50% 50%;background-size:contain;cursor:pointer;height:100%;margin:0;padding:0;position:relative;width:100%}.vjs-poster img{display:block;margin:0 auto;max-height:100%;padding:0;width:100%}.video-js.vjs-using-native-controls .vjs-poster{display:none}.video-js .vjs-text-track-display{text-align:center;position:absolute;bottom:4em;left:1em;right:1em}.video-js.vjs-user-inactive.vjs-playing .vjs-text-track-display{bottom:1em}.video-js .vjs-text-track{display:none;font-size:1.4em;text-align:center;margin-bottom:.1em;background-color:#000;background-color:rgba(0,0,0,.5)}.video-js .vjs-subtitles{color:#fff}.video-js .vjs-captions{color:#fc6}.vjs-tt-cue{display:block}.vjs-default-skin .vjs-hidden{display:none}.vjs-lock-showing{display:block!important;opacity:1;visibility:visible}.vjs-no-js{padding:20px;color:#ccc;background-color:#333;font-size:18px;font-family:Arial,sans-serif;text-align:center;width:300px;height:150px;margin:0 auto}.vjs-no-js a,.vjs-no-js a:visited{color:#F4A460} 样式中引用了几张图片，从锤子网http://www.smartisan.com/上下载就可以了\n","permalink":"https://jeremyxu2010.github.io/2013/10/video.js%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/","tags":["javascript","video"],"title":"Video.js简单使用"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","permalink":"https://jeremyxu2010.github.io/search/","tags":null,"title":"Search Results"}]